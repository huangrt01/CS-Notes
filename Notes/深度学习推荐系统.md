[toc]

# 深度学习推荐系统

> 工业界推荐系统实践 https://github.com/Doragd/Algorithm-Practice-in-Industry

## Intro

* 推荐系统与做菜
  * 原材料如何，丰富不丰富，新鲜不新鲜
  * 厨艺好不好，有没有丰富的经验
  * 现场发挥，能不能物尽其用，充分展现厨艺
* 能力要求
  * 知识
  * 工具
  * 逻辑
  * 业务
  

### 如何选 Paper

- **领域开创性的**：国外偏研究、创新的大公司，如Google、OpenAI (+国内DeepSeek)、Meta少部分工作
- **务实的incremental的工作，注重分析和小步迭代**：国外的各公司，以Meta为典型（+国内ByteDance）
  - 一个例子是，推荐系统在22年前，airbnb、pininterest等公司的工作有一定的影响力
- **大而全，囊括该领域所有相关潜在的技术**：国内 快手
  - Paper和技术如流水，从而读者眼花缭乱不知道到底哪些点work
  - 建议作为百科全书使用，作为具体决策参考你就输了
- **注重提出领域的新范式，Idea为先**：国内 Alibaba
  - 和 第一类 领域开创性的区别在于工作影响力

## 算法心得

### 算法范式

* 加特征：某些维度上高低估的情况，新增特征之前计算calibration
* 分析全链路，链路瓶颈在哪
  * 比如保送精排有收益说明召回粗排不准
* 是否有信息增益
* high-low范式
  * low fusion：划分领域、提取领域内特征，FM、DIN
  * high fusion：捕捉跨领域交互信息，MLP、LHUC

* **通用深度学习推荐模型范式**
  * **特征查找 (Feature Lookup)**: $$Code_i = \text{IDLookUp}([v^i_1, \dots, v^i_K] \oplus [r^i_1, \dots, r^i_L])$$
  * **特征表示 (Representation)**:
    * **Item Representation**: $$ItemCodeRep = \text{ItemNet}(Code_{Target})$$
    * **User Representation**: $$UserCodeRep = \text{UserNet}([Code_1, \dots, Code_n])$$
  * **特征交互 (Feature Interaction)**:
    * $$CrossCodeRep = \text{CrossNet}(ItemCodeRep, [Code^{Search}_1, \dots, Code^{Search}_n])$$
  * **多任务预测 (Multi-task Prediction)**:
    * $$\hat{y}^{ctr}, \hat{y}^{lvtr}, \dots = \text{MoE}([UserCodeRep, ItemCodeRep, CrossCodeRep, OtherFeaRep])$$
  * **损失函数 (Loss Function)**:
    * $$\mathcal{L} = - \sum_{xtr} (y^{xtr} \log(\hat{y}^{xtr}) + (1 - y^{xtr}) \log(1 - \hat{y}^{xtr}))$$


* [推荐系统里的“七伤拳”：那些高ROI但长期有损的优化](https://zhuanlan.zhihu.com/p/1930155262179807978)
  * 交叉特征
  * 大规模uid emb隐式交叉
  * cascade模型
    * 解决链路一致性问题
    * 导致了严重的data-loop问题



### 搜广推的差异

> 王喆：排得更好VS估得更准VS搜的更全
>
> 「推荐、广告、搜索」算法间到底有什么区别？https://zhuanlan.zhihu.com/p/430431149

* 问题定义

  * 搜索：搜索的经典场景是用户主动输入查询query，较明确地表达需求，然后搜索引擎从数据库中检索得到topK最优结果，最后展现给用户，这是一个用户主动获取信息的过程。

  * 推荐：推荐一般不存在用户主动提供query，而是系统根据其用户画像（性别、职业...）以及历史行为（浏览、点击、收藏...）等，排出最可能使用户消费的内容展现出来，这是一个用户被动获取信息的过程。

  * 广告：一般广告的参与有三个角色：广告主、平台和用户。与搜推的为用户找信息的过程不同，广告则是为信息找人，目的也非常直接，纯粹就是为公司增加收入，可以说是离 最近的业务了。

* 业务目标

  - 搜索：相比与推荐和广告，搜索在某种意义上是存在『正确答案』的，即用户是带着明确目的来完成这次行为的。
    - 所以搜索第一目标就是相关性，围绕『搜索词』展开优化，将"正确答案"展现给用户；
    - [推荐算法](https://zhida.zhihu.com/search?content_id=183780223&content_type=Article&match_order=2&q=推荐算法&zhida_source=entity)强调的个性化永远只是搜索算法的补充。“围绕着搜索词的信息高效获取问题“才是搜索算法想解决的根本问题。
    - 其次目标才是CTR/CVR/GMV等。随着相关商品量和算法的发展，现在也越来越强调类似推荐的 **个性化/千X千面** 。

  - 推荐：推荐算法的预估目标就不尽相同，视频类更多倾向于预测观看时长，新闻类预测CTR，电商类预估客单价等等这些 **跟用户参与度最相关的业务指标** 。
    - 只有用户的参与度高了，才能让广告系统有更多的inventory，进而增加公司营收。

  - 广告：各大公司广告算法的预估目标非常统一，就是 **预估CTR和CVR**，增加收入

* 评价指标：

  * 搜索：**非常看重能否把这些正确答案给召回回来**，针对召回率、MAP、NDCG这些指标进行优化
  * 推荐：由于推荐系统的推荐场景是生成一个列表，所以更加关注item间的相对位置，因此评估阶段更倾向于用AUC，gAUC，MAP这些指标作为评价标准。
  * 广告：因为CPC和CPA计价仍是效果类广告系统的主流计价方式。所以只有预估出CTR和CVR，才能反向推导出流量的价值，并进一步给出合理的出价。所以针对这样的目标，广告算法非常看重把**预估偏差**当作首要的评价指标。

* 算法设计

  * 搜索：**强调搜索词的关键性，以及对搜索词的理解**
    * 搜索词与其他特征组成的交叉特征，组合特征，以及模型中的交叉部分是异常重要的
    * 一定程度上要抑制个性化的需求，更多把质量和权威性放在更重要的位置
  * 推荐：list level，page level
    * 对推荐内容的**多样性，新鲜度**有更高的要求，这就让探索与利用，强化学习等一些列方法在推荐场景下更受重视。
  * 广告：对calibration方法的严苛要求，就算模型训练的过程中存在偏差，比如使用了负采样、weighted sampling等方式改变了数据原始分布，也要根据正确的后验概率在各个维度上矫正模型输出。此外，因为广告是很少以列表的形式连续呈现的，要对每一条广告的CTR，CVR都估的准，**广告算法大都是point wise的训练方式**。

* 辅助策略和算法上的区别

  * 搜索：NLP技术，对大量内容进行预处理，embedding化
  * 推荐：照顾用户的长期兴趣，需要一些补充策略做出一些看似“非最优”的选择，**比如探索性的尝试一些长尾内容**
  * 广告：pacing，bidding，budget control，ads allocation

* 模型结构差异

  * 搜索：搜索词和item之间天然是一个双塔结构，因此在模型构建的时候各种交叉特征，模型中的各种交叉结构往往是搜索类模型的重点。
  * 推荐：如果不抓住用户兴趣的连续变化，是很难做好推荐模型的，因此利用sequential model来模拟用户兴趣变化往往是有收益的
  * 广告：兴趣是不那么连贯的，因此容易造成sequential model的失效，attention机制可能会更加重要一些
    * 解释了为什么DIN简单实用

* 系统痛点

  * 搜索：往往把重心放在**搜索词和item的内容理解上**，只要能做好这一点，模型结构本身反而不是改进的关键点了，但是在多模态的时代，图片、视频内容的理解往往是制约搜索效果的痛点。
  * 推荐：**问题往往卡在长期利益与短期利益的平衡上**，在模型结构红利消失殆尽的今天，如何破局是推荐算法工程师们做梦都在想的问题。
  * 广告：**各模块协同工作找到平台全局利润最大化方法的难度非常大**，系统异常复杂到难以掌控的地步

### 如何做个性化

个性化召回的经典方式是特征工程+DLRM，能否设计出好的特征是推荐系统能否表现良好的关键问题。在过去的DLRM系统中，存在大量经过人工挑选的特征，除去表征物品本身语义的特征之外，还有一些特殊设计的交叉特征，表征用户交互的统计特征等等。

提升模型能力有以下常见路线:
1.  **增加输入的信息量**，即特征工程加特征；
2.  **增加输入的信息质量**，比如LLM领域清洗数据；
3.  **增加输入的数据量**，比如LLM训练采用合成数据，推荐系统中的多路样本流；
4.  **增加模型参数量**，提升模型智能；
5.  **探索更高效的模型结构**，提升模型对数据的利用效率。

在Wide&Deep及之前的时代，推荐系统极度依赖特征工程, 本质上是因为「1」这条路的低垂果实较多，因此常见的模型迭代范式是投入大量算法人力，进行“加特征——模型效果提升——特征裁剪——模型效率提升”，渐进阶梯式地迭代。但在推荐ToB领域，相比于投入大量人力追求少量的边界效益，更重要的是项目维度Scalable的能力，用合理高效的方式拿到大部分收益，用可复用、推广、Scalable的技术给更多客户带来价值。此际，重特征工程带来的长迭代周期与高人力成本，将阻碍拓客效率的提升。

| 阶段 | 增加输入信息量 (特征工程) | 增加输入信息质量 (数据清洗/增强) | 增加输入数据量 (样本) | 增加模型参数量 | 更高的数据利用效率 (模型结构) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Wide&Deep 时代** | ID特征, 行为特征, 交叉特征, 统计特征, Match特征 | 样本归因, 特征清洗 | 海量用户行为日志, 多场景样本流 | Embedding维度增加, 网络层数加深 (1M-10M) | Wide&Deep结构 (记忆+泛化) |
| **LLM 大语言模型初现** | 文本内容特征 | 高质量预训练语料 | Web级文本与代码数据 | 1B-100B | Transformer / Attention 机制 |
| **LLM 的算法工程迭代** | 图像, 视频等多模态特征 | RLHF对齐, 指令精调, 图文对齐 | 合成数据, 多模态数据 | 1B-300B | MoE, MLA, Long Context技术, RL策略优化 |
| **下一代个性化技术** | 行为特征 + 内容特征 | LLM数据增强, LLM自动特征映射与生成 | 推荐领域的行业模型, 世界模型 | 100M-?B | 字节长序列建模, 生成式推荐 (COBRA, URM, LUM) 等算法创新; 基于RL的Agent |

### 条数

* 导致精排条数随机性的因素：

  * 多层漏斗系统多次拆请求

  * 广告业务multitask，针对一个候选只会走一个task

  * 混排再精排

### 纯行为与多模态下的质量学习

* 背景

当推荐模型**没有统计特征**（如点赞、评论数、历史CTR），仅依赖**用户行为序列**和**Item多模态特征**时，能否有效识别“水贴”（低质高点击）与“好贴”？

* 核心判断

**能学到，且泛化性更好。** 
模型将从“记忆历史统计值”转向“理解内容语义与行为模式的映射”。

* 学习机制

1.  **行为即标签 (Behavior as Label)**
    *   **水贴模式**：`Click` + `Short Dwell Time` / `Skip`。
    *   **好贴模式**：`Click` + `Long Dwell Time` / `Finish` / `Interaction`。
    *   模型通过梯度下降，建立 $Embedding(Item_{multimodal}) \rightarrow Label(Quality)$ 的映射。
2.  **多模态的泛化 (Generalization)**
    *   统计特征是**后验**的（ID类特征），多模态是**先验**的（Content类特征）。
    *   模型能学到“模糊图片+震惊体文字”即为水贴的视觉/语义模式，从而对0曝光的新贴进行准确打分。

* 关键手段 (Action Plan)

1.  **Label Engineering (标签工程)**
    *   严禁仅使用 `Click` 作为正样本，否则会训练出“标题党”推荐器。
    *   **公式**：$Label = Click \times sigmoid(DwellTime - \tau)$。
    *   **负样本增强**：将“点击但秒退”的样本显式标记为负样本（Hard Negative）。

2.  **Model Architecture (模型架构)**
    *   **Auxiliary Task (辅助任务)**：增加辅助塔预测“完播率”或“停留时长”，强迫Shared Embedding包含质量信息。
    *   **Contrastive Learning (对比学习)**：在序列内部，对比用户“看完的Item”和“滑过的Item”，拉大其在特征空间的距离。

3.  **序列上下文**
    *   长序列Transformer能捕捉用户的**相对阈值**（看了好贴后再看水贴，跳出率更高），从而隐式建模质量。

## chpt1 互联网的增长引擎——推荐系统

### Intro

概念：

* UGC(User Generated Content)，典型如Youtube、Tiktok
* CVR (Conversion Rate)、观看时长(Youtube Recommendations)

推荐系统架构：数据与模型

* 物品信息、用户信息、场景信息
* 数据离线批处理、实时流处理

### 推荐系统的挑战

* item cardinality, the huge number of total items
* impression skew, the fact that only a few items comprise most user impressions or conversions (Milojević, 2010)
* ID drifting, or the majority of items entering and leaving the system within short time periods (Gama et al., 2014).

### 各种业务Overview

![image-20251027023317150](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251027023317150.png)



## chpt2 前深度学习时代——推荐系统的进化之路

> 单一特征的表达能力很弱（例如，[辛普森悖论](https://zh.wikipedia.org/wiki/辛普森悖论)）

演化关系图  p13

### 多种方法的对比

|                           | 信息             | 数学假设                                                     | 建模方式                                                     | 优点                                                         | 缺点                                                         |
| ------------------------- | ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| UserCF                    | $$E_C$$          | $$d=N, E_U={E_C}^T$$                                         | 用 User Embedding 检索相似 Users，获取 Users 的 Bhv Seq (Behaviour Sequence) |                                                              | 用户多，存不下矩阵不适用于正反馈获取困难、用户历史数据向量稀疏的场景 |
| ItemCF                    | $$E_C$$          | $$d=u, E_S=E_C$$                                             | 1. 用共现信息计算物品相似度，描述 Seq/Item Embedding<br />2. 用 User Bhv Seq Embedding 检索相似 Item Embedding |                                                              | 商品多，存不下矩阵头部效应较明显                             |
| MF (Matrix Factorization) | $$E_C$$          | $$E_C=E_S{E_U}^T$$                                           | 用 User Embedding 检索相似 Item Embedding                    | 泛化能力强空间复杂度低更好的扩展性和灵活性                   | 和UserCF/ItemCF一样，不方便加入context信息                   |
| STAR                      | $$E_C$$、$$E_S$$ | $${E_S^u=\frac{1}{n} (\bold{r\lambda^t})_u^T E_S}$$$${E_C^u=\frac{1}{n} (\bold{r\lambda^t})_u^T E_C}$$ | 1. User Bhv Seq -> User Bhv Emb<br />2. 用 User Bhv Emb 做检索，两路 Merge<br />第一路：用$${E_C}^u$$检索相似 Item<br />第二路：用$${E_S}^u$$检索相似 Item<br />注：STAR paper的实现，实际上不是两路做Merge，而是对全量商品做暴力计算STAR paper按User Bhv Seq顺序做了recency decay | 引入了预训练的Item Embedding，增加了语义信息显式引入协同过滤信息 | 计算复杂度较高，在线计算的挑战性大；仅用 User Bhv Seq 做检索，存在信息茧房 |

### 协同过滤(CF)

[一篇详细的介绍文章](https://zhuanlan.zhihu.com/p/80069337)

* UserCF (1994)

  * 用户相似度计算：余弦相似度/皮尔逊相关系数/引入物品平均分

  * 最终结果的排序：按相似度加权

  * 特点：社交特性更强

  * 缺点：
    * 用户太多，在线存储系统存不下矩阵。 
      * 不适用于正反馈获取困难的应用场景（酒店预定、大件商品购买），用户历史数据向量稀疏


* ItemCF

  * $$w(u, i^{'}) = \sum_{i \in U2I(u), i^{'} \in I2I(i)} w_{U2I}(i) w_{I2I}(i^{'})$$
  * 计算物品相似度矩阵 `I2I`
  
    * 基于共现用户的统计
    * 基于语义相似度
  
  * 用户对一个新物品的兴趣度，等于这个新物品与他所有历史喜好物品(`U2I`)的相似度的加权总和
  
* UserCF 和 ItemCF 的对比
  * UserCF 适合用在个性化需求不强，热点很明显的领域，比如新闻，电影推荐

  * ItemCF 适合用在个性化需求比较强，长尾比较长的领域，比如书，电商的推荐


* 协同过滤的缺点

  * 推荐系统的头部效应较明显，处理稀疏向量的能力弱 => MF用更稠密的隐向量表示用户和物品

  * 无法引入场景信息和更精细的用户/物品信息 => LR模型、机器学习模型

#### Swing 相似度 [Alibaba]

> 论文: [Large scale product graph construction for recommendation in e-commerce](https://arxiv.org/pdf/2010.05525.pdf)

*   **核心动机**: 传统的 ItemCF 算法仅通过共现次数计算相似度，容易受到**热门物品**和**活跃用户**（“购物狂”用户）的噪声影响。例如，如果两个热门商品（如“牛奶”和“面包”）被一个购买了上千种商品的用户同时购买，这次共现并不能有效证明这两个商品之间有很强的关联性。
*   **Swing 算法思想**: Swing 算法的核心思想是，当两个物品 `i` 和 `j` 同时被许多**兴趣相似且聚焦**的用户购买时，它们才被认为是相似的。如果一个用户购买了大量物品，那么他为 `i` 和 `j` 之间贡献的相似度权重就会被降低。
*   **算法公式与直觉**:$ sim(i, j) = \sum_{u \in U_i \cap U_j} \frac{1}{\alpha + |I_u|} $
    *   $U_i$ 和 $U_j$ 分别是购买了物品 `i` 和 `j` 的用户集合。
    *   $I_u$ 是用户 `u` 购买过的所有物品的集合。
    *   $\alpha$ 是一个平滑因子，防止分母为0。
    *   **直觉**: 遍历同时购买了 `i` 和 `j` 的用户 `u`。如果用户 `u` 的购买列表 $|I_u|$ 非常大（即该用户是“购物狂”），那么他贡献的权重 $ \frac{1}{\alpha + |I_u|} $ 就很小。反之，如果用户 `u` 是一个兴趣很专一的用户，他的贡献权重就更大。




### 矩阵分解(MF)

* 用户和物品的隐向量通过分解协同过滤生成的共现矩阵得到
* 矩阵分解的方法：
  * 特征值分解（方阵，不适用）
  * SVD（计算量大、要求稠密）
  * 梯度下降法

* 矩阵分解可加入偏差向量
  * $$r_{ui}=\mu+b_i+b_u+q_i^Tp_u$$

* 优点：
  * 泛化能力强
  * 空间复杂度低
  * 更好的扩展性和灵活性
* 缺点
  * 同样不方便加入场景信息和更精细的用户/物品/context信息



* [Impl](https://medium.com/@eliasah/deep-dive-into-matrix-factorization-for-recommender-systems-from-basics-to-implementation-79e4f1ea1660)
  * **Alternating Least Squares (ALS)**
    * Fix the user matrix and optimize the item matrix
    * Fix the item matrix and optimize the user matrix
    * Repeat steps 1 and 2 until convergence





* paper
  * Online Learning for Matrix Factorization and Sparse Coding



### 逻辑回归 (LR)

将推荐问题转换为一个CTR预估问题

流行原因：1）数学含义上的支撑 2）可解释性强 3）工程化的需要



### 从FM到FFM——自动特征交叉的解决方案

* Poly2——特征交叉的开始
  * 两两特征交叉，缺点是交叉特征稀疏

* FM模型——隐向量特征交叉 (2012-2014)
  * 用两个特征隐向量的内积作为交叉特征的权重； 隐向量分解的思想扩展到所有特征
  * 参数减少到n*k，减小训练开销，增大收敛可能

* FFM模型——引入特征域的概念 (2015)
  * ![image-20241229013921950](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241229013921950.png)
  * 参数：nkf (f个特征域、hidden len k)
  * Features are empirically categorized into several groups
  * Within each group, features are further identified as user-side and ad-side features
  * For each group, user-side and ad-features are sum-pooled, respectively, followed by element-wise multiplication.
  * Limitations
    * Feature-level interactions are lost due to sum-pooling
    * Using element-wise multiplication of sum-pooled embeddings leads to high dimensional output.




### GBDT+LR——特征工程模型化的开端

* Intro

  * 组合模型，一定程度上解决特征交叉的问题
  * 树深度～特征交叉阶数

  * 优点：
    * e2e特征工程模型化

    * 能选一些能够显著区分正负样本的特征变化方式

  * 缺点：
    * 容易过拟合
    * 丢失大量特征的数值信息
    * 不容易上线，延时高
    * **GBDT无法流式更新**

* 实现：

  * 所有叶子节点组成的向量，构成了LR模型输入 [0,1,0,0,1,0,1]



### LS-PLM —— alibaba曾经的主流推荐模型

* Large Scale Piece-wise Linear Model
  * 也称为MLR(Mix Logistic Regression)，在逻辑回归的基础上加入聚类的思想，其灵感来自对广告推荐领域样本特点的观察
  * $$f(x)=\sum_{i=1}^{m}\frac{e^{\mu_i\cdot x}}{\sum_{j=1}^me^{\mu_j\cdot x}}\cdot\frac{1}{1+e^{-w_i\cdot x}}$$
  * 先对样本分片，再分而治之
  * DL角度：注意力机制的三层模型

* 超参数：分片数m，阿里的经验值为12

* 优势：

  * 端到端的非线性学习能力

  * 模型稀疏性强


* 引申：
  * L1范数比L2范数更容易产生稀疏解，因为加入正则化项的损失函数最小值更容易在参数空间的顶点处取得，对应稀疏参数




## DL在推荐系统中的应用 (chpt3)

### Intro: 从 DL 到 推荐系统

* AlexNet 在 ImageNet 竞赛中的突破性成功表明了 DNN 巨大潜力，搜推广开启 DNN 时代；
* Word2Vec 奠定了表征基础，启发了 Embedding 技术在搜推广的广泛应用； Attention 机制对翻译任务的大幅提升，深刻影响用户行为兴趣建模；
*  基于 Transformer 结构的训练范式的普及，推动了对比学习、掩码学习、预训练 & 迁移学习等各种迭代模式的兴起。

演化关系图：p51

![image-20250102011634184](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250102011634184-5751797.png)

### Sparse Feature

> Meta: embedding Representation Stability

* Many modern deep learning recommendation models use trained embeddings to
  represent categorical (“sparse”) features (Covington et al ., 2016;
  Naumov et al. , 2019; Naumov, 2019).

* A simple solution to high item cardinality is to use random hashing (Weinberger et al. ,
  2009), but random hash collisions can be undesirable. One
  option is to modify the hashing procedure. Under this category,
  collision-free hashing (Liu et al., 2022) introduces individual
  embeddings for each item by dynamically free the memory of
  embeddings for retired items.

* Double hashing (Zhang et al. , utilizes two independent hash functions to reduce memory usage, but still has random collision.

  Learning to hash methods (Wang et al., 2017) focus on similarity preserving by training ML-
  based hash functions.

* There have also been works that address impression skew through contrastive learning or clustering (Yao
  et al., 2021; Chang et al., 2024); we view these as complementary approaches

### [AutoRec](https://zhuanlan.zhihu.com/p/159087297) —— 单隐层神经网络推荐模型

* Intro
  * 结合了auto-encoder和协同过滤的思想，本质上是训练自编码器 auto-encoder保存推荐系统的泛化信息，输入一个“不完整”的、“真实”的评分条目列表

* I-AutoRec, U-AutoRec
  * U-AutoRec输入user评分，计算一次即可，但效果会受用户数据稀疏性影响

### 特征交叉

#### Deep Crossing模型 —— 经典的深度学习架构

* Intro

  * 微软Bing搜索广告场景

* 特征：

  * query

    * query

  * ad

    * keyword
    * title

    * Landing page
    * match type
    * history ctr
    * Click prediction
    * 广告计划：预算、定向条件

* 模型：

  * Sparse&dense embedding连起来，过 多层残差网络

* 如何解决稀疏特征向量稠密化的问题？

  * 除数值类特征，进入Embedding层

* 如何解决特征自动交叉的问题？
  * 无人工特征交叉，由 **残差神经网络** 加强提取非线性特征和组合特征信息的能力
  * $$f(x) = Fc2(ReLU(Fc1(x)))+x$$

* 如何在输出层中达成问题设定的优化目标？
  * 最后一层逻辑回归

#### Deep&Cross模型 (DCN): 对 W&D 的改进

* 多层交叉层: $x_{l+1}=x_0x_l^Tw_l+b_l+x_l$ 
  * 参数引入较为克制，增强模型的非线性学习能力
  * 解决了Wide&Deep模型人工组合特征的问题

#### CAN: Revisiting Feature Co-Action for Click-Through Rate Prediction

[想为特征交互走一条新的路 - 周国睿的文章 - 知乎](https://zhuanlan.zhihu.com/p/287898562)

feature co-action

1. sum-pooling: DIN 系列
2. graph-based
3. combinatorial embedding methods: DCN、PNN

1+2: the edges are only used for information aggregation but not information augmentation；edge weight 的维度不高，可能信息不足以刻画好 feature co-action

3: 同时进行 representation learning and co-action modeling，可能有冲突

![image-20250408190957309](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250408190957309.png)

* CAN网络结构：

  * 核心思路是有限度地扩充交叉特征参数，CAN独立参数

  * Pitem做MLP参数，这样选取是考虑到 candidate ads 的量级比 user history少

  * CAN本质上感觉是一种更“深度”的“朴素特征交叉”，既直接由 Pitem + Puser 输出 embedding，又不引入新的 dense MLP 参数，保证“穿越性”


* 相比笛卡尔积方案的优势是：1）参数解耦；2）参数量折中；3）冷启动

* Multi-level Independence

  * parameter independence
    * 这篇文章核心思路在这里，稀疏特征的场景将 表征学习 与 特征交叉 解耦，这一思想与CV领域解决长尾分布问题，[表征学习 与 分类器 解耦](https://arxiv.org/abs/1910.09217)的思路异曲同工（本质还是“有限地”增加参数）

  * combinations independence

  * orders independence


* 和Dynamic Weight的类比



##### DCN-V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems

1.Introduction

* DNN比较难学好二阶、三阶特征交叉 ---> implicit 转 explicit 的思路

* DCN的问题：Cross网络的参数量 O(input size) is overwhelmed by Deep网络

2.Related Work

* Parallel Structure: Wide & Deep, DeepFM, DCN, xDeepFM, AutoInt, InterHAt

* Stacked Structure: PNN(IPNN, OPNN), NFM, DLRM, AFN

* 一些对比的要点：特征交叉的方式、高阶特征交叉、定长/变长特征交叉

3.Proposed Architecture: DCN-V2

stacked and parallel structure

* cross network(DCN-M) 在W为对角阵时退化为DCN
* Cost-Effective Mixture of Low-Rank DCN：本质上是矩阵分解减参数，insights:
  * learn feature crosses in a subspace -> Mixture-of-Experts(MoE)
  * 利用低秩特性，先降维再升维 -> 在低维空间做非线性

6.Emprical Understanding

* DCN-V2的交叉能力优于DNN

* CrossNet ~ ReLu 学习非线性

* 朴素情况下的类比：rank threshold ~ feature num
  * 第8小节声明 rank=input_size/4 时无效果损失

9.Conclusion

DCN-V2: to model explicit crosses in an expressive yet simple manner. 

DCN-Mix: Observing the low-rank nature of the weight matrix in the cross network, to propose a mixture of low-rank DCN，是效果和延时的折中

* * 

### NeuralCF模型——CF与深度学习的结合

* MF的缺点：输出层简单，容易欠拟合

* NeuralCF
  * 用神经网络替代CF最后一层打分的点积操作

* NeuralCF混合模型：
  * concat（原始NeuralCF模型的输出，以element-wise product为互操作的广义矩阵分解模型）

* 局限性和CF一致：没引入其他类型的特征、具体的互操作有待探索



### PNN模型——加强特征交叉能力

Product层：线性操作 + 乘积操作
* 乘积操作 = 内积(IPNN)/外积(OPNN)
* 叠加外积互操作矩阵：本质上是让所有embedding通过一个平均池化层后，再进行外积互操作
  * $$p=\sum_{i=1}^N\sum_{j=1}^Nf_if_j^T=(\sum_{i=1}^Nf_i)(\sum_{i=1}^Nf_i)^T$$
  * [embedding的意义](https://www.zhihu.com/question/374835153/answer/1042845667)
  * 简单的sumpooling会忽略一些有价值的信息，对不同类embedding的avg pooling需要谨慎
  



### Wide&Deep模型——记忆能力和泛化能力的综合

#### Wide & Deep learning for Recommender Systems, RecSys 17

* Introduction

  * Wide ~ Memorization: 模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力
    * LR能学到“强特征”
    * 输入特征：已安装应用list、target item


  * Deep ~ Generalization: 模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力
    * 输入特征：全部特征


  * Generalized linear models with nonlinear feature transformations

  * cross-product transformations: 特征工程的概念，交叉积变换，缺点是无法 generalize 没出现过的 query-item feature pairs


* 问题：输入是稀疏高秩矩阵，缺少 interactions，难以利用它学到合适的低维 embedding

WIDE & DEEP Learning

* The Wide Component
  * 利用 cross-product transformation 提供多个特征的非线性
    * $$\phi_k(\mathbf{x}) = \prod_{i = 1}^{d} x_i^{c_{ki}} \quad c_{ki} \in \{0, 1\}$$
    * [Explanation](https://datascience.stackexchange.com/questions/57435/how-is-the-cross-product-transformation-defined-for-binary-features)
  * 对比：[Deep Neural Networks for YouTube Recommendations ] 用平方 和 平方根项 提供单个特征的非线性

* Joint Training of Wide & Deep Model

  * 注意辨析 joint training 和 ensemble 的区别

    * 前者是共同训练，后者不是

    * 后者模型可以更大

    * 前者，Wide 只需要给 Deep 补少量 cross-product feature transformations


* System Implementation

  * Model Training

    * warm-starting system

    * dry run

    * sanity check


* Appendix

  * 概念：AUC：ROC曲线下方的面积，ROC横坐标FPR，纵坐标TPR

  * 资源：
    * 这个大佬的专栏很实用，讲解tensorflow和推荐系统，https://zhuanlan.zhihu.com/learningdeep


* 思考：可否联系到IRLS方法，最优化稀疏矩阵的秩，用一个类似的矩阵学习秩的表示

#### [DLRM] Deep Learning Recommendation Model for Personalization and Recommendation Systems

![image-20250524032009190](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250524032009190.png)

* 模型：
  * dense特征相连之后过一个MLP也变成和Embedding维度一样的tensor A
  * 所有sparse + tensor A两两做内积，相连，再连上A，过MLP
* 训练：并行策略
  - DLRM 参数多，训练时间长，需高效并行。其嵌入部分参数多、内存和带宽需求大，不适用于数据并行；MLP 内存需求小但计算量大，适合数据并行。
  - 因此采用嵌入模型并行和 MLP 数据并行结合的方式。在实现中，需处理 top MLP 和交互算子的数据通信，通过自定义实现模型并行和蝴蝶洗牌算子实现All-to-all通信，并优化数据并行的参数更新。

* 机器：
  * Big Basin platform
  * Xeon 6138 CPU@2.00GHz + 8*Nvidia Tesla V100 16GB GPUs

* 结论：DLRM > DCN

#### 变种：Bias NN Tower

* bias sum加在logit
* bias tower output加在hidden

#### 变种：Multi-Tower形式

* 1-Tower、2-Tower、3-Tower
* Shared Bottom





### FM与深度学习模型的结合

* FNN——用FM的隐向量完成Embedding层初始化
  * 背景：Embedding层收敛速度慢 <= 参数多、输入向量稀疏

![image-20250103195716225](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103195716225.png)

* DeepFM——用FM代替Wide部分
  * embedding作为FM的隐向量
  * 对比Deep&Cross：用FM而不是Deep&Cross做特征交叉
* NFM——FM的神经网络化尝试
  * 在Embedding层和多层神经网络之间加入**特征交叉池化层**
    * 对embedding，两两
  * NFM和FM的关系：FM是一种浅层的线性模型, 可以看作是不带隐层的NFM
  * 相比Wide&Deep, Deep&Cross用concat的方法连接特征，NFM方法更侧重特征之间的深度交互



### 注意力机制在推荐模型中的应用

* AFM——引入注意力机制的FM
  * 在NFM的Pair-wise Interaction Lyaer和池化层之间加入注意力网络
  * 注意力网络 = 全连接层 + softmax
    * 全连接层网络输入是 element-wise 交叉的特征vector，输出是score
    * 所有score做softmax



#### (DIN) Deep Interest Network for Click-Through Rate Prediction ——引入注意力机制的深度学习网络

  * 核心动机是常见的 Embedding&MLP 模型，user 特征的表达能力受限于 fixed-length vector。user特征表达能力难以通过简单的翻倍方式来提升（将多个域的user特征连接起来），考虑到线上压力以及过拟合的风险，需要寻求算法上的突破 => weighted sum-pooling
  * Attention 技术：NLP、搜索领域，通常用来刻画context（比如用target词加权句子、用target广告加权最近的query）。本文创新性地用广告特征来加权user序列特征
  * 应用于淘宝的电商广告推荐场景，感觉在电商场景，推荐和广告天然结合地紧密（广告就是推荐的商品）；短视频场景，两者更割裂一些，并且用户兴趣更长期
      * 电商场景，搜索个性化的必要性也更强，和推荐区分不严格
    * 此外，搜索广告场景先通过相关性做召回，严格意义上进入推荐系统的候选数量少
  * 用户侧的embedding是对每次行为的embedding通过注意力加权得到，注意力权重受广告特征影响
    * 广告侧：论文中有 goods id, shop id, cate id；实践中可以pooling ad id, category id, position encoding (特征抽取时做分钟的sqrt，相比 log 对长期历史行为有区分度)
    * 商铺id只和用户历史行为中的商铺id序列发生作用，商品id也如此 <=> 注意力轻重更应该由同类信息相关性决定
  * $\textbf{V}_u=f(\textbf{V}_a)=\sum_{i=1}^N\omega_i·\textbf{V}_i=\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a)·\textbf{V}$ 
      * 注意力激活单元：
        * 元素减操作的embedding（原论文中是取外积向量）, concat两个原输入embedding
      * $\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a) \neq 1$ ，描述 an approximation of the intensity of activated user interests to some degree
  * Training Techniques
    * Mini-batch Aware Regularization：引入L2范数，通过“只对输入中有的sparse fc算”减少运算量
    * Data Adaptive Activation Function：PReLu -> Dice
  * Experiments
    * 新的AUC计算方式：GAUC (averaging AUC over users)
    * Regularization 实验：Mini-Batch Aware L2 > Occurrency Filter > DropOut > Regularization in DiFacto > Base

#### DIEN——序列模型与推荐系统的结合

> DIEN work的前提：1）场景存在兴趣进化；2）数据流完整保留兴趣的进化过程

* 尝试刻画 latent interest 而非用 behaviour 描述 interest
* 兴趣进化网络
  * 行为序列层：普通的Embedding层
  * 兴趣抽取层：GRU
    * auxiliary loss: extra supervision information, uses consecutive behavior to supervise the learning of hidden state at each step. which makes hidden state expressive enough to represent latent interest. 算法实现上，是让第i次的 hidden state 向量更接近第 i+1 次的正样本 item 向量
  * 兴趣进化层：(attentional update gate) AUGRU，引入注意力机制，与 DIN 相似，更有针对性地模拟与目标广告相关的兴趣进化路径
    * 文章中讨论了 attention 结合 GRU 的几种方法：AIGRU、AGRU、AUGRU

#### (DSIN) Deep Session Interest Network for Click-Through Rate Prediction

* multiple sessions：30 mins gap
* session内用户兴趣抽取：multi self-attention mechanism(AutoInt) with bias encoding
* session间兴趣进化：Bi-LSTM
* 聚合兴趣预估ctr：local activation unit

#### UIC+MIMN

[从阿里的User Interest Center看模型线上实时serving方法](https://zhuanlan.zhihu.com/p/111929212)

* system: UIC

* algorithm: MIMN = improved NTM with two designs of memory utilization regularization and memory induction unit

3.REALTIME CTR PREDICTION SYSTEM

- **电商领域，特征中 90% 的规模是 user behaviour features**
- long sequence 架构上的挑战主要在于 storage 和 latency

4.MULTI-CHANNEL USER INTEREST MEMORY NETWORK

- long sequence 算法上的挑战：
  - RNN 结构并不最适合刻画 long sequence:
    - hidden state 偏向于刻画 predicting target 而非 history；
    - 存储所有历史信息的方案较冗余


=> MIMN 

![image-20250111233228050](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111233228050.png)

- **NTM: Neural turing machines.**
- memory utilization regularization: to increase the expressive ability of memory tensor in UIC by increasing the utilization of memory
  - 解决 NTM 的 memory 被热点行为 dominate 的问题：1）LRU 将信息写进不同的 slot，有利有弊；2）Memory Utilization Regularization 
- memory induction unit: to help capture high-order information
  - 继续用GRU进化兴趣，MIMN的特点，兴趣进化是 multi-channel 的（channel间共享GRU的参数），**多通道分别生成记忆向量**
  - ![image-20250111233446127](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111233446127.png)



![image-20250111233647489](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111233647489.png)

#### [SIM] Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction

Search-based Interest Model (SIM) -> Sub user Behavior Sequence (SBS)

- General Search Unit (GSU)
  - soft-search and hard-search
- Exact Search Unit (ESU)
  - multi-attention; encode temporal distance information

training方案：

* ESU: multi-head attention

* GSU: soft-search 联合 ESU 学习，auxiliary CTR model 学习 behavior embedding

serving方案：

* hard-search: user behavior tree (UBT), uid->category id->behavior items

there indeed exists massive noise in original long-term behavior sequences which may undermine long-term user interest learning. 信息并非越多越好，本文也和 Youtube 论文[2] 一样，体现了过滤噪音信息的思想

#### MIND (Multi-Interest Network with Dynamic routing), CIKM 2019

* 与 match 特征的关系：本文是把 match 做到模型里，如果模型能学到 历史 item 到 interest 的转换关系，就相当于是有 match 能力了（match 的 key 我感觉就像是 interest）

* 本质上是捕捉用户兴趣做召回，利用胶囊网络去“压缩”序列特征成兴趣特征，multi-interest embedding 尝试克服 user embedding 的维数局限性
  * multi-interest extractor layer: soft-cluster user interests

* label-aware attention: to help learn a user representation with multiple vectors.

* Related Work
  * User Representation: sequence model; from word embedding
  * Capsule Network. The concept of "Capsule", a small group of neurons assembled to output a whole vector, is firstly proposed by Hinton [13] at 2011. 
    Instead of backpropagation, dynamic routing [21] is used to learn the weights on the connections between cap-sules, which is improved by utilizing Expectation-Maximization algorithm [14] to overcome several deficiencies and achieves better accuracy.

* Method
  * 3.3 Multi-Interest Extractor Layer
    * 由序列特征生成 user embedding，是基于胶囊网络的 user tower，生成 
      multiple representation vectors
  * Dynamic Routing
    * In a nutshell, capsule is a new kind of neuron represented by one vector instead of one scalar used in ordinary neural networks.
    * dynamic routing：胶囊网络中利用 bilinear 矩阵，学习low-level到high-level表征关系
    * 最后一层用 a non-linear "squash" function 处理
  * B2I dynamic routing
    * learn interest capsules from behavior capsules.
    * Shared bilinear mapping matrix：设计的一方面因素是因为 user behaviour 是变长的
    * Randomly initialized routing logits
    * Dynamic interest number: 根据历史序列长度动态调整兴趣数量

  * 3.4 Label-aware Attention Layer
    * 训练时用
    * Q: 这个 dynamic routing 怎么训练？

* Experiments
  * 实验指标 HitRate
  * hard attention scheme 效果最好

### 动态权重：推荐算法的新范式

#### [先入为主：将先验知识注入推荐模型](https://zhuanlan.zhihu.com/p/442845759)

* 正如DCN作者在论文中所宣称的那样，“People generally consider DNNs as universal function approximators, that could potentially learn all kinds of feature interactions. However, recent studies found that DNNs are inefficient to even approximately model 2nd or 3rd-order feature crosses.”。正因如此，**如果你先重要的特征加到DNN底部，层层向上传递，恐怕再重要的信息到达顶部时，也不剩下多少了。另外，推荐系统中，DNN的底层往往是若干特征embedding的拼接，动辄几千维是小意思，这时你再新加入一个特征32维embedding，“泯然众人矣”，恐怕也不会太奇怪**。
  * 这是DNN的情况，Attention可能更擅长保留信息
* **思路1: 重要特征加的浅**
  * 登录用户 vs. 未登录用户、不同国家的用户
  * 与用户分层类似，在multi-domain的推荐场景下（比如：首页推荐与猜你喜欢，混合训练），淘宝的**STAR《One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction》**也是将domain-id这样的强bias特征，喂入一个非常简单的dnn，得到的logit再叠加到主模型的logit上，算是wide&deep的一个变种。文中还强调，“the auxiliary network is much simpler ...... The simple architecture makes the domain features directly influence the final prediction”
* **思路2: 重要特征当裁判**
  * SENet、LHUC
  * ![image-20250408175744093](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250408175744093.png)
  * **将一些先验认为重要的强bias的特征，放到裁判的位置，决定其他特征的重要性**。比如，如果产品设计不允许未登录用户转发或评论，那么显然“用户未登录”这个特征值，就“应该”（suppose）将转发、评论相关特征的权重置为0，因为它们不能代表用户的真实意图。

* 交叉统计特征是重要先验
  * 使用这样的统计特征，有两大难点：
    - **泛化能力差**。统计特征只能涵盖样本中经常出现的特征组合，对于罕见的特征组合，假设<男性，口红>，要么统计结果不置信，要么干脆在样本中就从未出现过。
    - **存储量巨大**，需要存储任意两个特征值组合对应的统计数据。
  * 为了更好地利用“统计特征”这一先验知识，阿里妈妈在SIGIR 21**《Explicit Semantic Cross Feature Learning via Pre-trained Graph Neural Networks for CTR Prediction》**一文中提出了用预训练来解决以上难题的思路：
    - 预训练一个模型，输入两个特征，输出这一对特征组合上预估的xtr
    - 预训练的时候用了GNN
      - 顶点就是样本中出现过的categorical feature，样本中常见共现特征之间建立边，边上的值就是这一对儿特征离线统计出的xtr
      - 按照link prediction的方式来训练GNN
  * 阿里这种Pre-trained Cross Feature Learning的好处在于，**用“预测”代替了“存储”**，从而节省了存储量，而且**对于不常见、甚至未出现过的特征组合也能够给出统计特征**。
* 先验知识帮助稀疏目标

#### 动态权重

> 石塔西 https://zhuanlan.zhihu.com/p/500934745

* 大型推荐系统要面对多场景，而多场景的训练数据存在分布差异。比如：
  - 对一个跨国app，不同国家的用户的消费习惯存在差异
  - 同一个app，不同使用场景，比如单列 vs. 双列，用户行为也有较大差异
  - 新老用户的行为模式也存在很大的不同
* 面对这种差异，一种作法是将拆分数据，每个场景（不同国家、单双列、新老用户）单独训练、部署模型。但是这种作法也有缺点：
  - 有的场景数据少，单独训练根本训不出来，必须依靠大数据场景的知识迁移。
  - 每个场景，单独训练和推理，资源占用太多，而且日后的升级、维护都比较麻烦。
* 单独建模行不通，把所有数据合在一起训练就行了吗？最大的问题在于，**各个场景的数据量严重不均衡**。合一起训练，整个模型会被数据丰富的场景主导，从而在数据量少的场景表现欠佳。

* 思路：
  * 特征：场景敏感特征
    * 比如，多国家场景下，用户和作者的国籍、语言；新老用户场景下，是否新用户、用户是否登录、用户使用天数等
  * 模型：多塔结构，star、hmoe https://zhuanlan.zhihu.com/p/458843906
  * Dynamic Weight
* Dynamic Weight
  * 方法：
    * 将“场景敏感”特征z，喂入一个小网络，输出一个向量W，即`W=G(z)`。
    * 把W reshape成一个子模型Dynamic Weighted Network(DWN)。比如，W一共640维， 640=32×16+16×8 ，从而可以reshape成一个32->16->8的三层MLP。
    * 拿以上根据“场景敏感”特征动态生成权重的DWN，接入整个推荐模型的关键位置。即$$y=F_W(x)=F_{G(z)}(x)$$，注意这个动态网络的输入是x，就是普通特征，场景敏感与非敏感特征都包括。
  * **DW模式特别突出了“场景敏感”特征的作用，让它们像一个“滤波器”控制住了其他信息的向上传递通道。其他信息向上传递过程中，都要经过“场景敏感”特征的“调制”，根据不同场景，对前一层发现的模式局部放大或衰减**。而且在将动态产生的权重reshape成DWN的过程中，DWN层与层之间可以插入非线性的激活函数，从而允许实现比较复杂的“调制”功能。后面介绍了几篇paper

* 和阿里CAN的联系：
  * CAN和DW针对的问题很像，都是针对“合不上，分不开”的问题
    - 合不上：如果每个特征只有一套embedding，需要与其他所有embedding交叉，可能相互干扰。
      - 这和，DW将所有场景数据合一起训练，面临的“模型被数据多的场景带偏”问题，很相似。
    - 分不开：如果每对儿交叉特征都有自己独立的embedding，特征空间太稀疏不好训，而且也占用太多资源。
      - 这和DW为每个场景单独建模，面临的“数据少场景不好训、占用资源多、不好维护”问题，很相似。
  * CAN和DW解决的方法很像
    - CAN把target item id/category embedding reshape成一个MLP，与user feature交叉时，就把user feature喂入这个dynamic generated MLP
    - DW利用“特征敏感”特征动态生成一个MLP，把其他所有特征喂入这个dynamic generated MLP

##### 微信PTUPCDR

>  《Personalized Transfer of User Preferences for Cross-domain Recommendation》

![image-20250408185038509](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250408185038509.png)

##### 阿里M2M

> 《Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling》

![image-20250408185618445](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250408185618445.png)

##### 阿里APG

> 《APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction》

* 解决DW的性能，做低秩分解，控制可学习的矩阵

#### 基础结构

#### LHUC

* motivates the idea of using LHUC– Learning Hidden Unit Contributions – **for test-set adaptation**.
* The key idea of LHUC is to explicitly parametrise the
  amplitudes of each hidden unit (either in fully-connected
  and convolutional layers after max-pooling), using a speaker-
  dependent amplitude function.

## 模型训练

### 流式训练和实时特征

- [推荐系统里的“七伤拳”：那些高 ROI 但长期有损的优化 - 九老师的文章 - 知乎](https://zhuanlan.zhihu.com/p/1930155262179807978)
- [推荐系统里流式训练、UID 特征、实时特征 - 被推荐的人生回路的文章 - 知乎](https://zhuanlan.zhihu.com/p/1931450475925992604)
- [也谈推荐系统的流式学习、ID 类特征 - booster 的文章 - 知乎](https://zhuanlan.zhihu.com/p/1931660935539421893)
- [再论流式训练与实时统计特征 - BUGs 的文章 - 知乎](https://zhuanlan.zhihu.com/p/1933153750140318902)

> 很多宣称流式训练及其有效的场景，忽略了 **特征完备性假设**，通俗讲就是因为ID可以实时学习，就忽略了特征的构造和实时性优化。特征完备和流式并不矛盾，阿里的系统，默认是batch，很多系统是后面接入流式的，但是AB收益并不大，甚至普遍没有收益；某司的系统，默认是流式的，宣称流式是非常有效的。这里面有两个原因，阿里的系统是从LR演化来的，做了非常多的特征工程，比如大量秒级实时统计特征，来到NN后，也做了秒级实时行为序列，超长行为序列，它切流式就没有太大效果。某司的系统，从FFM演化来的，特征工程即模型里的手工隐式交叉，依赖训练产生新的模式捕捉，来到NN之后，一段时间也没有手工统计特征和非常实时的序列架构，那流式自然就非常有效。

* 构建泛化特征，能避免对流式学习的过度依赖，避免long term的潜在损失。
* 流式虽然反馈快速，但是在不考虑额外处理delayed feedback的前提下，一般要从曝光后设置一个窗口，根据不同的行为收集比率，这个窗口从15分钟到1小时不等，比如电商等待转化label，直播等待关注label。流式的label并不足够实时，这也是实时特征永远能带来增益的原因。

### 一些技巧

* slow start，降低大并发异步训练的gradient staleness在模型训练初期的影响
* lr rate scaling







## 多目标任务

### Intro

* 显式信息迁移的典型是阿里的ESMM模型。`pCTCVR=pCTR*pCVR`。CTR任务的数据更多，预测精度更高，会给CTCVR的预测任务以提示
* --> 前端环境的hidden emb进入后端环节

### MMOE

### PLE



## 新闻推荐

### Literature Review

* 数据增强
  * LLM生成category description https://arxiv.org/pdf/2405.13007
* 广告对推荐的影响
  * [2,10] [YAHOO]

### [YAHOO] Embedding-based News Recommendation for Millions of Users

* 技术发展
  * idrec和低秩分解，缺失语义信息；
  * word-based methods，涉及构造query、orthographical variants（拼写变体）、query生成

* An article is regarded as a collection of words included
  in its text. A user is regarded as a collection of words included
  in articles he/she has browsed. 

* 核心思路：
  * Start with distributed representations of articles based on
    a variant of the denoising autoencoder (which addresses
    the first issue in Section 3).
  * Generate user representations by using an RNN with brows-
    ing histories as input sequences (which addresses the sec-
    ond issue in Section 4).
  * Match and list articles for each user based on the inner
    product of article-user for relevance and article-article for
    de-duplication (outlined in Section 2).

* 业务流程：
  * Identify: Obtain user features calculated from user his-
    tory in advance.
  * Matching: Extract articles from all those available using
    user features.
  * Ranking: Rearrange list of articles on certain priorities.
  * De-duplication: Remove articles that contain the same
    information as others.
    * 贪心算法消重
  * Advertising: Insert ads if necessary

* 特征
  * the expected number of page views and freshness of each article, in addition
    to the relevance used for matching
* 模型
  * a **denoising autoencoder** [19] with weak supervision
    * ![image-20250103183530883](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103183530883.png)
    * 根据category信息，构建一个loss
    * ![image-20250103185036772](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103185036772.png)

* User行为建模

  * session和browse
  * 有可能一个browse却没有session（网页搜索进入）
  * ![image-20250103190827268](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103190827268.png)

  * 建模：考虑负例、position bias
    * ![image-20250103191736717](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103191736717.png)

  * word-based方法的局限性：

    * the sparseness of the representation
    * intensity

  * 优化：

    * ![image-20250103192355241](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103192355241.png)

    * 缺点：指数遗忘

* 模型：

  * RNN
  * LSTM
  * GRU

* 评估：

  * 在线：user分类
    * Heavy: Users who have visited for more than five days
      during the previous week.
    * Medium: Users who have visited for between two and five
      days during the previous week.
    * Light: Users who have visited for less than two days dur-
      ing the previous week.

* 实时性：

  * article/user representation model，更新比较慢

* 结论：

  * distributed-representation比BoW强很多
  * 对browsing序列做简单的decay效果不好，GRU效果比较好
  * 在线指标：
    * a good recommendation model first increases clicks, and multiple click experiences
      encourage users to gradually use the service more frequently.
    * Light user提升大

### 微软新闻

* msnews.github.io
  * MIND acl2020

* bert noisytune 大模型finetune
* responsible的目标
  * [cprs: objective beyond click](https://www.ijcai.org/proceedings/2020/0418.pdf)
* fairness：fairrec 学习一个serving用的无偏网络，比如让模型估不出来性别

### Empowering News Recommendation with Pre-trained Language Models

* 模型

![image-20250102003946208](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250102003946208.png)

![image-20250102004310634](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250102004310634.png)

* 训练
  * used the titles of news
  * finetuned the last two Transformer layers
* 结论：
  * attention pooling > avg pooling > CLS token embarrassment
    * https://github.com/wuch15/PLM4NewsRec/blob/main/model_bert.py

## 视频推荐

### Intro

* 业务目标：
  * 时长 -> 用户满意度 -> 留存、DAU、ARPU
    * 反例：youtube推荐，无法提升留存和DAU，一直涨时长 -> 推的视频越来越长
    * -> 长视频的目标是 LT、LTV
* 业务特点（by 王喆）：首页播放率大概在1.5%左右，广告更低 0.5%以下

### Deep Neural Networks for Youtube Recommendations, RecSys 16

* 业务
  * 内容来自UGC

  * 头部效应不明显，商业模式不同于Netflix、爱奇艺
    * 时长更重要

* Intro: three major perspectives
  * scale
  * freshness: reponsive, exploitation
  * noise

  * DL在推荐系统中的应用：recommending news or music, citations, review ratings, collaborative filtering, autoencoders
* System Overview
  * candidate generation + ranking，一个two-staged的机制，本质上是对数据和用户行为细粒度做分层
  * 训练时利用offline信息，但测试是用A/B testing
* Candidate generation

![image-20250111225209942](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111225209942.png)

* Recommendation as Classification

  * item emb的学习
    * collaborative filtering + word2vec，类似airbnb的方法学习候选embedding向量
    * a non-linear generalization of factorization techniques，用分类的方法做推荐
      * 输出经过softmax处理，输入的是video向量和user向量的内积
  * user向量的学习：利用implicit feedback
  * 对类别数过多的处理
    * sample负样本再correct via importance weighting[10]，损失函数只涉及true label和sampled negative classes
    * hierachical softmax效果不好
    * serving阶段
      * approximate scoring scheme sublinear in the number of classes 
      * softmax对serving没用，转化为nearest neighbor search问题

* Model Architecture：Figure 3.

  * watch和search vector是对variable-length vectors求平均

  * Heterogeneous Signals
    * DL的优势是能方便地cat各种信息，性别、登入状态、年龄等作为[0,1]变量输入
    * 输入"Example Age" Feature，用来fresh信息，用户喜欢，可能有viral效应[11]
  * Label and Context Selection 
    * generate a fixed number of training examples per user，每个user的权重一致
    * training examples包括用户看的所有视频，而不仅仅是推荐给用户的视频
    * **withhold information from the classifier，问题的来源：推荐用户刚搜索的视频很蠢 =>** 
      **discarding sequence information and representing search queries with an unordered bag of tokens**
      * 感觉更多地适用于搜索+推荐综合场景（用户主动+被动接受信息）
    * asymmetric consumption patterns => Figure 5. predicting future watch
      * ![image-20250111230537204](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111230537204.png)
    * 总结：There is more art than science in selecting the surrogate problem for recommendations

  * Experiements with Features and Depth





![image-20250111225946384](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111225946384.png)

* 数据：

  * 对每个用户，提取等数量的训练样本，减少高度活跃用户对模型的影响

* Ranking

  * The primary role of ranking is to use impression data to specialize and calibrate candidate predictions for the particular user interface.

* 特征 Feature Representation

  * 引入univalent和multivalent的概念对变量分类
    * whether they describe properties of the item (“impression”) or properties of the user/context (“query”), Query features are computed once per request while impression features are computed for each item scored.

  * 主要的挑战是1）时序动作 2）与impression相关的时序动作如何处理
  * 业务特征：
    * 该用户自上次观看**同频道视频**的时间（time since last watch）
    * 该视频已曝光给用户的次数（previous impression）

  * Embedding Categorical Features
    * 维度压缩，对数关系
    * categorical features in the same ID space also share underlying emeddings
    * 占据了模型大部分参数
  * Normalizing Continuous Features
    * 线性插值估算CDF来归一化
    * **输入x的次方和开方，获取非线性特性**

* Modeling Expected Watch Time

  * weighted logistic regression
  * 数学含义：预测Odds
    * $$p=\frac{1}{1+e^{-(Wx+b)}}$$
    * $$Odds=p/(1-p)=e^{Wx+b}$$  ，**机会比**
    * logit函数：$$logit(p)=ln(\frac{p}{1-p})$$
    * Weighted LR后，正样本i的Odds：$$Odds(i)=\frac{T_ip}{1-T_ip}\approx T_ip=E(T_i)=期望观看时长$$
  * 细节分析：正样本加权 <-> Odds变成w倍
    * https://bourneli.github.io/recommendation/2020/07/19/notes-of-youtube-recommendation-2016.html
  * ![image-20250116220810022](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250116220810022.png)
    * 《Simple and scalable response prediction for display advertising》

* 处理用户对新视频的偏好：Example Age

  * training：训练样本产生的时刻距离当前时刻的时间
  * serving：0或者很小的负值
  * 思考：为什么不用视频的days since upload，原因可能是会导致时间分布过于分散，无法集中描述近期的变化趋势

* Note

  * 长尾视频embedding用全零向量替代
  * 特征长度的可变性：在输入特征之前再加可变长的神经网络？

* 资料

  * [Youtube推荐系统的变迁](http://www.datagrand.com/blog/youtube.html)

#### 和 Word2Vec 的联系

* 这里的最后一个 hidden 层和 softmax 层组成的结构与 word2vec 一模一样
  * 唯一的区别是 word2vec 用输入层与 hidden 层之间的权重作为 word embedding，没用 hidden 层与输出层的权重；
  * 这里用了 hidden 层与输出层的权重作为 video embedding，用 hidden 层作为 user embedding

* 为什么input和output不在一个空间，又通过点积预测
  * 《word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method》
  * One motivation for making this assumption is the following: consider the case where both the word dog and the context dog share the same vector v. Words hardly appear in the contexts of themselves, and so the **model should assign a low probability to p(dog|dog)**, which entails assigning a low value to v·v which is impossible.

## 电商推荐

* 《value aware recommendation baased on reinforcement profit maximization》

#### eval

* Transaction Count：It is a direct metric that quantifies the number of items successfully purchased by users during a given page view. As such, it serves as one of the most critical indicators of online performance, capturing the real-world impact of the methods in industry. A higher transaction count indicates that the recommended items are not only relevant but also compelling enough to lead to actual purchases.

## Push推荐

Push（通知）推荐场景与传统的 Feed 流（Pull）推荐有显著差异，面临独特的挑战：

1. **触发机制 (Triggering / Timing)**
   * **Pull vs Push**：Feed 流是用户主动请求（Pull），系统响应；Push 是系统主动发起（Push），用户被动接收。
   * **挑战**：
     * **时机选择 (Timing)**：需要在不打扰用户（如深夜、工作繁忙时）的前提下，预测用户最可能点击的时间点（Personalized Send Time）。
     * **实时性 (Timeliness)**：对于突发新闻、热点事件，秒级触达至关重要。

2. **候选集与受众选择 (Audience Selection / Targeting)**
   * **全量 vs 个性化**：除了常规的“给 User 选 Item”，Push 场景更强调“给 Item 选 User”（例如突发一条体育新闻，应该推给哪些对体育感兴趣的用户）。
   * **挑战**：
     * **计算量大**：全库用户筛选（Full-base scanning）对工程架构要求极高。
     * **精准度要求高**：误推的代价比 Feed 流高得多（Feed 流刷过去就行，Push 会打扰用户）。

3. **负反馈与频率控制 (Negative Feedback & Frequency Capping)**
   * **硬损失**：Push 的负反馈极其昂贵。用户反感会导致直接关闭通知权限（System-level Opt-out）甚至卸载 App，这是不可逆的损失。
   * **挑战**：
     * **全局配额管理 (Global Quota)**：需要对全天、全周的推送数量进行严格限制（Frequency Capping）。
     * **多业务竞争**：不同业务线（如新闻、活动、社交提醒）争夺同一个用户的 Push 配额，需要建立统一的打分与仲裁机制（Arbitration）。

4. **上下文缺失 (Lack of Real-time Context)**
   * **问题**：用户未打开 App，系统无法获取用户当前的实时行为序列（Last Actions）或所处场景。
   * **挑战**：
     * **依赖离线/近线特征**：主要依赖用户的历史画像、长期兴趣，而非实时意图。
     * **冷启动更难**：对于新用户，缺乏足够信息判断是否应该 Push。

5. **评估指标 (Evaluation Metrics)**
   * **复杂权衡**：不仅关注点击率 (CTR)，更要关注：
     * **到达率 (Delivery Rate)**：受限于通道能力和用户设备状态。
     * **打开率 (Open Rate)**：真正进入 App 的比例。
     * **DAU 贡献 (Session Start)**：Push 是否带来了有效的日活。
     * **负向指标**：关闭通知率 (Opt-out Rate)、卸载率。

6. **物品冷启动 (Item Cold Start)**
   * **Feed 验证 (Warm)**: 部分 Push 内容源自 Feed 热帖，已有充分交互数据，此时不涉及冷启动。
   * **突发 Breaking News (Cold)**: Push 的核心价值在于“快”（如突发地震、重大赛事比分）。
     * **矛盾**：追求秒级触达，意味着没有时间在 Feed 积累行为数据。
     * **重要性**：**极高**。这是 Push 区别于 Feed 的杀手锏场景。
     * **解法**：强依赖 **内容理解 (Content-based)**、语义匹配 (Semantic Matching) 和 知识图谱，而非 ID 类协同过滤。

## Push 推荐系统

### 核心差异与挑战
Push（通知）推荐与 Feed 流推荐有着本质的区别，主要体现在**主动性**与**约束性**上：
1.  **Interruption (打扰性)**：Push 是系统主动发起的（System-initiated），用户处于被动接收状态。错误的推荐不仅是“不点击”，更可能导致“关闭通知权限”甚至“卸载 App”，负反馈代价极高。
2.  **Context-less (无上下文)**：用户未打开 App，缺乏实时的 Session 上下文（如当前想看什么），只能依赖离线画像和长期兴趣。
3.  **Winner-take-all (赢家通吃)**：通常只有 Top-1 的展示机会（锁屏界面第一条），对 Precision@1 要求极高，导致 Exploratoin（探索）极其困难。
4.  **Timeliness (时效性)**：Push 往往承载突发热点（Breaking News）的分发，要求秒级触达。

### Volume Control (Push Specific)
- Paper: [Near Real-Time Optimization of Notification Volume](https://www.kdd.org/kdd2018/accepted-papers/view/near-real-time-optimization-of-notification-volume) (LinkedIn, WWW 2018)
- Core Problem: Push 通知的核心挑战不仅是 Ranking（推什么），更是 Volume Control（推多少）。过度推送会导致用户关闭通知权限（不可逆的 Churn），这是 Feed 流没有的硬约束。
- Methodology:
  - Budget Constrained Optimization: 将问题建模为带约束的优化问题。目标是最大化 Total Engagement (Sessions)，约束是 "Badness" (Unsubscribes/Disable) < Budget。
  - Lagrange Multipliers: 使用拉格朗日乘子法将约束转化为无约束问题，引入 $$\lambda$$ 作为“打扰成本”。
  - Send Decision: $$Score = P(Click) \times V_{click} - \lambda \times P(Unsubscribe) \times V_{churn}$$。只有当 Score > 0 时才发送。
- Insight: Push 推荐必须考虑 Long-term Value (LTV)，不能只看短期 CTR。

### Exploration & Cold Start (Exposure Optimization)
老虎机调物品超参
相似物品打散高于这个
- Paper: [A Contextual-Bandit Approach to Personalized News Article Recommendation](https://arxiv.org/abs/1003.0146) (Yahoo!, WWW 2010)
- Core Problem: 物品冷启动（Cold Start）和动态环境下的探索（Exploration）。在 News/Push 场景中，新物品源源不断，且生命周期短，传统的 Collaborative Filtering 无法处理（因为没有历史交互）。
- Methodology: LinUCB (Linear Upper Confidence Bound)
  - Contextual: 利用上下文特征（用户特征 + 物品特征）来预测回报。
  - UCB (Upper Confidence Bound): 不仅预测点击率的期望值 (Mean)，还预测置信区间 (Variance)。
  - Selection: 选择 $$Score = \mu + \alpha \cdot \sigma$$ 最高的物品。对于新物品，由于数据少，$$\sigma$$ 大，会被优先展示（探索）；随着数据积累，$$\sigma$$ 减小，逐渐过渡到利用（Exploit）。
- Insight: 解决“曝光物品数少”的关键是引入确定性的探索机制，而不是随机探索。LinUCB 是工业界解决冷启动最经典的 Baseline。

### Targeting & Incrementality (Uplift Modeling)
- Context: 业界（如美团、阿里）在 Push 场景中常采用 Uplift Modeling（因果推断）来解决 "Targeting" 问题。
- Core Problem: 传统的 CTR 模型无法区分 "Sure Things"（自然转化用户）。对这类用户发 Push 是浪费预算且增加打扰。
- Methodology: Uplift Modeling
  - Goal: 预测 $$Lift = P(Click|Treatment) - P(Click|Control)$$，即 Push 带来的增量点击概率。
  - User Segmentation:
    - Persuadables (高 Lift): 只有推了才点（核心目标）。
    - Sure Things (高 CTR, 低 Lift): 不推也会点（应减少推送）。
    - Lost Causes (低 CTR, 低 Lift): 推不推都不点。
    - Do Not Disturbs (负 Lift): 推了反而反感（绝对不推）。
- Insight: Push 的核心价值在于挖掘 Persuadables，从而提升 DAU 的增量 (Incremental Gain)，而非仅仅提升 CTR。

## 搜索

* 历史：https://www.vantagediscovery.com/post/ecommerce-search-transcended-for-the-ai-age
  * PageRank
  * As search engines become more personalized and user-focused, traditional ecommerce SEO tactics based on keyword optimization and backlinking are giving way to more sophisticated strategies that prioritize user intent and experience. 

### 核心技术与工具

* 传统的Indexing技术
  * [Suffix tree](https://en.wikipedia.org/wiki/Suffix_tree) 
    * Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [trie](https://en.wikipedia.org/wiki/Trie). Tries support [extendible hashing](https://en.wikipedia.org/wiki/Extendible_hashing), which is important for search engine indexing.[[8\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-8) Used for searching for patterns in [DNA](https://en.wikipedia.org/wiki/DNA) sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.[[9\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-Gus97-9) An alternate representation is a [suffix array](https://en.wikipedia.org/wiki/Suffix_array), which is considered to require less virtual memory and supports data compression such as the [BWT](https://en.wikipedia.org/wiki/Burrows–Wheeler_transform) algorithm.
  * [Inverted index](https://en.wikipedia.org/wiki/Inverted_index) (倒排索引)
    * Stores a list of occurrences of each atomic search criterion,[[10\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-10) typically in the form of a [hash table](https://en.wikipedia.org/wiki/Hash_table) or [binary tree](https://en.wikipedia.org/wiki/Binary_tree).[[11\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-11)[[12\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-12)
    * **定义补充**：现代搜索引擎的核心数据结构。它将文档中的内容（Terms）映射到文档的位置（DocIDs），类似于书籍末尾的索引。
    * 缺点：inherently emphasizes surface-level lexical overlap, thereby limiting generalization and hindering retrieval effectiveness (Dai and Callan, [2019](https://arxiv.org/html/2509.24632v1#bib.bib7))
    * **核心组成**：
        * **Term Dictionary (词典)**：记录所有文档中出现的唯一词汇（Term）。为了快速查找，通常使用 B+ Tree, Hash Table 或 FST (Finite State Transducers) 存储。FST 在 Lucene 中广泛使用，能极大压缩词典空间并保持快速查找。
        * **Posting List (倒排表/倒排链)**：对于词典中的每个 Term，维护一个包含出现该 Term 的所有文档 ID (DocID) 的有序列表。
            * **Payload**: 除了 DocID，还可以存储 Term Frequency (TF, 用于打分)、Position (用于短语搜索)、Offset (用于高亮)。
    * **查询过程 (Boolean Retrieval)**：
        * 搜索 "Apple AND Banana" 时，分别取出 "Apple" 和 "Banana" 的倒排链，进行**交集 (Intersection)** 运算。
        * 由于倒排链是有序的，可以使用 Skip List (跳表) 或 Frame of Reference (FOR) 等算法加速合并。
    * **压缩技术 (Compression)**：
        * 为了减少 I/O 和内存占用，倒排链通常会被压缩。
        * **Delta Encoding**: 不直接存 DocID，而是存 DocID 的差值 (d-gap)。
        * **Bit Packing / VByte / PForDelta**: 对差值进行变长编码或位压缩。
    * **现代优化**：
        * **Tiered Indexing**: 热数据存内存，冷数据存磁盘。
        * **Segment Merge**: 类似于 LSM Tree，新文档先写入内存 buffer，定期 flush 成不可变的 segment 文件，后台异步合并 (Merge) 小 segment。这是 Lucene/Elasticsearch 的核心写入机制。
  * [Citation index](https://en.wikipedia.org/wiki/Citation_index)
    * Stores citations or hyperlinks between documents to support citation analysis, a subject of [bibliometrics](https://en.wikipedia.org/wiki/Bibliometrics).
  * [*n*-gram index](https://en.wikipedia.org/wiki/N-gram)
    * Stores sequences of length of data to support other types of retrieval or [text mining](https://en.wikipedia.org/wiki/Text_mining).[[13\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-13)
  * [Document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix)
    * Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix).
* faceted search
  * https://www.sparq.ai/blogs/ecommerce-faceted-search

### 核心算法：相关性与排序

#### 向量空间模型 (Vector Space Model, VSM)

##### TF-IDF
* Term Frequency–Inverse Document Frequency
* 指标定义：TF-IDF = TF * IDF。其中，TF（Term Frequency）指的是词频，即某个词在文本中出现的次数。IDF（Inverse Document Frequency）指的是逆文档频率，即该词在整个语料库中出现的文档数的倒数。
* 应用场景：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
* **工程技巧与优化**:
    *   **过滤低权重词**: 这是 TF-IDF 应用中的一个关键步骤。权重极低的词（通常是停用词或极度稀有的词）对最终相似度得分的贡献微乎其微，但仍会参与计算。在计算相似度（如余弦相似度）前，可以设定一个阈值，过滤掉权重低于该阈值的词项，从而显著减少计算量，尤其是在处理长文档时效果明显。
    *   **停用词表 (Stop Words)**: 建立一个包含“的”、“是”、“在”等高频但无实际意义的词的列表。在计算 TF-IDF 之前，直接从文本中移除这些词，可以有效降噪并减少计算开销。
    *   **词干提取/词形还原 (Stemming/Lemmatization)**: 将词语的不同形态（如 "running", "ran", "runs"）统一为其基本形式（"run"）。这有助于合并相似的词项，减少特征空间的维度，增强模型的泛化能力。

#### 概率模型

##### BM25 (Best Match 25)
*   **核心思想**: BM25 是在 TF-IDF 基础上发展而来的经典排序算法，被认为是当前最先进的排序算法之一。它同样基于词频和逆文档频率，但引入了两个关键的改进：
    1.  **词频饱和度**: 与 TF-IDF 中词频权重可以无限增长不同，BM25 认为一个词在文档中反复出现，其对相关性的贡献会达到一个上限（饱和）。它通过一个超参数 `k1` 来控制这个饱和速度。
    2.  **文档长度归一化**: BM25 会对长文档进行“惩罚”。因为长文档天然包含更多词，更容易获得高的词频分数。它通过超参数 `b` 来控制文档长度归一化的程度。
*   **与 TF-IDF 对比**: BM25 通常比 TF-IDF 效果更好，因为它对词频和文档长度的处理更符合直觉，更鲁棒。

#### 稠密检索 (Dense Retrieval)

> Dense retrieval is more complex and demands greater computational resources. Additionally, it is more sensitive to increases in retrieval scale, such as the expansion of candidate sets, compared to inverted indexing recall.

*   **定义**: 区别于基于关键词匹配的倒排索引，稠密检索是一种互补范式。它将 Query 和 Document 编码到共享的 Embedding 空间中，通过向量相似度（如点积、余弦相似度）进行语义匹配。
*   **流程**: 
    *   **Offline**: 文档编码 (Document Encoding) + 索引构建 (Indexing, 如 ANN)。
    *   **Online**: 查询编码 (Query Encoding) + 相似度搜索 (Similarity Search)。
*   **早期研究**: 
    *   基于 PLM (如 BERT) 的 **Dual-Encoder** 架构 (Devlin et al.; Lee et al.)，在语义检索基准上表现强劲。
*   **优化方向**:
    *   **Task-specific Pretraining**: 针对检索任务进行预训练，如 `Condenser` (Gao et al.), `coCondenser` (Gao and Callan), `PAIR` (Ren et al.)。
    *   **Negative Sampling**: 改进负采样策略（如 Hard Negatives），提升模型判别能力 (Yang et al.; Zhan et al.)。
    *   **Knowledge Distillation**: 从更强的 Teacher 模型（如 Cross-Encoder）蒸馏知识 (Qi et al.; Qu et al.)。
    *   **Efficiency**: 为了减少内存占用和延迟，探索了 Embedding 压缩、积量化 (Product Quantization) 和二值编码 (Binary Encoding) (Yamada et al.; Zhan et al.)。
*   **LLM for Retrieval**: 
    *   利用 LLM 强大的语义理解、多任务适应性和长上下文建模能力，实现更复杂的 Query-Document 交互和长文档检索 (Chen et al.; Liu et al.)。
*   **对比**: Dense Retrieval 依赖计算密集的神经相似度搜索，而倒排索引框架依赖 Term-matching 且天然高效。
*   **代表性工作 (Literature Review)**
    *   **DPR (Karpukhin et al., 2020)**: 使用 **Dual-Encoder** 框架将 Query 和 Passages 映射为低维稠密向量，通过点积相似度实现高效检索。
    *   **ANCE (Xiong et al., 2021)**: 引入 **Approximate Nearest Neighbor Contrastive Learning**，解决了稠密检索训练中的负采样瓶颈 (Training Bottleneck)。
    *   **ColBERT (Khattab and Zaharia, 2020)**: 增强了 Query 和 Document 的表示，采用 **Token-level Late-interaction** 机制，支持多向量检索 (Multi-vector Retrieval)。
    *   **TriSampler (Yang et al., 2024)**: 进一步优化了困难负采样策略 (**Hard Negative Sampling Strategies**)。

#### 学习型稀疏检索 (Learned Sparse Retrieval, LSR)

*   **背景**: 传统的稀疏检索（如 BM25）面临**词汇不匹配 (Vocabulary Mismatch)** 问题；而稠密检索（Dense Retrieval）虽能捕捉语义，但缺乏精确匹配能力且可解释性差。LSR 旨在结合两者优势。

##### 稀疏增强 (Sparse Augmentation)

*   **原理**: 利用生成模型或上下文感知模型来丰富文档的词汇表示，从而让 BM25 更容易命中语义相关但词不重叠的查询。
*   **Document Expansion (文档扩展)**:
    *   **doc2query** (Nogueira et al., 2019a): 使用 seq2seq 模型为每篇文档生成可能的查询 (Predicted Queries)，并将这些查询附加到文档中。
    *   **docTTTTTquery (docT5query)** (Nogueira et al., 2019b): 使用 **T5** 生成多条“伪查询”拼接到文档，效果比 seq2seq 更强。
    *   **Reference**: ColBERT 论文 ([arXiv:2004.12832](https://arxiv.org/abs/2004.12832)) 中作为 baseline 进行了对比。
*   **Term Weighting (词项加权)**:
    *   **DeepCT**: 使用 BERT 预测每个词在文档中的“重要性”分数，用以替代或修正 TF (Term Frequency) 权重，底层仍走稀疏倒排索引。

##### Interaction Modeling (交互建模)

*   **原理**: 直接建模 Query/Document Token 与整个词表（Vocabulary）的交互，构建交互矩阵并聚合得到 Term-level 的重要性分数。
    *   **代表模型**:
        *   `SparTerm` (Bai et al., 2020): 使用 Summation aggregation。
        *   `SPARTA` (Zhao et al., 2021) & `EPIC` (MacAvaney et al., 2020): 使用 Max pooling。
    *   **局限**: 表示不够稀疏（通常需要额外的 Top-k pruning）或缺乏显式的稀疏正则化。
*   **SPLADE Family** (Formal et al., 2021a, b; Lassance et al., 2024; Bruch et al., 2024)
    *   **原理**: 结合了 **Sparsity Regularization (稀疏正则化)** 和 **Lexical Expansion (词汇扩展)**。它利用 MLM (Masked Language Model) 预测每个 Token 在词表上的权重，并通过 ReLU 和 Log 变换得到稀疏的 Term 权重。
    *   **优势**:
        *   **效率**: 保持了倒排索引的高效性。
        *   **可解释性**: 也就是生成的权重直接对应具体的词。
        *   **检索效果**: 在 Zero-shot 和 In-domain 任务上通常优于 Dense Retrieval。
    *   **局限**: 仍受限于 Token-level 交互和启发式规则，泛化能力受限，未能完全利用基于模型的表示。
*   **Unified Semantic Modeling (统一语义建模)**
    *   `UniDex`: 旨在消除复杂的手工规则，增强泛化能力，提供更统一的语义建模视角。

#### 细粒度交互与多表示模型 (Fine-grained Interaction & Multi-representation)

*   **核心思想**: 为了捕捉细粒度的 Query-Document 交互信息，研究者提出了 Token-level 或 Multi-representation 的编码器。这些方法旨在平衡**检索效果**（接近 Cross-Encoder）与**计算效率**（接近 Dual-Encoder/Sparse）。
*   **代表模型**:
    *   `ColBERT` (Khattab and Zaharia), `ColBERTv2` (Santhanam et al.)
    *   `ME-BERT` (Luan et al.)
    *   `COIL` (Gao et al.), `uniCOIL` (Lin and Ma)
    *   `MVR` (Zhang et al.)
*   Multi-Representation 的价值
    * UniDex Paper 举了一个生动的例子：
      * For instance, consider the query “apple”, which may refer to the fruit, the technology company, or even the record label. A document discussing global technology trends may contain sections on Google, Microsoft, and Apple. Even if only the “Apple Inc.” aspect overlaps with the query, the document should still be retrieved.

##### **ColBERT (Late Interaction)**

*   **论文**: [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832) (SIGIR 2020)
*   ![image-20260113193728089](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260113193728089.png)
*   **Late Interaction (迟交互)**: 区别于 Bi-Encoder (完全解耦，交互太少) 和 Cross-Encoder (完全交互，计算太慢)，ColBERT 提出了一种“折中”方案。
*   它独立编码 Query 和 Document（如 Bi-Encoder），但在最后计算相似度时，保留了 Query 和 Document 的 **Token-level Embeddings**，通过 **MaxSim** 操作进行细粒度的交互（如 Cross-Encoder 的效果）。
*   **模型架构**:
    *   **Query Encoder**: BERT 输出后接线性层 + Normalize，得到 $E_q = \{q_1, q_2, ..., q_m\}$。
    *   **Document Encoder**: 类似 Query Encoder，但过滤掉标点符号，得到 $E_d = \{d_1, d_2, ..., d_n\}$。
    *   **相似度计算 (MaxSim)**: 对 Query 中的每个 token embedding $q_i$，找到它在 Document 中相似度最高的 token embedding $d_j$，然后将这些最大相似度求和。
        $$ S_{q,d} = \sum_{i \in E_q} \max_{j \in E_d} (q_i \cdot d_j^T) $$
*   **优势**:
    *   **高效性 (Efficiency)**: 支持离线预计算 Document Embeddings（建立索引）。查询时只需计算 Query Embeddings 并进行 MaxSim 检索（可利用 FAISS 优化）。
    *   **有效性 (Effectiveness)**: 捕捉了细粒度的语义匹配信息，效果接近 Cross-Encoder。
*   **ColBERT v2 核心改进**:
    *   **Residual Compression (残差压缩)**: 解决 v1 存储成本过高的问题（每个 Token 一个 dense vector）。
        *   **原理**: 将向量表示为 `Centroid + Residual` 的形式。
            *   $$ v \approx C_{I_v} + \text{Quant}(r_v) $$
            *   其中 $C_{I_v}$ 是通过 K-means 聚类得到的中心点（Centroid），$r_v$ 是残差向量。
        *   **效果**: 残差部分被量化为极低比特（如 1-2 bits），使得每个 Token 的存储开销大幅降低，同时保留了高精度的语义信息。
    *   **Denoised Supervision (去噪监督)**:
        *   利用 Cross-Encoder 对难负例 (Hard Negatives) 进行蒸馏 (Distillation)。
        *   通过筛选高质量的负例和正例，提升训练数据的信噪比，从而在更小的模型规模下达到更好的效果。

###### **工程实践: Multi-Stage & Multi-Vector Querying (Qdrant Implementation)**

> 参考: [Advanced Multi-Stage, Multi-Vector Querying Using the ColBERT Approach in Qdrant](https://medium.com/@learn-simplified/advanced-multi-stage-multi-vector-querying-using-the-colbert-approach-in-qdrant-1742f6016391)

*   **Multi-Stage Retrieval Paradigm (多阶段检索范式)**
    *   **Philosophy**: 类似于图书馆找书，先看分区（Initial Filtering），再看具体书名（Refined Scoring）。
    *   **Stage 1: Initial Filtering (Coarse)**
        *   使用计算成本低、检索速度快的表示（如 Document-level Dense Vector 或 Quantized/Binary Vector）。
        *   **目标**: 快速从海量库中召回 Top-K (e.g., 1000) 候选集。
    *   **Stage 2: Refined Scoring (Fine)**
        *   对 Top-K 候选集使用 **Multi-Vector (ColBERT)** 进行 **Late Interaction** 重排序。
        *   **目标**: 精准计算语义匹配度，获取最终 Top-N。

*   **Multi-Vector Implementation in Vector DB**
    *   **Data Structure**:
        *   不同于传统单向量检索，ColBERT 模式下，一个文档被存储为 **一组向量 (List of Vectors)** (Matrix)。
        *   例如：描述一个人不仅仅用“名字”向量，而是用 [身高, 发色, 瞳色...] 多组向量共同描述。
    *   **Late Interaction Support (MaxSim Operator)**:
        *   现代向量数据库（如 Qdrant）原生支持 `MaxSim` 算子。
        *   查询时，计算 Query Vectors 与 Document Vectors (Matrix) 的交互：$\sum_{i \in Q} \max_{j \in D} (q_i \cdot d_j)$。
        *   这允许在数据库层面高效完成 Late Interaction，无需将所有向量取回应用层计算。

###### **End-to-End ColBERT: 工程挑战与解决方案 (PLAID & MUVERA)**

*   **核心矛盾**: ColBERT 的 Multi-Vector 机制导致索引体积爆炸（是单向量的 50-100 倍）且检索计算量巨大。若直接在全库（如 10M+ 文档）上跑 MaxSim，延迟和内存都无法接受。
*   **PLAID (Performance-optimized Late Interaction Driver)**: ColBERT 团队推出的工程优化方案。
    *   **核心机制: Centroid Pruning (质心剪枝)**
        1.  **聚类索引**: 离线使用 K-Means 将所有文档 Token 聚类成 $K$ 个簇 (Centroids)，构建倒排索引。
        2.  **动态剪枝**: 检索时，Query Token 只需计算与 Centroids 的相似度。根据分数分布，**动态选择少量高分 Centroids**，忽略绝大多数无关簇。
        3.  **效果**: 极大地缩减了搜索空间，只需扫描极少量的倒排链即可召回候选，实现毫秒级延迟。
*   **MUVERA (Multi-Vector Retrieval Accelerator)**:
    *   利用硬件指令集（如 AVX-512）或 GPU 并行原语，加速 MaxSim 这种非标准的向量算子。
*   **Residual Quantization (残差量化)**: 结合 PQ，将 32-bit 浮点向量压缩为 1-2 bits，显著降低内存占用并利用 SIMD 加速计算。

##### **ColPali (Multimodal Late Interaction)**

*   **论文**: [ColPali: Efficient Document Retrieval with Vision Language Models](https://arxiv.org/abs/2407.01449)
*   **核心思想**: 将 ColBERT 的 Late Interaction 机制扩展到多模态领域（Visual-Language Models, VLMs），实现高效的文档检索。
*   **架构**: 基于 PaliGemma（SigLIP Vision Encoder + Gemma LLM）。
    *   **Document**: 视为图像（Image），通过 Vision Encoder 切分为 Patch Embeddings，直接作为多向量表示（无需 OCR）。
    *   **Query**: 文本（Text），通过 LLM 生成 Token Embeddings。
    *   **Interaction**: 使用 Late Interaction (MaxSim) 计算 Query Tokens 和 Image Patches 之间的相似度。
*   **优势**:
    *   **无需 OCR**: 直接处理复杂排版、图表、公式，避免了 OCR 错误级联。
    *   **Fine-grained**: 利用 Patch-level 的细粒度交互，精准定位文档中的视觉元素（如特定的图表或段落）。

#### 其他搜索相关技术

*   **统计检索策略 (Statistical Retrieval Strategies)**
    *   作为补充策略，与模型检索结合使用。
    *   **Item-to-item Retrieval**: 利用物品间的共现或相似性 (He et al.)。
    *   **Collaborative Filtering (CF)**: 利用 User-Content 关联扩展候选集 (Shi et al.; Fkih)。
    *   **优缺点**: 能有效捕捉行为相关性 (Behavioral Relevance)，但存在冷启动问题和马太效应 (Matthew Effect)。
    *   **定位**: 通常不单独使用，而是作为辅助路融入更健壮的检索框架。

* beam search 
  * https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24
  * Beam search is an algorithm used in many NLP and speech recognition models as a final decision making layer to choose the best output given target variables like maximum probability or next output character.   
  * 核心在于LM generate output的地方
  * Greedy Search: A Naïve Approach
  * Beam Search: **Using Conditional Probability** ,多次计算Decoder
    * The algorithm can take any number of N best alternatives through a hyperparameter know as Beam width

![image-20241005231947598](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/translation.png)

#### UniDex：利用 Multi Vector + SID 优化 Inverted Index

> **关联思考**: UniDex 的架构完美体现了 **Multi-Stage Retrieval** 的思想：
>
> *   **Stage 1 (Filtering)**: 使用 **UniTouch** 生成的 Semantic IDs (SIDs) 构建倒排索引，进行快速候选召回。
> *   **Stage 2 (Refining)**: 使用 **UniRank** (Dense Vectors) 进行细粒度的 Late Interaction 排序。
> 这种设计在保证检索效率（倒排索引）的同时，通过多向量和迟交互最大化了语义匹配的精度。

> https://arxiv.org/html/2509.24632v1
>
> UniTorch 是端到端的建模，从 embedding 到 SID，用的 cotrain 的方式，引入协同信号

* **解决的问题**：
  * 传统倒排索引 term-centric paradigm 的局限性：inherently emphasizes surface-level lexical overlap, thereby limiting generalization and hindering retrieval effectiveness (Dai and Callan, [2019](https://arxiv.org/html/2509.24632v1#bib.bib7))
    * ![image-20260112124339637](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260112124339637.png)
  * 提供了一种 emb 和 量化 module 进行 cotrain 的方式
  * 利用 multi-vector 的价值增强搜索：ensuring broad semantic coverage and robust recall performance

* **核心设计**:
  * ![image-20260112134508023](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260112134508023.png)
  * **UniTouch**:
    * 目标：将 queries 和 documents 映射为 Semantic IDs (SIDs) 以改进检索。
    * **Multi-Vector Encoding**:
      * Append $M$ learnable CLS Tokens to input.
      * 文档语义更丰富，故设定 $N > M$ (Doc tokens > Query tokens)。
      * Query/Doc encoders 共享参数以保证语义对齐。
    * **Quantization Module (量化模块)**:
      * **Down-projection**: 将 query embedding $Q_i \in \mathbb{R}^d$ 投影到低维空间 $Q'_i \in \mathbb{R}^{d_q}$ ($d_q \ll d$).
      * **Finite Scalar Quantization (FSQ)**: 离散化 $Q'_i$ 的每个维度到 $K$ 个 bins，生成 Semantic IDs。
        * $S_i = \text{FSQ}(Q'_i) = \text{Round}((K-1)\sigma(Q'_i))$
        * 典型配置: $K=2$ (binary), $d_q=19$，生成 $[0, 2^{19}]$ 范围内的整数 ID。
      * **Up-projection**: 将离散码 $S_i$ 映射回原始维度 $\hat{Q}_i \in \mathbb{R}^d$ 用于下游任务。
    * **Optimization**:
      * FSQ
        * 使用 **Element-wise Gradient Scaling (EWGS)** (Lee et al., 2021) 替代 STE，通过自适应梯度缩放解决量化不可导和训练不稳定的问题。
        * 由于是 cotrain，配套了 **Quantization Regularization**，避免 embedding 数值趋于 0.5，难以量化
      * 对比学习：negatives include documents with lower relevance labels as well as documents from other queries within the same batch
      * 协同信号引入：Matching Loss，强制 SID 学习的一致性
        * ![image-20260113202312221](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260113202312221.png)
    * online retrieval：
      * build an inverted index where each entry corresponds to a particular SID
      * eash query generates M SID
  * **UniRank (Ranker)**: employs semantic matching (MaxSim) to rank results effectively.
    * **Modeling**: 独立的 End-to-End Ranking Model。
      * 不同于 UniTouch (Encoder) 需要 Co-train (Emb + Quantization) 来生成离散 SID，UniRank 直接在连续空间建模。
      * 它将 Query/Video 编码为 **4个 128维的 Dense Vectors** (High-dim Continuous Vectors)。
      * 并不直接复用 UniTouch 的 Embedding，而是有独立的 Encoder (或独立微调) 针对 Ranking 任务进行端到端建模。
    * Max Sim: 取 token 两两匹配的最大相似值。
    * list-wise contrastive objective

* 结论：
  * 效果大幅领先 sparse retrieval，逼近双塔召回
    * ![image-20260116203046957](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260116203046957.png)
  * 端到端效果成本
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260118015126753.png" alt="image-20260118015126753" style="zoom:50%;" />
    - 关于 UniDex v.s. Dense Retrieval，论文有讨论：Dense retrieval is more complex and demands greater computational resources. Additionally, it is more sensitive to increases in retrieval scale, such as the expansion of candidate sets, compared to inverted indexing recall.
    * **离线层面 (Recall Only)**: 单独考察 **UniTouch** (Recall)，由于量化带来的精度损失，其召回指标（HitRate）微负于全精度双塔（Dual Tower）。
    * **在线层面 (System Level)**: **UniDex 整体**（**UniTouch** 召回 + **UniRank** 排序）效果**优于**双塔系统。
      * **原因**: UniRank 作为配套的端到端排序模块，通过细粒度语义交互（MaxSim）弥补了召回阶段的微小损失；且得益于极低的资源消耗（Cost $\downarrow$），系统可以支持更大规模的候选集，从而实现整体业务指标（CTR/VPD）的反超。
  * **消融实验：Max-Max Loss 的重要性**
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260118012215608.png" alt="image-20260118012215608" style="zoom:50%;" />
    * **Max-Max (Global Max)**:
      * **作用对象**: UniTouch (Encoder) 训练阶段的 Loss 计算。
      * **逻辑**: $S_{\text{Max-Max}} = \max_{i \in Q, j \in D} \text{Sim}(q_i, d_j)$。
      * **与 ColBERT (Max-Sim) 的区别**:
        * **ColBERT (Max-Sim)**: $\sum_{i} \max_{j}$。追求 Query 的**每一个** Token 都有匹配，是 "Sum of Max"。适合 Ranking，精度高但对噪声敏感。
        * **UniDex (Max-Max)**: $\max_{i} \max_{j}$。追求**只要有一对** Token 匹配最强即可，是 "Global Max"。适合 Recall/Indexing 阶段，类似于布尔检索的 `OR` 逻辑（Hit 任意一个核心语义 ID 即召回），容忍度更高。
    * **Matching Loss**: 引入协同信号，强制 SID 学习的一致性。
  * SID count：内容越丰富（如document），增加SID count的收益越大
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260118015701101.png" alt="image-20260118015701101" style="zoom:50%;" />
    * 召回路重要性大、对冷启动增益大
* **Datasets.**
  * real-world search logs of the Kuaishou App
    * the training dataset comprises approximately 120 million user search sessions, which cover over 3 billion query-video pairs and span multiple stages of the online search pipeline, including recall, pre-ranking, and ranking. Each session contains up to 30 candidate videos, which are uniformly sampled across different stages and divided into multiple tiers to better capture varying degrees of relevance.
    * **Hard negative samples are sampled from the earlier stages of the search pipeline, while high-quality positive samples are constructed by integrating scores from the online fine-grained ranking model with explicit user feedback.**
    * Each query-video session is represented with rich features, including textual features of both queries and videos, video consumption data, and user feedback signals. For offline evaluation, we select 10 million videos from the video library to form the large-scale candidate pool and extract 50,000 user search sessions from the online system to construct the test set.


* **Details of the UniDex.**
  * The UniTouch encoder is implemented using a 24-layer BERT (Devlin et al., [2019](https://arxiv.org/html/2509.24632v1#bib.bib8)), initialized with internally pre-trained weights to leverage domain-specific knowledge. The model employs a hidden dimension of 1024, and training is conducted with a batch size of 32 using the Adam optimizer. We adopt an initial learning rate of 2×10−5, with a linear warm-up for the first 2,000 steps, followed by a cosine decay schedule.
  * Maximum sequence lengths are set to 32 for queries and 256 for videos, balancing computational efficiency with sufficient contextual coverage.
  * Semantic information is encoded through 19-dimensional 2-bit quantized vectors, with **3 SIDs generated per query and 8 SIDs per video**, enabling fine-grained semantic alignment between queries and video candidates. For the UniRank module, the settings of all training-related experimental hyperparameters are consistent with those adopted in UniTouch, and UniRank encodes the semantic information of both the query and the video into four 128-dimensional dense vectors each.



### 应用实例：Qdrant (Multivector & Late Interaction)

> Qdrant 原生支持多向量表示（Multivector Representations），通过 Late Interaction 模型（如 ColBERT）实现高精度的语义匹配。

*   **背景：Late Interaction 与 ColBERT**
    *   **Single Vector**: 传统方法将整个文档压缩为一个向量，容易丢失长文档的细节信息。
    *   **Multivector (Late Interaction)**: 
        *   每个文档表示为多个 Token-level 向量。
        *   查询时，Query 的每个 Token 向量与文档的所有 Token 向量计算相似度，取最大值 (**MaxSim**)，然后求和。
        *   这种方式保留了细粒度的语义交互，精度更高，但计算量大。

*   **Qdrant 的优化策略：Rescoring (重排序)**
    *   直接对海量多向量进行索引（HNSW）会带来巨大的内存开销（RAM Usage）和插入延迟。
    *   **解决方案**: **两阶段检索**
        1.  **Retrieval**: 使用 Dense Vector (单向量) + HNSW 进行快速初筛，召回 Top-K 候选。
        2.  **Reranking**: 使用 Multivector (ColBERT) 对候选集进行精确重排序。
    *   **配置技巧**:
        *   对 Dense Vector 开启 HNSW。
        *   对 Multivector **关闭 HNSW** (`hnsw_config=models.HnswConfigDiff(m=0)`)。仅存储向量用于重算分，不构建图索引。

*   **代码实现**
    *   使用 `fastembed` 生成 ColBERT 向量，`qdrant-client` 进行存储和检索。
    *   [qdrant_colbert_multivector.py](./snippets/qdrant_colbert_multivector.py)

### 应用实例：Elasticsearch

* Intro

  * https://www.elastic.co/guide/en/elasticsearch/reference/8.15/release-highlights.html

  * ES基于Lucene引擎，其核心是基于关键词的倒排索引，关键组件包括分词、倒排索引、相关性计算等。其中相关性计算通常采用TF-IDF和BM25等基于词频的算法

  * denormalization
    * inverted index
      * An index is a collection of documents.
      * index: <word -> [documents]>
      * document: <field -> [values]>
      * 用stopword处理


  * document oriented tool

* Usage

  * term关键字，不能用于text，要用于keywords类型

    * https://stackoverflow.com/questions/21933787/elasticsearch-not-returning-results-for-terms-query-against-string-property

  * phrase匹配

    * ```
      "query": {
          "multi_match": {
                  "query": "严有兵",
                  "fields": [
                      ...
                  ],
                  "operator": "OR",
                  "type": "phrase"
              }
        }
      ```

  * 前缀匹配 query_string

  * 分类汇总

    * HyperLogLog:  cardinality关键字

    * ```
      "aggs": {
          "count_unlisted_companies": {
            "cardinality": {
              "field": "company_id" // 假设这里有一个代表公司唯一标识的company_id字段
            }
          }
        }
      ```

    * ```
      "aggs": {
          "count_active_companies": {
            "value_count": {
              "field": "_id"
            }
          }
        }
      ```

### 排序

#### 干预排序

```
"relevance_score + exp((-timestamp_diff(last_update_timestamp)/86400)) + exp(-(1/(click_cnt+1)))"
```

#### 动态加降权 (Dynamic Boost)

在推荐、搜索、广告系统中，排序模型（如pCTR、pCVR模型）给出的分数决定了物品的自然排名。然而，纯粹的模型排序无法完全满足复杂的业务需求。因此，需要引入人工干预机制，其中“动态加降权”是核心手段之一。

**定义**: 一种根据**实时或近实时变化的条件**，对特定物品的排序分进行**强制调整**的机制。

*   **实现方式**: 通常在模型预估分的基础上，乘以一个权重因子或加上一个权重值。
    *   `Final_Score = Model_Score * Boost_Factor`
    *   `Final_Score = Model_Score + Boost_Value`
*   **“动态”的体现**: `Boost_Factor` 或 `Boost_Value` 并非固定，而是与外部条件挂钩，自动变化。

**常见动态条件与应用场景**:

1.  **实时热点**: 运营可对突发事件、节日等相关内容进行临时加权，抓住流量高峰。
2.  **库存水平**: 电商场景中，对高库存商品加权以清仓，对低库存商品降权以防超卖。
3.  **用户上下文**: 根据用户实时位置（LBS）、时间等信息，调整不同内容的权重。例如，饭点时间加权外卖内容。
4.  **商业化目标**: 为达成GMV或利润目标，可动态加权高毛利或重点合作品牌的商品。
5.  **实时效果反馈**: 系统可自动化监控，对实时CTR远超预期的“爆款”内容进行自动加权，反之则对效果差的内容进行降权。

**价值**: 动态加降权是连接算法与多变业务需求的桥梁，它为系统注入了灵活性和实时性，是实现精细化运营的关键工具。

#### 流量作弊与反作弊 (Anti-Spam)

除了通过策略进行主动干预，系统还需要识别并对抗各种试图操纵排序结果的作弊行为。

**关键词堆砌 (Keyword Stuffing) / 流量劫持 (Traffic Hijacking)**

*   **定义**: 一种常见的黑帽SEO策略，商家在商品标题中填充大量高热度但相关性不强的搜索词，以“蹭”流量，试图操纵搜索排名。
*   **例子**: 商品为《世事如烟》，但标题写成 `【“活着”作者余华作品世事如烟】`。这里就是利用更高知名度的“活着”为“世事如烟”引流，属于典型的流量劫持。
*   **影响**: 短期可能为商家带来流量，但长期会污染搜索结果，降低用户体验，破坏平台公平性。
*   **平台对策**: 反作弊系统会通过NLP技术识别此类行为，并采取**降权 (De-boosting)** 甚至下架商品的惩罚措施。


### Evaluation

* Recall & NDCG
  * ndcg影响用户价值
  * recall影响商家价值

* F-score: 精确率和召回率的调和平均数
* GSB：Good-Same-Bad

### Case Analysis

* 点击和相关性逆序比较大的case

### Case Examples

- “母亲节给妈妈买什么”
- “一个有趣的夜晚外出的衬衫”
- running shoes
- casual red Nike sneakers for summer
  - sparse: 'red', 'Nike', and 'sneakers'
  - dense: 'casual' and 'summer'
- recipes for a 6-year-old's birthday party
- recipes for a family dinner
- graduation garden party
  - floral sundress
  - wide-brim sunhat
- skin protection for outdoor concert
  - sunshine savior
- I need a stylish, comfortable dress for a summer wedding
- gear for a weekend camping trip
- eco-friendly sneakers
- what to wear to a beach party
- long floral dress with short sleeves.
- 菠萝
  - -X-> 马可菠萝火腿肠
- 生姜
  - -X-> 生姜洗发水
- 蒙牛牛奶
- 口条=猪舌
- 角瓜=茭瓜=西葫芦
- Redmi
- 烧烤（泛意图）
  - -> 烤羊肉串、烤羊腿
- 康师傅方便面
  - 品牌：康师傅
  - 商品：【袋装面、桶装面、泡面】
  - 类目：粮油调味
- 品牌搜索
  - 安慕希
- 基础单词
  - 苹果
  - 鱼
  - 水
    - -X-> 柔肤水
    - -X-> 丰水梨
- 同义词搜索
  - 圣女果
- 纠错
  - 平果、pingguo 和 pinguo

![9236e387-c3c1-4c80-b3a5-699c71e9299e](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/9236e387-c3c1-4c80-b3a5-699c71e9299e-3938325-3938327.png)

##### 对话式

- a camping backpack for an upcoming trip
  - the material that the backpack is made of, its volume, its straps, and its storage compartments.
- 更贵的xxx
  - 需要retrieve商品信息

### Ranking relevance in yahoo search

* Intro

  * ranking functions + semantic matching features + query rewriting
  * 挑战：1）query和item的空间不一致 (how much / price)；2）行为稀疏性；3）语义query
    * Developing semantic matching features including click simi-
      larity, deep semantic matching, and translated text matching.
  * 其它因素：时效性、地理位置
    * recency sensitive relevance and location sensitive relevance

* overview

  * 分词，求term检索的交集，uniquing（such as a limit on the num-
    ber of documents from a single host or domain），Core Ranking Function

* 特征

  * WebGraph（PageRank）
  * Document statistics
  * Document classiﬁer
  * Query Features：number of terms, frequency of the query and of its terms,
    click-through rate of the query
  * Text Match：computed from different sections of the document (ti-
    tle, body, abstract, keywords) as well as from the **anchor text（锚文本）** and
    the URL.
    * There are also proximity features which try to quantify how far in the document are the query
      terms (the closer the better) [23].
    * title is the most informative and representative content about the document,
      but in a deep understanding model, **domain** provides another im-
      portant signal for speciﬁc queries, e.g., wiki, weather, imdb, etc.
  * Topical matching:
  * Click
  * Time

* 算法

  * a uniﬁed method for web search which is **based on logistic loss and incorporates the Perfect, Excellent and Good information into the model though scaling the gradient for**
    **GBDT**
    * 加权pseudo-response
  * Contextual Reranking
    * 动机：所有候选的平均相似度
    * (1) Rank: sorting URLs by the feature value in ascending order
      to get the ranks of speciﬁc URLs. (2) Mean: calculating the mean
      of the feature values of top 30 URLs. (3) Variance: calculating
      the variance of the feature values of top 30 URLs. (4) Normalized
      feature: normalizing the feature by using mean and standard devia-
      tion. (5) Topic model feature: aggregating the topical distributions
      of 30 URLs to create a query topic model vector, and calculating
      similarity with each individual result.
    * In practice, we found that it is **more**
      **robust to use the ranks of the results** as a feature instead of directly
      using the core-ranking function’s scores

* semantic matching feature

  * 动机：The documents relevant to tail queries are often lacking anchor text

  * three novel features: **click similarity, translated text matching, deep semantic matching,**

  * **click similarity：** **vector propagation algorithm**
    * bipartite click graph构建
    * 根据coclick关系，不断迭代获取 QV^n, DV^n
    * trim top-K terms可以加速收敛
    * ![image-20250120030945033](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250120030945033.png)
    * ![image-20250120032806492](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250120032806492.png)

    * ![image-20250331173314663](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250331173314663.png)

    * 《Learning Query and Document Relevance from a Web-scale Click
      Graph. In SIGIR ’16.》

  * “Translated Text Matching” (TTM).
    * ![image-20250120033451861](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250120033451861.png)

* Query Rewriting: 视作机器翻译问题
  * e.g. how much tesla
  * a typical phrase-based machine translation framework to
    learn phrase-level translations
    * word alignment, phrase extraction and phrase scoring
    * Note that titles are usually longer than queries;
      hence we need to adjust the null-word alignment probability to rel-
      ative large values (say 0.9) to ﬁlter out noisy alignments.
  * adopt widely used feature functions in traditional statistical machine translation systems
    * including translation scores provided by the learning phase, lan-
      guage model of original queries, word penalty, phrase penalty and
      distortion.
    * We also develop feature functions speciﬁc to the QRW
      problem that aim to improve search relevance performance.
    * （重要特征）**Pair feature functions: h11-Jaccard similarity of URLs shared by q and qc in the query-URL graph, h12-difference between the**
      **frequencies of the original query q and the rewrite candidate qc,**
      **h13-word-level cosine similarity between q and qc**
* RECENCY-SENSITIVE RANKING
  * recency-demoted relevance
  * ![image-20250131213609150](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250131213609150.png)
  * ![image-20250131213814737](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250131213814737.png)
* LOCATION-SENSITIVE RANKING
  * Queries with speciﬁc location names
    (e.g., “restaurants Boston”) are referred to as **explicit local queries**,
    and queries without locations but with location-sensitive intention
    (e.g., “restaurants”) are referred to as **implicit local queries**.
  * location boosting ranking model
    * ![image-20250131215532604](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250131215532604.png)
    * ![image-20250131215654272](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250131215654272.png)
  * 特征：
    * query：提取explicit、默认user implicit
    * url：click graph；提取explicit
* 工程
  * 根据url md5分shard server
  * export features from the index serving nodes，给重排使用
    * 30-50个primitive feature
  * coclick feature：一个月重建一次
  * TTM基于cache，未命中的用pruning model

* 评估
  * human labeling (e.g., professional editor’s judgment) and user be-
    havioral metrics (e.g., click-through rate, query reformulation rate,
    dwell time).
  * DCG
    * employ the Wilcoxon T-test to report p-value
  * 按query频次分类：top, torso and tail.



## 电商搜索 / 电商搜索广告

### Intro

> 电商搜索全链路（PART I）Overviewhttps://mp.weixin.qq.com/s/8-JtKugK-zf9In2ZdI3lrg

![图片](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/640)

* 电商搜索和网页搜索的区别
  * 亿级 vs 万亿级
  * 数据结构化 vs 非结构化
  * 相关性时效性+CTR/GMV
* 阿里KDD'21的论文：《Embedding-based Product Retrieval in Taobao Search》，经典架构

![图片](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/640-20241010191312101)



* 总结：
  - 演进路线：传统lexical matching -> 深度模型 -> NLP-based检索 -> 个性化模型
  - **NLP相关性模型/策略的能力**是发展重点
  - 常见的精排模型结构仍是双塔模型（算法效果依赖 query*doc cross feature）
  - LLM兴起后，可能 **低成本打平甚至超过** 以往基于NLP技术的算法迭代

#### 业务理解

* 商品搜索中的用户强意图场景，召回率/MAP/NDCG指标要求高，因为不能让一些商品永远没有曝光的机会 —— 第四范式
* 商品搜索对个性化的要求高于网页/视频/文字搜索，比如搜索的时候，不同的人消费能力的不同，那么排序时，需要考虑用户的消费能力，返回合适价格的商品
* 电商场景出于商家生态考虑，商品覆盖率要求高



### Query Rewrite

> 先参考「AI-Applied-Algorithms」

* 改写是搜索广告场景极具业务特色的技术模块，用户输入搜索词 Query 表达搜索需求，广告主通过广告平台设置和自己产品有关的竞买词 Bidword 表达想要触达的流量，改写的目标是对 Query 和 Bidword 做高效匹配。匹配效率体现在两方面，分别是相关性和流量价值，前者是基础，后者是在前者的基础上挑选流量变现价值更高的 Bidword。
* 核心挑战主要有两个：1）精准理解 Query 背后的真实购物需求，尤其是手机文本输入成本高，用户和广告主的表达习惯千差万别，Query 和 Bidword 之间的语义鸿沟对于相关性挑战很大；2）相关性和高价值的平衡。
* 经典方案需要有两类模型相配合，深度语义模型解决相关性问题，基于协同过滤的深度价值模型解决流量价值问题。该方案有两方面问题，一方面存在老生常谈的问题即对长尾 Query 理解和改写能力不足，另一方面两段式目标融合往往会顾此失彼。

#### 阿里最新

* 大模型 LLM 的出现可以极大改善前者长尾流量上的相关性问题，LLM 蕴含的世界知识对于文本理解和推理能力非常强大，我们在 2023 年初就开始推进 LLM 在改写方向的落地，探索生成式改写的提效潜力。电商广告领域知识的 SFT 和在线动态 RAG 是迭代初期的常规优化手段，效果不错。生成式改写也是 LLM 在搜索广告业务中第一个上线项目。
* 但是简单将 LLM 适配成改写任务仍然会存在两个问题，一个是 LLM 的生成结果无法保证一定是在竞买词库中，导致生成结果不可用；另一个是生成结果虽然能够极大保证相关性但是无法提供流量价值的判断。所以系统往往需要有一个第二段改写的模块，给上述两个问题兜底或者改善。为了进一步优化改写效果，我们提出基于带权 Trie 树的 LLM 生成式改写技术（VALUE）。一方面通过将全库竞买词构建成 Trie 树，使得 LLM 生成过程在 Trie 树约束搜索下进行，确保生成结果一定是在竞买词库中；另一方面离线环节构建高低价值的反馈判断（哪个词的变现效率更高）进行 DPO 训练，在线环节将 Trie 树升级为带权（权重即变现效率的层层汇聚）且实时更新的模式，两相结合使得一段式生成过程兼顾了高价值判定。如上，基于 LLM 的生成式改写方向，两年时间总共上线 4 期，提效显著。

![image-20251025155928937](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251025155928937.png)

#### Large Language Model based Long-tail Query Rewriting in Taobao Search

* BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries
  * multi-instruction supervised fine tuning (SFT)
    * based on rejection sampling and auxiliary tasks mixing to fine-tune LLM
  * offline feedback
  * objective alignment.
  * beam search to generate multiple candidate rewrites

* 现有查询改写方法的局限
  - 基于嵌入的检索范式结果难解释
  - “查询改写 & 精确匹配” 范式中判别式方法难以控制语义范围和确保相关性
  - 生成式方法受限于模型规模对长尾查询理解不足，基于大语言模型的改写方法缺乏微调与目标对齐

![image-20241117235808683](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241117235808683-1379317.png)

* 多指令 SFT
  - 收集改写相关任务数据微调大语言模型，包括:
  - 构建查询改写数据集（经两轮拒绝采样提升质量并结合辅助任务数据）
  - 利用辅助任务数据集（质量分类、产品标题预测、思维链任务）增强模型对长尾查询的理解

* Evaluation: 利用taobao rele score function，定义hit rate

### 相关性和个性化排序

#### 召回

* 电商搜索有别于传统的文本搜索，Query、User 和 Item 是异构实体且有不同模态，向量化检索模式有天然的优化瓶颈
* ES

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/d350bbd78199bc8b214e81bc6a387820.png)

##### 经典双塔

> Embeddingbased retrieval in facebook search

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211013645109.png" alt="image-20251211013645109" style="zoom:50%;" />



#### 粗排

* 当召回数量较大时，粗排截断
* 相关性、时间、热度、销量、好评数和收藏数等等特征，训练出简单的模型，做一些粗排的排序，进行截断
  * 找出核心的特征，做加权平均也可以。

#### 排序：相关性模型

##### Intro

> [搜推广生死判官：重排技术发展](https://mp.weixin.qq.com/s/oSxtpVuoTFQGsVxbZelGQg)

* 相比于向量化检索，为什么需要rerank
  * 一方面是基于相似度量的索引构建与检索模型相分离会导致优化目标不统一，另一方面基于性能考虑实体间的计算只能局限在简单的线性计算模式。
* 相关性模型作为 NLP 技术在搜索广告中的核心应用，主要用于判断用户搜索需求（Query）的文本表达与商品展示的图文信息是否匹配。该技术体系包括实体识别模型、关键属性识别模型，以及贯穿召回与排序各阶段的相关性判别模型等多个关键模块
* 技术发展路线主要经历过两个阶段：
  * 挖掘行为数据作为弱标签，借助图学习和表征学习的能力做数据层面 Scale up；
  * 借鉴 BERT 系列的文本类多任务预训练 + 下游任务微调的范式，进行模型层面 Scale up。

* 随着自回归模式的 GPT 架构兴起，模型的进一步规模化还能涌现出逻辑推理能力，而这正是相关性模型可以代际性进阶的突破机会。逻辑推理和可解释性对于相关性任务判定很重要，一方面我们实践论证通过思维链 CoT 慢推理的任务设计可以显著提升判定结果的准确性，另一方面推理的过程信息对于模型的再一次迭代以及业务应用都有助益。

##### 阿里 ELLM-rele：基于思维链模式的聚焦逻辑推理的相关性大模型

> https://arxiv.org/abs/2411.13045

* 我们研发了基于思维链模式的聚焦逻辑推理的相关性大模型，并且升级了智能化标注系统，设计机器标注和人工校验的协同机制，彻底改变标注数据稀疏且昂贵的窘境。同时，考虑到相关性大模型无法在线毫秒级实时响应，我们设计一系列电商业务特色的细粒度蒸馏手段包括数据蒸馏、隐层蒸馏和过程蒸馏等，大幅提升在线传统相关性模型的预估能力。如上，基于 LLM 的相关性模型全面落地，配合 Case 驱动方法论践行，今年在相关性体验上做的提效收益高于过去三年之和。

![image-20251025161157712](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251025161157712.png)

##### 模型微调

[“阿里灵杰”问天引擎电商搜索算法赛--季军经验分享](https://mp.weixin.qq.com/s/sfFZpttGqf_f4sBFpQIiyg)

* ![image-20241010195711640](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010195711640.png)

* ![image-20241010195813065](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010195813065.png)

* 领域数据后训练
  * 动机：大量corpus未曝光，未经过训练
  * 预训练模型：Chinese-roberta-wwm-ext, Whole World MLM
* 召回任务微调
  * 数据构造：
    * 正样本：标注document
    * 负样本：
      * in-batch-negative: batch内其它query的标注document
      * random-negative：随机在corpus采样非标注document
        * 动机：如果仅使用in-batch-negative，会影响非标注document的学习过程

![image-20241010202306817](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202306817.png)

* 精排任务微调
  * 均值变换：相似程度较高的召回，区分度增强，增加梯度

![image-20241010202519864](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202519864.png)

![image-20241010202709678](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202709678.png)

![image-20241010202740700](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202740700.png)

##### 各种Tricks

* **数据增强**

  - **训练bart生成模型，使用document生成query构造伪标签**（还可以生成时随机替换一些阿拉伯数字、拼音），同时使用一定规则修正生成的伪标签query

  - 反向翻译（不确定是否有效）

  - **把query中长度小于4的拿出来，加入词表构建新词**

  - 把document分词后随机抽几个拼接作为新doc（不确定是否有效）

* **召回模型：**
  - 降维时可以使用无监督PCA作为降维矩阵初始化

  - 训练交互式精排模型去蒸馏非交互式召回模型

  - 难负例构造：使用训练好的召回模型+faiss召回，取相似度中等水平（30左右）的document作为难负例

  - infoNCE loss的温度：可以首先设定一个比较小的，然后慢慢变大，但不能太大

  - 集成使用提交的线上得分作为系数加权求和embedding（不确定是否有效）

* **精排模型：**
  - **可以使用ES引擎先召回query对应top-k doc，然后经过规则过滤（字面相似度高，向量余弦相似度不低）出过滤doc，然后把query和这些doc的向量表示按照一定规则相加，作为新query向量表示**

* **召回模型融合：**
  - **不能直接取均值，而是使用stacking的方法，把多个模型的输出向量使用一个训练后的FFN网络融合。**

#### 排序：个性化模型

* LR、GBDT

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/690ce3a3bbc3f407bc453ebc35ceb18b.png)

### 阿里妈妈搜索广告

> https://mp.weixin.qq.com/s/hgs_BzFZdjrDSbf9d74ilA

![image-20251025155047495](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251025155047495.png)

### 美团搜索广告

> https://www.bilibili.com/video/BV1gM4m1r7DQ
>
> https://tech.meituan.com/2024/07/05/the-practice-of-search-advertising-recall-technology-in-meituan.html
>
> 从架构演进的角度讲解：
>
> 重点：关键词挖掘技术、用户个性化信息和语义个性化信息分别学习、搜索推荐化解决泛意图弱供给

* 业务特点
  * 搜商品（80%+） + 搜商家 + 猜你喜欢
  * 百万级商家、十亿级别商品
  * 中小商家多，内容质量不高
  * LBS属性，供给不充分，对召回率要求更高

![image-20241004205944993](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/meituan0.png)

![img](https://p1.meituan.net/travelcube/d1e0aed8bb38220792a3337d9ac211e8728900.png)

![img](https://p0.meituan.net/travelcube/68f8473fef2b195795238fda49311e4d767762.png)

![img](https://p1.meituan.net/travelcube/30482573c6a09cb8e3384db6dc660a0e829404.png)

* 阶段一：多策略关键词挖掘
  * SPU通过离线方式，挖掘核心关键词，在线与Query精确匹配
  * **特点**：
    * 只聚焦于通过离线方式覆盖高频流量；
    * 缺乏线上的行为数据，以NLP的挖词技术为主；
    * 为了追求更多的覆盖，采用了多策略并行的方式，不断叠加新的召回策略，以达到更高的流量覆盖
  * 一由于Query很短，我们很容易通过信息抽取，把词或实体核心信息抽取出来；二是因为头部效应比较明显，Top2万的Query覆盖了很多流量，采用这种离线方式能快速拿到大部分收益；三是由于商家没有买词能力，如果用Query直接匹配商品，会涉及到传导文本匹配问题，匹配难度会更高，所以我们最后采用模型从商家商品里挖掘核心词，在线做短串匹配的方式。
  * 第一版：更多采用基于规则的挖掘式策略，把流量分成了商家词、商品词和品类词。商品词通过分词和词频贡献的算法，挖掘核心关键词，由于品类字面没有完全匹配的信息，我们通过互信息，构建词之间的权重去挖掘。但问题一是规则能力较弱；第二是只能挖掘出连续的短词，比如“炒西红柿鸡蛋”，它只能挖掘出“炒西红柿”，挖掘不出“炒鸡蛋”。
  * 第二版：抽取式模型
    * 序列标注模型：只能挖掘出连续短串，好处是挖掘效率比基于规则的挖掘模式高，但会导致很多关键词受限于连续短串的方式而挖掘不出来
    * 标注组合模型和指针组合模型：标注组合模型能够跨越连续短串挖掘，但它有一个顺序概念在里面；指针组合模型可以在原有短串里随机组合词，突破顺序和连续的局限。但抽取式模型的准确率较高，探索空间不足
  * 第三版：生成式模型
    * 深度分类模型：将SPU商品文本直接分类到这2万个Query标签里，做词和Query间的匹配，但这种多分类模型较难优化，也不能泛化出更多的Query，时效性和更新频率也有限
    * 深度生成模型：受限于模型规模和样本丰富度，准确性不太好，所以我们在后面加了标注和生成模型，在具备生成泛化性的同时，尽量控制Query质量

![img](https://p1.meituan.net/travelcube/ce161f376ffa89b2baed47bc8e7c4765967044.png)

* 阶段二：分层召回体系
  * 特点：
    * 在一个业务范畴内，通过把技术做深能够取得业务效果的极大提升；
    * 随着基建能力的提升，更多的是把召回由离线切换成在线，以此覆盖更多的流量；
    * 在单通路的召回能力上，我们突破了传统单一NLP技术瓶颈，开始大规模使用个性化/图/多模态等新的召回技术。在2022年底，整个分层召回体系取得了不少成效。
  * 第一是强意图有供给，通过关键词就能较好满足，因此在这个象限里，我们更多是在迭代关键词召回技术。
    * 一是通过离线统一到生成式的方式。前面介绍离线关键词挖掘策略可能会有十几个通道，不管迭代哪个通道，策略召回的覆盖面都是有限的，而且团队也没那么多人迭代，但这种情况下，我们把整个离线关键词十多路的挖掘策略通过规模较大的生成式模型做了统一，引入了多模态信息，做到了数据更多、模型更多以及召回目标更多的情况，后期只需要通过优化模型能力，就能取得线上全流量覆盖的效果；
    * 二是通过离线关键词的方式做到了在线。我们并没有采用业界传统的布尔检索，这种方式有两个局限，一是Query改写以及商品分词基于较浅层的模型，整体效果会受限于模型效果。二是它没有做到检索和最终目标的匹配。
      * 在线稀疏化检索方式类似于双塔向量检索，但每个模型出来不是一个稠密的向量，而是一个几万维稀疏的term粒度，通过端到端的建模方式，把Query和商品映射到一个稀疏的几万维槽位的几个槽位里，离线训练时通过槽位端到端的建模，实现目标检索和目标一致性，在线检索时，基于槽位构建倒排检索，具备一定的可解释性。
  * 第二个是泛意图有供给，体现了用户的个性化偏好，通过迭代向量召回模型覆盖这个场景。向量召回经过了三版迭代。
    * 第一版是基于传统语义相关性约束的双塔模型，和业界的做法类似；
    * 第二版将用户个性化提上了日程，但如果只把用户个性化特征和传统语义特征融合在一起，**黑盒式学习很容易被用户个性化信息带偏**，最后我们让**用户个性化信息和语义个性化信息分别学习**，通过显式叠加的方式做端到端的建模。这种检索方式能够兼顾个性化和语义相关性信息；
    * 第三版是基于平台的多样化目标，我们需要对齐后链路的精排目标，在召回阶段考虑整体商业价值。
  * 第三个是泛需求弱供给，比如搜索“汉堡王”，但给TA一个“肯德基”，TA也会下单，通过**搜索推荐化**的方式覆盖和解决。
    * 这个场景比较复杂，从业务来看，它需要做引导和推荐，在结果页里也做偏泛结果的推荐，涉及到搜索前和搜索中，搜索中既有商家也有菜品，既涉及要推荐什么样的菜品，也涉及推荐什么样的商家；
    * 另外推荐本身是一个关系建模。我们最后选择基于图模型的迭代，因为图模型首先是一个基于关系的建模，而且图模型具备多场景海量信息的容纳能力，在图建模里，一是构建了异构的多节点百亿规模图，通过图预训练加微调的方式识别多个场景，我们最近也在尝试做图和大模型训练相结合的方式；
    * 二是我们把整个图检索搬到在线，因为在搜索场景中，用户需求是即时需求，属性较强，只有把检索搬到在线，通过图在线的实时检索聚合到用户当前最有可能的潜在兴趣情况下，才能实现收益最大化。
  * 第四个是没有供给的场景，通过流量结合供给运营化的方式解决。

![img](https://p0.meituan.net/travelcube/cb8c69f866c07b7bbe28f99acbc845f7640525.png)

* 阶段三：生成式召回
  * 核心思路是按照流量和供给特点分类，强意图是直接搜索一个商品；泛意图比如搜索“烧烤”这个品类，泛意图用户虽然表达了需求，但满足需求的候选可以很广，甚至可以替代；供给层面分为有供给、弱供给和没有供给三个象限
  * 核心思路是结合大模型或生成式技术思想，提高召回算法的决策空间，提升模型的匹配能力。经过一段时间迭代，我们抽象出广告子模块结合LLM落地的三类思想及方式，分别是用思想、学能力、用LLM。具体和子模块结合的一些探索如下：
    * 一是离线关键词召回方向。如刚才介绍，我们已经把整个离线关键词召回技术方式统一到了规模不错的生成式模型方式上。大模型出来后，直接用大模型其实还存在着算力及效果的2个挑战。但我们认为大模型的两个核心技术思想：**Cot（Chain-of-thought，能使大型语言模型能够更好地理解人类的语言请求）推理和RLHF（Reinforcement Learning from Human Feedback，一种基于人类偏好的强化学习方法）对齐人类反馈思想**，对我们现有模型的优化也是有帮助的，因此我们使用大模型的这些技术思想来改造离线生成式召回模型。
    * 二是在向量召回方向。我们已经将向量表征升级为多模态模型，进一步我们思考，**LLM语言大模型对于离散Token的信息归纳及表征是有比较大的提升的**，但是在稠密表征领域，一个值得借鉴的方法是扩散模型，因为扩散模型也是通过多步去噪的方式来生成目标，通过扩散多步过程，在其中引入多元信息多步融合的思路，提升整个向量召回的向量表征能力。
    * 三是随着我们探索的深入及对应算法能力的提升，我们构建了美团领域广告大模型，尝试直接把大模型用到美团实际场景里做关键词召回，将离线中等规模的生成式模型直接替换成大模型，并探索大模型在线化。
    * 第四个是蒸馏大模型能力，主要在相关性场景落地，目前蒸馏了两块能力，Cot推理能力和模型隐层知识能力蒸馏
  * 生成式关键词召回
    * 生成式召回主要借鉴大模型思想，我们已经升级为统一的生成式模型，它的工作方式是基于beamsearch的方式，一次生成多个结果，但结果之间是互相看不到的，我们认为这种方式会存在问题，另外，从线上和实际生成结果来看，词之间是有关系的，按照概率方式来看，如果一个关键词能够推理出另一个关键词，大概率前面这个关键词要比下一个关键词的信息含量多，那能否借鉴大模型推理思想，**按照序列生成方式逐步推理出多个关键词**。
    * 我们通过构建概率贡献图的方式，采样得到关键词之间的导出关系，在一次生成时，直接生成多个关键词，这多个关键词之间有推理关系，比如要给“花仙女鲜花店”商家生成关键词，第一个关键词就是相对具象的“鲜花店”，它的含义和商家的商品描述是确定的，在生成“鲜花店”时，可以推理成“花店”，进一步可能会生成新关键词，通过这种序列推理方式，能够很好地利用关键词之间的关系。
    * 在序列推理生成关键词时，比如生成了5个关键词，有一个关键词不相关，剩下的4个关键词是相关的，那如何通过模型识别出这种不一致现象，能否借助人类反馈方式，实现模型序列好坏端到端的判断。模型生成的关键词序列与人工标注是否一致，通过这种反馈对齐的方式喂给模型，提升整个序列生成结果的一致
    * ![img](https://p0.meituan.net/travelcube/0a64745f26ec8939c7f4e17424273d161277430.png)
* 对于离线关键词，前面是中等规模的模型，我们最近把整个离线关键词替换成大模型，之前没有替换是因为开源通用大模型能力在领域场景里，挖掘词的准确性和通用性有限，我们一直在构建美团广告领域的大模型，通过激发大模型知识，生成更全面准确的模型，我们做了3个阶段的优化。
  * 第一是融合领域知识，比如健身和轻食相关，这是领域知识，通过领域全参数训练得到一个基础的广告领域模型。
  * 第二是融入场景知识，美团有很多店铺和商品名，比如川菜和眉州东坡在店铺里有很多相关数据。通过这种指令微调的方式学习店铺知识，在实际应用时，再学习偏实际的知识，比如搜索“猪手”时，发现他之前检索过很多“猪肘切片”，通过这种检索方式增强大模型当前推理知识能力。
  * 最后通过构建领域大模型和检索增强范式，在一些场景里替换传统大模型，这样，我们发现召回效率明显提升。
  * ![img](https://p0.meituan.net/travelcube/b9d1d0d7bcc6265f296c2ff425f5954a774735.png)

* **多模态生成式向量召回——结合扩散模型，多阶段生成向量表征**
  * 我们改造或优化多模态向量召回，在表征里结合扩散模型做了优化，如下图左边所示，传统的多模态向量召回更多是在item侧表征里，将商品图片和文本模态信息融合在一起，得到一个表征，那能否通过一些方式在Query侧也实现多模态表征。一个用户在美团场景里搜索一个Query时，大概率他的脑海里已经有关于这个Query所对应菜品图片的大致印象。那我们如何通过模型建模的方式还原图片的印象，核心在于还原用户的潜在意识。
    * 我们的做法是，一是把Query历史点击的图片信息汇集在一起，表征Query所代表的通用视觉信息；二是将用户历史点击图片代表用户个性化视觉信息，把这两类视觉信息叠加在一起，可以在一定程度上反映用户在当前搜索框架下，想要得到的流量侧多模态信息，最后通过多模态表征匹配技术，整个离线召回效率也有提升。

* 但这种方式也是基于传统的判别式表征，比如现在大家都在做个性化向量召回，相关性和个性化之间有递进关系，最浅层的需要保证相关性，第二层才需要在相关性里挑选更个性化、更符合用户偏好的候选集，给到下游链路。
  * 但传统的判别式方式一般在特征阶段叠加不同特征，通过建模、多目标落实反向迁移方式，不能很好的显式学习到不同目标间的递进关系，但SD生成模型比较适合这种稠密向量生成，通过多步还原过程，本质上也是一个不断推理的生成式过程。

* 我们希望向量表征具备不同信息的推理能力，SD的多步加噪去噪过程类似于推理过程，可以相结合，在不同步骤中引入不同维度的信息，做到多维信息的显式理解及融合。
  * 在正向编码过程中，先将item通过编码器编码成向量后，逐渐加噪还原成白噪声，在反向去噪还原过程中，在噪声里分阶段添加用户Query以及side info信息，通过多步还原的方式，还原出Query所代表的信息。并有两个对比的操作，一是传统的样本Paiwise学习，通过对比学习方式拉近Query与相似Item的表征；二是我们认为相似item有类似的标准过程，通过对比学习拉近相似item之间在扩散中间过程的表征，这是整个建模过程。
  * 在还原阶段，我们会显式还原中间步骤叠加相关性信息、个性化信息，通过对比方式让模型在还原过程中显式相关性和个性化信息，最后在模型结果里能看到，如下图左边是传统的判别式模型里最好的一个Baseline，它能够较好区分Query和正样本信息，但它在个性化样本和相关性样本里基本是混在一起的，通过这种扩散模型方式，相关性样本和个性化样本就有一定程度区分开来的能力。

![img](https://p0.meituan.net/travelcube/eb8c6c661c488af1801306944b08b8ff683001.png)

### todo [京东] Towards Personalized and Semantic Retrieval : An End-to-End Solution for E-commerce Search via Embedding Learning

> https://zhuanlan.zhihu.com/p/465504164

### [第四范式] 如何构建一个好的电商搜索引擎？

> https://www.infoq.cn/article/ixobeuyc5q0b1dmhrwh7

* 商业逻辑：
  * 搜索，是电商 app 非常重要的一个流量入口，可能很多电商 app 来自搜索的流量都会占过半以上。
  * 搜索行为背后是巨大的UV价值

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/5cb85359f486ff64c45d24790572daef.png)

## Sug (Search Query Suggestion)



## Two-sided marketplace 搜推

### Real-time Personalization using Embeddings for Search Ranking at Airbnb

> 解决转化稀疏的问题：user type & listing type

* 业务场景
  * Airbnb, a short-term rental marketplace, search and recom-
    mendation problems are quite unique, being a **two-sided market-**
    **place** in which one needs to optimize for host and guest preferences,
    in a world where a user rarely consumes the same item twice and
    one listing can accept only one guest for a certain set of dates
    * Example industries include accommodation (Airbnb), ride sharing (Uber, Lyft), online shops (Etsy),
  * 短租平台
  * 搜索个性化、相似推荐
    * query： location、dates、guests、map、filters
* 业务特征
  * given an **input query** with **location and trip dates** we need to rank high listings whose
    **location, price, style, reviews**, etc. are appealing to the guest and, at the same time, are a good match in terms of host preferences for **trip duration and lead days.**
  * **bad reviews, pets, length of stay, group size**
  * session特征：
    * **clicks, host contacts**
    * **skips of high ranked listings**
* 核心思路：
  * **训练泛化user type embedding，冷启动（由于旅客的行为过于稀疏，一年几次）**
    * 在同一个空间训练 listing type embeddings
  * **Adapting Training for Congregated Search** - Unlike in Web search, the search on travel platforms is often congregated, where users frequently search only within a certain market, e.g. Paris., and rarely across different markets. We
    adapted the embedding training algorithm to take this into account when doing negative sampling, which lead to capturing better within-market listings similarities.
  
* 参数/数据
  * 800 million search clicks sessions -> listing emb
  * 50 million users -> user/listing type emb

* listing embeddings的学习

  * session定义：
    * A new session is started whenever there is a time
      gap of more than 30 minutes between two consecutive user clicks
    * 详情页停留30秒才算数据点
    * 至少2个有效clicks
  * ![image-20250111131009803](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111131009803.png)
  * 思路：
    * 基于context window，对比学习，类似CBOW的思想
      * 共现的标准：滑动窗口内共现
    * Mn项：强调类内的负例，同一market下的随机负例

  * 冷启动：地理位置相似 + 类型一致 + 价格bucket一致，3个取平均

* User-type & Listing-type Embeddings

  * 动机：book（预订）行为相比点击行为非常稀疏，session长度仅有1，学好某个item embedding至少需要5-10条样本
  * 两个目标交替训练，从而user-type和listing-type在一个空间
  * ![image-20250111132736294](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111132736294-6573257.png)

  * ![image-20250111132909510](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111132909510.png)

  * 双边市场：考虑Host rejections（listing type对user type的rejection）
    * 作为explicit negatives

* 模型，在基线gbdt&lr model的基础上，加入上面的embeddings

  * ![image-20250111135137098](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111135137098.png)
  * ![image-20250111135539502](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111135539502.png)
    * 加特征：规则+score

* 训练

  * 数据
    * session划分
    * we removed accidental and short clicks, i.e. clicks for which user stayed on the
      listing page for less than 30 seconds, and kept only sessions con-
      sisting of 2 or more clicks
    * oversampled booked sessions by 5x in the training data
  * Daily Training.
  * listing types
    * trained embeddings for 500K user
      types and 500K listing types using **50 million user booking sessions.**
      Embeddings were d = 32 dimensional and were trained using a
      **sliding window of m = 5** over booking sessions

* Evaluation

  * ![image-20250111134240296](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111134240296.png)

  * ![image-20250111140037550](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111140037550.png)
  * 离线 NDCU (Normalized Discounted Cumulative Utility)
    * rejections -0.4
    * Clicks 0.01
    * contacts 0.25
    * bookings 1

## 广告

### Intro

* 广告的预估CTR具有物理意义
  * calibration

### Practical Lessons from Predicting Clicks on Ads at Facebook, KDD 2014

> GBDT+LR、在线学习、Online Joiner、负样本降采样

**2.指标**

* Normalized Entropy: the average log loss per impression divided by what the average log loss per impression would be if a model predicted the background click through rate (CTR) for every impression. 
  * 用 background CTR 给 loss 做 normalize
* RIG (Relative Information Gain) = 1 - NE
* Calibration: the ratio of the average estimated CTR and empirical CTR
* AUC(Area-Under-ROC): 衡量排序， 忽略低估与高估

**超参**

* GBDT
  * 子树600；几天更新一次

**3.Prediction Model Structure**

* BOPR (Bayesian online learning scheme for probit regression): 假定高斯分布，在线学习分布的参数
  * Both SGD-based LR and BOPR described above are stream learners as they adapt to training data one by one.
  * BOPR 相比 SGD-based LR 的区别在于，梯度下降的 step-size 由 belief uncertainty $\sigma$ 控制，也是在线更新的参数3
* 3.1 Decision tree feature transforms
  * bin the feature
  * build tuple input features
    *  joint binning, using for example a k-d tree
    *  boosted decision trees
  * follow the Gradient Boosting Machine (GBM) [5], where the classic L2-TreeBoost algorithm is used
  * We can understand boosted decision tree based transformation as a supervised feature encoding that converts a real-valued vector into a compact binary-valued vector.

* 3.2 Data freshness
  * The boosted decision trees can be trained daily or every couple of days, but the linear classifier can be trained in near real-time by using some flavor of online learning.

* Experiment result for different learning rate schmeas for LR with SGD
  * NE: per weight > global > constant > per weight sqrt > per coordinate

* BOPR 与 LR 对比
  * LR's model size is half
  * BOPR provides a full predictive distribution over the probability of click. This can be used to compute percentiles of the predictive distribution, which can be used for explore/exploit learning schemes

**4.Online Data Joiner**

* length of waiting time window: 定义"no click"，需要 tune
  * 定义：曝光发生后，等待多久判定是否产生了点击
  * 过长会增加buffer、影响"recency"实时性
  * 过短会影响"click coverage" => the empirical CTR that is somewhat lower than the ground truth

* 数据结构：HashQueue 缓存曝光记录
  * consisting of a First-In-First-Out queue as a buffer window and a hash map for fast random access to label impressions.
  * operations: enqueue, dequeue, lookup

* Anomaly detection mechanisms
  * 监测到数据剧烈变化，断流训练器
  * e.g. 点击数据流由于action id的bug无法与曝光数据流正确join

**5.Containing Memory and Latency**

* number of boosting trees: 500个比较折中
* boosting feature importance
  * the cumulative loss reduction attributable to a feature
  * 对多个 trees 的 reduction 相加
* features
  * contextual features: local time of day, day of week, device, current page
  * historical features: ctr of the ad in lask week, avg ctr of the user
  * **historical features 明显比 contextual features 重要**
  * contextual features 更需要 data freshness

**6.Coping with Massive Training Data**

* Uniform subsampling: sample rate 10% 比较合适

* Negative down sampling: sample rate 2.5% 效果反而更好

* Model Re-Calibration: $q=\frac{p}{p+\frac{1-p}{w}}$
  * 负采样带来ctr飘移

### 生成式做广告 [美团] EGA-V2







## 简历 Job Market 匹配

### Personalized Job Recommendation System at LinkedIn





## Embedding 技术在推荐系统中的应用 (chpt4)

### Intro

* 定义：用低维稠密向量“表示”一个对象 
  *	向量“表示”对象特征
  *	向量距离“表示”相似性
  *	具有本体论哲学层面上的意义
* 意义
  * 高维稀疏特征向量 转 低维稠密向量
  * 本身表达能力强，可引入任何信息进行编码(Graph Embedding)
  * 对物品、用户相似度的计算是常用的推荐系统召回层技术

* embedding的学习
  * 预训练
  * e2e
* embedding的获取方式：
  * word2vec
  * 在线学习
  * gbdt也可以视作embedding



### Word2vec——经典的Embedding方法

> 参考 Machine-Learning.md

### Item2Vec——Word2vec在推荐系统领域的推广

* 和Word2Vec的区别：
  * 不限制上下文长度
* 广义的Item2Vec：粗排双塔模型

### Graph Embedding——引入更多结构信息的图嵌入技术

#### DeepWalk

* 生成物品图：根据短时间内的物品序列
* 随机游走生成多个序列
  * 唯一的参数：v_i到v_j的跳转概率
* 用序列训embedding

#### Node2Vec

* Intro
  * 同质性和结构性的权衡
  * 同质性 -> DFS
    * In-out parameter越小，随机游走到远方节点的概率越大，越突出同质性
  * 结构性 -> BFS
    * return parameter越小，游回节点的概率越大，越突出结构性
* 关于Node2vec算法中Graph Embedding同质性和结构性的进一步探讨 - 王喆的文章 - 知乎
  https://zhuanlan.zhihu.com/p/64756917

#### EGES: Enhanced Graph Embedding With Side Infomation

* 核心思想：
  * 知识图谱学习类目向量，提升商品冷启动
    * 根据行为序列构成有向图
  * 学习 $$e^{\alpha_j}$$作为系数，weighted avg pooling的每类embedding的权重

#### Graph Service

* 基于Euler改造
  * 支持全图节点遍历，支持按时间戳采样

* 高性能采样
  * 节点生成前缀和，二分查找随机数
  * 全局点采样：shard有权（shard上所有节点的权重），shard内再次按权
* Graph Embedding on GPU: sample和worker分离
* GE应用
  * 预训练：利用uid与author间的finish关系，构造双向异构图。边权重用finish视频数量，点权重用finish视频总数。
    * 不用gid而用author是为了减小图结构随时间的变化
    * 正负例构造：正例是user及其finish的节点，负例随机采样
  * end2end training
    * 利用uid和cid之间的click关系，边权重是click次数，点权重是click总数
    * 采样：有放回按权采样
    * 优点：训练目标和LTR任务一致
    * 缺点：1.全局图的拟合能力不如业务图 2.end2end结构更自由，可以用node2vec等
  * Finetune BERT/ResNet
    * cid和cid的co-click，边权重用交并比，点权重用click总数，点特征用广告id类泛化特征

## 特征工程

* 原则：

  * 尽可能保留推荐环境及用户行为过程中的所有有用信息，尽量摒弃冗余信息

* 原始特征分类：

  * 用户行为数据
  * 用户关系数据
  * 属性、标签类数据
    * 电商类目打标
    * 豆瓣“社交化”让用户添加收藏，用户打标
  * 内容类数据
  * context特征
    * 时间、GPS地点、季节、月份、节假日、天气、社会大事件
  * 统计类特征
    * 历史ctr、历史cvr、物品热门程度、物品流行程度
  * match特征

* 特征举例：

  * ![image-20250105002547819](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250105002547819-6007955.png)

  * ![image-20250105002610024](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250105002610024.png)

* 特征处理：

  * dense特征：
    * 归一化、离散分桶
    * 非线性变换，一起加进来：$$x^a, log_a(x), log_a(\frac{x}{1-x})$$

  * direct特征：categorical（比如物品类目）、用户id
    * one-hot/multi-hot编码
    * embedding化
  * 统计特征：过去7天用户U1点击次数，过去 7 天物品$$I_1$$被点击的次数
  * cp(click polarity)特征
  * match特征
  * combine特征：交叉



## 召回算法

### 多路召回

* 新闻场景的N路：
  * 热门新闻
  * 兴趣标签
  * 协同过滤
  * 最近流行
  * 朋友喜欢
* Embedding召回
  * EGES一定程度上能融合多路
  * 评分具有连续性

### 负例采样与负例生成

* 概念与动机：在隐反馈场景（点击/观看/购买）下，未交互即“负例”并不等价于不喜欢，合理的负例采样/生成能让模型看到足够“困难”的对比，提升排序与召回的辨识力，同时控制训练效率与分布偏差。
* 常见采样策略：https://zhuanlan.zhihu.com/p/456088223
  * Uniform/Popularity-aware：从全库按均匀或按热度分布采样，前者易于实现但负例过“容易”，后者能打压热门项但可能引入偏差。
  * In-batch/Cross-batch/Memory-bank：批内其它样本作为负例；跨批或动量内存库（如 MoCo）扩展负池，提升难度与稳定性。
    * pros：提高训练速度、打压热度；cons：SSB（离在线不一致）
  * MNS采样：融合 inbatch 与 uniform，见 https://blog.csdn.net/codename_cys/article/details/121304716
  * CBNS采样：跨批利用前后 batch 信息缓解 SSB。
  * Hard Negative Mining：用已训练召回模型+向量检索（如 FAISS）挑选与正例相似但未互动的项；排序阶段可用“曝光未点击”，召回阶段需谨慎避免假负例。
  * Dynamic/Self-adversarial：依据当前模型分数动态选择高分负例，或按分数分布自适应采样，提升学习信号密度。
* 补充解法：
  * 取上一轮召回模型排名靠后的样本为负例，结合业务逻辑构造 hard 负例。
  * 对靠前曝光位的负样本在采样的时候做降权（尤其baidu app这种，大部分用户意图不是feed流的场景）
* 典型损失与公式：
  * BPR Pairwise：
    $$\mathcal{L}_{\mathrm{BPR}}=\sum_{(u,i,j)}-\ln\,\sigma\big(s(u,i)-s(u,j)\big)$$
    其中负例采样决定 $j$ 的质量；常与难负例挖掘结合。
  * InfoNCE（对比学习，两塔常用）：
    $$\mathcal{L}_{\mathrm{InfoNCE}}=-\ln \frac{\exp(\mathrm{sim}(u,i)/\tau)}{\sum\limits_{k\in\{i\}\cup \mathcal{N}(u)} \exp(\mathrm{sim}(u,k)/\tau)}$$
    负池来自 in-batch/cross-batch/memory-bank，$\tau$ 为温度。
  * Sampled Softmax/NCE（大类目近似）：以提议分布 $q(c)$ 近似归一项并做重要性加权：
    $$\tilde{Z}=\sum_{c\in\{y\}\cup S} \frac{\exp(z_c)}{q(c)},\quad \mathcal{L}_{\mathrm{ss}}=-\log \frac{\exp(z_y)/q(y)}{\tilde{Z}}$$
    或 NCE 形式：
    $$\mathcal{L}_{\mathrm{NCE}}=-\mathbb{E}_{(u,y)\sim p}[\log\sigma(f(u,y))]-K\cdot\mathbb{E}_{i\sim q}[\log\sigma(-f(u,i))]$$
* 偏差与校正：
  * SSB（Sample Selection Bias）与离/在线不一致：采样改变了训练分布；可用重要性加权 $w(i)=1/q(i)$、曝光建模、后验校准缓解。
  * 假负例与标签噪声：对“曝光未点”下调权重/设定时间窗；过滤同域强共现、同会话正例冲突。
* 工程实践：
  * 召回（两塔）：优先 in-batch + cross-batch + memory-bank；定期离线挖难负（teacher 模型+FAISS）；控制负例数 K 与温度 $\tau$。
  * 排序：以曝光未点为主并做权重/时间窗控制；正负去重；冷热启动场景结合内容相似难负。
  * 校准：采样训练后做概率校准，保证线上可解释性与多业务一致性。
* 参考：
  * BPR: https://arxiv.org/abs/1205.2618
  * IRGAN: https://arxiv.org/abs/1705.02364
  * Sampled Softmax（TF）: https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss
  * MoCo: https://arxiv.org/abs/1911.05722

### Streaming-VQ

> https://arxiv.org/abs/2501.08695

### Evaluation

* 后验概率：
  * 发生点击行为后，用户对item感兴趣的概率
* HitRate@K (命中率@K)
  *   **定义**: 衡量在为用户推荐的 Top-K 个物品中，是否命中了用户最终实际交互（如点击、购买）的物品。它是评估**召回模型**能力的核心指标之一。

  *   **计算逻辑**: 对于单个用户，如果在 Top-K 推荐列表中出现了至少一个用户实际交互过的物品，则记为“命中”（Hit=1），否则为“未命中”（Hit=0）。HR@K 是对所有测试用户的命中情况求平均值。

  *   **公式**: 
      $$ HR@K = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(|I_{K,i} \cap I_{click,i}| > 0) $$
      其中，$N$ 是测试用户总数，$\mathbb{I}(\cdot)$ 是指示函数（条件成立为1，否则为0），$I_{K,i}$ 是为用户 $i$ 推荐的 Top-K 物品集合，$I_{click,i}$ 是用户 $i$ 实际点击的物品集合。

  *   **应用与局限**:
      *   **应用**: 主要用于评估召回阶段的有效性，即“有没有把对的东西捞出来”。
      *   **局限**: 不关心命中物品在 Top-K 列表中的具体排序。对于排序效果的评估，需要结合 **NDCG**、**MAP** 等指标。
* PVR
  * PVR computes the ratio of items displayed to users (P Vi) that are successfully retrieved by our retrieval model
* Dual Evaluation Strategy (RS & CS)
  * 源自 UniDex，一种通用的召回评估手段，旨在兼顾系统相关性与用户参与度。
  * **Ranking Subset (RS)**
    * **定义**: 搜索/推荐系统实际展示给用户的物品集合。
    * **目的**: 评估模型是否拟合了现有的系统级偏好 (alignment with **system-level preferences**)，即模型召回的物品是否符合线上系统的整体排序逻辑。
    * **侧重**: Systemic relevance。
  * **Click Subset (CS)**
    * **定义**: 用户实际点击的物品集合（正样本）。
    * **目的**: 评估模型是否捕捉了用户的即时兴趣 (**immediate user interest**) 和真实交互信号。
    * **侧重**: User-centric engagement。
  * **意义**: 通过双重评估，全面分析模型在“拟合系统”与“服务用户”两方面的表现。


## 多角度审视推荐系统 (chpt 5)

* 特征工程
* 召回算法

### 实时性

* Literature Review

  * Facebook用gdbt+LR做实验，更新间隔越长，loss越大

* Online Learning
  * BOPR

* 特征实时性

  * 端上：缓存session内部行为，一个session内（比如3分钟一个session）点击的几篇文章
  * 流处理：简单的统计特征
    * 某个item在时间窗口内的曝光次数
    * 某个user在时间窗口内的点击话题分布
  * 批处理：
    * 数据join
    * 延迟信号的合并
  * ![image-20250105011214868](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250105011214868.png)

* Embedding实时性

* 模型实时性

  * 全量更新
  * 增量更新
    * 可以和全量更新相结合，过一段时间全量更新一次
  * online learning
    * 核心问题是兼顾模型效果和稀疏性
    * FOBOS和FTRL
  * 策略：局部更新
    * GBDT天级更新、LR实时更新
    * Embedding预训练、Dense实时更新

  * 端上的模型实时性
    * 讨论了端上User Embedding的实时更新，我觉得和端上实时特征的区别不大

### 推荐系统的优化目标

* youtube视频推荐系统的目标
  * 业务场景：视频播放前和中间的广告
  * 优化时长而不是ctr
  * 视频长短和质量都重要
  * 把播放时长转化为正样本权重，输出用加权逻辑回归，预测用$$e^{Wx+b}$$预测时长
    * 【上次观看时间】也是重要特征

* 电商推荐：预估cvr
  * 解决【曝光点击转化】的问题，阿里esmm

* 协商好优化目标——技术、产品、运营、内容编辑团队，解耦工作，最大化效率

### 优化不限手段

* roku：右滑某一类目tab，作为特征

#### 素材生成

* [Netflix artwork personalization](https://netflixtechblog.com/artwork-personalization-c589f074ad76)
  * https://www.youtube.com/watch?v=UjQMEjkrUGo

### 离在线一致性

*  特征 / 模型 / 策略 / 数据方面原因
   * 特征不一致：纠偏特征与请求上下文或模型自身状态相关，易出现不一致
     * rank (without per-rank serving)
     * time特征
     * model name特征
   * 特征参数不一致：
     * 时间整点特征
     * UE实时性的一致性（落盘 / 实时取）
   * 模型结构：
     * Batch Normalization
   * 精度：BF16、FP16等
   * 融合公式
   * 数据分布: sample selection bias (SSB)
*  系统方面原因
   * Serving/Training有bug
   * Model delay: Kafka lag / 模型停更 / 动作反馈延迟
   * Latency / timeout / failure
*  排查技巧
   * 静态模型预估，diff Training & Serving
   * 人工构造model delay，模拟其影响
   * 同一份数据用两个模型预估，评估数据的影响
   * 尝试解决SSB：加大流量，增大反馈，观测趋势



## LRM 长序列建模 (Large Recommendation Model) 

> Data->Sparse->Token->Dense
>
> Sparse 参数的 SOTA 规模应该是 1T～10T (Meta)，对应的存储空间（FP32）是 4TB～40TB
>
> wukong https://arxiv.org/abs/2403.02545

### Intro

* 业界：
  * meta新闻：https://www.cnbc.com/2024/03/06/facebook-working-on-single-ai-model-to-power-all-video-recommendations.html

### Literature Review

* 一些算法 [star]
  * KNN is a user-based collaborative
    filtering method that finds the top 10 most similar users to a given
    user and averages their ratings to score a specific item
  * Caser uses convolution neural networks to model user interests [54]; 
  * HGN uses hierarchical gating networks to capture both long and
    short-term user behaviors [55];
  * GRU4Rec employs GRU to model user action sequences [56];
  * FDSA uses a self-attentive model to learn feature transition patterns [57];
  * SASRec uses a self-attention mechanism to capture item correlations within a user’s action sequence [58];
    * SASRec [ 17 ], which used self-attention similar to decoder-only transformer models. Inspired by the success of masked language modeling in language tasks 【tiger】
  * BERT4Rec applies a masked language modeling (MLM) objective for bi-directional sequential recommendation [9];
    * BERT4Rec [ 32 ] and Transformers4Rec [ 6] utilize transformer models with masking strate-
      gies for sequential recommendation tasks 【tiger】
  * S3-Rec extends beyond the MLM objective by pre-training with four self-supervised objectives to learn better item representations [59].
    *  goes beyond just masking by pre-training
      on four self-supervised tasks to improve data representation
  * P5 fine-tunes a pre-trained LM for use in multi-task recommendation systems by generating tokens based on randomly assigned item IDs [40];
    * P5 [ 8] fine-tunes a pre-trained large language models for multi-task recommender systems. The P5
      model relies on the LLM tokenizer (SentencePiece tokenizer [ 29 ]) to generate tokens from randomly-
      assigned item IDs.【tiger】
* ZESRec [8], UniSRec [15], and RecFormer [21], have emphasized cross-domain
  transferability by incorporating textual features and employing con-
  trastive learning techniques. RecFormer, in particular, has unified
  language understanding and sequence recommendation through
  bidirectional Transformers.

### 字节 LONGER

> LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders

* 部分参考「AI-Algorithms」
  * token merge

### [美团] 用transformer做序列特征交叉

https://tech.meituan.com/2020/04/16/transformer-in-meituan.html

* 用transformer做序列特征交叉
  * 将transformer的输出结果和target item做din

### SASREC

https://arxiv.org/abs/1808.09781

### 序列降噪

#### Self-Attention 是低通滤波器

参考「AI-Algorithm」

#### Denoising Self-Attentive Sequential Recommendation

https://arxiv.org/pdf/2212.04120

## 冷启动 Cold-start 问题

* 问题定义：
  * 用户冷启动
  * 物品冷启动
    * pretrained embedding
    * 根据“价格、房屋属性、距离近”进行聚类，用相似物品的embedding
  * 系统冷启动

### 基于规则

* 用户冷启动
  * 热门、最流行、最高评分
  * 根据年龄、性别、地址，构建榜单
* 物品冷启动
  * pretrained embedding
  * 根据“价格、房屋属性、距离近”进行聚类，用相似物品的embedding

### 丰富特征

* 注册信息
  * 根据IP、GPS推断地址信息
* 第三方DMP（Data Management Platform）提供的信息
  * 低阶和高阶特征
* 物品内容特征：
  * 演员、年代、风格
* 引导用户输入：
  * 音乐风格
  * 喜欢的电影

### 主动学习、迁移学习、Exploit-Exploration

* 主动学习
  * 实际效果：给用户推荐最大聚类的物品

```
for j = 1,2,..., totalIterations do
	foreach q_j in potentialQueries do
		Evaluate Loss(q_j)
	end foreach
	Ask query q_j for which Loss(q_j) is the lowest
	Update model M with query q_j and response (q_j, y_j)
end for
return model M
```

#### bandit算法 —— 非个性化的探索利用

* 问题定义：
  * Multi-Armed Bandit Problem
  * 假设了每个Item对不同用户的效益一视同仁
* $$\epsilon-Greedy$$算法
  * 每次$$\epsilon$$概率随机选择，$$1-\epsilon$$概率选择平均收益最大的
  * 缺点：需要启发式，动态调节探索和利用的比例

> [专治选择困难症——bandit算法](https://zhuanlan.zhihu.com/p/21388070)

* Thompson sampling算法
  * 假设赢钱的概率p服从beta分布
    * beta分布是伯努利分布的共轭先验分布
  * `choice = numpy.argmax(pymc.rbeta(1 + self.wins, 1 + self.trials - self.wins))`
* UCB算法：Upper Confidence Bound
  * 利于工程实现
  * $$UCB(j) = \bar{x}_j + \sqrt{\frac{2 \ln n}{n_j}}$$
* Epsilon-Greedy算法：有点类似模拟退火
  * 一定的概率做纯随机决策
* 总结：
  * UCB算法和Thompson采样算法显著优秀一些，两者都是给explore bonus，给出explore bonus的方式不同：
    * UCB是频率学派，假定了大数定律收益收敛，显式的通过试验次数给出的置信度上界
    * Thompson是靠分布刻画不确定性
  * 至于你实际上要选哪一种bandit算法，你可以选一种bandit算法来选bandit算法。。。

#### 个性化探索利用

> Contextual-Bandit Algorithm

* LinUCB： 针对线性模型，找到探索的公式
  * $$E[\textbf{r}_{t,a}|\textbf{x}_{t,a}]=\textbf{x}_{t,a}\theta_a^*$$
  * Ridge Regression岭回归
  * ...

#### 基于模型探索利用

* 强化学习DRN，扰动模型参数，起到类似的作用
* 多样性探索：消重、打散、部分流量未知兴趣

## 多模态 + 搜广推

### Intro

> 推荐系统中的semantic feature https://www.xiaohongshu.com/explore/68c7ce6a000000001b031804

* “重新审视 ID 形式的特征表达，认为 ID 仅仅是客观世界的代理表达，但是模型对世界的感知应该更加原生和直接。常理思考，用户对于一个 item 是否感兴趣、是否会发生点击行为，本质是 item 的内容视觉表达是否吸引到用户，所以直接建模原生视觉表达会更为本质” —— 阿里妈妈搜索广告

* 多模态emb和其它特征做交叉，而不要直接加进item emb作为bias
  * 问题一：参数空间异质性：This issue arises because the parameters associated with multimodal representations, e.g., the parameters of the MLP that are connected with multimodal representations, are not adequately learned during the joint training process with the ID embeddings [ 29 ]. 

  * 问题二：多epoch v.s. one epoch
    * 多模态适合多epoch学习；ID-based模型适合one epoch学习
    * ![image-20251001194036697](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251001194036697.png)

* 思路一：将高维emb压缩为低维/标量，进模型，比如taobao paper的思路 SimTier，算相似度进模型

* 思路二：cotrain

* 思路三：semantic ID

  * 参考下文「生成式 + 语义ID」



### Pretrain + 多模态排序: Enhancing Taobao Display Advertising with Multimodal Representations

![image-20251001193454687](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251001193454687.png)

**2.1 预训练阶段：Semantic-aware Contrastive Learning（SCL）**

目标是生成能区分商品语义相似性的多模态表征，核心设计如下：

- 数据集构建：基于用户行为构建正负样本对，贴合电商场景语义：

  | 模态 | 语义相似正样本对             | 负样本来源                            |
  | ---- | ---------------------------- | ------------------------------------- |
  | 图像 | <用户图像查询，购买商品图像> | 大内存库（基于 MoCo 策略）            |
  | 文本 | <用户文本查询，购买商品标题> | 大内存库 + 难负样本（点击未购买商品） |

- 优化策略

  1. 损失函数：采用**InfoNCE 损失**（公式 1），通过 L2 归一化向量的点积衡量相似度，最小化正样本距离、最大化负样本距离，其中内存库大小$$K=196800$$，$$τ$$为可学习温度参数；
  2. 负样本增强：结合**MoCo（Momentum Contrast）** 扩展负样本池，引入**Triplet Loss**区分 “难负样本”（如外观相似但图案不同的枕头），提升表征细粒度区分能力。

**2.2 整合阶段：多模态与 ID 模型融合**

针对多模态与 ID 模型的适配问题，提出两种核心方法：

- **方法 1：SimTier**（简化多模态使用，提升性能）
  1. 计算相似度：目标商品多模态表征$$v_c$$与用户历史交互商品表征$$\{v_i\}$$的点积相似度$$s_i = v_i \cdot v_c$$（公式 2）；
  2. 区间划分：将相似度范围$$[-1.0, 1.0]$$划分为N个区间，统计每个区间的相似度计数；
  3. 特征输入：生成N维向量，与其他 ID 嵌入拼接后输入 MLP，实现高维多模态表征的低维转化。
- **方法 2：MAKE（MultimodAl Knowledge Extractor）**（解决训练 epoch 差异）
  1. 多 epoch 预训练：基于**DIN（Deep Interest Network）** 构建行为建模模块，输入多模态表征生成$$v_{MAKE}$$（公式 3），通过 4 层 MLP 输出预测 logit$$\hat{v}$$（公式 4），用交叉熵损失（公式 5）优化多模态相关参数（训练多 epoch，确保收敛）；
     1. sigmoid(v)是logit，本质是用multi-epoch的ctr预测任务来“纯粹地”学习多模态表征和ctr任务的关联
  2. 知识复用：将$$v_{MAKE}$$与 MLP 中间输出拼接至下游 ID 模型，解耦多模态与 ID 参数优化，避免 ID 模型 one epoch 训练导致的多模态参数欠拟合。

![image-20251001194444375](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251001194444375.png)

### ICDE'26 Lazada: Decoupled Multimodal Fusion (DMF)

> **Paper**: [Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction](https://arxiv.org/html/2510.11066v2) (ICDE 2026 Accepted)
> **Background**: 工业界多模态 CTR 预估通常采用两阶段：离线提取 Embedding -> 冻结后作为特征输入下游 ID 模型。

*   **痛点 (Pain Points)**:
    *   **Semantic Gap**: 直接融合多模态 Embedding (内容语义) 与 ID Embedding (行为协同) 存在分布差异。
    *   **Efficiency vs Effectiveness**:
        *   **Early Fusion (Modality-enriched)**: 效果好 (Deep Interaction)，但计算昂贵 (Target-aware 特征需对每个 Candidate 重算)。
        *   **Late Fusion / Pre-computed (Modality-centric)**: 效率高，但缺乏细粒度交互 (Coarse-grained)。
    *   ![image-20260123150145289](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260123150145289.png)
    
*   **核心方案 (DMF Framework)**:
    *   **DTA (Decoupled Target Attention)**:
        *   **Input**: 使用 Target Item 与 History Items 的预计算 **Multimodal Similarity Scores** (而非原始 Embedding)。
        *   **Similarity Calculation**: 使用余弦相似度计算 Target ($m_c$) 与 History ($m_j$) 的相似度：
            $$ c_j = \frac{m_c^T m_j}{||m_c||_2 ||m_j||_2}, \forall j \in \{1, ..., L\} $$
        *   **Discretization**: 将相似度分数 $c_j$ 离散化为 Bucket ID，查表得到 **Similarity Embeddings**。
        *   **Decoupling**: 复用 Target-agnostic 的 ID-based Value 向量，仅在 Attention Weight 计算时引入 Target-aware 的相似度信号。
            *   **Attention Score**:
                $$ \alpha_{ij} = \frac{\exp(e_{ij}/\tau)}{\sum_{k=1}^L \exp(e_{ik}/\tau)} $$
                其中 $e_{ij}$ 融合了 ID 特征交互与多模态相似度信号。
            *   **Value Calculation**:
                $$ \mathbf{v}_j^{final} = \text{Fusion}(\mathbf{v}_{id}, \mathbf{E}_{sim}^{V}) $$
                虽然 Value 融合了相似度 Embeddings ($\mathbf{E}_{sim}^{V}$)，但这里的 $v_{id}$（来自 $S$ 的 ID 投影）是 Target-agnostic 的，可以复用；而 $\mathbf{E}_{sim}^{V}$ 查表开销极小。
                > **Note**: 相比 Early Fusion 需要对 `[ID, Sim]` 拼接后的长向量做全连接投影（Linear），Decoupled Fusion 仅需对 ID 做投影（可复用），Sim 部分直接查表，最后轻量级 Fusion，从而大幅降低了计算量。
            *   **Aggregation**:
                $$ u_i = \sum_{j=1}^L \alpha_{ij} \mathbf{v}_j^{final} $$
        *   **收益**: 相比 Early Fusion，单机吞吐提升 **3倍** (避免了重算 Value 投影)。
    *   **CMM (Complementary Modality Modeling)**:
        *   融合 **Modality-centric** (如 Similarity Histogram, SH) 和 **Modality-enriched** (DTA) 两路信号。
        *   兼顾 **Semantic Generalization** (泛化) 和 **Behavioral Personalization** (细粒度)。
    *   ![image-20260123150358010](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260123150358010.png)
    *   ![image-20260123151025043](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260123151025043.png)

*   **效果 (Performance)**:
    *   **Lazada** 生产环境 A/B (12天): CTCVR **+5.3%**, GMV **+7.43%**.
    *   延迟微涨但可接受，适合高并发场景。

### TRM (Token-based Recommendation Model): Farewell to Item IDs

> **Paper**: [Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens](https://arxiv.org/html/2601.22694v1) (ByteDance, Arxiv 2026)
> **Blog**: [字节，多模态引入推荐模型的新方法](https://mp.weixin.qq.com/s/E3GtWE-SMIwHkw6GoyuZ0w)

*   **核心思想 (Core Idea)**:
    *   在大型排序模型 (Large Ranking Models, LRMs) 中，用 **Semantic Tokens** (语义 ID) 替代传统的 **Item IDs**。
    *   **Scaling Laws**: 传统的 Item ID 是独立的 categorical symbol，分布不稳定且难以扩展 (Scaling)；而 Semantic Tokens 构成了结构化的闭集，训练过程中分布更稳定，能够更好地支持 Dense Parameters 的 Scaling。

*   **痛点 (Motivation)**:
    1.  **Item ID 缺陷**:
        *   **Cold Start**: 新 ID 缺乏历史交互，难以学习。
        *   **Dynamic Shift**: ID 分布随时间剧烈变化，阻碍 Dense 参数的学习。
    2.  **现有 Semantic Token 的问题**:
        *   **Ignore Collaborative Signals**: 仅基于 Visual/Text 内容聚类，忽略了用户行为域 (User-action domain) 的信息。
        *   **Coarse-grained**: 粗粒度聚类牺牲了**记忆能力 (Memorization)**，导致对高频老物品的效果下降。
        *   **Ignore Structure**: 忽略了 Token 序列内部的结构信息。

*   **解决方案 (TRM Framework)**:
    *   ![image-20260205190321902](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260205190321902.png)
    *   **Collaborative Signal Injection (注入协作信号)**:
        *   使用 **MLLM (Qwen2.5-VL)** 作为基座。
        *   **Fine-tuning**: 基于交互日志构建 Item-Item 对 (共点击)，使用 **Contrastive Loss** 微调 MLLM，使其输出的 embedding 包含协作信息。
    *   **End-to-End Semantic ID Generation (端到端语义ID生成)**:
        *   **Coarse-grained (RQ-Kmeans)**: 对 MLLM 输出的 Item Embedding 做残差量化 (RQ-Kmeans)，生成基础语义 Token 序列。
        *   **Fine-grained (BPE-based)**: 将 Token 序列视为“单词”，使用 **BPE (Byte Pair Encoding)** 算法识别高频 k-gram 组合（如 "cake" + "candle" -> "birthday"），分配独立的可学习 Token。这增强了对高频 Item 的细粒度记忆能力。
    *   **Joint Training (联合训练)**:
        *   **Architecture**: 采用 **Wide&Deep** 或 **RankMixer** 架构。
            *   **Deep side**: 使用 Coarse-grained tokens。
            *   **Wide side**: 使用 Fine-grained tokens (BPE tokens)。
        *   **End-to-End**: Token 模块（Embedding & Projector）与排序模型联合训练。
    *   **Auxiliary Task (辅助任务)**:
        *   **NTP (Next Token Prediction)**: 在训练中增加 NTP 任务，预测语义 Token 序列的下一个 Token，辅助提升语义 ID 的质量。
    
*   **效果 (Results)**:
    *   **Performance**: AUC +0.85%, 用户活跃天数 (User Active Days) +0.26%.
    *   **Efficiency**: 稀疏存储 (Sparse Storage) 减少 33%。
    *   **Scaling Potential**: 随着模型参数量增加，TRM 的效果持续提升，验证了 Semantic Token 在 Scaling 方面的潜力。
        *   **Norm Variance Analysis**: 实验显示 Semantic Tokens 的 Norm Variance 在 Scaling 过程中保持稳定，而 Item IDs 的 Norm Variance 剧烈波动，解释了为何 Item IDs 难以支持大规模参数学习。
        *   **Scaling Law**: 论文通过 $L \propto N^{-\beta}$ 验证了 Semantic Tokens 符合 Scaling Law，且 $\beta$ 值优于 Item IDs。
    *   ![image-20260205190306907](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260205190306907.png)

### 多模态召回

#### STAR: A Simple Training-free Approach for Recommendations using Large Language Models

![image-20241225184836438](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241225184836438.png)

* Intro
  * ![image-20241225204332446](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241225204332446.png)
* 结论：
  * 召回效果很好
    * “头重脚轻”的召回排序架构
    * 语义比CF重要
    * recency decay 0.7
    * length=3，只能建模短时序列
  * LLM做pairwise排序，能提升效果
    * Table 5: 相比以往的N选M任务，window-based排序任务降低了难度，效果更好
    * window size=4，stride=2，参与排序的recall len=20
    * Table 6: 排序prompt中，popularity, co-occurrence的作用，热度信息没用
      * previous research indicating that simple popularity bias
        is ineffective in addressing recommendation problems [60–62].
    * Table 7: LLM模型能力对pairwise排序任务的效果影响有限
  * collaborative information在召回和排序中很重要
  * 比较有趣的结果：不考虑rating，效果更好，原因是目标是ctr，不考虑rating的后验

![image-20241225210031783](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241225210031783.png)

![image-20241226015142625](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241226015142625.png)

![image-20241226014245937](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241226014245937.png)

* 比Avg Pooling再召回强很多：原因是更细腻的交互，不丢失信息，本质上已经是一个策略排序模型了。
  * 启发：用短序列中的每个Item做召回，每个History Item可以作为一路召回，三路Merge

![image-20241226020026743](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241226020026743.png)

* 召回

  * using a combination of semantic similarity and collaborative
    commonality to the items in a user’s history.
    * a temporal factor gives priority to user’s recent interactions

  * Semantic relationship：
    * 入库：We construct a prompt based on
      the item information and metadata, including the title, description,
      category, brand, sales ranking, and price.
  * Collaborative relationship：
    * ItemCF
  * 融合：
    * 时间衰减、rating（行为等级）
    * ![image-20241226012231967](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241226012231967.png)

* 排序：LLM as Ranker

  * pointwise
  * pairwise
    * 从下往上
  * listwise
    * a window size 𝑤 and a stride d
  * 特征：
    * Popularity
    * Co-occurence

![image-20241226012908127](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241226012908127.png)

* Prompt:

```
Analyze the user’s purchase history to identify user preferences and purchase patterns. Then, rank the 4 items above based on their alignment
with the user’s preferences and other contextual factors. All the items should be included and listed using identifiers, in descending order of the user’s
preference. The most preferred recommendation item should be listed first. The output format should be [] > [], where each [] is an identifier, e.g., [1] >
[2]. Only respond with the ranking results, do not say any word or explain. Output in the following JSON format:
{
"rank": "[] > [] .. > []"
}
```



* Evaluation
  * 倒数第一个：test； 倒数第二个：validation



### 协同信号训练语义 Embedding

#### Intro

* 动机：
  * the **causal relationship** between different categories of goods is beneficial for **online-shopping short-videos services**
  * the **same category item relationship** is more important for **usual short-videos recommendation**
* Trivial 方法
  * 对比学习拉近多模态 emb 和 id emb 的距离：such contrastive loss is weak and easily over-fitting because of the ground-truth is not diverse enough, e.g., the item MLLM only has one ground-truth, its item ID embedding.

#### 快手 QARM：Quantitative Alignment Multi-Modal Recommend

> https://mp.weixin.qq.com/s/vmLZQGQwTwTsguy0awD0fA
>
> QARM: Quantitative Alignment Multi-Modal Recommend

因为多模态模型（multi-modal large language models ，MLLMs）属于大模型，难以端到端应用，所以工业界广泛使用级联范式作为建模架构（图1）：

1. 首先预训练多模态模型，为下游服务提供多模态表示；
2. 下游推荐模型将多模态表示作为附加输入以适应真实的用户-项目行为。

![image-20251225204203503](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251225204203503.png)

这种级联范式存在两个问题：

1. 表示不匹配（Representation Unmatching）：预训练的多模态模型始终受到经典 NLP/CV 任务的监督，而推荐模型由真实的用户-项目交互进行监督。结果，两个根本不同的任务目标相对独立，表述上缺乏一致的目标；
2. 表示不可学习（Representation Unlearning）：生成的多模态表示总是存储在缓存中，并作为推荐模型的额外固定输入，因此无法通过推荐模型梯度进行更新，对下游训练更加不友好。

于是，本文引入定量对齐多模态框架，为不同的下游模型定制专门的、可训练的多模态信息。包括：

1. **项目对齐（Item Alignment）**：转换原始的多模态表示以匹配真实的用户-项目行为分布。
   * Based on User2Item retrieval model, for each user positive clicked target item, select the highest similar item in ID representation space as trigger item from historical lastest 50 his/her positive clicked item set.
   * Based on Item2Item retrieval model, utilizing existing models learned stable item pairs with high similarity as data sources, e.g., export data from our Swing retrieval model.
2. **定量编码（Quantitative Code）**: 借鉴语义ID，将对齐的多模态表示转换为下游任务的可训练的 code ID。

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260105185632739.png" alt="image-20260105185632739" style="zoom:50%;" />

![image-20260105185809858](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260105185809858.png)

* 实现：
  * VQ：用 item embedding set 直接作为码表
  * RQ-KMeans
* 结论：
  * ![image-20260105213041184](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260105213041184.png)
  * 冷物品曝光增加、orders增加；热物品曝光减少、CTR增加
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260109183811209.png" alt="image-20260109183811209" style="zoom:50%;" />
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20260109183926334.png" alt="image-20260109183926334" style="zoom:50%;" />
  



##### **[Bootstrap Latent Representations for Multi-modal Recommendation](https://arxiv.org/abs/2207.05969)** (WWW 2023)

* **Background**: Existing multi-modal methods (e.g., LATTICE) typically construct item-item graphs and use GCNs to aggregate multi-modal information, which is computationally expensive and may introduce noise.
* **Method (BM3)**:
  * **Graph-less**: Avoids explicit graph construction and heavy GCN message passing.
  * **Bootstrapping**: Adopts a self-supervised learning approach inspired by BYOL (Bootstrap Your Own Latent). It predicts the latent representation of one view (e.g., augmented view) from another, avoiding negative sampling.
  * **Contrastive/Reconstruction**: Jointly optimizes the recommendation task (BPR Loss) and multi-modal reconstruction loss (aligning ID embeddings with multi-modal features).
* **Performance**: Achieves SOTA performance with significantly improved efficiency (e.g., much faster training speed compared to LATTICE).

#### todo 字节 SAIL

> [https://mp.weixin.qq.com/s/H7Q6nM500crysu7xS_E4qQ](抖音SAIL团队联合港中文MMLab推出SAIL-Embedding：打通「视、文、音」的全模态嵌入)

#### todo 阿里妈妈 MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior Modeling

> https://mp.weixin.qq.com/s/hgs_BzFZdjrDSbf9d74ilA
>
> https://arxiv.org/abs/2502.00321

* 高质量 MM Embedding 生成的核心是承载语义信息的内容空间与承载协同信息的兴趣空间如何有效对齐，模型架构就是多模态领域的稠密模型。稠密模型和 CTR 任务的稀疏模型相比，语义理解比统计判别任务相对更难，几十层的模型架构更为主流，给 Deeper 方向规模化带来空间。Pre-train 职责是 Encode，负责内容空间的理解与迁移，关注图文是什么，多模态对齐能力的持续优化是基础，将开源世界知识往电商知识迁移是关键；Post-train 职责是 Align，负责内容空间与兴趣空间的对齐，关注用户行为反馈、凸显图文吸引要素，高质量的训练样本和找到与下游 CTR 任务正相关的中间指标是关键。
  * 训练模式，包括分类、对比学习、掩码学习、自回归学习等，且 backbone 紧随主流更迭，包括 BEiT3、BGE、BLIP2、EVA2 等。
  * 数据质量，图文质量包括视觉强相关的主体和关键词识别，难正负样本挖掘，结合行业特色挖掘兴趣样本例如拍立淘的图搜场景等。
  * 规模效应，包括图片尺寸、训练样本和模型参数，模型尺寸经历了 0.1B、1B 和 10B 的升级过程，是 Deeper 方向规模化的主要路径。



### 纯 Dense 化探索

#### [Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://arxiv.org/pdf/2303.13835)

* Intro：
  * 结论是：MoRec is already comparable to its IDRec counterpart with an expensive end-to-end training method, **even for warm item recommendation**
  * https://github.com/westlake-repl/IDvs.MoRec
  * Q(i): Equipped with strong modality encoders (ME), can
    MoRec be comparable to or even surpass IDRec in regular, especially in warm-start item recommendation scenario?
    * two-tower based DSSM [24, 50] and session-based SASRec [25])，公平的实验setting对比
  * Q(ii): If Q(i) is yes, can the recent technical advances devel-
    oped in NLP and CV fields translate into accuracy improve- ment in MoRec when using text and visual features? 
  * Q(iii): Are the representations learned by these founda-
    tion models as general as claimed? How can we effectively use item modality representations derived from an NLP or CV encoder network?

* 算法：
  * User表征：User Emb、User BHV、User Profile
  * Item表征：Item Emb、模态Emb
  * 基于DSSM和SASREC研究IDRec和MoRec
    * SASRec is a well-known se- quential recommendation model based on multi-head self-attention (MHSA) [59] which describes a user by her interacted item ID sequence.
* 结论：
  * seq2seq训练 + SASREC相比双塔，更能发挥MoRec的能力
  * E2E训练效果比two stage好很多
    * “唯一The good thing” is that by proper adaption (i.e., TS-DNN), TS-based MoRec have some potential to compete with E2E MoRec for text recommendation in the future (16.66 vs 18.23).
    * representation fea- tures are not universal enough, at least for item recommendation.

![image-20241003233046500](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/morec.png)

* 关于Training Cost：
  * the best MoRec (with SASRec as user encoder and Swin-B as ME) takes an astonishing more than 100x compute and training time than IDRec
  * inference time差不多
  * 优化思路：
    * 只finetune top-layer
* 其它算法相关：
  * extra pre-training：在e2e morec的基础上，比较难做效果
  * Combing ID & modality features：效果差
  * it is sometimes necessary to set different learning rate for item ME and other modules. This may be because item ME has been pre-trained on NLP and CV datasets before, and its learning stride may be different from other modules trained from scratch.

#### Text-based collaborative filtering (TCF)

> Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights

* Intro
  * Text-based collaborative filtering (TCF)
  * We examine whether these extremely large LMs could enable a universal item representation for the recommendation task.

* 算法：
  * loss：either be a pairwise BPR [38] loss or a cross-entropy classification loss [54].

* 结论
  * Q1: How does the recommender system’s performance respond to the continuous increase in the item encoder’s size? Is the performance limits attainable at the scale of hundreds of billions? 
    * sasrec效果好于DSSM
    * the TCF model with a 175B parameter LM may not have reached its performance ceiling
  * Q2: Can super-large LMs, such as GPT-3 with 175-billion parameters, generate universal text representations?
    * even the item representation learned by an extremely large LM (e.g., GPT-3) may not result in a universal representation, at least not for the text
    * ![image-20241006172858506](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/tcf-result.png)
    * Finetune LM效果好（top two layers）![image-20241006173055402](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241006173055402.png)
  * Q3: Can recommender models with a 175-billion parameter LM as the item encoder easily beat the simplest ID embedding based models (IDCF), especially for warm item recommendation?
    * ![image-20241006173158353](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/tcf-result2.png)
  * Q4: How close is the TCF paradigm to a universal recommender model?
    * while TCF models with large LMs do exhibit a certain degree of transfer learning capability, they still fall significantly short of being a universal recommender model, as we had initially envisioned
    * Table 3
    * For a universal recommender system model, not only should item representations be transferable, **but also the matching relationship between users and items needs to be transferable.** However, the matching relationship is closely related to the exposure strategy of the specific recommender system.
  * Q5: Will the classic TCF paradigm be replaced by a recent prompt engineering based rec- ommendation method that utilizes ChatGPT (called ChatGPT4Rec)?

![image-20241006171904133](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/TCF.png)

* 其它：
  * appendix有sasrec在不同数据集的训练超参

### 借鉴自动驾驶VLA的演进

- http://xhslink.com/a/1z0HdJWDhc1ab DriveVLM
- http://xhslink.com/a/Q2xet1Q1cg1ab LMDrive



## 多场景建模

### [Meta] 跨域序列建模

https://mp.weixin.qq.com/s/b2QXIxOtFWNpY95N9_GRAw

### ADS

> https://arxiv.org/pdf/2502.05523

* Model
  * Personalized Candidate Representation Generation (PCRG).
    * 多 Query 生成的思想，将多场景信息进一步融入至 **item 粒度个性化的多个 query** 中，进一步提高整个序列结构的多场景表征能力
    * 输入scenario feature和target feature，生成多个query头
  * PSRG：使用**动态权重**的思想，通过**多场景个性化特征生成 Meta Network** 用于对原有行为序列表征进行自适应调整
    * 用scenario feature作为dynamic weight，处理user bhp item
  * domain之间的item share embedding

![image-20250605201320222](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250605201320222.png)

* 算法工程优化
  * chunked query attn
    * ![image-20250605204349345](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250605204349345.png)
    * 和GQA的区别：GQA在多个头之间share KV，该方案在多个item之间share query



## 生成式推荐

> 思考：
>
> * 生成式推荐的价值，更多在于生成的list具备连贯的语义（逻辑等）能力，list-wise的建模
> * user history seq显著影响生成的seq

### Intro

* 大模型时期的迭代主线：弱化归纳偏置，强化数据驱动，设计通用且高效的模型结构，让模型从数据中自动学习复杂模式，充分挖掘算力潜能，探索出稀疏 Wider 方向往稠密 Deeper 方向扩展的新路径
* 最关键的认知破局点在于，CTR 任务的判别式模式太简单了，让模型判别是否点击这类的 1bit 信息量的答案，相较于 Next Token Prediction 的生成式而言，求解空间过小。如此，在不改变判别式任务的情况下，模型仅依靠强能力的高维稀疏 ID Embedding 就能做好大部分的记忆工作，浅层的 Dense 参数只需要承担部分的泛化能力就好，这样模型始终有 Deeper 方向规模化的瓶颈。所以，我们认为三阶段的迭代范式 ——“Pre-train + Post-train + CTR” 可以破局，Deeper 方向规模化的重任交由 Pre-train 和 Post-train 完成。下面分别介绍新范式下我们对 Embedding 建模和用户行为兴趣建模的改造，对应两个关键词 ——“多模态” 和 “生成式”。

#### [明线、暗线、辅助线：阿里妈妈搜索广告2024大模型思考与实践](https://mp.weixin.qq.com/s/hgs_BzFZdjrDSbf9d74ilA)

* 明线，归纳偏置（Inductive Bias）的合理设计，是模型能力提升的核心驱动力。
  * 语义协同融合

* 暗线，硬件算力的指数级提升，为模型的规模化提供了强力支撑。
  * scaling law

* 辅助线，CV 和 NLP 领域的代际性技术升级，给搜推广领域带来重要启发。
  * semantic id

* 阿里妈妈自主研发了广告领域专属大模型 LMA（Large Models for Advertising）
  * LMA 持续优化，认知分支聚焦多模态表征，推理分支聚焦搜推广领域的用户行为大模型等。这些技术进展不仅推动预估环节实现多个版本迭代上线，还深度改造了召回、改写、相关性和创意等核心技术模块，推动技术体系全面升级。

![image-20251025151445996](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251025151445996.png)

#### 生成式 --> 搜推一体化

- LUM：用 Conditional Token 做搜推一体化

  - 搜索 Token：LLM Emb（不要求必须是原始文本 token，可是固定词表触发或外部向量编码）

  - 推荐 Token（引导大模型学习推荐pattern）：Scenario Token

  - 行为 Token：用 Token Encoder 整合异质的 Item 信息（ID、Category、内容）

- FORGE：

  - 搜索 Token：可变的文本 LLM Tokenizer

  - 推荐 Token（引导大模型学习推荐pattern）：固化的文本 Prompt + 可变的文本 LLM Tokenizer

  - 行为 Token：SID Token，加入 LLM Tokenizer

### 生成式召回

#### Intro

* 两个探索方向：
  * 参考 LLM 的自回归建模思路，基于 Transformer 架构自行构建 Next Item Prediction；
    * LUM
  * 将用户行为和 Query 一样文本化，直接借助 LLM 的世界知识和推理能力进行 Next Token Prediction。
    * 相关技术：
      * 蕴含协同过滤信息的 ID Embedding 以特殊 Token 的方式引入
      * 利用行为序列信息进行领域迁移的 SFT
      * Next Token 实际应用成 Next CPV（商品关键属性，结构化信息天然有聚类效果） —— 类似Next SID
    * 实践表明该召回方式能够提升召回通道独占比，带来明确业务收益

> 聊聊生成式召回的本质 https://zhuanlan.zhihu.com/p/1955356511661458958

* 预测下一个item embedding的生成式召回，本质还是双塔
  * 因为最终是点积建模而不是cross-encoder


* 预测下一个cluster的生成式召回，本质是索引
  * cluster是个有限集合，所以在预测的时候可以使用softmax做分类。但是，同个cluster内部一般是没有序的，这就和deep retrieval中的一条path，streaming VQ中的一个cluster（这个是特殊情况，有办法使得它内部可以排序，详情见 [现在，我们最好的召回模型来了](https://zhuanlan.zhihu.com/p/21751563983)）是一样的情况
* Semantic ID和协同cluster
  * 训练样本不充分的情况下，前者可能更好
  * 训练数据充足的情况下，后者更好：因为emb可变可训练
* 目前的生成式召回都是streaming VQ的子集
* 整体同意这篇文章的观点，但需要注意这篇文章的讨论setting更多是字节等超大规模场景
  * “数据量大时，id emb的学习能力无限，能强到让多模态收益微弱” --> 可能仅在超大规模场景，甚至字节的数据规模也难以达成这一结论
  * “id emb的重要性强于多模态”，该论断大体正确，但【“纯id”强于“纯多模态”】并不等价于【“id + 多模态”时，id的feature importance一定更高】，后者更接近于真实setting

#### 语义ID：Sparse & Dense 混合召回

##### LIGER [Meta]: Dense + TIGER

> Unifying Generative and Dense Retrieval for Sequential Recommendation
>
> * 核心结论：
>   * 加入多模态emb的dense retrieval模型（类似双塔），效果好于TIGER（由此推断，序列建模的模型 > dense retrieval模型 > TIGER生成式）
>   * 因此TIGER的优劣势：
>     * 优势主要在于SID降低了工程成本、效果有想象空间
>     * 劣势是效果有待商榷、冷启动能力较弱
>   * 这篇文章基于此探索dense retrieval和TIGER的融合
>     * 和COBRA的核心区别：COBRA的dense encoder是可学习的

* 问题：序列建模（SASRec、S3-Rec）的效果比 TIGER 强
  * 原因一：基于SID的模型：tends to overfit to items encountered during training, resulting in a lower probability of generating cold-start items.
  * 原因二：以往的基线对比时，没有将多模态content emb加入到dense retrieval模型中，存在不公平

![image-20250724165713380](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724165713380.png)

* 因此这篇文章先进行公平对比：构建dense retrieval
  * dense retrieval的定义：双塔召回，user塔使用transformer建模，输入id emb和多模态emb，输出预测的item emb
  * ![image-20250724171132291](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724171132291.png)
  * ![image-20250724171645054](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724171645054.png)

* 实验一：针对 1）、2）、3）三个Tiger和Dense Retrieval的核心差异点，做了消融
  * ![image-20250724172618121](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724172618121.png)
* 实验一结论：
  * ![image-20250724172836596](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724172836596.png)
  * 分析：1）TIGER的生成式 Next Item Prediction Loss 的训练效率不高（相比于cos sim的对比学习训练目标）；2）TIGER的冷启动效果不好
* 实验二：分析TIGER的冷启动能力，结论是基于SID的模型，tends to overfit to items encountered during training, resulting in a lower probability of generating cold-start items.
  * ![image-20250724173408571](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724173408571.png)
  * 调研到 Rajput et al. (2024) 的一种解决方案，根据cold-start和non cold-start的item比例，调整生成item的策略
* 然后提出混合的LIGER方案
  * ![image-20250724171333423](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724171333423.png)
  * LIGER的核心思路：
    * 同时接受Item Rep、SID作为输入（但没有Item ID），同时预测Item Rep和Item SID
    * ![image-20250724173739291](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724173739291.png)
    * **use generative retrieval to retrieve K items and then rank them with cold-start items using dense retrieval**
  * **实验结论：LIGER能narrow the performance gap，但仍不能追上dense retrieval**
    * ![image-20250724174427498](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724174427498.png)
    * 无法追上 --> 参考 Table 4: K=N时，是基于多模态改进版的dense retrieval模型
      * ![image-20250724175119027](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724175119027.png)

* 更多消融实验
  * ![image-20250724180458242](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724180458242.png)

##### COBRA [Baidu]: Dense Encoder + TIGER

> Cascaded Organized Bi-Represented generAtive retrieval (COBRA)
>
> 和LIGER的核心差异是训练了一个dense encoder

* Intro
  * 通过级联的稀疏 - 稠密表示整合生成式推荐与稠密检索的优势。该框架先生成**稀疏语义 ID**（捕捉物品类别本质），再以此为条件生成**稠密向量**（捕捉细粒度细节），通过端到端训练（结合稀疏loss和稠密loss）动态优化表示；推理阶段采用**粗到细生成过程**，并引入**BeamFusion 机制**（融合beam search分数与最近邻相似度）平衡推荐精度与多样性。实验显示，COBRA 在公共数据集（如 Beauty 数据集 Recall@5 达 0.0537）和工业数据集上均优于基线模型，在线 A/B 测试实现 3.60% 转化率和 4.15% ARPU 提升，验证了其有效性。

![image-20250721004612054](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250721004612054.png)

* Cascaded Bi-Represented Retrieval Framework
* Learnable Dense Representations via End-to-EndTraining
* Coarse-to-Fine Generation Process
* Comprehensive Empirical Validation



> Model

* 如何理解semantic id
  - 一种经过压缩（压缩为ID）的多模态特征

- Cascaded Representation：$$(ID_t,v_t)$$

![image-20250721154129987](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250721154129987.png)

>  Training

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250721155458955.png" alt="image-20250721155458955" style="zoom:50%;" />

> Infra

- 召回的工程实现
  - beam fusion
  - <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250721154234451.png" alt="image-20250721154234451" style="zoom:50%;" />
- 语义id的 算法分类 X 分客户 X 分行业 存储
  - COBRA: RQVAE sparse id
  - IDGenRec: 引入了额外的 Text ID
  - LIGER: LIGER’s IDs and dense representations share the same granularity, and the dense representations are pre-trained and fixed.
- semantic id的工程链路
  - sparse semantic id --> 多模态模型
    - pretrained model
    - finetuned model
      - 如何finetune
    - cotrain model
      - 语义id训练更新的实时性
- COBRA的特殊之处
  - dense representation --> trainable encoder

### 生成式 + 语义ID

#### Intro

![image-20251012235634551](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251012235634551.png)[snap]

* 量化算法

  - RQ-VAE

  - RQ-Kmeans

  - VQ-VAE
  - 码本共享程度
    - 单一共享码本

    - 每级一个码本

* 训练稳定性
  * 码本更新方式

    - EMA

    - 梯度更新
  * reset unused codebook vectors

* 和推荐模型的集成

  - tokenization方式
    - SID粒度：TIGER、LIGER、COBRA
    - 将Item SIDs分组：
      - N-gram [google]
      - SPM [google]
    - 更复杂的表示Item SIDs的方式
      - All-bigrams [meta]
      - prefix-ngram [meta]

##### 解码策略

  - 解码冲突和重复
    * SID去重：append item token [tiger]
    * 冲突解码
      * 约束beam search与无约束beam search [snap]

  - Trie 约束解码 (Trie-Constrained Decoding)

    * **核心思想**：利用 Trie (前缀树) 数据结构在解码过程中动态约束模型的输出，确保生成的 SID 一定是有效的、存在于物品库中的。

    * **Trie 树构建**：将所有有效的 SID 构建为前缀树。从 Root 到任意终点节点的路径即为一个有效 SID。

    * **执行流程**：
      1. 根据当前已生成路径，定位 Trie 中的当前节点
      2. 获取该节点的所有子节点（下一步允许生成的 token 集合）
      3. 修改模型 Logits：将不在允许集合中的 token 的 logit 设为 $-\infty$
      4. 从约束后的分布中采样

    * **数学表示**：
      $$
      \text{logits}_t'[v] = 
      \begin{cases} 
      \text{logits}_t[v] & \text{if } v \in V_{\text{valid}}(t) \\
      -\infty & \text{otherwise}
      \end{cases}
      $$
      其中 $V_{\text{valid}}(t)$ 为第 $t$ 步允许生成的 token 集合。

    * **在 MIMIGenRec 中的应用**：
      - 动态构建 Logits Processor，根据 tokenizer 动态构建 template
      - 自动构建 Trie 树，避免报错
      - 强制以最高优先级传入解码超参，解决部分 instruct 模型自带 topk 较小导致 valid token 被过滤的问题

    * **优劣势**：
      - 优势：100% 保证生成结果有效，减少无效采样，无需后处理
      - 劣势：限制模型探索能力，Trie 构建维护有开销
  - Sparse & Dense
    - LIGER
    - COBRA
  - prepend user token [snap]
  - 生成式时的训练推理分布不一致 —— label smoothing



#### Literature Review

##### SID, A Practitioner’s Handbook [Snap]

> Generative Recommendation with Semantic IDs: A Practitioner’s Handbook
>
> https://mp.weixin.qq.com/s/esHJFzOOm2pnL6uVgqrKKQ、https://arxiv.org/pdf/2507.22224

* SID生成算法：
  * 不同量化方法（如RQ-VAE、RQV、Residual K-Means）对推荐性能（Recall@5/10、NDCG@5/10）有显著影响。表1显示了具体性能比较。
  * 大部分时候Residual K-Means胜出。一个可能的原因：它在处理高维嵌入时更高效，避免了RQ-VAE的变分噪声或R-VQ的简单量化损失。

* SID序列模型最前面的user token
  * 移除user token获得最佳结果，表明当前个性化设计未达预期。
  * user token是额外的离散ID词汇表（vocabulary），用于捕捉用户偏好或个性化信息。
  * user token本意是提升个性化（e.g., 区分不同用户偏好），但结果显示它未能有效实现目标——可能因随机分配未捕捉真实个性化，或训练不足导致噪声。

* 模型架构
  * 编码器-解码器（Enc-Dec）架构显著优于仅解码器（Dec-only）架构，因为前者能更好地捕捉用户历史序列的模式。

* 数据增强：使用滑动窗口增强显著提升性能，有助于泛化和缓解过拟合。有些数据集，用户行为是很稀疏的，对于这样的，效果明显。
  * 具体做法：在标准滑动窗口增强中，通常使用固定窗口大小 w（如 w=5 或根据序列长度设定），从用户交互序列的开头逐步滑动生成样本。这会为每个长序列产生多个固定长度的子序列样本，帮助模型学习更多局部模式。

* SID去重：TIGER的去重策略与简单随机选择相当，但前者计算开销更大，不适合大规模物品集。

* 解码策略：约束beam search与无约束beam search性能相当，但后者更高效。

##### VAE

* the posteriors and priors in VAEs are assumed normally distributed with diagonal covari-
  ance, which allows for the Gaussian reparametrisation trick to be used [ 32, 23 ]. Extensions include
  autoregressive prior and posterior models [ 14 ], normalising flows [31 , 10], and inverse autoregressive
  posteriors [22]. 【VQ-VAE】

##### RQ-VAE模型相关：TIGER系列

* LC-Rec [53] aligns semantic and collaborative information using RQ-VAE with a series of align-
  ment tasks.
* ColaRec [45] **combines collaborative filtering signals with content information by deriving generative identifiers from a pretrained recommendation model.** 
* SEATER [34] focused on maintaining semantic consistency through balanced k-ary tree-structured indexes refined by contrastive and multi-task learning.
* TIGER also fine-tunes LMs to predict item IDs directly, but these IDs are semantic, meaning they are learned based on the content of the items [21];
* IDGenRec [38] leverages large language models to generate unique, concise, and semantically
  rich textual identifiers for recommended items, showing strong potential in zero-shot settings. 【COBRA】
  * IDGenRec goes further by extending semantic IDs to textual IDs, enriching the IDs with more detailed information [44].

* 码本更新方式

  - EMA

  - 梯度更新

* 码本共享程度

  - 单一共享码本

  - 每级一个码本

* 训练技巧

  * reset unused codebook vectors



##### 语义ID分析

* 为什么需要语义ID【Tiger】
  * With semantic token representations for items, the model is less prone to the inherent feedback loop [1, 26, 39] in recommendation systems, allowing the model to generalize to newly added items to the corpus. Furthermore, using a sequence of tokens for item representation helps alleviate the challenges associated with the scale of the item corpus; the number of items that can be represented using tokens is the product of the cardinality of each token
    in the sequence. Typically, the item corpus size can be in the order of billions and learning a unique
    embedding for each item can be memory-intensive. While random hashing-based techniques [16 ]
    can be adopted to reduce the item representation space, in this work, we show that using semantically
    meaningful tokens for item representation is an appealing alternative
  * 【FORGE】缓解random hashing时负采样带来的离在线不一致
* 语义ID的缺陷 【COBRA】
  * methods based on discrete IDs may lack fine-grained details and suffer from information loss, which can limit their abil-
    ity to accurately capture user preferences [42].
  * approaches that rely on natural language may struggle to align linguis-
    tic expressions with the requirements of recommendation tasks,
    potentially leading to suboptimal performance [19]

##### 量化编码方式

| 研究方向     | 代表性工作     | 核心方法                      | 局限 / 本文改进点                    |
| ------------ | -------------- | ----------------------------- | ------------------------------------ |
| AR 图像合成  | VQ-VAE、VQ-GAN | VQ+AR 模型，VQ-GAN 加对抗损失 | 低分辨率特征图近似精度低，需大码本   |
| 向量量化应用 | PQ（乘积量化） | 线性独立向量求和近似          | 仅适用于近邻搜索，无法直接用于 AR    |
|              | AQ（加法量化） | 依赖向量求和近似              | 码选择为 NP-hard 问题                |
|              | RQ（残差量化） | 迭代量化向量与残差            | 多码本需调参，本文用**单一共享码本** |

>  【tiger】Vector Quantization as the process of **converting a high-dimensional vector into a low-dimensional tuple of codewords**. 

* 最早期的进展，参考VQ-VAE paper的Related Work
* Other alternatives for quantization. A simple alternative to generating Semantic IDs is to use
  Locality Sensitive Hashing (LSH). We perform an ablation study in Subsection 4.2 where we find that
  **RQ-VAE indeed works better than LSH**.
  * Locality Sensitive Hashing (LSH) [14 , 13 ] is a popular technique used for clustering and approximate nearest
    neighbor search. The particular version that we use in this paper for clustering is SimHash [ 2], which
    uses random hyperplanes to create binary vectors which serve as hashes of the items. Because it has
    low computational complexity and is scalable [13 ], we use this as a baseline technique for vector
    quantization.
* Another option is to use **k-means** clustering hierarchically [ 34 ], but it **loses semantic meaning between different clusters** [ 37].
  * clusters created in a particular iteration are further partitioned into sub-clusters in the next iteration
  * 丢失了不同簇之间的语义含义
* We also tried VQ-VAE, and while it performs similarly to RQ-VAE for generating the candidates during retrieval, it **loses the hierarchical nature of the IDs which confers many new capabilities that are discussed in Section 4.3.**




##### sparse id 和 dense 联合

* COBRA
* LIGER’s IDs and dense representations share the same granularity, and the
  dense representations are pre-trained and fixed.

##### 消除冲突、生成、解码

* 消除冲突，增加token
  * 随机token
    * FORGE：circular添加
  * 热度token
  * KNN-based strategy samples multiple candidate SIDs and evaluates them sequentially based on their associated scores until a SID with fewer than σ corresponding items is found【FORGE】
* 生成
  * top-k
  * beam search
* 解码
  * 直接映射
  * 前缀匹配

##### 协同信号引入语义 ID 生成

* FORGE paper 引用中的 [41,5,42,11]

#### 量化编码方式 (Quantization Encoding Methods)

##### K-Means 与 层次化 K-Means 的语义损失

将连续的 Embedding 空间离散化的常用方法是聚类，但这类方法会造成语义信息的损失。

*   **简单 K-Means 的局限：完全丢失簇间语义**
    *   **核心问题**: K-Means 将空间分割成几个完全独立、地位平等的簇。它完全丢失了簇与簇之间的任何关联信息。
    *   **举例**: 假设对动物 Embedding 聚类，得到 `簇1(陆地哺乳动物)`、`簇2(海洋哺乳动物)`、`簇3(鱼类)`。在原始空间中，`鲸鱼(簇2)` 与 `老虎(簇1)` 的语义距离可能比与 `鲨鱼(簇3)` 更近。但量化后，我们只得到离散ID `1, 2, 3`，模型无法知道 `簇1` 和 `簇2` 的关系比 `簇1` 和 `簇3` 更近。

*   **层次化 K-Means 的改进与局限**
    *   **改进**: 通过构建树状结构，层次化 K-Means **部分保留了粗粒度的语义关系**。例如，`鲸鱼`和`海豚`是兄弟节点，同属`海洋哺乳动物`父节点。
    *   **局限**: 关系被固化为“父-子”或“兄弟”，**无法表达连续的远近概念**。例如，模型无法知道 `海狮` 其实介于“陆地”和“海洋”哺乳动物之间。并且，模型只知道 `簇1` 和 `簇2` 是兄弟，但不知道它们在语义上挨得很近；而 `簇1` 和 `簇3` 可能在树的另一大分支上，离得很远。这种**连续的距离感丢失了**。

**结论**: 无论是简单的还是层次化的K-Means，都将连续、灵活的向量空间转换为了离散、刚性的结构，无法避免细粒度、连续性语义信息的损失。

#### VQ-VAE

##### Paper

> VQ-VAE: Vector Quantised-Variational AutoEncoder
>
> https://papers.cool/arxiv/1711.00937
>
> [Neural Discrete Representation Learning](https://papers.cool/arxiv/1711.00937)
>
> todo 苏剑林 https://www.spaces.ac.cn/archives/6760

* Intro

  * 假设码本$$\mathcal C$$是一个有限集$$\{(k, \mathbf{e}(k))\}_{k\in [K]}$$，集合内是若干 pair，pair 内是编码$$k$$及其对应的码向量$$\mathbf e(k) \in \mathbb{R}^{n}$$，其中$$K$$是码本的大小，$$n$$是 code embedding 的 dimensionality. 给定$$\mathbf{z} \in \mathbb{R}^{n}$$,$$\mathcal{Q}(\mathbf{z}; \mathcal C)$$表示$$\mathbf{z}$$的 VQ，也就是离$$\mathbf{z}$$最近的 embedding 的编码。

    $$\mathcal{Q}(\mathbf{z}; \mathcal C) = \mathop{\arg\min}\limits_{k\in K} \Vert \mathbf{z} - \mathbf{e}_k \Vert_2^2 $$

  * Vector-Quantized Variational AutoEncoder (VQ-VAE), which was introduced in [35 ] as
    a way to encode natural images into a sequence of codes. The technique works by first passing the
    input vector (or image) through an encoder that reduces the dimensionality. **The smaller dimensional**
    **vector is partitioned and each partition is quantized separately, thus resulting in a sequence of codes:**
    **one code per partition.** These codes are then used by a decoder to recreate the original vector (or
    image).【tiger】

  * combining the variational autoencoder (VAE) framework with discrete latent representations through a novel parameterisation of the posterior distribution of (discrete) latents given an observation


![image-20251018023755713](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251018023755713.png)

* discrete相比continuous的价值： underlying modality is inherently discrete的场景
  * Language/speech/image description，本身是离散表征
  * discrete representations are **a natural fit for complex reasoning**, planning and predictive learning

* 关于VQ
  * one of the simplest dictionary learning algorithms
  * circumvent issues of **“posterior collapse”** -— where the latents are ignored
    when they are paired with a powerful autoregressive decoder
* decoder：自回归模型
  * VQ-VAE 和自回归模型的关联是一种 能力互补、分而治之 的绝佳策略：

    - VQ-VAE 负责 感知 (Perception) ：它将复杂的连续信号（像素）转换为抽象的离散概念（视觉词汇），并学会如何将这些概念渲染成逼真的细节。
    - 自回归模型 负责 认知 (Cognition) ：它在抽象的、离散的符号空间中进行操作，学习这些概念之间的逻辑和结构关系
  * 还原先验
    * ![image-20251018025838401](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251018025838401.png)

  * **ancestral sampling**

* 超参：
  * we use a field of 32 x 32 latents for ImageNet, or 8 x 8 x 10 for CIFAR10
* 结论：
  * perform as well as its continuous model counterparts in log-likelihood

* 应用
  * 从语音数据学到 prior knowledge about phonemes or words
  * decoder用来改变说话人的音色

* 数学理解，参考mathematics——「解构 VQ-VAE：一个关于变分贝叶斯方法应用的特例」

##### Scaling the Codebook Size of VQGAN

> Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%

* Utilization Rate: a codebook entry is considered utilized for the epoch if it is used at least once
* 思路是先Kmeans找到初始化码本，然后冻结码本，训一个码本的projector，从而做codebook size的scaling

##### Finite Scalar Quantization (FSQ)

> **Finite Scalar Quantization: VQ-VAE Made Simple** (Mentzer et al., 2024)

* **核心思想**: 提出一种极其简化的向量量化方法，去除了传统 VQ-VAE 中显式的码本（Codebook）学习和最近邻搜索，直接利用标量量化。
* **机制**:
    1.  **Down-projection**: 将高维隐变量 $z$ 投影到低维空间（如 $d=5$ 或 $d=8$）。
    2.  **Element-wise Quantization**: 对每个维度进行独立的标量量化，将其映射到 $L$ 个固定的离散层级（Levels）。
        $$ \hat{z}_i = \text{round}((L_i - 1) \cdot \sigma(z_i)) $$
        或者直接使用 $\text{floor}$ / $\text{ceil}$ 操作。
    3.  **Implicit Codebook**: 组合所有维度的离散值，形成一个隐式的、巨大的码本。例如，若有 $d$ 个维度，每个维度 $L$ 个层级，则总的离散状态数为 $L^d$。
*   **优势**:
    *   **避免 Codebook Collapse**: 由于没有显式码本，不存在某些码字从未被选中的问题（死码）。
    *   **效率极高**: 训练和推理时无需进行昂贵的最近邻搜索（Nearest Neighbor Search），计算复杂度大幅降低。
    *   **实现简单**: 代码实现仅需几行，易于集成到现有架构中。
*   **应用**: 视觉生成（如 LlamaGen, VAR）、推荐系统（如 UniDex）等领域，作为一种高效的离散化瓶颈（Discretization Bottleneck）。

#### 码本崩塌 (Codebook Collapse)

在涉及向量量化（Vector Quantization, VQ）的模型（如VQ-VAE, VQ-GAN）中，码本崩塌是一个常见的训练难题。

* 定义：**码本崩塌**指在训练时，一个预设大小的码本（Codebook，即 embedding 向量的集合），只有一小部分向量被频繁使用和更新，而大部分向量从未或很少被使用，最终成为“死码”（dead codes）。这导致码本的**有效容量**远小于其设计容量。

* 码本崩塌源于一个“富者愈富”的恶性循环：

1.  **早期选择**: 训练初期，编码器（Encoder）的输出随机地靠近码本中的某些向量。
2.  **梯度更新**: 被选中的码本向量获得梯度并更新，从而更贴近它所代表的数据特征。
3.  **恶性循环**: 更新后的向量因更优，后续更容易被再次选中。而从未被选中的向量得不到更新，离数据特征越来越远，被选中的概率也越来越低。
4.  **最终结果**: 编码器的输出被“困在”少数码本向量周围，导致大部分码本空间被浪费。

* 根本原因：从编码器输出到选择“最近”码本向量的 Argmax 操作是**不可导**的，梯度无法直接引导编码器去探索未使用的码本向量。

* 解决方案

1.  **VQ-VAE Commitment Loss**: 通过一个额外的损失项，鼓励编码器的输出向其选择的码本向量靠拢，防止编码器输出随意跳动，间接稳定码本使用。

2.  **码本重置 (Codebook Resetting)**: 周期性地检查“死码”，并将其重置到某个高频使用的向量附近（可加随机扰动），给其“重生”的机会。

3.  **Gumbel-Softmax**: 一种使离散选择过程“可微”的技巧，允许梯度更有效地回传，但会增加计算复杂度。

4.  **FSQ (Finite Scalar Quantization)**: 一种从根源上解决问题的设计。它放弃“找最近邻”的思路，而是将一个向量的**每一维度独立地**量化到固定的几个离散值上。这种网格化的结构保证了所有可能的量化组合都能被触及，从而**从根本上消除了码本崩塌**的问题。

##### VQ的训练稳定性

> https://arxiv.org/pdf/2005.08520
>
> VQVAE在训练时vq_loss不稳定是什么原因？ - 章彦博的回答 - 知乎
> https://www.zhihu.com/question/452385227/answer/3479897074

###### EMA or loss: $$‖sg[z_e(x)] − e‖^2$$的学习方式

* ![image-20251018024612390](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251018024612390.png)


##### 纯RQ结构的码本崩塌问题

当仅使用残差量化(Residual Quantization, RQ)而不结合VAE（Variational Autoencoder）架构时，非常容易出现码本崩塌（大部分数据点都只映射到少数几个码本向量上）。其核心原因是：**纯RQ的优化目标是局部的、贪婪的，而VAE提供了一个全局的、带有约束的优化框架。**

具体原因如下：

1.  **缺少编码器-解码器的反馈约束**：
    *   **在RQ-VAE中**：编码器(Encoder)负责将输入数据压缩成一个有意义的、高度概括内容的潜在向量。解码器(Decoder)则需要根据量化后的向量重建原始数据。这个过程为RQ提供了一个有语义的、更规整的输入空间。
    *   **在纯RQ中**：RQ直接作用于原始高维向量，它不理解向量的语义，其每一步量化都只是一个独立的“最近邻搜索”任务，缺乏高级信息引导。

2.  **缺少重建损失的全局监督**：
    *   **在RQ-VAE中**：重建损失(Reconstruction Loss)是一个强大的全局监督信号。如果码本崩塌导致信息大量丢失，解码器将无法很好地重建数据，巨大的重建损失会反向传播，“惩罚”这种无效的量化方式，迫使模型更均衡地利用码本。
    *   **在纯RQ中**：没有重建任务，优化目标仅仅是**最小化当前的局部量化误差**（即向量与码本的L2距离）。这种贪婪策略会使模型倾向于选择“万金油”式的码本，而不会费力去探索和使用那些表达细节的冷门码本。

3.  **残差累积效应的放大**：
    *   RQ是一个序贯过程，下一步的输入是上一步的残差。
    *   在纯RQ的贪婪策略下，如果前几步的量化已出现偏差或崩塌，产生的残差分布会变得非常不均衡，导致后续步骤的量化也更容易崩塌，问题逐级放大。

4.  **缺少潜在空间的正则化**：
    *   **在RQ-VAE中**：VAE的KL散度损失项会鼓励编码器生成的潜在向量分布接近于一个标准正态分布，这种正则化使得潜在空间更平滑，间接促进了码本的均匀使用。
    *   **在纯RQ中**：没有这种对向量分布的正则化约束。如果输入向量本身分布不均，纯RQ会忠实地（甚至放大地）反映这种不均衡，导致码本使用上的严重倾斜。



#### RQ-VAE

> RQ-VAE: Residual Quantized Variational AutoEncoder
>
> 《Autoregressive image generation using residual quantization》
>
> 《Product Quantization for Nearest Neighbor Search》

* 和VQ-VAE的对比：
  * VQ-VAE，the rate-distortion trade-off [42]

  * VQ-VAE的码本利用率不足：相同聚类数的情况下，codebook参数量显著低于VQ

  * VQ-VAE的重构质量受限
    * RQ-VAE [ 40, 21] applies residual quantization to the output of the encoder of VQ-VAE to achieve
      a lower reconstruction error. We discuss this technique in more detail in Subsection 3.1. 【tiger】

* 核心创新：
  * **残差**量化（Residual Quantization）不是用一个码本量化全部信息，而是用**多个码本级联**，每个码本只量化上一级残差。这样模型能够用较小的码本组合出更复杂、更丰富的表达，显著提升了码本利用率和重构精度。
    * RQ 可覆盖$K^D$个向量簇（等价于 VQ 的$K^D$码本），但无需增大 K，避免码本崩溃。

  * **多码本自回归组合** 每一级码本的选择依赖于前一级的残差，量化过程像“分层编码”，让信息分布在多个层次上，提升了模型的表达能力。
  * **灵活的信息分配** 允许每个级别的码本专注于信息的不同部分，低级别码本捕捉主要结构，高级码本细化细节。
    * 本文将 RQ 技术用于 AR 图像生成，且所有量化深度共享一个码本，避免多码本的超参搜索，最大化码利用率。

##### RQ-Transformer: 降低计算成本与提升码预测精度

![image-20251018235535505](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251018235535505.png)

* 先spatial再depth

* 损失函数：

  1. 重建损失：$$L_{recon} = \|X - \hat{X}\|_2^2$$（保证像素级保真）；
  2. 承诺损失：$$L_{commit} = \sum_{d=1}^D \|Z - sg[\hat{Z}^{(d)}]\|_2^2$$（sg为停止梯度，确保量化向量逐步逼近 Z）；
  3. 对抗与感知损失：patch-based 对抗损失（提升感知质量）+ 感知损失（对齐图像语义特征）。

* **计算复杂度对比**

  | 模型             | 序列长度             | 计算复杂度                            | 优势                                         |
  | ---------------- | -------------------- | ------------------------------------- | -------------------------------------------- |
  | 传统 Transformer | T×D                  | $$O(NT^2D^2)$$                        | 无                                           |
  | RQ-Transformer   | T（空间）+ D（深度） | $$O(N_{spatial}T^2 + N_{depth}TD^2)$$ | 复杂度大幅降低（如 T=64,D=4 时，约降 64 倍） |

#####  实验设置

- **数据集**：无条件生成（LSUN-{cat/bedroom/church}、FFHQ）、类条件生成（ImageNet 256×256）、文本条件生成（CC-3M）；
- **基线模型**：VQ-GAN、ImageBART、StyleGAN2、DDPM、BigGAN 等；
- **关键参数**：RQ-VAE 码本 K=16384（LSUN/ImageNet/CC-3M）、K=2048（FFHQ），量化深度 D=4；RQ-Transformer 参数规模 370M-3.8B。

- 局限
  1. 小规模数据集（如 FFHQ）性能不及 StyleGAN2，存在 AR 模型过拟合问题；
  2. 文本到图像生成未扩大模型 / 数据规模，未充分挖掘零样本能力；
  3. 仅利用单向上下文，无法支持图像修复、扩展等双向任务。

##### 训练技巧：label smoothing

* 目的：解决 “训练时用 RQ-VAE 的真实码，推理时用 RQ-Transformer 的预测码” 的偏差 —— 通过训练中引入随机码，让模型适应 “非最优码” 的情况，减少推理时的误差累积。
  * 用温度 τ 的软化分布$$Q_τ(k|r_{t,d-1}) \propto e^{-\|r_{t,d-1}-e(k)\|_2^2/τ}$$（而非 one-hot 标签）监督码预测，保留码嵌入的几何关系。
* 配合随机采样

##### RQ-VAE with GCN + 协同信号: MMGRec

> [MMGRec: A Multi-modal Generative Recommender System](https://arxiv.org/pdf/2404.16555)

MMGRec 首次将**生成式范式**引入多模态推荐，旨在解决传统“嵌入-检索”范式**推理成本高**、**交互建模不足**和**假阴性**三大问题。其核心思想是为每个物品生成一个唯一的 `Rec-ID`，然后通过 Transformer 模型自回归地生成用户偏好的物品 Rec-ID 序列。

*   **Rec-ID 设计**：由 `M-1` 个**语义令牌**和 `1` 个**流行度令牌**构成，兼顾了语义性和唯一性。语义令牌相同的物品，通过其交互热度的排名作为流行度令牌，以解决 ID 碰撞问题。

*   **Graph RQ-VAE (用于 Rec-ID 分配)**：
    1.  **特征融合**：通过 GCN 将物品的**多模态特征**（视觉、文本等）与**协同过滤信号**（来自用户-物品交互图）进行融合，得到物品的综合表征 $$h_i$$。
        *   图卷积层：$$\mathbf{h}_i^{(n)} = \text{LeakyReLU} (\mathbf{h}_i^{(n-1)} \mathbf{W}_1 + \frac{1}{|\mathcal{N}_i|} \sum_{u \in \mathcal{N}_i} \mathbf{h}_u^{(n-1)}\mathbf{W}_2)$$
    2.  **层级量化**：使用 RQ-VAE 对融合后的表征 $$h_i$$ 进行层级量化，生成语义令牌。
    3.  **联合优化**：损失函数结合了 BPR Loss 和 RQ-VAE Loss，确保表征既满足协同过滤的偏好假设，又能高质量地被量化和重构。
        *   BPR Loss: $$\mathcal{L}_{\text{bpr}} = \sum_{(u,i,j) \in \mathcal{R}} -\ln \sigma(\mathbf{h}_u \mathbf{h}_i^\top - \mathbf{h}_u \mathbf{h}_j^\top)$$
        *   RQ-VAE Loss: $$\mathcal{L}_{\text{rqvae}} = \text{重构损失} + \text{量化损失}$$

*   **Transformer 生成器 (用于 Rec-ID 生成)**：
    *   **关系感知自注意力 (Relation-aware Self-Attention)**：针对推荐场景中用户行为的**非序列性**，该机制取代了固定的位置编码。它为每个用户学习一组独立的 Q/K/V 映射矩阵，从而在计算注意力时能建模用户个性化的物品间关系。
        *   注意力权重计算：融合了**通用**的Q/K和**用户特异性**的Q/K：
            $$ \epsilon_{ij} = \frac{(e_i W^Q)(e_j W^K)^\top + (e_i W_u^Q)(e_j W_u^K)^\top}{\sqrt{D}} $$
    *   **推理效率**：由于生成过程与总物品数无关，在大规模物品库（如Kwai数据集）上，其推理效率远超传统的矩阵分解方法。

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251115005042349.png" alt="image-20251115005042349" style="zoom:50%;" />





#### Eval semantic id 指标

> 如何离线评估语义ID效果？ http://xhslink.com/o/8xM17krlzlP 
>
> Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%

* 重构损失 📉 
  * 观察 input embedding 和每一层累积 quantilized embedding 的重构损失，越小越好。 
* 码本利用率 📊
  * Codebook Utilization (Zhu et al., 2024)
  * **a codebook entry is considered utilized for the epoch if it is used at least once**
  * reflecting how effectively the model leverages available resources to represent
* Token Distribution Entropy (Bentz and Alikaniotis, 2016): 越大越好，但存在上限且浅层收敛慢。
  * Utilizing Shannon entropy, this metric quantifies the uniformity of token distribution, providing insight into the diversity and balance of token allocation across the model.
* Gini coefficient：
  * 类似 Token Distribution Entropy，但从 SIDs {C10C20C30,..., C18191C28191C38191}，更细粒度统计分布均匀性

* **SID 训练完成后**可以看这些：
* 可视化分布
  * 使用 T-SNE 或 UMAP 可视化多模态 embedding 与 quantilized embedding。
  * 肉眼观察相似 item 的分布变化情况。
  * 压缩过程中不可避免有信息丢失，但若仍能聚在一起则可以接受 ✅。
* 码本冲突率 collision rate
  * 计算码本冲突率，并查看相同 SID 的 item 进行归因。
  * 有些冲突是必然的，特别是在没有对 item 质量进行控制的情况下。
  * 也可训练中监控一大段
* 相似物品验证 🔍
  * SID 前 N 层相同的理当视为相似物品。
  * 可以使用线上相似物品索引（如 ItemCF）计算 Recall@K进行验证。
  * ⚠️ 注意：这个值一般不会太高，可以想想为啥？
  * ![image-20251204062807133](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204062807133.png)
* 📖 参考 - 方法（1、2）可参考 OneRec Technical Report-v1的 4.4 Tokenizer部分

#### TIGER系列：RQ-VAE用于推荐

> * 其它一些比较fancy的多模态+生成式的文章：https://mp.weixin.qq.com/s/7Ec8BZvRzu7tHo4d42kWdg
>   * “行为-语义对齐”量化模块
>   * 多个RQ-VAE ckpt，对item的token进行数据增强

* VQ-VAE 缺乏物品概念层级，无法平衡泛化与记忆

##### TIGER[Google]

> 《Recommender Systems with Generative Retrieval》https://arxiv.org/abs/2305.05065
>
> Transformer Index for GEnerative Recommenders (TIGER) 
>
> - TIGER（RQ-VAE）的训练/推理框架，相比于Paper，需要考虑：
>   - 小时级的二阶段模型训练
>   - (天级的?) RQ-VAE模型训练
>   - 几组table的在线更新：
>     - an Item ID to Semantic ID table：实时更新
>     - a Semantic ID to Item ID table：实时更新
>     - semantic id的embedding table：天级更新？

* TIGER leverages a  RQ-VAE [20] to encode item content features into hierarchical semantic IDs, allowing the model to share
  knowledge across semantically similar items without the need for individual item embeddings.
  * 核心是为每个物品生成**语义 ID（Semantic ID）**—— 通过**RQ-VAE（残差量化变分自编码器）** 对物品的内容嵌入进行量化，得到由语义相关代码词组成的序列。随后，基于 Transformer 的序列到序列模型被训练以预测用户下一个交互物品的语义 ID。实验表明，TIGER 在 Amazon 的 Beauty、Sports and Outdoors、Toys and Games 三个数据集上，在 Recall@K 和 NDCG@K 指标上显著超越现有 SOTA 模型（如 S³-Rec、SASRec 等），且在**冷启动推荐**（对无交互历史物品的检索性能提升）和**推荐多样性**（通过温度参数可调）方面展现出新能力。
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724014542189.png" alt="image-20250724014542189" style="zoom:67%;" />



![image-20250724022122724](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724022122724.png)

* 一阶段：训练 RQ-VAE：
  * The Autoencoder is jointly trained by updating the quantization codebook and
    the DNN encoder-decoder parameter
  * This loss **jointly trains the encoder, decoder, and the codebook**
  * ![image-20250724022821137](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724022821137.png)
  * **k-means clustering-based initialization，防止码本崩溃**
    * apply the k-means algorithm on the first training batch and use the centroids as initialization.
* RQ-VAE 实现细节
  * RQ-VAE is used to quantize the semantic embedding of an item. We use the pre-trained Sentence-T5 [27 ] model to obtain the semantic embedding of each item in the dataset. In particular, we **use item’s content features such**
    **as title, price, brand, and category to construct a sentence**, which is then passed to the pre-trained
    Sentence-T5 model to obtain the item’s semantic embedding of 768 dimension
    * ![image-20250724023505350](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724023505350.png)
  * Handling Collisions.
    * Depending on the distribution of semantic embeddings, the choice of codebook
      size, and the length of codewords, semantic collisions can occur (i.e., multiple items can map to the
      same Semantic ID).
    * To remove the collisions, we **append an extra token** at the end of the ordered
      semantic codes to make them unique. For example, if two items share the Semantic ID (12, 24, 52),
      we append additional tokens to differentiate them, representing the two items as (12, 24, 52, 0)
      and (12, 24, 52, 1). To detect collisions, we maintain a lookup table that maps Semantic IDs to
      corresponding items.
    * Note that collision detection and fixing is done only once **after the RQ-VAE**
      **model is trained**. Furthermore, since Semantic IDs are integer tuples, the lookup table is efficient in
      terms of storage in comparison to high dimensional embeddings
  * Invalid IDs的讨论：4.5节
    * 基本能预测出来valid ids
    * 可以做prefix matching
    * ![image-20251112012503896](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251112012503896.png)
* 二阶段：semantic id序列模型
  * **在最前面加user id**，且user id经过hash到2000 tokens

* 结论：
  * 编码方式：
    * RQ-VAE效果比LSH和random好
  * RQ-VAE
    * Effects of Semantic ID length and codebook size：效果上比较鲁棒，比如4-size或6-size codebooks；性能上size小的序列生成模型开销低
  * Tiger效果比直接用语义embedding做KNN效果好
  * 冷启动：效果好
  * 多样性：可结合temperature-sampling in the decoding stage，提升多样性
  * 可多场景共建RQ-VAE：
    * ![image-20250724025357526](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724025357526.png)
  * 性能：
    * 内存开销：
      * an Item ID to Semantic ID table
      * a Semantic ID to Item ID table. 
      * semantic id的embedding table
    * semantic id的embedding table小很多

##### Better Generalization with Semantic IDs [Google]

> https://arxiv.org/pdf/2306.08121v2

###### Intro: SPM、N-gram

* 探索用N-gram model / SPM（Sentence Piece Models），将Item SID subwords表征为embedding，在推荐模型中学习

  * SPM 通过动态学习子词，优于固定长度的 N-gram
  * the key idea is to create content-based hashing through sub-pieces of item SIDs
    * while SPM provides a learnable approach from item distribution by grouping sub-pieces with variable lengths;

* 作为对比：

  * LIGER、COBRA等，将subwords作为序列单位，这里将SID作为序列单位

* 对物品的内容嵌入（2048 维，由 Video-BERT 生成）进行压缩，输出离散的 SID 序列：

  1. RQ-VAE 结构：含 3 个联合训练组件

     - 编码器（ε）：将内容嵌入$$x \in \mathbb{R}^{2048}$$映射为 latent 向量z；
     - 残差量化器：共$$L=8$$层，每层含$$K=2048$$个码本向量$$e_k^l \in \mathbb{R}^{D'}$$，递归量化残差$$r_l$$；
     - 解码器（D）：将量化后的$$\hat{z}$$重构为原嵌入空间$$\hat{x}$$。
       - we use a 1-layer encoder decoder model with dimension 256.

  2. 损失函数：

     $$L = L_{recon} + L_{rqvae}$$

     - $$L_{recon} = \|x - \hat{x}\|^2$$（重构内容嵌入）；
     - $$L_{rqvae} = \sum_{l=1}^L \beta\left(\|r_l - sg[e_{c_l}]\|^2 + \|sg[r_l] - e_{c_l}\|^2\right)$$（推动残差与码本向量对齐，sg为停止梯度），其中$$\beta=0.25$$。

###### SPM、N-gram: SID 在排序模型中的适配方法

* 动机：capture relationships between sequential codes in Semantic ID
* 核心思路：将 SID 序列拆分为子词，通过子词嵌入实现 “语义哈希”，两种适配方式对比如下：

| 适配方法 | 子词生成逻辑                                                | 嵌入表设计                          | 优势                                               | 局限                                        |
| -------- | ----------------------------------------------------------- | ----------------------------------- | -------------------------------------------------- | ------------------------------------------- |
| N-gram   | 固定长度分组（N≤2）：如 Unigram（单码）、Bigram（双连续码） | 每个 N-gram 单独建表，大小为\(K^N\) | 实现简单，小嵌入表下略优                           | 嵌入表大小随 N 指数增长，无法自适应物品分布 |
| SPM      | 动态学习子词：高频共现码合并为子词，低频用单码              | 单张嵌入表（固定大小）              | 自适应物品分布，平衡泛化与记忆，大嵌入表下性能最优 | 小嵌入表下子词 vocab 有限，语义捕捉不充分   |

* N-gram
  * N=2 bigram为例：$$ (c_1^v, c_2^v), \ldots, (c_{L-1}^v, c_L^v)$$
* SPM：
  * using SPM to dynamically learn Semantic ID subwords based on the distribution of impressed items. This allows dynamic length subwords such that popular co-occuring codes are automatically comined as a single subgroup,
    whereas codes that rarely co-occur together may fallback to unigram.

###### 评估冷启动，CTR/1D AUC

- CTR AUC：评估整体泛化能力（0.1% 变化即显著）；
- CTR/1D AUC：评估对 “第 N+1 天新引入物品” 的泛化能力。

- 训练阶段：用 “第 1 天到第 N 天” 的 “过去数据” 训练模型，模拟 “模型在第 N 天刚训练好”。
- 测试阶段：用 “第 N+1 天的新物品” 和 “第 N 天及之前的用户” 在 “第 N+1 天及之后的真实交互”，模拟 “模型在第 N 天遇到第 N+1 天新物品时的预测任务”。

* **结论：使用user seq时，SPM(大emb table size) > bigram > random hashing > unigram**
* 关于Dense Input
  * Figure 2a/b：冷启动+多层时，效果好于 random hashing
  * ![image-20251019004932192](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251019004932192.png)

###### 训练技巧：reset unused codebook vectors

* 1kw样本
* **reset unused codebook vectors at each training step to content embeddings of randomly sampled videos**
  **from within the batch** Zeghidour et al. (2021), which significantly improved the codebook utilization

![image-20251019005445656](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251019005445656.png)

##### Enhancing Embedding Representation Stability [Meta] 

> Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID

###### Intro: prefix-ngram、All-bigrams

![image-20251020013104318](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251020013104318.png)

* 实验setting
  *  with L = 3 and K = 2048. We use the prefix 3-gram paramaterization
  * For production models, we train RQ-VAEs with **L = 6 and K = 2048**, and Semantic ID follows the design of prefix-5gram from Section 4.2 with H = O(50M ).

###### 三种序列建模

* Bypass, Transformer, and Pooled Multihead Attention (PMA),

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251020014955263.png" alt="image-20251020014955263" style="zoom:50%;" />

###### Impression Skew、ID Drifting

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251020013435106.png" alt="image-20251020013435106" style="zoom:50%;" />

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251020013504552.png" alt="image-20251020013504552" style="zoom:50%;" />

###### Embedding Representation Stability

* 一些观测：
  * attn logits中，Padding token attention score降低
  * CTR的计算，A/A prediction difference显著降低
  * tail items/new cold start items 的效果显著上升



##### FORGE [Alibaba]：生成式搜推、SID 和文本融合

> 《Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets》
>
> 阿里FORGE: 工业级生成式召回语义ID锻造术 大模型时代... http://xhslink.com/o/6eBwYelVRLl
>
> https://github.com/selous123/al_sid
>
> https://huggingface.co/AL-GR

![image-20251204033714884](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204033714884.png)

* ![image-20251204043322591](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204043322591.png)
* ![image-20251204053915059](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204053915059.png)
* RQ-VAE 上下游优化：
  * encompassing diverse data modalities
    * text和image是独立encoder，同时会训练一个fusion模块
    * **embedding训练用i2i信息：对HR@1000帮助很大，对HR@20帮助有限**
      * 实现方式：
        * 存储i2i信息在item中：the item i + with the most collaborative relations of each item i (i2i for simplicity) is provided as a field of each item and aids in SID generation in Section 3.1
        * 微调embedding
          * ![image-20251204040420838](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204040420838.png)
          * 正样本用i2i，负样本用in-batch negative sampling
  * ID collision mitigation
    * Random效果比KNN好：**“码本的公平利用” 比 “细粒度语义一致” 对性能的贡献更大**
    * ![image-20251204051308575](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204051308575.png)
* 生成
  * dynamic beam search, which adaptively increases the beam size during generation (e.g., 300 → 600 → 1200), thereby balancing computational efficiency and generation quality
* Eval:
  * embedding hitrate
    * 评估embedding，zero-shot召回率
  * Gini coefficient
  * 这俩指标能有效指导 GR 后续指标
    * Other metrics like feature fidelity do not show a clear correlation with retrieval effectiveness. Multiple-VQ achieves the best scores on two of them, but in Table 17 we can observe that it obtains the worst retrieval results. A similar pattern is observed for RQ-Kmeans, which significantly outperforms base in feature fidelity but still lags in overall retrieval performance
  * ![image-20251204040011824](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204040011824.png)
* SID 如何快速迭代：4.4 Practical Insights for Codebook Upgrade and Convergence (RQ4)
  * 问题：新的一套 SID，训二阶段推荐 GR 模型，收敛慢
  * ![image-20251204044227071](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204044227071.png)
  * ![image-20251204042618456](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204042618456.png)
  * 结论：From User Action 最好
* 其它结论
  * 二阶段decoder越大效果越好，Table 4（Qwen-0.5B到3B）
  * SIDs trained on the recommendation task **can generalize to the search task** （且3层码本的跨场景泛化能力比2层码本好）
  * i2i：Both the query2i and samecate (items within the same category) approaches underperform the base method, which utilizes only limited i2i data for alignment 
  * 码本结构，三层最合适，且形如 1024_4096_32768 ，recall@1000有提升
  * 量化方式
    * ![image-20251204063629483](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204063629483.png)
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204064322614.png" alt="image-20251204064322614" style="zoom:50%;" />

* dataset不错，参考后面benchmark笔记

* 超参和实现细节
  * 3×8192 as the 3-level codebooks
  * ![image-20251204054046528](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204054046528.png)
  * ![image-20251204054109012](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204054109012.png)
  * All the SIDs are regarded as new tokens and included in the tokenizer of LLMs upon initialization
  * 工程优化
    * ![image-20251204064809841](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204064809841.png)
  * **RQ-VAE 代码实现解析：Sinkhorn 防止码本坍塌**
    * 在 FORGE 的官方实现中，为了缓解 Codebook Collapse（即某些 Code 长期未被选中，导致“富者越富，穷者越穷”），在训练阶段引入了 Sinkhorn 算法。
    * **核心逻辑**：
      1.  **计算距离**：计算输入特征与 Codebook 中每个向量的 L2 或 Cosine 距离。
      2.  **标准化**：对距离矩阵进行标准化 `(d - mean) / std`，这一步对 Sinkhorn 的数值稳定性至关重要。
      3.  **Sinkhorn 优化**：将距离矩阵转化为相似度（负距离），通过 Sinkhorn 迭代（交替行/列归一化），得到一个**双随机矩阵 Q**。
      4.  **软分配**：使用 Q 来确定每个样本所属的 Code Index (`argmax(Q)`)，而不是直接使用原始距离的 `argmin`。
    * **作用**：强制要求在一个 Batch 内，Codebook 中的每个 Code 被选中的次数是大致均衡的（Equipartition）。这相当于给 VQ 过程加了一个“负载均衡”的约束，有效提升了离散编码的熵（Entropy）和表达能力。
#### RQ-KMeans vs. RQ-VAE：为何 RQ-KMeans 在推荐中更优？

> 趋势：近期的研究（如 "Revisiting the Codebook for Generative Recommendation" 或 MiniOneRec 等实践）表明，在构建 Semantic ID 时，**RQ-KMeans 往往优于 RQ-VAE**。

*   **核心差异**：
    *   **RQ-VAE**: 联合训练 Encoder、Codebook 和 Decoder。依赖重构损失 ($L_{recon}$) 和量化损失 ($L_{rqvae}$)。
    *   **RQ-KMeans**: 直接对**固定的**预训练 Embedding（如 BERT、SASRec 产出的 Item Embedding）进行迭代式残差聚类。

*   **RQ-KMeans 的优势 (The "Huge Advantage")**：
    1.  **保真度更高 (High Fidelity)**：
        *   推荐系统通常已经拥有高质量的 Item Embedding (来自 Content Tower 或 Collaborative Filtering)。
        *   RQ-KMeans 的目标非常纯粹：**最小化量化误差** (Euclidean Distance)。它能贪婪地、精准地保留原始 Embedding 的几何结构。
        *   相比之下，RQ-VAE 引入了变分推断的噪声（KL 散度）和复杂的优化目标，可能导致量化后的表征与原始高质量 Embedding 发生“漂移”。
    2.  **无码本坍塌 (No Codebook Collapse)**：
        *   RQ-VAE 训练极不稳定，容易出现“死码”（Dead Codes），即某些码本向量从未被使用。虽然有 Sinkhorn 等技巧，但仍需精细调参。
        *   RQ-KMeans 通过聚类算法（如 K-Means++ 初始化），天然保证了码本的高利用率和分布的均匀性。
    3.  **简单高效 (Simplicity & Efficiency)**：
        *   **Training**: K-Means 收敛速度快，无需反向传播和复杂的梯度估计 (STE)。
        *   **Inference**: 只需要计算距离，无需经过 Encoder 网络。
    4.  **解耦设计 (Decoupling)**：
        *   将“表征学习”（Representation Learning）与“索引构建”（Indexing/Quantization）解耦。
        *   这允许我们使用最强的 Encoder（如大语言模型产出的 Embedding）而不受 VAE 结构的限制。

*   **结论**: 在 Item Embedding 质量已经很高的情况下，**RQ-KMeans 是 Semantic ID 构建的首选方案**。它用最低的成本实现了最高的量化还原度。

#### RQ-KMeans 算法细节

> QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou

* 动机：避免 hourglass phenomenon 沙漏现象

* 定义：分层 K-Means
  * 在第一个level（𝑙 = 1），初始残差定义为 $$\boldsymbol{r}_i^1 = \boldsymbol{e}_i$$
  * 在后续每个level 都有一个码表 (codebook) $$\boldsymbol{C}_l = \{\boldsymbol{c}_1^l, \boldsymbol{c}_1^l, \cdots, \boldsymbol{c}_K^l\}$$, 其中$$K$$是码表大小。通过 $${s_i^l} = arg\ min_k \|\boldsymbol{r}_i^l - \boldsymbol{c}_k^l\|_2^2$$ 生成最接近的质心节点嵌入的索引, 下一层的残差定义为$$\boldsymbol{r}_i^{l+1} = \boldsymbol{r}_i^l - \boldsymbol{c}_{{s_i^l}}^l$$
  * 这个过程重复$$L$$次，得到$$sematic\_ID = \{s_i^1, s_i^2, \cdots, s_i^L\}$$
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251114231156408.png" alt="image-20251114231156408" style="zoom:50%;" />

* 构造：Balanced K-means Clustering
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251114231214319.png" alt="image-20251114231214319" style="zoom:67%;" />
  * Line1: 平衡的主要来源

##### 沙漏现象 [京东]

> 《Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of
    Generative Retrieval》 https://arxiv.org/pdf/2407.21488

* RQ-SID 存在 **“沙漏现象”**—— 中间层码本token过度集中，形成 “一对多”“多对一” 映射，导致路径稀疏与长尾分布，制约 GR 的表征能力与性能。

* ##### 3.2 沙漏现象的成因

  - **RQ 的内在机制**：
    1. 第一层：输入向量均匀分布→聚类后每个码本桶含 N/M 个点，令牌入度相等；
    2. 第二层：输入为第一层残差，小幅度点（近聚类中心）占比高（N×ρ）、outliers 少，聚类为降低损失偏向 outliers→小幅度点共享少量令牌，形成长尾；
    3. 第三层：残差重回均匀→令牌分布均匀，第二层集中令牌 “发散” 为多个令牌，形成 “多对一→一对多” 结构。
  - **数据分布加剧**：若输入向量为长尾分布（如电商商品的热门 / 冷门分布），残差分布更不均，沙漏现象更严重。

* 沙漏现象会显著制约生成式检索（GR）性能
  * **性能偏差实验**：基于 LLaMA2-0.8B（RQ3x12），将测试集按第二层令牌分为 “头部” 与 “尾部”，头部测试集的 Recall@1 达 0.3617，而尾部仅 0.2131，差距显著（如表 1 所示），证明沙漏现象导致模型偏向头部令牌，忽略尾部；
  * **层干预实验**：① 固定第一层令牌、预测第二 / 三层：Recall@1 从 0.2480→0.340；② 交换第一 / 二层 + 固定第一层：Recall@1 骤升至 0.6600，说明消除中间层长尾后性能大幅提升；
  * **一致性验证**：在 Qwen1.5-7B、Baichuan2-7B 等不同规模模型，以及 RQ3x11、RQ4x12 等不同 RQ 参数下，均观察到 “头部令牌性能优于尾部”“层干预后性能提升” 的规律，证明沙漏现象对 GR 性能的制约具有跨模型、跨参数的一致性。

* 解决方案：两种缓解方法

| 方法类型       | 操作细节                                                     | 空间容量变化                  | 优势                   | 不足                                         |
| -------------- | ------------------------------------------------------------ | ----------------------------- | ---------------------- | -------------------------------------------- |
| 启发式方法     | 生成 L 层 SID 后，直接移除第二层                             | Mᴸ → Mᴸ⁻¹（如 RQ3x12→RQ2x12） | 简单直接，消除长尾影响 | 空间容量不足，可能丢失信息                   |
| 自适应变长令牌 | 按阈值 p 移除第二层顶部 K 个令牌（如 top@400），SID 变为变长结构 | Mᴸ → Mᴸ+K(Mᴸ⁻²-Mᴸ⁻¹)          | 保持分布一致，容量充足 | 无法完全消除沙漏现象，需 ablation 测试确定 K |

* 实验结果：
  * 最优：移除第二层 top@400

#### OPQ 用于推荐

##### PQ

PQ 的核心思想是“分而治之”，将高维向量分解到多个低维子空间中进行量化。

1. **向量分解**: 将一个 D 维向量 `x ∈ R^D` 切分为 `M` 个 `D* = D/M` 维的子向量。   `x = [x_1, x_2, ..., x_M]`, 其中 `x_m ∈ R^{D*}`

2.  **子空间量化**: 为每个子空间 `m` 学习一个独立的码本 `C_m`，其中包含 `K` 个码字 `c_{m,j}`。子向量 `x_m` 被其在码本 `C_m` 中的最近邻码字量化。\n    `q_m(x_m) = argmin_{c ∈ C_m} ||x_m - c||_2^2`

3.  **编码与学习**: 向量 `x` 的编码由 `M` 个码字索引构成，学习目标是最小化均方量化误差，这等价于在每个子空间独立运行 k-means。\n    `E_{PQ} = E[||x - q(x)||^2] = E[\sum_{m=1}^{M} ||x_m - q_m(x_m)||^2]`

PQ 的主要局限在于，固定的维度切分方式可能不是最优的，会破坏维度间的相关性，导致较大的量化误差。

##### OPQ: Optimized Product Quantization

* OPQ 通过在量化前学习一个最优的旋转矩阵来解决 PQ 的局限。
  * **向量旋转**: 引入一个正交矩阵 `R ∈ R^{D×D}` ( `R^T R = I` )，先对向量进行旋转：`y = Rx`。
  * **学习目标**: 联合优化旋转矩阵 `R` 和所有码本 `{C_m}`，以最小化旋转后向量的量化误差。
    *   `E_{OPQ} = argmin_{R, {C_m}} E[||Rx - q(Rx)||^2]`
  *  **交替优化**: 该问题通过交替迭代解决：
    * **固定 R, 优化 {C_m}**: 此步等同于对旋转后的向量 `{Rx_i}` 进行标准的 PQ 学习。
    * **固定 {C_m}, 优化 R**: 此步变为一个经典的正交普鲁克问题 $$argmin_R \sum ||Rx_i - z_i||^2$$ (其中 `z_i` 是 `Rx_i` 的量化版本)。其闭式解可通过对 $$Z^T X$$ 进行 SVD 分解得到 $$R = VU^T$$。
    * 通过迭代，OPQ 找到的旋转 `R` 能使向量能量在各子空间分布更均衡，从而显著降低量化误差，提升检索精度。

### 端到端生成式推荐

#### Intro

* 生成方式
  * 直接生成：容易生成重复items，原因是生成的list没有实际被展现
  * beam search

* 生成式loss
  * 正例，inbatch softmax
  * 最后一层Item 0 emb和target emb相似度loss
  * token scaling（loss系数逐渐增加）

##### 生成式 v.s. 判别式

> LUM paper

* 建模方式：
  * 生成式 p(x,y)，判别式 p(y|x)
* 生成式的问题：
  * **Inconsistency Between Generative Training and Discriminative Application**
  * **Efficiency Challenges**
  * **Lack of Flexibility**：The inherent structure of E2E-GRs necessitates that any modification to the input schema, including the addition of new elements, triggers a requirement for retraining the entire model.
  * **Limited Compatibility**



#### [Meta] HSTU

https://arxiv.org/pdf/2402.17152v1

> - Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
> - 算法创新点：
>   - 将用户行为视作一种新模态
>   - 改变了特征排列（序列构造方式）
>   - HSTU模型结构
>   - 将target item做进了模型底座
> - 工程创新点：序列采样、 M-FALCON、激进的kernel fusion、casual mask（KV cache）

* Intro
  * reformulate recommendation problems as sequential transduction tasks within a generative modeling framework
  * HSTU
  * power-law of training compute
* 分析难以scale的原因
  * heterogeneous features的重要性大
  * A billion-scale dynamic vocabulary，候选多
  * 成本大：recommendation systems need to handle a few orders of magnitude more tokens per day than what language models process over 1-2 months.
    * GPT-3 was trained on a total of 300B tokens over a period of 1-2 months with thousands of GPUs

* In this work, **we treat user actions as a new modality in generative modeling**
  * core ranking and retrieval tasks in industrial-scale recommenders can be cast as generative modeling problems given an appropriate new feature space
  * this paradigm enables us to systematically leverage redundancies in features, training, and inference to improve efficiency
  * --> **three orders of magnitude more computationally complex** than prior state-of-the-art,

* Recommendation as Sequential Transduction Tasks: From DLRMs to GRs
  * Generative Recommenders (GRs)
    * 本质上是用transformer学一个hidden embedding
  * sparse features：
    * **Target item从底层引入**
  * dense features:
    * an important observation is that the categorical features (e.g., item topics, locations) over which we perform these aggregations are already sequentialized and encoded in GRs. Hence, we can remove numerical features in GRs given a sufficiently expressive sequential transduction architecture coupled with a target-aware formulation
  * 顺序转换任务
    * 按时间merge主序列和user profile序列
  * 辅助时间序列 - 随时间缓慢变化的时间序列
    * 只在变的时候merge进去
  * 当下一个token表示与参与无关的(non-engagement related)分类特征（例如人口统计学特征）时，$$y_i$$ 未定义, 对于这些情况，我们将 $$m_i$$ 设置为 0。
  * 精排：**内容位置的预测**转换为**多任务预测**
    * casual mask: https://zhuanlan.zhihu.com/p/698447429

![image-20240716221553540](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/hstu1.png)

* HSTU
  * Pointwise aggregated attention
    * HSTU在Transformer中采用了一种新的点对点（pointwise）聚集注意力机制，而不是softmax注意力。这是出于两个因素的考虑。
    * 在推荐系统中，与目标相关的先前数据点的**数量**作为一个强大的特征，指示用户偏好的强度，在经过softmax归一化后很难捕捉到。这一点很关键，因为我们需要预测参与度的强度，例如在给定item上花费的时间，以及item的相对顺序，再例如预测候选的排序以最大化AUC。
    * 虽然softmax激活函数对噪声具有鲁棒性，但它不太适合流式设置中的非平稳词汇表。
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250630204627133.png" alt="image-20250630204627133" style="zoom:50%;" />
  * 通过随机长度（Stochastic Length，SL）进一步从算法上增加用户历史序列的稀疏性
    * 对用户序列做采样：
      * 一种说法：在一个user的request/session结束时，以1/n的概率采样这个user，其中n是这个user的序列长度。
      * 另一种说法：一个session采样一次

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/hstu2.png" alt="image-20240716222635364" style="zoom:67%;" />

* 工程优化
  * 优化activations的内存占用
  * 单kernel
* 实验insight
  * 生成式推荐模型与LLM一样遵循scaling law，但传统推荐模型不遵循
  * 同等参数量的情况下，在参数达到一定规模的threshold后，生成式推荐模型才能有比传统推荐模型更好的效果。精排模型需要比召回模型更大的threshold(约100x)
  * Scaling law的最大配置时：8,192 sequence length, 1,024 embedding dimension, 24 layers of HSTU。**对精排模型，约在最大配置的1/10处，GR表现超过传统模型，对应的配置约为：4000 sequence length, 1,024 embedding dimension, 6 layers**
  * training flops的scaling law，且传统DLRM不具备该特性
    * ![image-20250703193457393](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250703193457393.png)
* Question
  * 用户token数量n_i 和用户的时序行为数量（上张图中，老推荐模型的时序样本数量）是什么关系？
  * 为什么在用户session结束时生成样本，相当于做采样？

##### M-Falcon

> M-Falcon: **M**icrobatched-**F**ast **A**ttention **L**everaging **C**acheable Operati**oN**s

* to perform inference for m candidates with an input sequence size of n
* We optionally divide the overall m candidates into ⌈m/bm⌉ microbatches of size bm to leverage encoder-level KV caching (Pope et al., 2022) either across forward passes to reduce cost, or across requests to minimize tail latency
* 实现思路：
  - 通过图中的“X”号，让候选Item之间不互相感知，从而实现Batch Inference
    - $$O(mn^2d) \rightarrow O((n+m)^2d)$$) 
  - 利用MicroBatch，令m可忽略
    - $$O((n+m)^2d) \rightarrow O(n^2d)$$

![image-20250630205946478](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250630205946478.png)

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250630205347181.png" alt="image-20250630205347181" style="zoom:50%;" />

* Cross-Request KV Cache

#### 字节 HLLM

> HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling
>
> https://github.com/bytedance/HLLM/tree/main
>
> - **思路：Item LLM和User LLM一起训，本质是用推荐广告的数据，训练一个多模态模型的基座**
>   - 模型1 (Item LLM): ItemEmb=F(Item)
>   - 模型2 (User LLM): UserProfile=G(List[ItemEmb])
> - 对比 HSTU：the model architecture is upgraded to **large language models with pre-trained weights**, and the **input features are changed from** **IDs** **to text-input** **LLM** **features**

> - Load&Serving OpenSource LLM Ckpt
>   - 用于初始化User LLM和Item LLM的weight
> - HLLM的训练Pipeline支持：
>   - 先Item LLM和User LLM一起训：user seq len：150
>   - 再单独训User LLM：seq len 1000
> - HLLM的online embedding cache
>   - 新Item/用户行为，触发Cache Emb
> - 模型输入：
>   - 支持原始文本（dense feature），无需ID化
>   - 需要支持Timestamp特征

* three critical questions remain under-explored:
  * firstly, the real value of LLMs’ pre-trained weights, often considered to en-
    capsulate world knowledge;
  * **secondly, the necessity of finetuning for recommendation tasks;**
  * lastly, whether LLMs can exhibit the same scalability benefits in recommendation systems as they do in other domains.

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241228023941859.png" alt="image-20241228023941859" style="zoom:50%;" />

* Item LLM

  * Inspired by previous works (Devlin 2018; Neelakantan et al. 2022), **a special token [ITEM] is added at the end of the item’s text description to extract features.**

* User LLM

  * discard the word embeddings from the pre-trained LLM but **retain all other pre-trained weights.**
  * Experiments show that these pre-trained weights are very helpful for reasoning user interests.

* 训练

  * 生成式：本质上是生成一个embedding，用InfoNCE学习
    * ![image-20241228124429924](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241228124429924.png)
    * s is the similarity function with a learnable temperature parameter
  * 判别式
    * ![image-20241228124550514](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241228124550514.png)
    * ![image-20241228124712454](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241228124712454.png)

* 训练pipeline：

  * 先一起训：user seq len：150
  * 单独训User LLM：seq len 1000

* online serving：利用触发 + Cache，减少计算量

  - Item实时触发HLLM Emb产出
  - User天级触发HLLM Emb产出（也可以做成实时）
  - 产出后存入Cache

  * ![image-20241228132412134](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241228132412134.png)

* 相比HSTU的改动：

  * the model architecture is upgraded to large language models with pre-trained weights,
    and the input features are changed from IDs to text-input LLM features

* 参数：

  * LLM的pretrained token num：3T
  * TinyLLM：1.1B Params

* 实验结论：

  * SFT mainly enhances instruction-following abilities, which do not aid in recommendation tasks (Zhou et al. 2024)
  * data scaling能力比HSTU强
    * ![image-20241228131044343](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241228131044343.png)

  * 附录：
    * [ITEM] Token的方式效果比mean pooling好
    * LLM Emb + Timestamp效果非常好
      * ![image-20241228133059538](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241228133059538.png)
    * 把ItemLLM Emb和ItemId Emb加起来，没效果，估计是text能丰富表达Item



#### OneRec 系列 [快手]

> high-level https://zhuanlan.zhihu.com/p/1918350919508140128
>
> OneRec
>
> OneRec TechReport
>
> OneRec-v2
>
> 总结来说，OneRec是和长序列建模很不一样的路径，encoder中蒸馏进了大量的特征信息，然后用生成式的方式去生成token

OneRec 系列是快手团队提出的端到端生成式推荐框架，旨在解决传统级联推荐架构的计算碎片化与目标不一致问题。其中，OneRec 首次以统一生成模型替代多阶段检索-排序流程；OneRec-v2 针对其计算瓶颈与奖励模型局限，提出懒解码-only架构，大幅提升效率与扩展性；OneRec-TechReport 则全面阐述了其架构细节、工程落地与多场景效果。

##### OneRec

> 2502.18965v1

*   **核心目标**: 以统一生成模型替代传统“检索-预排序-排序”级联框架，实现端到端检索-排序一体化，解决各阶段独立优化、性能受限的问题。
*   **核心技术**:
    *   **架构**: 采用编码器-解码器结构，并引入稀疏MoE提升模型容量。
        *   RQ-Kmeans
    *   **生成方式**: 采用会话级生成（Session-wise Generation），一次生成5-10个视频的完整序列，自主学习物品间的关联性与多样性。
    *   **偏好对齐**:
        *   设计迭代偏好对齐（IPA）
        *   通过奖励模型（RM）对beam search结果排序
        *   在构建偏好对(preference pairs)时，我们从难负采样(hard negative sampling)中获得灵感，从beam search结果中创建self-hard rejected samples，而非进行随机采样
        *   再利用DPO进行优化
*   **核心结果**: 在快手主场景实现 **1.6%** 观看时长提升。【其实是降级流量场景】



##### OneRec-TechReport

> 2506.13695v4
>
> 协同信号pairs和caption生成任务训练embedding -> RQ-KMeans做量化 -> 生成式预测sid token

*   **核心目标**: 全面阐述OneRec的架构细节、训练流程与工程落地，解决传统级联架构的计算碎片化、目标冲突、技术脱节三大痛点。

![image-20251107173243703](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251107173243703.png)

*   **Tokenizer**: 采用 **RQ-Kmeans**，融合多模态特征与协作信号，生成3层粗-细粒度的语义ID。
    *   ![image-20251107174131601](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251107174131601.png)
    *   对于协作信号，可以通过以下方式构建高质量的物品对数据集$$\mathcal{D}_{Pair}$$ ：
        - **U2I 检索(User CF)**：对于每个用户，选取一个被积极点击的目标物品，并将其与该用户最近历史积极点击记录中协作相似度最高的物品配对
        - **I2I** **检索(Item CF)**：将具有高相似度得分（例如，Swing 相似度）的物品进行配对
        - $$\mathcal{L}_{I2I} = -\frac{1}{\left| \mathcal{D}_{\text{pair}} \right|} \sum_{(i,j) \in \mathcal{D}_{\text{pair}}} \log \frac{ \exp\left( \text{sim}\left( \tilde{\mathbf{M}}_i, \tilde{\mathbf{M}}_j \right) / \tau \right) }{ \sum_{(i',j') \in \mathcal{D}_{\text{pair}}} \exp\left( \text{sim}\left( \tilde{\mathbf{M}}_{i'}, \tilde{\mathbf{M}}_{j'} \right) / \tau \right)}$$
*   **编码器**: 设计了**4条多尺度特征路径**（用户静态、短期、正反馈、终身路径），通过Transformer与QFormer深度压缩用户兴趣。
    *   有点类似传统的推荐模型，把大量特征都encoder进来，许多特征做了特殊处理，比如 select the item closest to each cluster center as the representative of that cluster.
*   **解码器**: 采用MoE增强的Transformer解码器。
    *   从 BOS token 开始，进行逐个的 SID 生成
    *   <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251107181811328.png" alt="image-20251107181811328" style="zoom:50%;" />
*   **奖励系统**: 设计了P-Score（多目标学习型分数）、格式奖励和工业奖励（如打压低质内容）的组合。
*   **工程与部署**:
    *   **效率**: 训练/推理MFU（模型FLOPs利用率）分别达23.7%/28.8%，OPEX（运营成本）仅为传统pipeline的10.6%。
    *   **效果**: 处理25%总QPS，快手/极速版App停留时间提升 **0.54% / 1.24%**，7日留存提升 **0.05% / 0.08%**，本地生活场景GMV增长 **21.01%**。

##### OneRec-v2

> 2508.20900v4

*   **核心目标**: 解决OneRec计算分配低效（97%计算用于编码）和依赖奖励模型的局限。
*   **核心技术**:
    *   **架构**: 提出**懒解码-only架构 (Lazy Decoder-Only)**，移除编码器，将计算资源集中于生成阶段，总计算量减少94%，支持模型扩展至8B参数。
    *   **偏好对齐**: 完全基于真实用户交互进行对齐，设计了**时长感知奖励塑造**（缓解视频时长偏见）和**自适应比例裁剪 (GBPO)**（提升RL训练稳定性），摆脱了对奖励模型的依赖。
*   **核心结果**: 在快手/快手极速版部署，App停留时间分别提升 **0.467%** 与 **0.741%**。

##### 版本对比

| 对比维度 | OneRec (v1) | OneRec-v2 | OneRec-TechReport |
| :--- | :--- | :--- | :--- |
| **核心定位** | 首次提出统一生成式框架 | 优化计算与奖励，提升扩展性 | 全面阐述架构、工程与落地 |
| **架构设计** | Encoder-Decoder + MoE | **Lazy Decoder-Only** + GQA | Encoder(多尺度) - Decoder(MoE) + RQ-Kmeans Tokenizer |
| **偏好对齐** | 奖励模型 + DPO | **真实用户交互** + GBPO | P-Score + 多维度奖励 |
| **模型规模** | 最大 1B | 最大 8B | 最大 2.6B (MoE) |
| **核心指标** | 观看时长 +1.6% | App停留 +0.467%/+0.741% | App停留 +0.54%/+1.24%，LT7 +0.05%/+0.08% |
| **工程成本** | 未提及 | 未提及 | OPEX为传统pipeline的 **10.6%** |

##### 核心问题演进 (Q&A)

1.  **OneRec v1到v2架构的核心调整，为何能支持模型从1B扩展至8B？**

    核心在于将“编码器-解码器”架构改为“**懒解码-only架构**”。v1架构中97.6%的计算资源被用于上下文编码，计算分配严重失衡。v2移除编码器，将用户上下文作为静态条件通过简化交叉注意力接入，使计算资源集中于生成，总计算量减少94%。这种效率提升打破了计算瓶颈，让模型在同等资源下可扩展至8B，并实现了规模与性能的同步提升。

2.  **OneRec系列的“偏好对齐”策略经历了怎样的演进？**

    演进路径清晰地体现了从“模拟”到“真实”的转变：
    *   **v1**: 依赖“**奖励模型+DPO**”。通过RM对模型生成的候选进行排序，构建偏好对进行优化，但存在模拟偏差。
    *   **TechReport**: 提出“**P-Score+多维度奖励**”。P-Score通过神经网络学习多目标的个性化融合，并加入格式、工业奖励，首次将对齐与真实业务目标结合。
    *   **v2**: 完全落地“**真实用户交互驱动**”。基于用户实际观看时长构建奖励，并用GBPO约束梯度，完全脱离对奖励模型的依赖，更贴合用户真实偏好。

3.  **TechReport相比v1，在“用户兴趣建模”上补充了哪些关键细节？**

    v1仅笼统提及编码器，TechReport则补充了精细化的“**多尺度特征工程**”设计：
    *   **拆分4条特征路径**: 覆盖不同时间尺度——**静态路径**（核心属性）、**短期路径**（即时偏好）、**正反馈路径**（深度兴趣）、**终身路径**（长期偏好）。
    *   **解决超长序列**: 针对终身路径的10万级原始序列，提出“聚类代表选择+特征聚合”策略，用QFormer等技术进行压缩。
    *   **多尺度融合**: 各路径特征通过统一Transformer编码，形成融合的用户兴趣表示，更能捕捉用户兴趣的动态性与全面性。

#### SpecGR

> Inductive Generative Recommendation via Retrieval-based Speculation (AAAI 2026)
> 解决生成式推荐 (GR) 无法推荐未见过的新物品 (Inductive Setting) 的问题。

*   **背景 & 问题**:
    *   现有的 GR 模型 (如 TIGER) 依赖于训练期间见过的 Semantic IDs。
    *   对于新物品 (New Items)，虽然可以分配 Semantic ID，但 GR 模型从未在自回归训练中见过其 ID 序列，导致生成概率极低 (Logits 偏向于旧物品)。
    *   导致 GR 在 Inductive Setting 下几乎不可用。

*   **核心思路**: **Draft-then-Verify (推测式解码)**
    *   借鉴 LLM 的 Speculative Decoding 思想，但用途不同：不为了加速，而是为了**引入新物品**。
    *   **Drafter (Inductive)**: 负责“猜”候选，必须具备 Inductive 能力 (如 KNN, LightGCN)，能召回新物品。
    *   **Verifier (GR)**: 负责“验”候选，利用 GR 强大的序列建模能力对 Drafter 的候选进行打分 (Query-Likelihood)。

*   **SpecGR 框架**:
    1.  **Inductive Drafting**: 使用 Inductive Drafter (如 UniSRec 或 GR 自身的 Encoder) 检索出 Top-K 候选 (包含新物品)。
    2.  **Target-aware Verifying**: GR 模型计算候选 Item 的 Semantic ID 的生成概率。
        *   *Trick*: 对于新物品，忽略 ID 的最后一位 (Identifier Token)，只计算语义前缀的概率，避免 OOV 问题。
    3.  **Guided Re-drafting**: 如果验证通过的候选不足 K 个，利用 GR 当前 Beam Search 生成的语义前缀，约束 Drafter 进行下一轮检索。

*   **SpecGR++ (Self-Drafting)**:
    *   为了避免维护额外的 Drafter 模型，直接复用 GR 的 **Encoder**。
    *   **训练**: 引入 **Contrastive Loss** (Item Embedding 与 Sequence Embedding 对齐)，使 GR Encoder 具备检索能力。
    *   **推理**: GR Encoder 产出 Embedding -> KNN 检索候选 -> GR Decoder 验证。
    *   **优势**: 参数高效，且 Encoder/Decoder 语义空间对齐，效果更好。

*   **效果**:
    *   首次赋予 GR 模型 Inductive 推荐能力。
    *   推理加速：验证过程是并行的 (Parallel Verification)，比纯自回归生成更快 (1.7x Speedup)。

##### 并行 Decode [Meta]

> Generating Long Semantic IDs in Parallel for Recommendation
>
> https://arxiv.org/pdf/2506.05781
>
> https://github.com/facebookresearch/RPG_KDD2025



#### GR中的数据增强

> 《Is contrastive learning necessary? a study of data augmentation vs contrastive learning in sequential recommendation》

- 业界GR实践中，发现序列长度不均（data sparsity issue）, 易导致模型序列部分参数训练不充分（insufficient training signals to learn informative item representations for the downstream recommendations）

  - https://arxiv.org/pdf/2403.11136

- 对短序列进行数据增强是常用手段

  - Semantic ID practitioner guide https://arxiv.org/pdf/2507.22224
  - https://arxiv.org/pdf/2403.11136

  * Sliding window 应用于 OneSearch
    * 为引导模型学习用户兴趣偏好的变化，研究人员将滑动窗口数据增强应用于短行为序列 ($$Seq_{short}$$）：
      * 通过沿用户短行为序列滑动窗口，每次生成一段新序列及其后续商品作为预测目标，并限制窗口最大长度，可针对短行为序列（如仅含 1 个商品的序列）生成m个增强样本（例如第一个样本无序列、第二个样本仅含 1 个商品$$s_1$$）。
      * 这种方式不仅能提升序列推荐与生成式检索模型的鲁棒性，还能通过对更短 subsequence 的训练，有效处理搜索历史有限的新用户（冷启动用户）
    * ![image-20251010232823540](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251010232823540.png)



#### OneSearch [kuaishou]

**OneSearch**是快手团队提出的首个工业部署电商搜索端到端生成框架，旨在解决传统多阶段级联架构（MCA）的**碎片化计算**与**优化目标冲突**问题；其核心创新包括：（1）**KHQE 模块**（关键词增强分层量化编码），保留商品层级语义与独特属性并强化查询 - 商品相关性；（2）**多视图用户行为序列（Mu-Seq）注入策略**，通过行为驱动用户 ID、显式短期序列与隐式长期序列全面建模用户偏好；（3）**PARS 偏好感知奖励系统**，结合多阶段监督微调与自适应奖励加权排序捕捉细粒度偏好

![image-20251010010219628](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251010010219628.png)

##### RQ-OPQ建模残差

##### short + long seq建模

##### 短序列增强

见「GR中的数据增强」

##### Preference-Aware Reward System（PARS） 多阶段SFT、自适应奖励与混合排序

* 阶段 1：语义对齐
  * SID 与文本 / 类别匹配	
  * ① 文本→SID；② SID→文本；③ 文本→类别
* 阶段 2：共现同步	
  * 学习 Q-I 内在语义关联	
  * ① 查询→商品（如 “运动鞋” 对应 “运动品牌”）；
  * ② 查询 SID→商品 SID（忽略用户特征）
* 阶段 3：个性化建模	
  * 结合用户特征生成目标商品	
  * 输入 “用户 ID + 查询 + 短期序列 + 长期嵌入”，输出商品 SID（滑动窗口数据增强）

- **自适应奖励与混合排序**：先通过奖励模型对生成结果重排序，再用列表级 DPO 训练。
- PARS 的平衡机制：
  1. **相关性约束**：在奖励模型中引入离线计算的**查询 - 商品相关性得分（S_Rel）**，并赋予 10 倍权重（公式：$$Rscore=\lambda_1·CTR+\lambda_2·CVR+\lambda_3·CTCVR+10·\lambda_4·S_{Rel}$$），确保生成结果不偏离查询意图；
  2. **个性化优化**：通过 6 级用户行为权重（如 “搜索购买” 权重 2.0、“点击” 权重 1.0）与 CTR/CVR 校准，捕捉细粒度偏好，再通过列表级 DPO 训练（区分正负样本偏好差异），结合 SFT 阶段的负对数似然损失优化，让模型优先推荐高偏好商品。

#### OneSug [kuaishou]

> https://mp.weixin.qq.com/s/xfxB6rtSsewRpqO3YPfyyQ

![image-20251027024159622](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251027024159622.png)

* **Prefix2Query Representation Enhancement**
  * 信息增强：利用和prefix共现过的queries增强prefix emb
  * 生成SID，生成queries

* **User Preference Alignment**：通过两种方式进行用户偏好对齐。

  * **将用户对query suggestion的反馈分为6个等级**，并设置不同的reward权重。6个等级如下表所示：
    * ![image-20251027024529371](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251027024529371.png)
    * [2.0, 1.5, 1.0, 0.5, 0.2, 0.0]
    * ![image-20251027024608771](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251027024608771.png)

  * **混合排名框架**。 作者认为，DPO是pair-wise loss，能学到预估的排序，缺乏对预估值的拟合能力。作者认为他们在公式6中引入的rw弥补了这一点。然后又引入了一个大于0的 作为正负样本的margin
    * ![image-20251027024702997](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251027024702997.png)


#### [todo] ColaRec



#### MiniOneRec: 开源复现

> https://arxiv.org/pdf/2510.24431
>
> https://github.com/AkaliKong/MiniOneRec


#### MIMIGenRec: 灵活的生成式推荐框架

> https://github.com/cocoshe/MIMIGenRec
>
> **全名**: Modular, Integrated, Mutable, Interchangeable GenRec

* **定位**: 旨在解决 MiniOneRec 等框架在配置复杂、代码冗长、基座模型切换难、召回无效 item 等方面的痛点。

* **核心特性**:

  1. **SFT 阶段**: 基于 **LlamaFactory** 构建
     * 支持上百种模型，通过 YAML 配置轻松切换
     * 支持 LoRA、全量微调，单卡/多卡，Zero 优化，Unsloth 后端
     * 内置 WandB 等实验监视器

  2. **RL 阶段**: 高度集成 **TRL** 和 Hugging Face 生态
     * 使用 `GRPOTrainer` + Wrapper 替代手写 Trainer + GRPO
     * Accelerate 支持简单的单多卡切换、Zero 优化切换
     * 自定义奖励（如 NDCG、HR）用于策略优化

  3. **灵活的 Trie 设计**
     * 动态构建 Logits Processor
     * 抛弃固定 template 写法，根据 tokenizer 动态构建
     * 根据 template 自动构建 Trie 树，避免报错

  4. **解码配置修复**
     * 强制以最高优先级传入解码超参
     * 解决部分 instruct 模型自带 topk 较小导致 valid token 被过滤的问题

* **训练流程**:
  * **SFT**: `llamafactory-cli train` + YAML 配置
  * **RL**: 自定义 MIMIGenRec model wrapper + GRPOTrainer
  * **Rollout/Eval**: Trie 约束解码 + 自定义超参优先级

### 多阶段生成式：预训练 + 微调 / Contextualize LLM

#### Intro

* LUM 模型的优化核心要解决两个问题：
  * 问题一：Item 如何高效 Token 化。
    * 一方面 Item 规模相较 LLM 的 Token 词表过于庞大，另一方面如果参考初期文献直接文本化的做法对于长序列表达是个灾难，所以将语义信息压缩至小规模的 Token 非常有必要。目前 Token 化方法处于百花齐放中，包括语义 ID、LLM 总结、多模态表征等
  * 问题二：语义信息与协同信息如何高效融合
    * 虽然协同信息和语义信息的建模思路大同小异，都是在时序维度刻画 Token 之间的 “共现” 概率，但是背后的 Pattern 还是有很大差异。为了求解解耦可以各司其职，分层架构是理想方案，底层 Token 化聚焦语义信息的编码，上层 Transformer 结构聚焦协同信息的挖掘。如上，用户行为建模可以增强兴趣推理能力，并开启新的规模化路径。

#### BaseModel.ai

* BaseModel.ai https://www.basemodel.ai/
  * 和tiger对比：https://sair.synerise.com/basemodel-vs-tiger-for-sequential-recommendations/
  * multi purpose：https://sair.synerise.com/towards-a-multi-purpose-behavioral-model/
  * hstu：https://sair.synerise.com/basemodel-vs-meta-ais-hstu-for-sequential-recommendations/
  * https://docs.basemodel.ai/docs/the-how
  * On MovieLens-1M, **BaseModel had a clear advantage over both SASRec and HSTU**, and on MovieLens-20M both **BaseModel and HSTU** **were tied**, while SASRec remained far behind.

#### User-LLM: 文本模态训推荐任务

> USER-LLM: Efficient LLM Contextualization with User Embeddings
>
> 本质上是输入和输出都是文本模态，联合文本模态的encoder和decoder，和推荐系统的预测任务一起训练

* 本文提出**USER-LLM**框架，通过**user embeddings** 实现 LLM 的高效个性化 contextualization。该框架利用**自监督预训练的用户编码器**（如自回归 Transformer）从用户交互序列中提取潜在行为和兴趣嵌入，再通过**跨注意力（cross-attention）** 将其与 LLM 整合，相比文本提示方法实现**78.1 倍的推理加速**和最高**16.33% 的性能提升**。在 MovieLens20M、Amazon Review 和 Google Local Review 数据集上的实验表明，USER-LLM 在长序列用户行为理解、下一项预测、偏好类别预测和评论生成等任务中表现优异，且通过**Perceiver 层**进一步优化计算效率。

* 算法：当 LLM 被冻结时，其自注意力层、前馈网络等核心参数保持不变，但 USER-LLM 会**在 LLM 的 Transformer 层之间插入独立的投影层 + cross-attention 层**，因此可以只训练插入的结构（冻结了LLM）
  * ![image-20250724200327772](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724200327772.png)
  * 一系列效率提升手段，使推理 FLOPs 减少最高 78.1X：
    * user emb将用户历史压缩为固定长度向量（如 32 token），大幅减少 LLM 输入长度；
    * cross-attn仅在 LLM 中间层整合用户嵌入，避免长文本上下文的注意力计算开销
    * 引入 Perceiver 层进一步压缩嵌入序列，减少 token 数量（如从 50 减至 16）
  * **gated cross-attention**：引入可训练标量$$\alpha_i$$控制用户嵌入的影响权重：
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724200254623.png" alt="image-20250724200254623" style="zoom:50%;" />

* 结论：

  * **Enc approach (tuning encoder and projection layers and freezing the LLM)** 在效率和效果上的折中较好
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724191951036.png" alt="image-20250724191951036" style="zoom:50%;" />

  * ![image-20250724201230375](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724201230375.png)

#### LUM [Alibaba]: 生成兴趣token输入下游判别式模型

> Unlocking Scaling Law in Industrial Recommendation Systems with a Three-step Paradigm based Large User Model
>
> 淘天集团的算法技术-未来生活实验室 和 阿里妈妈-搜索广告团队
>
> 阿里妈妈搜索广告2024大模型思考与实践 https://mp.weixin.qq.com/s/hgs_BzFZdjrDSbf9d74ilA
>
> 本质上是用一个自回归大模型抽兴趣特征 soft embedding，给各类下游使用

##### LUM: Large User Model + Conditional Token 支持多场景

* 前序阶段参考 LLM 模型架构设计自回归生成式任务 ——Next Item Prediction，旨在从用户行为序列中以数据驱动的方式学习协同过滤模式，该阶段专注下游行为预测类模型的可迁移性。CTR 模型则依赖 LUM 的推理结果，进行 Target-Attention，除了传统的从历史行为中提取兴趣以外，还将从推理的未来信息中挖掘潜在兴趣，该方式高效融合了生成式与判别式任务的各自特点。

![image-20251211004953918](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211004953918.png)

* Conditional token + item token
  * seamlessly trigger the relevant knowledge from the LUM into the discriminative tasks by specifying various conditions during the second step of the process

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211013113653.png" alt="image-20251211013113653" style="zoom:50%;" />

* ![image-20251211013944221](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211013944221.png)
  * ![image-20251211023419402](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211023419402.png)
  * “下一条件的物品预测”（next-condition item prediction）：用看到 `c_l` 后的用户状态来预测紧随其后的 `i_l`。这在时间上是“上一时刻的状态”对“下一时刻目标”的标准 teacher-forcing 对齐，所以索引自然出现 `l-1 → l` 的一位偏移。
* 多场景，conditional token 有性能优化，类似 M-Falcon 合并 query 计算
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211023739996.png" alt="image-20251211023739996" style="zoom:50%;" />
* 超参
  * 模型最大 7B
* 结论：好于 HSTU
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211025648586.png" alt="image-20251211025648586" style="zoom:50%;" />
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211030202818.png" alt="image-20251211030202818" style="zoom:50%;" />
  * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251211031600512.png" alt="image-20251211031600512" style="zoom:67%;" />
* 工程细节
  * The sequence length is set to 4096, and the model sizes of LUM and E2E-GR range from 0.5 billion to 14 billion parameters. All models are trained on 128 GPUs
  * trigger预计算



##### 评估：UQABench，序列理解+动作预测+兴趣感知

> [LLM如何高效理解用户？淘天发布首个基于用户表征的问答基准UQABench](https://mp.weixin.qq.com/s/Ug5sGlFFuLfkcImK_rrVfA)

**1. 序列理解：**

分为直接特征理解和match类特征理解。前者要求模型回答用户序列中一些显而易见的特征，例如「用户最近点击的三个商品的品牌分别是什么」，而后者要求模型回答一些交叉类的特征，例如「用户共点击过多少次手机类商品」。序列理解任务涉及使用LLM从用户嵌入中提取和恢复历史用户信息。目标是评估用户嵌入在多大程度上可以作为桥梁，将用户交互序列中的必要信息传递给LLM。这个任务关系到在LLM时代用户嵌入是否可以替代大量的用户侧特征工程。

**2. 动作预测：**

预测用户下一个要点击的商品和要点击商品的属性，例如「基于用户的浏览历史，该用户下一个要点击的商品的标题是什么」。该任务的目标是评估用户嵌入如何能够帮助LLM完成诸如Top-k推荐和点击率（CTR）预测等传统工业推荐系统任务，这与电商平台的收入密切相关。

**3. 兴趣感知：**

预测用户的短期兴趣、长期兴趣以及兴趣的变化轨迹，例如「用户最喜欢的品牌是什么」或是「用户近期最喜欢什么类目的商品」。这反映了基于LLM做推荐的方法的愿景：准确理解用户兴趣和提升用户体验。基于LLM的推荐系统相比传统推荐系统的一个革命性进步是在引入显著的多样性方面。受限于训练范式和协同过滤框架，传统推荐系统往往集中在热门项目和频繁互动的用户上。研究人员希望用户嵌入能够帮助基于LLM的方法召回多样的用户兴趣项目，从而提高个性化并增强用户体验

#### URM [阿里妈妈] —— LLM + 矩阵分解 + 概率采样召回

> Large Language Model as Universal Retriever in Industrial-Scale Recommender System
>
> https://zhuanlan.zhihu.com/p/1900899926776471607
>
> 阿里妈妈 世界知识大模型，支持搜推一体化

* 技术要点：LLM做推荐系统的retrieval任务，且支持text instructions能力，能支持不同的业务目标，因此对主流LLM结构做了以下改进：

  * **multi-query representation**：URM significantly boosts the expressive capacity of generative retrieval models.
    * 理解：推荐任务对“细节”的要求高于LLM
  * **Matrix decomposition**： further enhances URM’s learnability, discriminability, and transferability
  * **probabilistic sampling**： effectively reduces training and generation costs when faced with an extremely large candidate set

* 推荐系统的多目标： We define objectives using different text descriptions, such as:

  * (1) Multi-scenarios: Please retrieve items for scenario A/B/C. 
  * (2) Multi-behaviors: Please retrieve items that
    the user will click on / purchase / favorite. 
  * (3) Long-tail item: Please retrieve long-tail items.
  * (4) Serendipity: Please retrieve items from new categories. 
  * (5) Long-term interest: Please find items that match user’s long-term interests. 
  * (6) Search: Please retrieve items that match the given query. We construct the training set with specific positive samples under these constraints to ensure that the model generates appropriate results for each objective
    * **--> 支持搜推一体化**

* 模型：

  * 多个query token
  * 输入emb的处理：1）混合表征；2）Position Emb；3）ID和文本混合
  * 输出emb的处理：1）多个output token；2）过一个大MLP；3）兴趣层面max pooling
    - $$max[W^TF(u,o),axis=1],\ W^T:|C|*D,\ F:D*M$$
  * 思考：W本质是也是一个大的item词表，这一设计在输入域和输出域，分别维护item emb，也许是因为输入域更偏文本，输出域更偏推荐
  * ![image-20250725142729304](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725142729304.png)

* 输入侧：物品 Embedding 的生成

  - The item title is {TITLE}. The category is {CATEGORY}. The price is {PRICE}. The shop name is {SHOP}. Over the past 7 days, its sales volume is {SALE} and click-through rate is {CTR}.

  * ![image-20250725151453367](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725151453367.png)

* 输出侧：对item表征矩阵W进行分解

  - $$W=U(V_{dis}+V_{trans})^T,\  W: D*|C|,\ U: D*H,\ V: H*|C|$$
    - D：emb dim
    - C：样本corpus
  - $$V_{trans}=Emb(text\_rep)*W_{content$$

* Probabilistic sampling:

  * 要解决的问题：召回时要对全量商品的item emb做线性映射，W的计算量太大
  * 思路：
    - 训练时，Noise NCE loss: 《Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.》
    - 推理时，迭代式的Probabilistic sampling
  * 理论依据：假设在映射矩阵W中距离接近的物品，其得分$$P(v|u, o$$也相似，通过邻居扩展保证高得分物品被覆盖（Appendix A.3）。
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725145200996.png" alt="image-20250725145200996" style="zoom:50%;" />
    * ![image-20250725145628367](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725145628367.png)
    * <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250801145427056.png" alt="image-20250801145427056" style="zoom: 50%;" />

* online serving

  * ![image-20250725145925199](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725145925199.png)
  * 核心是按10min的时间窗口，异步触发URM的计算，cache候选子集，用于后续轻量的在线召回计算

* 结论：

  * 指标SOTA
    * ![image-20250725150630376](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725150630376.png)

  * zero-shot调整任务目标的能力
    * ![img](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=YThiM2VjODk3ZGQ2ODVjODdjOTgwNTc0NzE4ODFjYWVfZ2ZueW5hT3ZxcGNqTEM5SFhoeVM5WjVaZXVUZVJJcjRfVG9rZW46TEw0cGI2RkMwb1JPMXh4MkVGa2NlTVNubldmXzE3NTQwMzE0Nzc6MTc1NDAzNTA3N19WNA)
    * ![image-20250801145838283](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250801145838283.png)

#### GPSD [Alibaba]: 从生成式迁移到判别式

* 本文提出**GPSD（Generative Pretraining for Scalable Discriminative Recommendation）** 框架，旨在解决判别式推荐模型（如 CTR、CVR 预测）的**过拟合问题**—— 包括**one-epoch overfitting**（ epoch 转换时性能骤降）和**within-one-epoch overfitting**（首 epoch 内验证性能停滞）。GPSD 通过生成式预训练（基于 Transformer 的自回归模型，预测下一个物品）学习参数，再通过**稀疏参数冻结策略**（冻结嵌入表等稀疏参数，仅微调稠密参数）初始化判别式模型。实验表明，GPSD 在工业数据集（如 CTR、CVR）和公共数据集（如 Taobao、Amazon Electronics）上显著提升性能（AUC 相对提升 2.36%-17.31%），且模型规模从 13K 扩展到 0.3B 稠密参数时，性能遵循**幂律（power laws）** 持续提升，为推荐模型与语言模型的架构统一奠定基础。
* 核心问题
  - 过拟合现象：两种类型过拟合阻碍模型性能提升（图 1a）：
    - **one-epoch overfitting**：epoch 转换时验证性能骤降，由特征稀疏性导致；
    - **within-one-epoch overfitting**：首 epoch 内训练性能持续提升但验证性能停滞。
  - **模型缩放无效**：增大模型规模时，判别式模型性能无显著提升，与语言模型的缩放律相悖（图 1b）。
  - <img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724185356955.png" alt="image-20250724185356955" style="zoom:50%;" />
* **关键观察**：生成式模型（自回归预测下一个物品）无明显过拟合，因大量随机负采样稳定了稀疏参数训练。
  * sparse parameters learning is problematic in discriminative training

* 从生成式模型迁移参数到判别式模型
  * ![image-20250724185507697](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250724185507697.png)
* **问题**：五种桥接策略中，哪种性能最优？其优势体现在哪些场景？
  * **答案**：ST&SF（Sparse Transfer & Sparse Freeze）和 FT&SF（Full Transfer & Sparse Freeze）策略性能最优。
    * ① ST&SF 在跨架构迁移（如从 Transformer 到 HSTU、Wukong）和增量训练中更灵活，适合工业场景；
    * ② FT&SF 在判别式数据集较小或模型规模较大时表现更优（表 3）。
    * 两者的共同优势是通过冻结稀疏参数，显著缩小了训练与验证性能的 generalization gap，使模型随规模扩大性能持续提升。

* 工程需求

  - 生成式模型的训练支持：
    - Transformer的生成式mask训练
    - paper中提到，也可以用HSTU等结构训练生成式模型，然后仅transfer sparse参数

  - Ckpt Save&Store：不同模型之间，Sparse参数的transfer

  - 多阶段训练的pipeline
    - 先multi-epoch训练生成式，再one-epoch训练判别式



#### todo [饿了么] IAK 多领域学习

https://mp.weixin.qq.com/s/wXYNOk7-Oak_N_bIord_bQ



### diffusion

https://arxiv.org/pdf/2304.04971



## LLM4Rec

> 核心问题：
>
> * 效果与泛化性的统一
> * 智能的外显性

### Intro

* https://github.com/WLiK/LLM4Rec-Awesome-Papers
* [LLM+Recommendation大模型推荐近期进展|含WWW, SIGIR, AAAI等顶会文章](https://mp.weixin.qq.com/s/m8DMgSt_r-HVNHHzA8ceVw)
* KDD 2024 工业界搜广推工作整理 https://mp.weixin.qq.com/s/io8bZRMTmt9rQ2pRh1T2pQ
* 一篇中文科普文章：https://36kr.com/p/2805108795192961
  * LLM MLSys比传统RecSys更通用
    * 传统RecSys涉及的中间件更多、更重
    * Langchain的调用流程通用性强
  * AI Paas引领推荐系统Saas由算法主导到工程主导的转型

![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2024%2F0604%2F94c56fc3j00sejlo6001bd200u000klg00hx00ca.jpg&thumbnail=660x2147483647&quality=80&type=jpg)

![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2024%2F0604%2Fa2c9deb7j00sejlo7002rd200u000npg00id00ei.jpg&thumbnail=660x2147483647&quality=80&type=jpg)

![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2024%2F0604%2Ff9887823j00sejlog005cd200u000i6g00hx00au.jpg&thumbnail=660x2147483647&quality=80&type=jpg)



#### [马坚鑫 阿里M6团队 MLNLP2023大模型与推荐系统论坛](https://www.bilibili.com/video/BV17u4y1N7zY)

* Qwen LLM介绍

  * 7B开源模型
  * ReAct prompting
    * 技巧：处理多轮问答上下文，将ReAct prompting贴在倒数第二个回答前面，而不是最后一个问题前，有助于模型理解上下文
  * 如何训练Agent能力
    * AI aligns AI
      * Step 0: 少量高质量数据，用作SFT和in-context examples
      * Step 1: self-instruct，即通过in-context prompt生成更多样本
      * Step 2: 规则处理，人工审核，（可选）人工改正
      * Step 3: 循环 Step 1-2

* LLM在RecSys的价值 —— 从生产者、平台、消费者视角

  * ![image-20240719185430334](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/llm-rec-text.png)

  * LLM + RecSys 有益于内容生产者

    * 核心诉求：流量，尤其是新内容的冷启动
    * ![image-20240719185656541](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/llm-recsys-1.png)

    * ali结果：小模型、少量样本、冷启动优秀

  * LLM + RecSys有益于推荐平台

    * 核心诉求：算法支持平台的运营企划
    * 时事热点：加快对事件的响应速度
      * 快中秋了 -> 推荐月饼，无需从行为学习
    * 人工干预：基于LLM的customized instruction/system prompt

  * LLM + RecSys有益于内容消费者

    * 推荐理由
    * 用户反馈
    * 导购、对话式推荐

* RecSys对LLM的挑战

  * 推理成本
    * 硬件
    * 工程：缓存；投机采样
    * 算法：大模型+小模型+规则；放弃通用；Linear Attn
      * e.g. GPT做数学题非常消耗token（CoT），但调用工具很轻量
    * UI/UX：好的产品设计能避开缺陷
      * e.g. chatGPT流式输出，巧妙地让生成长文的耗时可接受

* 模型结构尝试：

  * 延迟交互（late interaction）
    * 把浅层的cross attention干掉，高层再进行transformer的交互

![image-20240719191437165](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/llm-recsys-2.png)

* 挑战：模型更新 - RecSys每天都有新内容
  * 检索增强RAG的难点1：新知识易检索，新常识难检索
    * e.g. 有没有超过GPT-4的大模型
  * RAG的难点2：检索算法的精度、LLM的长序列支持
  * 预测：1-2年内会出现LLM的online learning

![image-20240719191754039](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/llm-recsys3.png)





### 电商 LLM4Rec

#### Amazon:  [基于大语言模型和推荐系统构建电商智能导购机器人](https://aws.amazon.com/cn/blogs/china/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system/)

* 基于 Amazon SageMaker、Amazon OpenSearch、AWS Lambda、Amazon Personalize 和 Amazon API Gateway 等基础云服务，结合大语言模型、开源大语言模型应用框架 langchain 以及开源前端架构 Stramlit
* 功能：智能问询、商品推荐、商品个性化营销文案
  * 多轮对话：挖掘用户需求，商品的品牌、价格、材质、用途、使用场景等角度
* 框架：
  * dynamodb存储“用户同session的对话记录”（类似OpenAI的thread概念）
* 测试集：https://github.com/aws-samples/retail-demo-store
  * 2000 多个虚拟商品数据、6000 多个虚拟顾客数据和 2 万多条虚拟交互信息

![build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system1](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system1.png)

![img](https://s3.cn-north-1.amazonaws.com.cn/awschinablog/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system3.png)

![img](https://s3.cn-north-1.amazonaws.com.cn/awschinablog/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system7.png)

![img](https://s3.cn-north-1.amazonaws.com.cn/awschinablog/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system8.png)

#### 阿里[LLM在电商推荐系统的探索与实践](https://www.53ai.com/news/qianyanjishu/357.html)、LLM4REC综述

> LLM+RS、LLM As RS
>
> 基于LLM知识能力的类目搭配推荐

* 对比RecSys和LLM：
  * 前者是一个数据驱动的系统，依赖电商ID体系来建模用户或物品，缺乏语义和外部知识信息，存在信息茧房、冷启动、多样性不足、无法跨域推荐等问题；
  * 后者缺乏推荐领域内的专有数据信息，不具备传统推荐模型的序列处理和记忆能力，同时计算复杂度高、训练和推理成本大。

* 两种范式：LLM+RS；LLM as RS
* LLM + RS
  * LLM Embedding: U-BERT[2]对用户评论内容进行编码来增强用户的个性化向量表征，最终得到稠密的embedding向量；UniSRec[3]通过对商品title/用户行为序列进行编码，来达成跨域序列推荐的目标。
  * LLM Summary:
    * 生成式新闻推荐框架GENRE[5]
    * GPT4Rec[6]将LLM模型用于用户意图理解，根据用户的行为历史，进行兴趣抽取并生成中间的语义query，用于后续的推荐召回。如下图所示，其主要分为两步：首先根据用户历史交互的商品和它们对应的标题，通过prompt格式化后，使用GPT2来生成可以表征用户多个兴趣的“search query”。然后将GPT2生成的query提供给搜索引擎，以检索要推荐的商品，从而提高推荐召回的相关性和多样性。

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkRGJlYzFpYTRhckl1N2ROcVVNNjFNTlhZZm03cU4wbTJtUEo5YWF1aWFxZ1A0TXY1TUJ3MzhkeXcvNjQwP3d4X2ZtdD1wbmc=)

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkaFJpYWljVVZINWJ5eDJpY1hMQzR2R0xXaFdxbkV0TERERFRNb1I2NkVDQ2c0R21XZ2dYb0N3YVlBLzY0MD93eF9mbXQ9cG5n)

* LLM As RS
  * LLM As Ranker
    * 此类工作[7] [8]将推荐问题形式化为给定条件的排序任务，其中用户的历史交互作为条件，推荐系统召回得到的商品作为候选。通过设计合适的prompt模版，结合条件、候选、排序指令，使得LLM为候选的商品进行打分或者排序。
    * 实验证明，LLM在Zero-Shot场景具有较好的零样本排序能力，但在排序时不可避免地有position bias和popularity bias问题。

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkNzI3aWFxb1ZXZnBNZHN3SWVmU2ljWjF2SGpVMlU3dk5nSjFFWUhNbjNpY1BTZVZqaWFUakVWZ3NkZy82NDA/d3hfZm10PXBuZw==)



* 算法方案：受限于LLM模型极大的推理耗时，无法满足在线推荐系统毫秒级的时延限制，短期内不具备将LLM模型用于在线推理的条件。于是我们更多地采用"LLM + 推荐"的方式，去利用大模型的知识和推理能力，提高推荐模型对商品信息、上下文、用户行为序列的知识表达，包括：
  * 借助LLM通用知识信息，构建类目搭配体系，引入推荐系统在推荐召回侧引入搭配I2I、排序侧进行类目兴趣扩展建模，提高推荐的多样性。
  * 借助LLM文本推理能力，辅助商品/用户理解。
    * 我们使用LLM将电商Item冗余文本信息进行去噪提纯和改写；
    * 结合用户行为序列、上下文以及用户画像，进行用户行为sumarry总结。并通过预训练语言模型，将文本知识结果进行embedding向量化表征，与传统的推荐模型进行知识感知嵌入，提高模型的知识表达。

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkSk1icEM1aWJER1FhUjdBN29udG5aZVhyTkt6T0hoSUgxQjJ3ZUFWTjJJTDhKdTE3NXk4NHRLdy82NDA/d3hfZm10PXBuZw==)

* 基于LLM知识能力的类目搭配推荐
  * 经过多年的沉淀，电商平台已经拥有了一套完整的类目体系。这套类目体系通常采用树状结构，通过层层拆解，最终将一个商品映射到一个末级类目，不同末级类目之间相对独立。现有的类目体系无法体现出这类目之间存在的搭配信息，缺乏跨类目的搭配关系表达。
  * 同时，相较于品牌和卖家，类目可以更加显式地与用户兴趣进行聚合和映射。在推荐场景之中，给用户准确地推荐相关商品的同时，如果能够挖掘不同兴趣之间的隐藏关系，基于搭配进行发散推荐，将给用户带来新的惊喜感、实现用户需求和兴趣的扩展。
  * 类目体系：休闲裤和衬衫分别属于一级类目（男装）下面的不同二级类目，而男帆布鞋又挂载在另一个一级类目（流行男鞋）上
  * 传统的类目关系挖掘往往基于知识图谱，采用距离度量、聚类、行业规则、协同过滤等方法。这些工作大都需要繁杂的数据清洗、算法挖掘和行业专家知识。LLM大模型的出现，让快速、高效的电商知识构建变成了现实。
  * Prompt:"1.用逗号分隔,2.返回格式为'''类目1,类目2,类目3...''',3.不包含【cate_name】这个词,4.搭配类目丰富"
  * 站内类目ID映射：由于LLM模型返回的是通用知识信息，存在与站内的类目体系无法完全对应的情况。为了便于后续推荐各个模块使用，兼容现有的电商推荐链路，我们进一步将LLM搭配类目映射成站内类目ID。站内类目ID映射可以采用以下两种方法：
    * 基于文本相关性的向量召回。将LLM搭配类目和站内类目分别表征成文本embedding向量，然后通过向量召回的方式，选取与LLM搭配类目距离空间最近的top站内类目进行映射。
    * 基于站内后验统计的query2cate映射。将搭配类目作为query，根据电商平台搜索query2cate的统计数据，使用该query下top的点击cate作为映射类目，实现LLM搭配到站内ID的映射。
  * 精排兴趣扩展

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkeWlhR2lhTlE3QzVVOWVkSGlhaE1EY0NOaWNWUTV6cUZQUTVrYWpZaWNoc2lhVU5KSXZKd1h5MUtKaWNhZy82NDA/d3hfZm10PXBuZw==)

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkVkIyMmVSWDJ2MjZzcEVub0JlWUI4Y0NIZ0x6eFFHRWxsQjZJSjgybGhzeW1OWTlmazdlQ0p3LzY0MD93eF9mbXQ9cG5n)



* **基于LLM文本能力的商品语义表征**
  * 对于商品类目以及属性信息，通常将其通过multi-hot的方式进行编码转化成特征向量。
    * 容易产生数据稀疏问题。
  * 商品标题语义上并不连贯，信息凌乱（包括“爆款”、“特价”等），直接进行mutli-hot或者文本编码难以得到很好的嵌入表示。
  * 一种可行的解决方案是将对商品零散的信息转换成语义连贯的文本，然后通过pre-train语言模型进行编码。对此，我们借助LLM蕴含的强大的语言表达能力和逻辑推理能力从商品标题中抽取出关键信息，从而实现对商品标题的正则化，得到语义连贯的文本描述，再对其进行编码，从而丰富商品的特征。
  * Prompt：你现在是一个买家。给定商品的描述词【A】以及各种属性【B】，请根据关键词和关键属性描述出商品是什么。要求是只需要回答是什么，不要补充其他内容，尽量从A和B中选出词语进行描述，字数不超过40，回答模版为:这个商品是...。比如当A=['giyo', '公路', '山地车', '专用', '自行车', '单车', '专业', '骑行', '手套', '半指', '夏季', '男', '硅胶', '减震', '女']，B=['尺码': 'XXL', '类目': '自行车手套', '适用对象': '通用', '颜色分类': '弧光半指-黄色-双面透气+GEL硅胶+劲厚掌垫', '上市时间': '2016年夏季', '货号': '1183', '品牌': 'GIYO/集优', '款式': '半指手套']，输出：这个商品是GIYO牌的自行车半指手套。现在A=...,B=...
  * 指标：平均困惑度 https://zhuanlan.zhihu.com/p/114432097

* 商品语义向量-引入排序模型：仅仅是加特征
  * 借助Modelscope的CoROM模型[15]，我们对正则化后的商品标题文本进行了向量化抽取，并作为特征加入基于双塔结构的DSSM粗排模型中[16]
    * https://www.modelscope.cn/models/damo/nlp_corom_sentence-embedding_chinese-base-ecom/summary
  * 特征降维方式是BERT-whitening[18]

* 更多方向：
  * 多模态推荐：利用多模态LLM大模型的多模态信息抽取和表征能力，提取包括图片、文本、视频关键帧，视频语音文字等不同模态的语义化信息，并通过离线特征工程进行表征，使线上推荐模型能够真正完整地感知到各种电商模态信息，并实现对用户不同信息偏好和意图的理解。
  * LLM推理加速：现阶段LLM存在推理时延过高的问题，无法满足推荐系统数十ms级别的rt要求，我们的LLM探索也止步于离线特征编码阶段。后续考虑通过蒸馏、剪枝、量化等手段，用一个小模型蒸馏出LLM的部分能力，从而降低推理的复杂性，使其能线上serving。
  * LLM as 重排: 利用LLM丰富的知识领域扩展能力，在商品已有丰富的语义标签基础上，结合用户历史交互兴趣、选择偏好、序列生成规则 和 prompt template为用户从top排序集合中选取合适的商品或话题，生成推荐列表。

#### 阿里云-施兴-推荐搜索技术的前沿探索

> https://github.com/alibaba/EasyRec/

![image-20241007223126666](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/pairec.png)

![image-20241007223250405](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/ali-ai.png)

![image-20241007223648967](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/easyrec.png)



![image-20241007223838777](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/pairec-opt.png)

![image-20241007224303869](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/ali-query-rewrite.png)

### 通用 LLM4Rec

> https://github.com/CHIANGEL/Awesome-LLM-for-RecSys
>
> https://github.com/WLiK/LLM4Rec-Awesome-Papers

#### Literature Review

* LLM增强数据 [hllm]
  * (Zhang et al. 2024a; Ren et al. 2024;
    Xi et al. 2023), such as summary of user behavior and item
    information expansion.
  * RLMRec (Ren et al. 2024) develops a user/item profiling paradigm em-
    powered by LLMs, and aligns the semantic space of LLMs
    with the representation space of collaborative relational sig-
    nals through a cross-view alignment framework.
  * LLMs are also employed to generate augmented training signals for
    coldstart items (Wang et al. 2024)

* LLMs as either feature encoders [9–24] [star]

  * 直接使用
    * star
    * [15]
  * mapping continuous LLM
    embeddings into discrete tokens using vector quantization and
    training a subsequent generative model [12, 13, 21, 22];
  * training sequential models by initializing the embedding layer with
    LLM embeddings [9, 14, 24];
  * training models to directly compute the relevance between item and user embeddings (i.e., embeddings of user selected items) [10, 11, 16–20, 23].

* LLM as scoring and ranking functions [25–31]. [star]

  * generative selection prompting, instructing the LLM to choose the top k items in ranked order from a set of candidates [25, 27, 28]
  * lag behind the performance of fine-tuned models due to a lack of collaborative knowledge
  * fine-tuning the models with interaction data, though this approach is also costly [40–45].

* LLM as a Ranker for Information Retrieval.[star]

  * 优势：
    * 构建简单
    * 顺便得到推荐解释
    * 少量交互量的数据下，效果好于传统模型
  * 劣势：
    * 延时高
    * 正常交互量的数据下，效果一般
    * 效果提升很难
  * point-wise: LLMs directly evaluate relevance using numerical scores or binary judgments [48, 49]
    * capturing the relative importance of passages
  * pair-wise: LLMs express preferences between item pairs
    * effective but inefficient due to the high number ofcalls required [50]
  * List-wise: LLMs compare multiple passages simultaneously [51],
    * performance heavily relies on the model’s semantic prior and
      reasoning capabilities [50]
  * adapt the recommendation domain data into conversational
    formats (Bao et al. 2023; Friedman et al. 2023; Zhang
    et al. 2023; Yang et al. 2023; Zhai et al. 2023). [HLLM]

* LLM接受ID Feature作为输入，并建模 [HLLM]

  * 改进处理文本行为序列耗时长的问题
  * LLaRA (Liao et al.2024) proposed a novel hybrid prompting method that inte-
    grates ID-based item embeddings with textual item features.
  * SFT mainly enhances instruction-following abilities, which
    do not aid in recommendation tasks (Zhou et al. 2024)

  * Ning et al. 2024; Zhai et al. 2024;

* * 

#### [LLMRec] Is ChatGPT a Good Recommender ? A Preliminary Study

> https://github.com/williamliujl/LLMRec

* Intro
  * taobao的尝试，Pretrained Model做推荐
    * M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems.
    * Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5)
* 实验结论：
  * 「未经finetune的ChatGPT」 performs well in rating prediction but poorly in sequential and direct recommendation tasks, achieving only similar performance levels to early
    baseline methods on certain metrics.
  * 人工评估结果，Explanation Generation、Review Sumarization效果较好
* 架构：
  * different prompts are constructed based on the specific characteristics of the recommendation tasks (Section 3.1)
  * these prompts are used as inputs for ChatGPT, which generates the recommendation results according to the requirements specified in the prompts
  * the output from ChatGPT is checked and refined by the refinement module, and the refined results are returned to the user as the final recommendation results (Section 3.2).
    * 检查gpt的输出是否符合格式
    * 如果输出item和item pool不匹配，则用BERT做相似度匹配

![image-20241003193718138](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/llmrec.png)

* 五种task
  * Rating Prediction
  * Sequential Recommendation
  * Direct Recommendation
  * Explanation Generation
  * Review Sumarization
* 实验设置：
  * 10 items、3 shots、gpt-3.5-turbo
  * direct rec：99负例、1正例
  * 指标：top-k Hit Ratio (HR@k), top-k Normalized Discounted Cumulative Gain (NDCG@k)
* 其它：
  * Figure2提供了针对不同recommendation task的一些prompt
* 结果：
  * rating predict效果还行
  * sequential predict效果不好：
    * focus more on semantic similarity rather than the transition relationships between items,
    * 无法把候选都输入prompt，输出了假数据
  * direct rec:
    * gpt有bias，更容易推荐prompt中排在前面和后面的item

![image-20241003202813843](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/llmrec1.png)

#### RecGPT [Alibaba]

> https://mp.weixin.qq.com/s/hfLXvMOhGBrddoQW-25IbQ
>
> https://arxiv.org/pdf/2507.22879
>
> todo 王喆解读 https://zhuanlan.zhihu.com/p/1963825375902699535

* **用户-商品-标签三塔检索框架**
* ![image-20250819111525913](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250819111525913.png)



#### GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation

> * Beam Search生成多query：似乎可被LLM能力替代

* Intro
  * we present GPT4Rec, a novel and flexible generative framework inspired by search engines.
    It first generates hypothetical "search queries" given item titles in a user’s history, and then retrieves items for recommendation by searching these queries.
  * a multi-query generation technique with beam search.

![image-20241005210152630](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/gpt4rec.png)

* 架构
  * GPT4Rec formats the item titles with a prompt and uses a generative language model
    to learn both item and user embeddings in the language space.
    The model then generates multiple queries that represent user’s
    interests, which will be fed to a search engine to retrieve items
    for recommendation.
  * prompt: "Previously, the customer has bought: <ITEM TITLE 1>. <ITEM TITLE 2>... In the future, the customer wants to buy"
  * beam search
  * BM25 matching score function [20], as it is one of the most widely used baseline search engines that accounts for the term frequency saturation and the document length with two corresponding parameters
  * **multi generation的算法**
    * ![image-20241005215520146](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/multi-generation.png)
* 训练细节
  * 基于对比学习的思想，T-1个推测第T个
  * 先训练好backbone，再调BM25的参数
  * ![image-20241005220901898](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241005220901898.png)



* 结论
  * The comparison with baseline methods suggests that both item
    content information and modern language modeling are key ingredients for achieving superior performance. One the one hand, while BERT4Rec has the best performance among the baseline methods by leveraging modern language modeling techniques, it fails to fully utilize the item content information by treating items as IDs. On the other hand, ContentRec’s use of item content information with bag- of-words embeddings and mean-pooling modeling is insufficient for achieving comparable performance.
  * In particular, generating K queries and retriev- ing one item per query yields the best performance of Recall@K. This finding suggests that each query contains enough detail to re- trieve a relevant item.
* 定性分析的角度
  * diversity：兴趣模糊时，推送新类别
  * coverage：兴趣固定时，推送固定类别

#### HiT-LBM [Kuaishou] LLM做行为兴趣建模

> Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model
>
> 核心思路是利用LLM抽取长期行为兴趣，并用“过程评分模型”，将LLM的基于prompt的兴趣抽取方式，一定程度上与推荐系统的业务目标进行对齐。
>
> “过程评分模型”是这篇paper的核心，可用于原生的LLM4Rec领域

##### 2.1 CUBE（Chunked User Behavior Extraction）

- **功能**：将用户长期行为按 LLM 上下文窗口大小分块（如长度$$L=50$$），实现级联式兴趣学习。
- **级联学习**：每块兴趣生成需结合前一块兴趣，通过提示词模板（$$prompt^{inst}$$）引导 LLM 捕捉当前兴趣及演化（示例见图 2）。
- **公式**：$$Inst _{i}= \begin{cases}LLM\left( prompt ^{i n s t}, Inst _{i-1}, B_{i}\right), & if i>1, \\ LLM\left( prompt ^{inst }, B_{i}\right), & otherwise. \end{cases}$$ 其中$$Inst_i$$为第i块的兴趣表示，$$B_i$$为第i块行为。

##### 2.2 HTS（Hierarchical Tree Search for Interests）

- **目的**：缓解级联式学习中的质量退化问题，确保每块兴趣的信息增益。
- 过程评分模型（3.3.1）：本质是再构建一些样本（比如采用不同兴趣时，实际auc是上升或下降），学习两个MLP，MLP的输入是
  - **SRM（Sequence Rating Model）**：评估当前兴趣与前K块兴趣的连续性（基于 AUC 对比）；
  - **PRM（Point Rating Model）**：评估当前兴趣对当前行为块的代表性（基于 AUC 对比）。
    - 将当前兴趣用LLM embedding化，输入PRM
- 层级树搜索
  - 每块行为生成N个候选兴趣（LLM 的 Best-of-N 采样）；
  - 用 SRM 和 PRM 评分，通过加权计算最终得分（$$S_{final}=\alpha S_c + (1-\alpha)S_e$$，$$\alpha=0.5$$）；
  - 选择最高分候选作为下一块的父节点，形成最优兴趣路径（算法 1）。

##### 2.3 TIF（Temporal-Ware Interest Fusion）

- **兴趣编码**：通过轻量级编码器（如 BGE）将文本兴趣转为稠密向量$$e_j$$。
  - 𝑒𝑗 = 𝐸𝑛𝑐𝑜𝑑𝑒𝑟 (𝐼𝑛𝑠𝑡𝑗 ) 
- 时间融合
  - 结合搜索得分和位置编码：$$e_j^* = e_j * S_{final}^j + pos_j$$；
  - 掩码自注意力（MSA）整合时序信息，保留时间顺序；
  - 交叉注意力结合物品知识（通过$$prompt^{item}$$提取），生成最终用户表示$$e^{user}$$。
- **应用**：作为辅助信息嵌入传统推荐模型，公式为：$$\hat{y}=f\left(r^{user }, r^{item }, e^{user }, e^{item } ; \theta\right)$$

### LLM Finetuning/对比学习

#### RecGPT [taobao]: 三塔

> [淘宝上线大模型推荐系统！淘宝联合人民大学推出RecGPT，有效改善推荐效果、缓解信息茧房](https://mp.weixin.qq.com/s/hfLXvMOhGBrddoQW-25IbQ)

* 利用item seq生成user interest tag
  * ![image-20251015005434513](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251015005434513.png)

* 三塔检索
  * ![image-20251015005507377](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251015005507377.png)

* 工程
  * Qwen3-14B
* LLM课程学习
  * ![image-20251015005805290](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251015005805290.png)
  * 兴趣挖掘、标签预测和推荐解释生成三大任务

#### NoteLLM [xiaohongshu] —— 用作i2i召回

> NoteLLM: A Retrievable Large Language Model for Note Recommendation

* **NoteLLM**是一种用于笔记推荐的可检索大型语言模型（LLM），旨在解决 item-to-item（I2I）笔记推荐任务。其核心创新在于通过**Note Compression Prompt**将笔记压缩为特殊 token，结合**生成式对比学习（GCL）** 学习相关笔记的嵌入，并利用**协作监督微调（CSFT）** 自动生成标签 / 类别，从而增强笔记嵌入质量。实验表明，NoteLLM 在小红书真实场景中表现优异，离线实验中 Recall@100 达 84.02%，显著优于 SentenceBERT（70.72%）等基线模型；在线实验中点击率提升 16.20%，且对低曝光和冷启动笔记推荐效果显著，验证了其在实际推荐系统中的有效性。

<img src="./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725160044134.png" alt="image-20250725160044134" style="zoom: 67%;" />

- **Note Compression Prompt**：
  - 统一模板整合 I2I 推荐与标签 / 类别生成任务
  - 结构为`[BOS]<Instruction> <Input Note> The compression word is:"[EMB]". <Output Guidance> <Output>[EOS]`，其中`[EMB]`为压缩笔记的特殊 token。
  - ![image-20250725192850480](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725192850480.png)
- 生成式对比学习（GCL）
  - 基于用户行为构建相关笔记对：
    - 计算共现分数$$s_{n_{A} \to n_{B}}=\sum_{i=1}^{U} \frac{1}{N_{i}}$$（U为用户数，$$N_i$$为第i用户点击笔记数）
      - 仅计算共现、考虑活跃程度的影响
    - 筛选 top-t 高分数笔记作为相关笔记。
  - 损失函数：$$L_{c l}=-\frac{1}{2 B} \sum_{i=1}^{2 B} log \frac{e^{sim\left(n_{i}, n_{i}^{+}\right) \cdot e^{\tau}}}{\sum_{j \in[2 B] \backslash\{i\}} e^{sim\left(n_{i}, n_{j}\right) \cdot e^{\tau}}}$$，通过对比学习获取协作信号。
- 协作监督微调（CSFT）
  - **联合训练标签 / 类别生成任务**，损失函数$$L_{g e n}=-\frac{1}{T} \sum_{i=1}^{T} log \left(p\left(o_{i} | o_{<i}, i\right)\right)$$。
  - 总损失：$$L=\frac{L_{c l}+\alpha L_{g e n}}{1+\alpha}$$（$$\alpha$$为超参数）。

#### NoteLLM2 [xiaohongshu]

> NoteLLM-2: Multimodal Large Representation Models for Recommendation
>
> 训练MLRM Embedding

多模态上下文学习（mICL）

- **核心思路**：分离视觉与文本内容，分别压缩为模态专属 token，强化模态内相关性。

- Prompt 设计

  ```plaintext
  Note content: {'image': <IMG>}, Compress into one word:"<IMG_EMB>"
  Note content: {'title':..., 'content':...}, Compress into one word:"<TXT_EMB>"
  ```

- **对比学习**：对视觉压缩 token（<IMG_EMB>）和文本压缩 token（<TXT_EMB>）分别进行批内对比学习，损失函数为： $$\mathcal{L}^{v}=-\frac{1}{2B}\sum_{i=1}^{2B}log\frac{e^{sim(\hat{n}_i^v, \hat{n}_i^{v+}) \cdot e^\tau}}{\sum_{j\neq i}e^{sim(\hat{n}_i^v, \hat{n}_j^v) \cdot e^\tau}}$$ （同理计算多模态损失$$\mathcal{L}^{m}$$）

2.2 晚期融合机制

- **核心思路**：延迟视觉与文本信息的融合，保留原始视觉特征，缓解文本偏向。
- **门控融合**：通过线性层将视觉编码器特征转换为 LLM 维度，再通过门控机制融合： $$z = sigmoid(W[v, n_i^v] + b),\ \hat{n}_i^v = z \odot v + (1-z) \odot n_i^v$$ （v为视觉特征，$$n_i^v$$为 LLM 输出的视觉表示）

![image-20250728164901446](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250728164901446.png)

### 

### 个性化语义画像

#### LLM原生流式生产

* DataSQRL + Flink https://www.datasqrl.com/blog/personalized-ai-search/
  * deduplicate the stream to get the most recent version for each product.



## RL + 推荐系统

### Intro

* 强化学习：“行动—反馈—状态更新”循环，收益点是在线学习
* DQN
  * 任何深度学习模型都可作为智能体的推荐模型
* DRN
  * 模型微更新：竞争梯度下降算法
    * dueling bandit gradient descent algorithm
  * 模型主更新：利用历史数据重 train
* dueling bandit gradient descent algorithm
  * 随机扰动、探索网络
  * interleaving然后推送结果
  * $$\Delta W=\alpha *rand(-1,1)\cdot W$$

* 意义：变静态为动态，增强模型学习的实时性
  * “重量”与“实时”的折中

### Rec-R1

> 看起来对搜索的价值高于推荐

* 思路：强化学习接收推荐系统的信号反馈，反馈到LLM的学习中
  * **目标函数**：最大化预期推荐性能，即$$max _{\theta} \mathbb{E}_{s \sim p(s), a \sim \pi_{\theta}(a | s)}[f(a | s)]$$，其中s为用户输入，a为 LLM 生成文本，$$f(a|s)$$为推荐系统反馈的奖励
  * **强化学习算法**：GRPO
  * **奖励机制**：直接使用推荐系统的评价指标（如 NDCG@100）作为奖励，无需额外训练奖励模型，避免偏差。

![image-20250725153710133](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725153710133.png)

* 结论：
  * 搜索任务：优于一些sparse&dense结合的检索算法，也**优于query rewrite的一些算法**
    * ![image-20250725154527981](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725154527981.png)
  * **推荐任务：冷启动场景（inductive setting）下效果不错，普通场景效果一般**
    * ![image-20250725154425962](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250725154425962.png)



## Evaluation (chpt 7)

- 资料
  - https://recbole.io/evaluation.html

### 离线评估 Evaluation

**实验setting**

* holdout实验
  * 7:3
* cross-validation 交叉验证
  * k-fold
  * leave-one-out validation
    * K-fold k=1
* 自助法 bootstrap
  * N次有放回，没抽到的作为验证集

**指标**

* 准确率 Accuracy：
  * 缺点是只把99%负样本分类对，准确率也很高
* Precision + Recall
  * Precision@N 认为前N个为模型预测的正例
  * F1-Score 调和平均
* RMSE
  * 解决离群点影响大的问题：$$MAPE=\sum_{i=1}^N|\frac{y_i-\hat{y_i}}{y_i}|\times\frac{100}{n}$$ 
* LogLoss (Binary Cross Entropy)
  * 二分类任务中常用的损失函数，评估预测概率与真实标签的差异。
  * 公式：$$LogLoss = - \frac{1}{N} \sum_{i=1}^N [y_i \log p_i + (1-y_i) \log (1-p_i)]$$
  * LogLoss 越小越好，但它的绝对值依赖于数据集的正样本比例（CTR），因此不同数据集间难以直接对比。

* Normalized Entropy (NE) / RIG (Relative Information Gain)
  * **背景**：为了消除数据集本身的 Label 分布（如 CTR 高低）对 LogLoss 的影响，引入了 NE 和 RIG。
  * **Normalized Entropy (NE)**：
    * 将模型的 LogLoss 除以 **Background LogLoss**（即仅使用平均 CTR 作为预测值时的 LogLoss）。
    * 公式：$$NE = \frac{LogLoss_{model}}{LogLoss_{background}}$$
    * 其中 $$LogLoss_{background} = - (\bar{p} \log \bar{p} + (1-\bar{p}) \log (1-\bar{p}))$$，$\bar{p}$ 为数据集的平均 CTR。
    * NE 越小越好。
  * **RIG (Relative Information Gain)**：
    * 定义：$$RIG = 1 - NE = \frac{LogLoss_{background} - LogLoss_{model}}{LogLoss_{background}}$$
    * **物理意义**：相比于盲猜平均 CTR，模型带来了多少**相对信息增益**（性能提升了多少百分比）。
    * **取值范围**：
      * $$RIG = 1$$：LogLoss 为 0，完美预测。
      * $$RIG = 0$$：模型效果等同于直接预测平均 CTR。
      * $$RIG < 0$$：模型效果不如直接预测平均 CTR。
    * **转换关系**：RIG 和 NE 之和为 1。RIG 越高越好（越接近 1 越好）。
  * ![image-20251020003212782](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251020003212782.png)
  

**直接评估推荐序列的离线指标**

* P-R 曲线
  * 横轴Recall，纵轴Precision
  * AUC：Area Under Curve
* ROC 曲线
  * the Receiver Operating Characteristic
  * 横轴 FPR = FP/N，纵轴 TPR = TP/P
  * 计算方法：可以对样本排序、遍历，遇到正样本，纵轴绘制 1/P，遇到负样本，横轴绘制 1/N
  * 经过 (0,0) 和 (1,1)
* mAP
  * mean average precision
  * 对每个用户的样本排序，计算AP
  * 按全部用户平均，计算mAP

**其它指标**

* NDCG
* coverage
* diversity

#### 关于AUC

* 评价指标：[AUC和GAUC](https://www.jianshu.com/p/03a11a083a6d)、[AUC不等于线上效果可能的原因](https://zhuanlan.zhihu.com/p/58152702)

* **GAUC (Group AUC)**:
  * 动机：AUC 是全局指标，但个性化推荐中，我们需要衡量模型在**每个用户内部**的排序能力，且不同用户的活跃度差异很大。全局 AUC 可能会因为用户本身分布的差异（user bias）而被高估或低估。
  * 定义：对每个用户单独计算 AUC，然后按样本量加权平均。
  * 公式：
    $$GAUC = \frac{\sum_{u \in U} w_u \times AUC_u}{\sum_{u \in U} w_u}$$
    其中 $w_u$ 通常为用户 $u$ 的样本数（曝光数或点击数），$AUC_u$ 是用户 $u$ 的样本子集的 AUC。

* **UAUC (User AUC)**:
  * 定义：UAUC 是 GAUC 的特例，即所有用户权重相等的情况（平均用户满意度）。
  * 公式：
    $$UAUC = \frac{1}{|U|} \sum_{u \in U} AUC_u$$
  * 场景：当关注长尾用户体验，不希望头部活跃用户主导指标时使用。

* 理解ROC曲线：ROC曲线是一个在二维平面上绘制的曲线，它描述了一个二分类模型在 所有可能阈值 下的表现。

  - 横坐标 (X轴) ： 假正例率 (False Positive Rate, FPR)
    
    - 公式 ： FPR = FP / (FP + TN)
    - 含义 ：在所有 真实为负 的样本中，被模型 错误地预测为正 的比例。FPR越低，说明模型对负样本的误判越少。
    
    - 纵坐标 (Y轴) ： 真正例率 (True Positive Rate, TPR) ，也叫召回率（Recall）或灵敏度（Sensitivity）。
      
      - 公式 ： TPR = TP / (TP + FN)
      - 含义 ：在所有 真实为正 的样本中，被模型 正确地预测为正 的比例。TPR越高，说明模型对正样本的覆盖越好。
      这里的 TP, FP, TN, FN 来自于混淆矩阵：
    
  - TP (True Positive) : 真实为正，预测也为正。
  
  - FP (False Positive) : 真实为负，预测为正。（误报）
  
  - TN (True Negative) : 真实为负，预测也为负。
  
  - FN (False Negative) : 真实为正，预测为负。（漏报） 如何绘制ROC曲线？
  分类模型通常不直接输出0或1，而是输出一个概率或分数（例如，样本为正的概率是0.75）。我们需要一个**阈值（Threshold）**来决定如何划分正负样本（例如，分数 > 0.5 则判为正）。


* ROC曲线的绘制过程如下：
  * 模型对所有样本进行预测，得到每个样本属于正类的分数。
  * 将所有样本 按预测分数从高到低排序 。
  * 从一个极高的阈值（高于所有分数，此时所有样本都被判为负）开始，逐步降低阈值。
  * 每当阈值越过一个样本的分数时，就重新计算一次FPR和TPR，并在图上描一个点 (FPR, TPR) 。
  * 将所有这些点连接起来，就构成了ROC曲线。
  * 曲线的起点是 (0, 0)，对应阈值最高，所有样本都判为负。
  * 曲线的终点是 (1, 1)，对应阈值最低，所有样本都判为正。
* 一个理想的模型，其ROC曲线会尽可能地靠近左上角（FPR=0, TPR=1）。
* 计算方式：
  * 1）ROC绘图，曲线下方面积
  * 2）AUC有一个非常重要的统计学含义： 从所有正样本中随机抽取一个样本A，再从所有负样本中随机抽取一个样本B，模型对A的预测分数高于对B的预测分数的概率。
    * AUC = ( Σrank_positive - P * (P + 1) / 2 ) / ( P * N )

### 更接近线上环境的离线评估方法 —— Replay

* 动态离线评估
  * 训练集 - 测试集1 - 测试集2 - 测试集3
* Netflix Time Machine
  * 避免“数据穿越” （样本里包含未来信息）

### A/B Testing

* Intro：为什么A/B测试无法被替代
  * 离线评估无法消除 data bias
  * 离线评估无法完全还原线上工程环境
    * 延迟、数据丢失、标签数据缺失
  * 线上系统某些商业指标，在离线评估中无法计算
    * CTR、ST、PV

* 分桶原则
  * 分桶的随机性
  * Overlapping Experiment Infrastructure: More, Better, Faster Experimentation
    * 层与层之间的流量正交
      * 同层之间的流量互斥

* 真实指标：

![image-20250107001513104](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250107001513104.png)

### 快速线上评估方法 —— Interleaving

* Intro
  * A/B资源有限 -> 快速筛选出“优胜”算法
  * A/B测试的统计学问题 -> 不区分A/B组，把不同的被测对象同时提供给受试者，根据受试者喜好得出评估结果
* 实现：
  * 整合算法A和算法B
* 结论：
  * p-value 5%: interleaving需要10^3样本，A/B testing需要10^5样本

* 局限性：
  * 工程实现复杂（需要大量辅助性的数据标识添加到数据流）
  * 只能得到偏好，无法得到真实业务指标提升的数据

## Benchmark

> 有评估代码的开源仓库：
>
> https://github.com/bytedance/HLLM

#### Amazon Book Review

https://arxiv.org/pdf/2403.03952

https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023

#### PixelRec

https://github.com/westlake-repl/PixelRec

#### ORBIT (Open Recommendation Benchmark for Reproducible Research with Hidden Tests) [Meta]

> 由CMU与Meta联合推出，旨在解决推荐系统研究中评测标准不一、数据集脱离真实场景、模型泛化能力难以评估等核心痛点。
> *   **GitHub**: [cxcscmu/RecSys-Benchmark](https://github.com/cxcscmu/RecSys-Benchmark)
> *   **arXiv**: [2510.26095](https://arxiv.org/abs/2510.26095)

*   **核心亮点**:
    *   **统一评测标准**: 整合5个主流公开数据集（如MovieLens, Amazon系列），提供一致的数据划分与评估指标。
    *   **隐藏测试集 (ClueWeb-Reco)**: 基于美国真实用户浏览数据构建，通过语义软匹配映射到公开网页，既保留了真实行为特征，又确保了隐私安全。
    *   **覆盖代表性模型**: 系统化测评了从GRU4Rec到HLLM等12种传统及新型内容增强模型。
    *   **LLM-QueryGen 创新基线**: 首次引入大模型（如GPT-4o, Gemini-2.5）生成检索查询的推荐方式，在零样本条件下展现出强大的泛化潜力。
    *   **开源透明生态**: 提供完整的代码、排行榜和数据访问渠道，推动社区共建。

*   **典型启示**:
    *   在ClueWeb-Reco隐测中，传统模型表现急剧下降，而 **LLM-QueryGen** 模型显著领先，证明了LLM在理解用户意图与跨域泛化方面的巨大优势。
    *   内容驱动的模型普遍优于纯ID模型。

#### Alibaba FORGE

* providing more than 14 billion user behaviors in 10 days, along with multimodal features of 250 million items

![image-20251204032916511](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20251204032916511.png)

## Potpourri

* [Inside TikTok's Algorithm: A WSJ Video Investigation](https://www.wsj.com/articles/tiktok-algorithm-video-investigation-11626877477)
  * https://www.youtube.com/watch?v=nfczi2cI6Cs

## Connections

* meta recsys - GPU Mode Lecture 18的作者
  * Discord (@sk4301)
  * Linkedin (https://www.linkedin.com/in/sharma-k/)
  * Twitter (@kapil_sh_)
  * Github (https://github.com/kapilsh)

## Push Notification & Cold Start Classics

### 1. Volume Control (Push Specific)
*   **Paper**: [Near Real-Time Optimization of Notification Volume](https://www.kdd.org/kdd2018/accepted-papers/view/near-real-time-optimization-of-notification-volume) (LinkedIn, WWW 2018)
*   **Core Problem**: Push 通知的核心挑战不仅是 Ranking（推什么），更是 **Volume Control**（推多少）。过度推送会导致用户关闭通知权限（不可逆的 Churn），这是 Feed 流没有的硬约束。
*   **Methodology**:
    *   **Budget Constrained Optimization**: 将问题建模为带约束的优化问题。目标是最大化 Total Engagement (Sessions)，约束是 "Badness" (Unsubscribes/Disable) < Budget。
    *   **Lagrange Multipliers**: 使用拉格朗日乘子法将约束转化为无约束问题，引入 $\lambda$ 作为“打扰成本”。
    *   **Send Decision**: $Score = P(Click) \times V_{click} - \lambda \times P(Unsubscribe) \times V_{churn}$。只有当 Score > 0 时才发送。
*   **Insight**: Push 推荐必须考虑 Long-term Value (LTV)，不能只看短期 CTR。

### 2. Exploration & Cold Start (Exposure Optimization)
*   **Paper**: [A Contextual-Bandit Approach to Personalized News Article Recommendation](https://arxiv.org/abs/1003.0146) (Yahoo!, WWW 2010)
*   **Core Problem**: 物品冷启动（Cold Start）和动态环境下的探索（Exploration）。在 News/Push 场景中，新物品源源不断，且生命周期短，传统的 Collaborative Filtering 无法处理（因为没有历史交互）。
*   **Methodology**: **LinUCB (Linear Upper Confidence Bound)**
    *   **Contextual**: 利用上下文特征（用户特征 + 物品特征）来预测回报。
    *   **UCB (Upper Confidence Bound)**: 不仅预测点击率的期望值 (Mean)，还预测置信区间 (Variance)。
    *   **Selection**: 选择 $Score = \mu + \alpha \cdot \sigma$ 最高的物品。对于新物品，由于数据少，$\sigma$ 大，会被优先展示（探索）；随着数据积累，$\sigma$ 减小，逐渐过渡到利用（Exploit）。
*   **Insight**: 解决“曝光物品数少”的关键是引入**确定性的探索机制**，而不是随机探索。LinUCB 是工业界解决冷启动最经典的 Baseline。

### 3. Targeting & Incrementality (Uplift Modeling)
*   **Context**: 业界（如美团、阿里）在 Push 场景中常采用 **Uplift Modeling**（因果推断）来解决 "Targeting" 问题。
*   **Core Problem**: 传统的 CTR 模型无法区分 "Sure Things"（自然转化用户）。对这类用户发 Push 是浪费预算且增加打扰。
*   **Methodology**: **Uplift Modeling**
    *   **Goal**: 预测  = P(Click|Treatment) - P(Click|Control)$，即 Push 带来的**增量**点击概率。
    *   **User Segmentation**:
        *   **Persuadables** (高 Lift): 只有推了才点（**核心目标**）。
        *   **Sure Things** (高 CTR, 低 Lift): 不推也会点（应减少推送）。
        *   **Lost Causes** (低 CTR, 低 Lift): 推不推都不点。
        *   **Do Not Disturbs** (负 Lift): 推了反而反感（绝对不推）。
*   **Insight**: Push 的核心价值在于挖掘 **Persuadables**，从而提升 DAU 的**增量** (Incremental Gain)，而非仅仅提升 CTR。
