[toc]

### 深度学习推荐系统 —— 王喆

#### chpt1 互联网的增长引擎——推荐系统

概念：

* UGC(User Generated Content)，典型如Youtube、Tiktok
* CVR (Conversion Rate)、观看时长(Youtube Recommendations)

推荐系统架构：数据与模型

* 物品信息、用户信息、场景信息
* 数据离线批处理、实时流处理



#### chpt2 前深度学习时代——推荐系统的进化之路

1. 演化关系图  p13
2. 协同过滤(CF)

[一篇详细的介绍文章](https://zhuanlan.zhihu.com/p/80069337)

UserCF (1994)
* 用户相似度计算：余弦相似度/皮尔逊相关系数/引入物品平均分
* 最终结果的排序：按相似度加权
* 特点：社交特性更强
* 缺点：
  * 用户太多，在线存储系统存不下矩阵。 
  * 不适用于正反馈获取困难的应用场景（酒店预定、大件商品购买），用户历史数据向量稀疏

UserCF 和 ItemCF 的对比
* UserCF 适合用在个性化需求不强，热点很明显的领域，比如新闻，电影推荐
* ItemCF 适合用在个性化需求比较强，长尾比较长的领域，比如书，电商的推荐

协同过滤的缺点
* 推荐系统的头部效应较明显，处理稀疏向量的能力弱 => MF用更稠密的隐向量表示用户和物品
* 无法引入场景信息和更精细的用户/物品信息 => LR模型、机器学习模型



3. 矩阵分解(MF)

* 用户和物品的隐向量通过分解协同过滤生成的共现矩阵得到
* 矩阵分解的方法：特征值分解（方阵，不适用）；SVD（计算量大、要求稠密）；梯度下降法
* 矩阵分解可加入偏差向量
* 优点：
  * 泛化能力强
  * 空间复杂度低
  * 更好的扩展性和灵活性
* 缺点
  * 同样不方便加入场景信息和更精细的用户/物品信息



4. 逻辑回归 (LR)

将推荐问题转换为一个CTR预估问题

流行原因：1）数学含义上的支撑 2）可解释性强 3）工程化的需要



5. 从FM到FFM——自动特征交叉的解决方案

* Poly2——特征交叉的开始
* FM模型——隐向量特征交叉 (2012-2014)
  * 参数减少到n*k，减小训练开销，增大收敛可能
* FFM模型——引入特征域的概念 (2015)
  * Features are empirically categorized into several groups
  * Within each group, features are further identified as user-side and ad-side features
  * For each group, user-side and ad-features are sum-pooled, respectively, followed by element-wise multiplication.
  * Limitations
    * Feature-level interactions are lost due to sum-pooling
    * Using element-wise multiplication of sum-pooled embeddings leads to high dimensional output.



6. GBDT+LR——特征工程模型化的开端

组合模型，一定程度上解决特征交叉的问题

优点：e2e特征工程模型化

缺点：容易过拟合；丢失大量特征的数值信息



7. LS-PLM —— alibaba曾经的主流推荐模型

也称为MLR(Mix Logistic Regression)，在逻辑回归的基础上加入聚类的思想，其灵感来自对广告推荐领域样本特点的观察

* 超参数：分片数m，阿里的经验值为12

优势：

* 端到端的非线性学习能力
* 模型稀疏性强



引申：

* L1范数比L2范数更容易产生稀疏解，因为加入正则化项的损失函数最小值更容易在参数空间的顶点处取得，对应稀疏参数



#### chpt3 浪潮之巅——深度学习在推荐系统中的应用

1. 演化关系图：p51

2. [AutoRec](https://zhuanlan.zhihu.com/p/159087297) —— 单隐层神经网络推荐模型

结合了auto-encoder和协同过滤的思想，本质上是训练auto-encoder保存推荐系统的泛化信息，输入一个“不完整”的、“真实”的评分条目列表，得到对应这个条目列表的相似条目列表

* I-AutoRec, U-AutoRec

3. Deep Crossing模型 —— 经典的深度学习架构

* 如何解决稀疏特征向量稠密化的问题？
  * 除数值类特征，进入Embedding层
* 如何解决特征自动交叉的问题？
  * 无人工特征交叉，由 **残差神经网络** 加强提取非线性特征和组合特征信息的能力
* 如何在输出层中达成问题设定的优化目标？
  * 最后一层逻辑回归

4. NeuralCF模型——CF与深度学习的结合

* 用神经网络替代CF最后一层打分的点积操作
* NeuralCF混合模型：concat原始NeuralCF模型的输出和以element-wise product为互操作的广义矩阵分解模型
* 局限性和CF一致：没引入其他类型的特征、具体的互操作有待探索



5. PNN模型——加强特征交叉能力

Product层：线性操作 + 乘积操作
* 乘积操作 = 内积(IPNN)/外积(OPNN)
* 叠加外积互操作矩阵：本质上是让所有embedding通过一个平均池化层后，再进行外积互操作
  * [embedding的意义](https://www.zhihu.com/question/374835153/answer/1042845667)
  * 简单的sumpooling会忽略一些有价值的信息



6. Wide&Deep模型——记忆能力和泛化能力的综合

具体参考[MLSys.md](https://github.com/huangrt01/CS-Notes/blob/master/Notes/Output/MLSys.md)



7. FM与深度学习模型的结合

* FNN——用FM的隐向量完成Embedding层初始化
  * 背景：Embedding层收敛速度慢 <= 参数多、输入向量稀疏
* DeepFM——用FM代替Wide部分
* NFM——FM的神经网络化尝试
  * 在Embedding层和多层神经网络之间加入**特征交叉池化层**
  * NFM和FM的关系：FM是一种浅层的线性模型, 可以看作是不带隐层的NFM
  * 相比Wide&Deep, Deep&Cross用concat的方法连接特征，NFM方法更侧重特征之间的深度交互



8. 注意力机制在推荐模型中的应用

* AFM——引入注意力机制的FM
  * 在NFM的Pair-wise Interaction Lyaer和池化层之间加入注意力网络
  * 注意力网络 = 全连接层 + softmax，网络输入是 element-wise 交叉的特征vector
##### (DIN) Deep Interest Network for Click-Through Rate Prediction ——引入注意力机制的深度学习网络

  * 核心动机是常见的 Embedding&MLP 模型，user 特征的表达能力受限于 fixed-length vector。user特征表达能力难以通过简单的翻倍方式来提升（将多个域的user特征连接起来），考虑到线上压力以及过拟合的风险，需要寻求算法上的突破 => weighted sum-pooling
  * Attention 技术：NLP、搜索领域，通常用来刻画context（比如用target词加权句子、用target广告加权最近的query）。本文创新性地用广告特征来加权user序列特征
  * 应用于淘宝的电商广告推荐场景，感觉在电商场景，推荐和广告天然结合地紧密（广告就是推荐的商品）；短视频场景，两者更割裂一些，并且用户兴趣更长期
      * 电商场景，搜索个性化的必要性也更强，和推荐区分不严格
    * 此外，搜索广告场景先通过相关性做召回，严格意义上进入推荐系统的候选数量少
  * 用户侧的embedding是对每次行为的embedding通过注意力加权得到，注意力权重受广告特征影响
    * 广告侧：论文中有 goods id, shop id, cate id；实践中可以pooling ad id, category id, position encoding (特征抽取时做分钟的sqrt，相比 log 对长期历史行为有区分度)
    * 商铺id只和用户历史行为中的商铺id序列发生作用，商品id也如此 <=> 注意力轻重更应该由同类信息相关性决定
  *  <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7BV%7D_u%3Df%28%5Ctextbf%7BV%7D_a%29%3D%5Csum_%7Bi%3D1%7D%5EN%5Comega_i%C2%B7%5Ctextbf%7BV%7D_i%3D%5Csum_%7Bi%3D1%7D%5ENg%28%5Ctextbf%7BV%7D_i%2C%5Ctextbf%7BV%7D_a%29%C2%B7%5Ctextbf%7BV%7D_i" alt="\textbf{V}_u=f(\textbf{V}_a)=\sum_{i=1}^N\omega_i·\textbf{V}_i=\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a)·\textbf{V}_i" class="ee_img tr_noresize" eeimg="1">  
    * 注意力激活单元：元素减操作的embedding（原论文中是取外积向量）, concat两个原输入embedding
    *  <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5ENg%28%5Ctextbf%7BV%7D_i%2C%5Ctextbf%7BV%7D_a%29%20%5Cneq%201" alt="\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a) \neq 1" class="ee_img tr_noresize" eeimg="1">  ，描述 an approximation of the intensity of activated user interests to some degree
  * Training Techniques
    * Mini-batch Aware Regularization：引入L2范数，通过“只对输入中有的sparse fc算”减少运算量
    * Data Adaptive Activation Function：PReLu -> Dice
  * Experiments
    * 新的AUC计算方式：GAUC (averaging AUC over users)
    * Regularization 实验：Mini-Batch Aware L2 > Occurrency Filter > DropOut > Regularization in DiFacto > Base



##### DIEN——序列模型与推荐系统的结合

* 尝试刻画 latent interest 而非用 behaviour 描述 interest

* 兴趣进化网络
  * 行为序列层：普通的Embedding层
  * 兴趣抽取层：GRU
    * auxiliary loss: extra supervision information, uses consecutive behavior to supervise the learning of hidden state at each step. which makes hidden state expressive enough to represent latent interest. 算法实现上，是让第i次的 hidden state 向量更接近第 i+1 次的正样本 item 向量
  * 兴趣进化层：(attentional update gate) AUGRU，引入注意力机制，与 DIN 相似，更有针对性地模拟与目标广告相关的兴趣进化路径
    * 文章中讨论了 attention 结合 GRU 的几种方法：AIGRU、AGRU、AUGRU



##### (DSIN) Deep Session Interest Network for Click-Through Rate Prediction

* multiple sessions：30 mins gap
* session内用户兴趣抽取：multi self-attention mechanism(AutoInt) with bias encoding
* session间兴趣进化：Bi-LSTM
* 聚合兴趣预估ctr：local activation unit



#####  UIC+MIMN

[从阿里的User Interest Center看模型线上实时serving方法](https://zhuanlan.zhihu.com/p/111929212)

* system: UIC

* algorithm: MIMN = improved NTM with two designs of memory utilization regularization and memory induction unit

3.REALTIME CTR PREDICTION SYSTEM

- 电商领域，特征中 90% 的规模是 user behaviour features
- long sequence 架构上的挑战主要在于 storage 和 latency

4.MULTI-CHANNEL USER INTEREST MEMORY NETWORK

- long sequence 算法上的挑战：RNN 结构并不最适合刻画 long sequence（hidden state 偏向于刻画 predicting target 而非 history；存储所有历史信息的方案较冗余）

=> MIMN 

- NTM
- memory utilization regularization: to increase the expressive ability of memory tensor in UIC by increasing the utilization of memory
  - 解决 NTM 的 memory 被热点行为 dominate 的问题：1）LRU 将信息写进不同的 slot，有利有弊；2）Memory Utilization Regularization 
- memory induction unit: to help capture high-order information
  - 继续用GRU进化兴趣，MIMN的特点，兴趣进化是 multi-channel 的（channel间共享GRU的参数）



##### SIM

Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction

Search-based Interest Model (SIM) -> Sub user Behavior Sequence (SBS)

- General Search Unit (GSU)
  - soft-search and hard-search
- Exact Search Unit (ESU)
  - multi-attention; encode temporal distance information

training方案：

* ESU: multi-head attention

* GSU: soft-search 联合 ESU 学习，auxiliary CTR model 学习 behavior embedding

serving方案：

* hard-search: user behavior tree (UBT), uid->category id->behavior items

there indeed exists massive noise in original long-term behavior sequences which may undermine long-term user interest learning. 信息并非越多越好，本文也和 Youtube 论文[2] 一样，体现了过滤噪音信息的思想


##### MIND (Multi-Interest Network with Dynamic routing), CIKM 2019

* 与 match 特征的关系：本文是把 match 做到模型里，如果模型能学到 历史 item 到 interest 的转换关系，就相当于是有 match 能力了（match 的 key 我感觉就像是 interest）

* 本质上是捕捉用户兴趣做召回，利用胶囊网络去“压缩”序列特征成兴趣特征，multi-interest embedding 尝试克服 user embedding 的维数局限性
  * multi-interest extractor layer: soft-cluster user interests

* label-aware attention: to help learn a user representation with multiple vectors.

* Related Work
  * User Representation: sequence model; from word embedding
  * Capsule Network. The concept of "Capsule", a small group of neurons assembled to output a whole vector, is firstly proposed by Hinton [13] at 2011. 
    Instead of backpropagation, dynamic routing [21] is used to learn the weights on the connections between cap-sules, which is improved by utilizing Expectation-Maximization algorithm [14] to overcome several deficiencies and achieves better accuracy.

* Method
  * 3.3 Multi-Interest Extractor Layer
    * 由序列特征生成 user embedding，是基于胶囊网络的 user tower，生成 
      multiple representation vectors
  * Dynamic Routing
    * In a nutshell, capsule is a new kind of neuron represented by one vector instead of one scalar used in ordinary neural networks.
    * dynamic routing：胶囊网络中利用 bilinear 矩阵，学习low-level到high-level表征关系
    * 最后一层用 a non-linear "squash" function 处理
  * B2I dynamic routing
    * learn interest capsules from behavior capsules.
    * Shared bilinear mapping matrix：设计的一方面因素是因为 user behaviour 是变长的
    * Randomly initialized routing logits
    * Dynamic interest number: 根据历史序列长度动态调整兴趣数量

  * 3.4 Label-aware Attention Layer
    * 训练时用
    * Q: 这个 dynamic routing 怎么训练？

* Experiments
  * 实验指标 HitRate
  * hard attention scheme 效果最好

##### explore-exploit问题 

* [专治选择困难症——bandit算法](https://zhuanlan.zhihu.com/p/21388070)
  * Thompson sampling算法
    * `choice = numpy.argmax(pymc.rbeta(1 + self.wins, 1 + self.trials - self.wins))`
  * UCB算法：Upper Confidence Bound
  * Epsilon-Greedy算法：有点类似模拟退火
    * 一定的概率做纯随机决策
  * 总结：
    * UCB算法和Thompson采样算法显著优秀一些，两者都是给explore bonus，给出explore bonus的方式不同：
      * UCB是频率学派，假定了大数定律收益收敛，显式的通过试验次数给出的置信度上界
      * Thompson是靠分布刻画不确定性
    * 至于你实际上要选哪一种bandit算法，你可以选一种bandit算法来选bandit算法。。。


##### 强化学习与推荐系统的结合

* 强化学习：“行动—反馈—状态更新”循环
  * DQN
    * 任何深度学习模型都可作为智能体的推荐模型
  * DRN
    * 模型微更新：竞争梯度下降算法（随机扰动、探索网络）
    * 模型主更新：利用历史数据重 train
* 意义：变静态为动态，增强模型学习的实时性
  * “重量”与“实时”的折中



#### chpt4 Embedding 技术在推荐系统中的应用

1. 什么是Embedding
* 定义：用低维稠密向量“表示”一个对象 
  *	向量“表示”对象特征
  *	向量距离“表示”相似性
  *	具有本体论哲学层面上的意义
* 意义
  * 稀疏特征向量转稠密
  * 本身表达能力强，可引入任何信息进行编码(Graph Embedding)
  * 对物品、用户相似度的计算是常用的推荐系统召回层技术



2. Word2vec——经典的Embedding方法

* CBOW模型和Skip-gram模型
  * 经验上，Skip-gram模型效果更好，输入是本体，输出是周边的词
* 模型训练过程
  * 输入向量的权重矩阵 = 词向量查找表
  * 负采样方法减轻训练负担
    *  <img src="https://www.zhihu.com/equation?tex=E%3D-log%5Csigma%28%7Bv%5E%7B%27%7D_%7Bw_o%7D%7D%5ETh%29-%5Csum_%7Bw_j%5Cin%20W_%7Bneg%7D%7Dlog%5Csigma%28%7Bv%5E%7B%27%7D_%7Bw_j%7D%7D%5ETh%29" alt="E=-log\sigma({v^{'}_{w_o}}^Th)-\sum_{w_j\in W_{neg}}log\sigma({v^{'}_{w_j}}^Th)" class="ee_img tr_noresize" eeimg="1"> 
    * 还有Hierarchical softmax方法加速训练



3. Item2Vec——Word2vec在推荐系统领域的推广



4. Graph Embedding——引入更多结构信息的图嵌入技术



##### Graph Service

* 基于Euler改造
  * 支持全图节点遍历，支持按时间戳采样

* 高性能采样
  * 节点生成前缀和，二分查找随机数
  * 全局点采样：shard有权（shard上所有节点的权重），shard内再次按权
* Graph Embedding on GPU: sample和worker分离
* GE应用
  * 预训练：利用uid与author间的finish关系，构造双向异构图。边权重用finish视频数量，点权重用finish视频总数。
    * 不用gid而用author是为了减小图结构随时间的变化
    * 正负例构造：正例是user及其finish的节点，负例随机采样
  * end2end training
    * 利用uid和cid之间的click关系，边权重是click次数，点权重是click总数
    * 采样：有放回按权采样
    * 优点：训练目标和LTR任务一致
    * 缺点：1.全局图的拟合能力不如业务图 2.end2end结构更自由，可以用node2vec等
  * Finetune BERT/ResNet
    * cid和cid的co-click，边权重用交并比，点权重用click总数，点特征用广告id类泛化特征

#### chpt6 深度学习推荐系统的工程实现

1. 推荐系统的数据流

四种架构：批处理、流计算、Lambda、Kappa

* Lambda: 流计算以增量计算为主，批处理进行全量计算；利用离线层数据对实时流数据进行校验和检错

2. Spark MLlib

3. Parameter Server

* Spark MLlib: 同步阻断式
* Parameter Server: 异步非阻断式

两者区别在于模型参数的分发是否同步

6. 工程与理论之间的均衡

* end2end：强调模型一致性的收益
* two stages：强调模型实时性的收益





##### Potpourri

* 多样性探索：消重、打散、部分流量未知兴趣

* 评价指标：[AUC和GAUC](https://www.jianshu.com/p/03a11a083a6d)、[AUC不等于线上效果可能的原因](线下AUC提升为什么不能带来线上效果提升? - 萧瑟的文章 - 知乎 https://zhuanlan.zhihu.com/p/58152702)

* 边缘计算

  * [EdgeRec：揭秘边缘计算在淘宝推荐系统的重要实践](https://developer.aliyun.com/article/742144)

* 弹性近线计算

  * [百度信息流和搜索业务中的弹性近线计算探索与应用](https://mp.weixin.qq.com/s/53KLAPphK9t4G3q-78S9mg)

  * 弹性近线计算系统主要包括几个子系统：

    * 触发控制系统：主要负责根据业务参数，控制近线计算的触发，达到削峰填谷的目的。

    * 动态算力和动态调参系统：它相当于弹性近线计算系统的大脑，根据集群的资源情况，分配近线计算算力；再根据算力情况，计算控制参数，从而控制跟算力匹配的负载。

    * 历史数据中心：保存近线计算历史的计算记录。可以根据资源的情况，复用历史计算结果，来调节对算力的使用。

    * 业务近线计算 & 控制系统：这个主要是和业务接入近线计算相关的一些架构机制设计，比如说输入输出缓存的读写，计算的拆包 / 并包等等，业务计算与失败信号的反馈等等。
      * 实践下来，通过错峰调度，预估资源需求并提前分配计算资源是比较有效的提升算力的办法，可以理解是如果在资源已经紧张的时候，再进行近线计算模块的调度，新的近线计算模块的算力消耗有概率本身就造成局部热点，导致扩展资源被回收，造成调度失败

    * 业务在线接入：部分主要是业务接入近线计算系统上的一些设计。这块主要考虑的是如何高效的接入业务，避免业务接入过高的架构人力成本。

  * 应用场景：

    * Feed 在线 & 近线混合计算架构：将多个推荐算法服务的召回结果进行缓存，再放到近线系统来进行综合打分之后生成一个用户对应的近线计算候选集，作为一路新的召回。之后在线请求来了之后，使用这路新的召回结果，进行轻量的在线计算之后就可以返回给用户，这样就把召回层初步排序打分的计算规模提升了 1 个数量级。
    * 搜索结果排序：



[Inside TikTok's Algorithm: A WSJ Video Investigation](https://www.wsj.com/articles/tiktok-algorithm-video-investigation-11626877477)

