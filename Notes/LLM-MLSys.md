# LLM MLSys

[toc]

## èµ„æº

* https://cs.stanford.edu/~chrismre/#papers

## Intro

* Intro
  * æœªæ¥ç¡¬ä»¶ï¼Œå†…å­˜äº’è¿å¾ˆå…³é”®
* æŠ€æœ¯å‘å±•
  * Memory Efficient Attention with Online Softmax (2021) -> FlashAttention in Megatron-LM (2022) 
  * Continuous Batching (2022), Paged Attention (2023) -> vLLM, TensorRT-LLM (2023) 
  * Speculative Sampling (2023) -> Everywhere in LLM Serving (2023)
  * Sequence Parallel (2023) ->  Megatron-LLM (2023) 
* ä¸šåŠ¡ç›®æ ‡ï¼šhttps://mp.weixin.qq.com/s/llalxX6miJRxy0-Vk8Ezpg
  * MFUï¼ˆModel FLOPs Utilizationï¼‰
  * æ•…éšœç‡ï¼šåœ¨å¤§è§„æ¨¡çš„é›†ç¾¤ä¸­ï¼Œæ¨ç†è¯·æ±‚çš„æ•…éšœç‡ï¼Œå› ä¸ºåœ¨ä¸€ä¸‡å¼ å¡çš„é›†ç¾¤ä¸­ï¼Œå¦‚æœæ¯å‡ åˆ†é’Ÿå°±æœ‰ä¸€å¼ å¡æŒ‚æ‰ï¼Œé‚£ä¹ˆè¿™ä¼šå½±å“æ•´ä½“æ•ˆç‡ï¼Œæˆ–è€…è¯´çœ‹æ•…éšœæ—¶é—´å åœ¨æ•´ä¸ªæœ‰æ•ˆè®­ç»ƒæ—¶é—´çš„å æ¯”ï¼Œå¦‚æœè¯´æ˜¯æ•…éšœçš„æ—¶é—´å è®­ç»ƒæ—¶é—´æ¯”ä¾‹è¶…è¿‡30%ï¼Œä¹Ÿéå¸¸å½±å“æ•ˆç‡ï¼›
  

## æˆæœ¬å’Œæ€§èƒ½è¯„ä¼°

* Intro
  * AIGCæ˜¯å¤§å›½çš„æ¸¸æˆ
    * æ¬§æ´²å—æ¬§ç›Ÿæ³•æ¡ˆå½±å“ï¼Œaiå‘å±•æ²¡è·Ÿä¸Š

  * AIç³»ç»Ÿï¼šè®°å½•æ•°æ®ã€ä¸äººäº¤äº’ã€æœºå™¨å­¦ä¹ åˆ†æã€é¢„æµ‹ã€å¹²é¢„äººçš„å†³ç­–

### MFUã€HFU

* Hardware FLOPS Utilization

  * è€ƒè™‘äº†è®¡ç®—æ¢ç©ºé—´

* MFUï¼ˆModel FLOPs Utilizationï¼‰ï¼š

  * è¯„ä¼°GPUç®—åŠ›çš„æœ‰æ•ˆåˆ©ç”¨ç‡

* | æ¨¡å‹          | å‚æ•°è§„æ¨¡ | MFU    | ç¡¬ä»¶é…ç½®   |
  | ------------- | -------- | ------ | ---------- |
  | PaLM          | 540B     | 46.2%  | 6144 TPUv4 |
  | Megatron-LM   | 530B     | 56.0ï¼… | 3072 A100  |
  | Mosaic ML     | 70B      | 43.36% | 128 H100   |
  | å­—èŠ‚MegaScale | 175B     | 55.2%  | 12,288 GPU |

### FLOPS

* Am,k * Bk,n : `2*m*n*k` FLOPS
  * ä¹˜å’ŒåŠ å„ç®—ä¸€æ¬¡
* transformer
  * è®¾Cä¸ºemb sizeã€Tä¸ºseq len
  * ä¸€å±‚Transformer
    * FLOPSï¼š `24BTC^2 + 4BCT^2` 
    * Paramsï¼š`12C^2+13C`
  
  * attnçš„è®¡ç®—å æ¯”æ˜¯$$\frac{4BCT^2}{24BTC^2+4BCT^2} = \frac{T}{6C+T}$$
  * GPT3-175B C = 12288, T = 8192
  

```Python
# x : [B, T, C]
# B : batch_size
# T : seq_len
# C : dimension

x = layernorm(x)
q, k, v = qkv_proj(x).split()
# [B, T, C] x [C, 3C] -> [B, T, 3C]: 6BTC^2 FLOPS
attn = q @ k.T
# [B, T, C] x [B, C, T] = [B, T, T] : 2BT^2C FLOPS
attn = softmax(attn)
# 3BT^2*n_h, softmaxè®¡ç®—é‡è¢«å¿½ç•¥
y = attn @ v
# [B, T, T] x [B, T, C] -> [B,T, C] : 2BT^2C FLOPS
y = proj(y)
# [B, T, C] x [C, C] -> [B, T, C] : 2BTC^2
y = layernorm(y)
y = fc1(y)
# [B, T, C] x [C, 4C] -> [B, T, 4C] : 8BTC^2
y = gelu(y)
y = fc2(y)
# [B, T, 4C] x [4C, C] -> [B, T, C] : 8BTC^2
```

* GPT decoderæ¨ç†
  * ç»“åˆGPUçš„FLOPSå’ŒDRAMå†…å­˜å¸¦å®½ï¼Œå®¹æ˜“è®¡ç®—å¾—åˆ°GPTçš„è®­ç»ƒæ˜¯compute boundï¼Œæ¨ç†æ˜¯MBW bound

```Python
# qkv_cache : [B, T-1, 3C]
# x : [B, 1, C]
# B : batch_size
# T : seq_len
# C : dimension

x = layernorm(x)
qkv = qkv_proj(x)
# [B, 1, C] x [C, 3C] -> [B, 1, 3C]: 6BC^2 FLOPS
qkv = concat(qkv, qkv_cache)
# [B, 1, 3C], [B, T-1, 3C] -> [B, T, 3C]
q, k, v = qkv.split()
attn = q[:, -1, :] @ k.T
# [B, 1, C] x [B, C, T] = [B, 1, T] : 2BTC FLOPS
attn = softmax(attn)
y = attn @ v
# [B, 1, T] x [B, T, C] -> [B,1, C] : 2BTC FLOPS
y = proj(y)
# [B, 1, C] x [C, C] -> [B, 1, C] : 2BC^2
y = layernorm(y)
y = fc1(y)
# [B, 1, C] x [C, 4C] -> [B, 1, 4C] : 8BC^2
y = gelu(y)
y = fc2(y)
# [B, 1, 4C] x [4C, C] -> [B, 1, C] : 8BC^2
```



### æ˜¾å­˜

#### è®­ç»ƒæ˜¾å­˜

![image-20250416153914190](./LLM-MLSys/image-20250416153914190.png)

* 7Bæ¨¡å‹ï¼š
  
  * float32: 70*10^8 * 4B = 26.7GB
  * å¾®è°ƒï¼šè€ƒè™‘ä¸­é—´ç»“æœï¼Œ100GBä»¥ä¸Š
* gpt-3ï¼š
  * 175B 700GB
    * Fp16 326GB
  * ç®—ä¸Šadamä¼˜åŒ–å™¨2100GB
  * æ··åˆç²¾åº¦è®­ç»ƒï¼š
    * fp16å‚æ•°ã€fp32å‚æ•°copyã€fp16æ¢¯åº¦ã€fp32æ¢¯åº¦ã€fp32å†å²æ¢¯åº¦æ»‘åŠ¨å¹³å‡ã€fp32å†å²æ¢¯åº¦å¹³æ–¹å’Œæ»‘åŠ¨å¹³å‡
    * `(1+2+1+2+2+2)*2*175=3,500 GB`

* the 1.5B parameter GPT-2 model trained with sequence length of 1K and batch size of
  
  32 requires about 60 GB of memory. 
  
  * Activation checkpointing reduce the activation memory by approximately the square root of the total activations. -> 8GB
  
  * For a GPT-2 like architecture the total activations is about 12 Ã— hidden dim Ã— batch Ã— seq length Ã— transformer layers.

#### æ¨ç†æ˜¾å­˜

* 8bité‡åŒ–æ¨¡å‹ï¼š å‚æ•°é‡1B å ç”¨ 1G æ˜¾å­˜ä»¥ä¸Š

### Token

```python
import tiktoken

def count_tokens(prompt):
    encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = len(encoding.encode(prompt))
    return num_tokens

prompt_text = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹prompt"
token_count = count_tokens(prompt_text)
print(f"Promptçš„tokenæ•°é‡ä¸º: {token_count}")
```

### æ€§èƒ½ã€å»¶æ—¶

* TTFTï¼štime to first tokenï¼Œå’Œinput tokené•¿åº¦ç›¸å…³
* TPOT


### è®­ç»ƒæˆæœ¬

* O(10k) è§„æ¨¡çš„ GPU / TPU é›†ç¾¤
* LLaMAï¼š2048 A100 21d
  * a100ä¸€ä¸ªæœˆå‡ ååˆ€ï¼Œè®­ä¸€ä¸ªå‡ åä¸‡
* äººåŠ›æˆæœ¬ï¼šè®­ç»ƒåŸºç¡€å¤§æ¨¡å‹ï¼Œå›¢é˜Ÿ20äºº
  * 6ä¸ªæœˆå‡†å¤‡ã€6ä¸ªæœˆè®­ç»ƒã€6ä¸ªæœˆå¾®è°ƒï¼Œ18ä¸ªæœˆè®­æ¨¡å‹
  * ä¸Šä¸‹æ–‡èƒ½åŠ›æå‡ä¹‹åï¼Œæ—¶æ•ˆæ€§ä¼šæ˜¾è‘—å¢å¼º

* Note
  * å’ŒèŠ¯ç‰‡çš„å¯¹æ¯”ï¼šThis â€œgrowthâ€ is strikingly similar to the one involved in chip evolution where as the number of transistors increases (higher density on a chip) the cost for plants manufacturing  those chips skyrocket.  In  the case of chip manufacturing  the economics remained viable because new plants did cost more but they also produced many more chips so that till the middle lf the last decade the cost per chip was actually  decreasing generation over generation (one effect captured in the Mooreâ€™s law).
  * As with chips one may  wonder if there is a limit to the economic affordability (there sure is, it is just difficult  to pinpoint!).
  * TODO: https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/

### GPU

* å­˜é‡å’Œå¢é‡

![image-20241019195324985](./LLM-MLSys/image-20241019195324985.png)

* åˆ†å¸ƒï¼š

![image-20241019195345714](./LLM-MLSys/image-20241019195345714.png)

### å”®ä»·

* https://tiktoken.aigc2d.com/
  * ç»Ÿè®¡tokenæ•°é‡
  * GPT-4o
    * outputï¼š15åˆ€/1M token
    * inputï¼š5åˆ€/1M token

## æ¨ç†&è®­ç»ƒéƒ¨ç½²

### Intro â€”â€” æ¨¡å‹&èµ„æºå†³ç­–

> * å¾®è°ƒçš„æ˜¾å­˜æ¶ˆè€—å°
> * å¯¹äºè®¸å¤šä¸éœ€è¦ H ç³»åˆ—æ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼ˆå¦‚æœ€é«˜å¸¦å®½çš„ NVLinkã€å…¨é¢çš„ ECC å†…å­˜ã€ç‰¹å®šçš„è™šæ‹ŸåŒ–æ”¯æŒæˆ–å•å¡æœ€å¤§æ˜¾å­˜ï¼‰çš„åœºæ™¯ï¼Œ4090 æ˜¯ä¸€ä¸ªæ›´ç»æµçš„é€‰æ‹©
>   * æ³¨æ„4090åŠŸè€—&æ•£çƒ­åƒäºï¼Œ32B+ æ¨¡å‹éœ€é«˜åŠŸç‡ç”µæºï¼ˆ1000W+ï¼‰å’Œæ•£çƒ­ç³»ç»Ÿ

![image-20250507030154296](./LLM-MLSys/image-20250507030154296.png)

* **ä½é…ä½¿ç”¨ï¼ˆè®¡ç®—èµ„æºæœ‰é™ï¼‰**
  * Int4é‡åŒ–ï¼Œçº¦2Kä¸Šä¸‹æ–‡

<table align="left">
<thead>
<tr>
<th style="text-align:center">æ¨¡å‹ï¼ˆint4ï¼‰</th>
<th style="text-align:center">æ‰€éœ€æ˜¾å­˜GB</th>
<th>æ¨èGPU</th>
<th>å‚è€ƒæ¨¡å‹</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.5B</td>
<td style="text-align:center">&lt;5G</td>
<td></td>
<td>Qwen2-0.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">1.5B</td>
<td style="text-align:center">&lt;3G</td>
<td></td>
<td>Qwen-1_8B-Chat, Qwen2-1.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">6B</td>
<td style="text-align:center">4G</td>
<td></td>
<td>Yi-6B-Chat-4bits</td>
</tr>
<tr>
<td style="text-align:center">7B</td>
<td style="text-align:center">&lt;11G</td>
<td></td>
<td>Qwen2-7B-Instructï¼ŒQwen-7B-Chat-Int4</td>
</tr>
<tr>
<td style="text-align:center">14B</td>
<td style="text-align:center">13G</td>
<td></td>
<td>Qwen-14B-Chat-Int4</td>
</tr>
<tr>
<td style="text-align:center">34B</td>
<td style="text-align:center">20G</td>
<td></td>
<td>Yi-34B-Chat-4bits</td>
</tr>
<tr>
<td style="text-align:center">57B</td>
<td style="text-align:center">&lt;35G</td>
<td></td>
<td>Qwen2-57B-A14B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">72B</td>
<td style="text-align:center">&lt;47G</td>
<td></td>
<td>Qwen2-72B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">130B</td>
<td style="text-align:center">-</td>
<td>8 * RTX 2080 Ti(11G) <br> 4 * RTX 3090(24G)</td>
<td>GLM-130B</td>
</tr>
<tr>
<td style="text-align:center">236B</td>
<td style="text-align:center">130G</td>
<td>8xA100(80G)</td>
<td>DeepSeek-V2-Chat</td>
</tr>
</tbody>
</table>























* ä¸­é…
  * int8ã€4k/6kä¸Šä¸‹æ–‡

<table align="left">
<thead>
<tr>
<th style="text-align:center">æ¨¡å‹ï¼ˆint8ï¼‰</th>
<th style="text-align:center">æ‰€éœ€æ˜¾å­˜GB</th>
<th>æ¨èGPU</th>
<th>å‚è€ƒæ¨¡å‹</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.5B</td>
<td style="text-align:center">6G</td>
<td></td>
<td>Qwen2-0.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">1.5B</td>
<td style="text-align:center">8G</td>
<td></td>
<td>Qwen2-1.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">6B</td>
<td style="text-align:center">8G</td>
<td></td>
<td>Yi-6B-Chat-8bits</td>
</tr>
<tr>
<td style="text-align:center">7B</td>
<td style="text-align:center">14G</td>
<td></td>
<td>Qwen2-7B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">14B</td>
<td style="text-align:center">27G</td>
<td></td>
<td>Qwen-14B-Chat-Int8</td>
</tr>
<tr>
<td style="text-align:center">34B</td>
<td style="text-align:center">38G</td>
<td></td>
<td>Yi-34B-Chat-8bits</td>
</tr>
<tr>
<td style="text-align:center">57B</td>
<td style="text-align:center">117G (bf16)</td>
<td></td>
<td>Qwen2-57B-A14B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">72B</td>
<td style="text-align:center">80G</td>
<td></td>
<td>Qwen2-72B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">130B</td>
<td style="text-align:center">-</td>
<td>8xRTX3090 (24G)</td>
<td>GLM-130B</td>
</tr>
<tr>
<td style="text-align:center">236B</td>
<td style="text-align:center">490G(bf16)</td>
<td>8xA100 (80G)</td>
<td>DeepSeek-V2-Chat</td>
</tr>
<tr>
<td style="text-align:center">340B</td>
<td style="text-align:center">-</td>
<td>16xA100(80G) <br>  16xH100(80G) <br>  8xH200</td>
<td>Nemotron-4-340B-Instruct</td>
</tr>
</tbody>
</table>



























* é«˜é…
  * Bf16ï¼Œ32Kä¸Šä¸‹æ–‡

<table align="left">
<thead>
<tr>
<th style="text-align:center">æ¨¡å‹ï¼ˆfb16ï¼‰</th>
<th style="text-align:center">æ‰€éœ€æ˜¾å­˜GB</th>
<th>æ¨èGPU</th>
<th>å‚è€ƒæ¨¡å‹</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.5B</td>
<td style="text-align:center">27G</td>
<td></td>
<td>Qwen2-0.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">1.5B</td>
<td style="text-align:center">30G</td>
<td></td>
<td>Qwen2-1.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">6B</td>
<td style="text-align:center">20G</td>
<td></td>
<td>Yi-6B-200K</td>
</tr>
<tr>
<td style="text-align:center">7B</td>
<td style="text-align:center">43G</td>
<td></td>
<td>Qwen2-7B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">14B</td>
<td style="text-align:center">39G(8k)</td>
<td></td>
<td>Qwen-14B-Chat</td>
</tr>
<tr>
<td style="text-align:center">34B</td>
<td style="text-align:center">200G(200k)</td>
<td>4 x A800 (80 GB)</td>
<td>Yi-34B-200K</td>
</tr>
<tr>
<td style="text-align:center">57B</td>
<td style="text-align:center">117G</td>
<td></td>
<td>Qwen2-57B-A14B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">72B</td>
<td style="text-align:center">209G</td>
<td></td>
<td>Qwen2-72B-Instruct</td>
</tr>
</tbody>
</table>



















### DeepSeek-V3 (MoE)

* prefill
  * The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The
    attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), com-
    bined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP
    communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that
    each expert processes a sufficiently large batch size, thereby enhancing computational efficiency.
    For the MoE all-to-all communication, we use the same method as in training: first transferring
    tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In
    particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP
    communication.
    * **redundant experts**ï¼šFor each GPU, besides the original 8 experts it
      hosts, it will also host one additional redundant expert
  * simultaneously process two micro-batches with similar computational workloads, **overlapping the attention and MoE of one micro-batch with the dispatch and combine of another.**
    * exploring a dynamic redundancy strategy for experts, where each GPU hosts
      more experts (e.g., 16 experts), but only 9 will be activated during each inference step

* decoding
  * The minimum deployment unit of the decoding stage consists of 40 nodes with 320 GPUs.
  * The attention part employs TP4 with SP, combined with DP80,
  * the MoE part uses EP320.
    * each GPU hosts only one expert, and 64 GPUs
      are responsible for hosting redundant experts and shared experts
    * the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation
      * **allocate only a small portion of SMs to dispatch+MoE+combine.**
  * é€šä¿¡ä¼˜åŒ–
    * leverage the IBGDA (NVIDIA, 2022) technology to further
      minimize latency and enhance communication efficiency.
    * **overlap the attention of one micro-batch with**
      **the dispatch+MoE+combine of another.**

## æ¨ç†&è®­ç»ƒä¼˜åŒ–

### Intro

> https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/

* KV cache
  * LLMæ¨¡å‹é¢„æµ‹çš„æ—¶å€™ä½¿ç”¨çš„æ˜¯KV cacheçš„æŠ€æœ¯ï¼Œä¹Ÿå°±æ˜¯ç¼“å­˜å·²ç»æ¨ç†å‡ºçš„å‰t-1ä¸ªtokençš„KV matrixï¼Œé‚£ä¹ˆåœ¨ç¬¬tä¸ªtokenå¼€å§‹å°±æ— éœ€å†è®¡ç®—è¿™éƒ¨åˆ†KVï¼Œç›´æ¥è°ƒç”¨ç¼“å­˜çš„KVå°±å¯ä»¥ã€‚å…·ä½“è€Œè¨€ï¼Œæ•´ä¸ªMHAåœ¨casual maskä¸‹ï¼Œå¯ä»¥è¡¨ç¤ºä¸ºï¼š $$Logit_{t_h} = \sum_{i \leq t}softmax(\frac{Q_{t_h}K^T_{i_h}}{\sqrt d})V_{i_h}$$,å› æ­¤é¢„æµ‹ç¬¬tä¸ªtokençš„æ—¶å€™ï¼Œqueryçš„multi headï¼ˆhè¡¨ç¤ºï¼‰éœ€è¦é‡æ–°è®¡ç®—ï¼Œä»¥åŠç¬¬tä¸ªkeyå’Œqueryçš„multi headï¼ˆhè¡¨ç¤ºï¼‰è¡¨ç¤ºéœ€è¦é‡æ–°è®¡ç®—ï¼Œå…¶ä½™çš„å°±å¯ä»¥ç›´æ¥ç”¨é¢„æµ‹t-1ä¸ªtokenç¼“å­˜çš„KVè¿›è¡Œè®¡ç®—ã€‚æ•´ä½“ä¸Šä¼šå¤§å¤§èŠ‚çœé¢„æµ‹æ—¶é—´ã€‚é™„ï¼šä½†æ˜¯è¿™éƒ¨åˆ†çš„KVéœ€è¦å ç”¨GPUç¼“å­˜ï¼Œè€Œå¤§æ¨¡å‹ä¸­ç¼“å­˜å ç”¨è¿‡å¤šï¼Œä¼šå¯¼è‡´é¢„æµ‹çš„æ—¶å€™Batch sizeè¿‡å°ï¼Œé‚£ä¹ˆæ•´ä½“çš„é¢„æµ‹ååç‡ä¼šé™ä½ï¼Œæ‰€ä»¥åç»­å¾ˆå¤šå·¥ä½œéƒ½åœ¨å¯¹äºKV cacheåšä¼˜åŒ–ã€‚
* Prefix Cache
  * https://docs.vllm.ai/en/latest/automatic_prefix_caching/apc.html
  
* Mooncakeï¼šå°† P / D åˆ†ç¦»è¿›è¡Œåˆ°åº• https://zhuanlan.zhihu.com/p/1711346141

### Literature Review

* Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. ã€FlashAttentionã€‘
  * These methods range from sparse-approximation [51, 74] to low-rank approximation [12, 50, 84],
    and their combinations [3, 9, 92].
  * æ ¸å¿ƒé—®é¢˜ï¼šthey focus on FLOP reduction (which may not
    correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).

* Eï¬ƒcient ML Models with Structured Matrices. ã€FlashAttentionã€‘
  * Matrix multiply is the core computational bottleneck of most machine learning models. To reduce the computational complexity, there have been numerous approaches to learn over a more eï¬ƒcient set of matrices. These matrices are called structured matrices, which have subquadratic (ğ‘œ(ğ‘›2) for dimension ğ‘› Ã— ğ‘›) number of parameters and runtime. Most common examples of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several more general classes of structured matrices proposed in machine learning: Toeplitz-like [78], low-displacement rank [49], quasi-separable [25]). The butterï¬‚y pattern we use for our block-sparse attention is motivated
    by the fact that butterï¬‚y matrices [15, 64] and their products have been shown to be able to express any structured matrices with almost optimal runtime and number of parameters [16, 20]. However, even though structured matrices are eï¬ƒcient in theory, they have not seen wide adoption since it is hard to translate their eï¬ƒciency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation, a phenomenon known as the hardware lottery [41]. Extensions of butterï¬‚y matrices [17, 18] aimed to make butterï¬‚y matrices more hardware-friendly.
* Sparse Trainingã€FlashAttentionã€‘
  * Our block-sparse FlashAttention can be seen as a step towards making sparse model
    training more eï¬ƒcient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23, 38, 39, 55, 76]. For model training, the lottery tickets hypothesis [28, 29, 30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network.
* Eï¬ƒcient Transformer.ã€FlashAttentionã€‘
  * Transformer-based models have become the most widely-used architecture in
    natural language processing [22] and computer vision [24, 91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12, 54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52, 57, 79, 89]. One can also attend over the states from previous sequences
    to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details.
    There are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31, 36, 37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models. They combine the strengths of CNNs (eï¬ƒcient training), RNNs (eï¬ƒcient inference), and continuous models (robust to change in sampling rates). LambdaNetworks [2], AFT [93] and FLASH [42] are other attempts at replacing attention in the context of image classiï¬cation and language modeling.

### Best Practicesï¼šä½¿ç”¨ GemLiteã€TorchAO å’Œ SGLang åŠ é€Ÿ LLM æ¨ç†

> https://pytorch.org/blog/accelerating-llm-inference/
>
> é€‰å‹ï¼šint4 weight only quantization (both tinygemm and GemLite version), float8 dynamic quantization

* ç°æœ‰çš„ä½ç²¾åº¦æ¨ç†æ–¹æ¡ˆåœ¨å° batch size åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

  - å½“ batch size å¢å¤§æ—¶ï¼Œæ€§èƒ½ä¸‹é™

  - å¯¹é‡åŒ–ç±»å‹çš„é™åˆ¶ï¼Œä¾‹å¦‚ï¼Œä¸€äº›è®¡ç®—æ ¸ï¼ˆkernelsï¼‰ä»…æ”¯æŒå¯¹ç§°é‡åŒ–ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹åœ¨è¾ƒä½æ¯”ç‰¹ä¸‹çš„å‡†ç¡®æ€§

  - é‡åŒ–ã€åºåˆ—åŒ–å’Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰çš„ç›¸äº’å½±å“ï¼Œä½¿å¾—åŠ è½½é‡åŒ–æ¨¡å‹å˜å¾—å›°éš¾ï¼Œå¹¶ä¸”å¯èƒ½éœ€è¦å¯¹ç”¨æˆ·æ¨¡å‹è¿›è¡Œä¿®æ”¹

* é›†æˆï¼š

  * GemLite ï¼šä¸€ä¸ªåŸºäº Triton çš„è®¡ç®—æ ¸ï¼ˆkernelï¼‰åº“ï¼Œè§£å†³äº†å¤§ batch size åœºæ™¯ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æ”¯æŒæ›´çµæ´»çš„é‡åŒ–æ–¹å¼ã€‚
  * TorchAO ï¼šä¸€ä¸ªåŸç”Ÿ PyTorch åº“ï¼Œä¸ºé‡åŒ–ã€ç¨€ç–æ€§å’Œå¼ é‡å¹¶è¡Œï¼ˆä¸ DTensor ç»“åˆä½¿ç”¨ï¼‰æä¾›äº†ç®€åŒ–çš„ç”¨æˆ·ä½“éªŒã€‚
  * SGLang ï¼šä¸€ä¸ªå¿«é€Ÿã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„ LLM å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒå¹¿æ³›çš„æ¨¡å‹ç±»å‹ã€‚

* a summary of the results in **8xH100 machine on Llama 3.1-8B for decode**. 

  * **int4 Weight-Only Quantization**: This method significantly reduces memory footprint and **accelerates decode for memory-bound workloads**, with minimal impact on performance in compute-intensive scenarios like prefill or larger batch sizes. We present results for bf16, GemLite, and tinygemm kernels below, across various batch sizes and tensor parallel configurations
  * **float8 Dynamic Quantization**: While offering less memory savings, this method often provides higher accuracy and balanced speedups for both memory-bound and compute-bound tasks. With Hopper-grade hardware and native fp8 support, the efficient cutlass/cuBLAS kernels used by AO contribute to a significant speedup

![image-20250409022139221](./LLM-MLSys/image-20250409022139221.png)

> æ›´è¯¦ç»†çš„å®éªŒç»“è®ºï¼šhttps://developers.redhat.com/articles/2024/10/17/we-ran-over-half-million-evaluations-quantized-llms#real_world_benchmark_performance





### è®¿å­˜ä¼˜åŒ–

#### FlashAttention: Fast and Memory-Eï¬ƒcient Exact Attention with IO-Awareness
> https://github.com/HazyResearch/flash-attention
>
> **flashattn + flash-decoding https://zhuanlan.zhihu.com/p/685020608**
>
> FlashAttn V1/V2/V3è®ºæ–‡ç²¾è¯» https://www.bilibili.com/video/BV1ExFreTEYa
>
> åŠ¨ç”»ï¼šhttps://www.bilibili.com/video/BV1HJWZeSEF4
>
> æ ¸å¿ƒæ´å¯Ÿï¼šattentionçŸ©é˜µN^2å¤ªå¤§äº†ï¼Œæ— æ³•åˆ©ç”¨192KBçš„SRAMç¼“å­˜
>
> ç›´è§‚ç†è§£ï¼šåˆ†å—è®¡ç®—æ³¨æ„åŠ›ï¼Œå‰é¢å—çš„æ³¨æ„åŠ›æ˜¯ä¸€ä¸ªå±€éƒ¨æ³¨æ„åŠ›ï¼Œå½“è¿›ä¸€æ­¥è®¡ç®—åé¢æ³¨æ„åŠ›æ—¶ï¼Œéœ€è¦å¯¹å‰é¢çš„å±€éƒ¨æ³¨æ„åŠ›åŠ æƒï¼Œå’Œåé¢çš„æ³¨æ„åŠ›æƒé‡ç›¸åŠ 

##### Attnè®¡ç®—

* 1 SM: â€œ1 head + no batch dimension"
  * å› æ­¤attnçš„head dimè¾ƒå°ï¼Œå¦åˆ™æ— æ³•mapåˆ°ä¸€ä¸ªSMå®Œæˆ
* tilingä¼˜åŒ–æ€è·¯
  * å¯¹contraction axisåštiling

```
for t_tile:
    load(Q[t_tile]) to shared, init O[t, d] = o
    for s_tile:
        load(K[s_tile], V[stile]) to shared;
        compute I[t, s] = Q[t_tile] @ Káµ€[s_tile] (compute p[t, s])
        O[t, d] += p[t_tile, s_tile] @ V[s_tile]
    write O[t, d] 
```

##### FlashAttn

* Intro
  * uses tiling to reduce the number of memory reads/writes
    between GPU high bandwidth memory (HBM) and GPU on-chip SRAM
  * also extend FlashAttention to block-sparse attention
  * 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3Ã— speedup on
    GPT-2 (seq. length 1K), and 2.4Ã— speedup on long-range arena (seq. length 1K-4K).

![image-20250131230121452](./LLM-MLSys/image-20250131230121452.png)

![image-20250201014853281](./LLM-MLSys/image-20250201014853281.png)

* æ€è·¯

  * Our main goal is to avoid reading and writing the attention matrix to and from HBM.
    This requires
    *  (i) computing the softmax reduction without access to the whole input
    *  (ii) not storing the large intermediate attention matrix for the backward pass.

  * (i) We restructure the attention computation to split the input into blocks and make several
    passes over input blocks, thus incrementally performing the softmax reduction (also known as **tiling**).
  * (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.

* ç»“è®ºï¼š
  * in sub-quadratic HBM accesses
  * seq-len 512ï¼Œæ¯”ä»»ä½•ç®—æ³•å¿«
  * seq-len 1000ä»¥ä¸Šï¼Œapproximateç®—æ³•æ›´å¿«
* 3.1 An Eï¬ƒcient Attention Algorithm With Tiling and Recomputation
  * The main challenge in making attention memory-eï¬ƒcient is **the softmax that couples the columns of K (and columns of V).**
  * online softmax
    * Reformer: The eï¬ƒcient transformer
    * **Online normalizer calculation for softmax.**
    * Self-attention does not need ğ‘‚ (ğ‘›2) memory
  * recomputation
    * This can be seen as a form of **selective gradient checkpointing** [10, 34]. While gradient checkpointing has been suggested to reduce the maximum amount of memory required [66],

![image-20250201015720233](./LLM-MLSys/image-20250201015720233.png)

![image-20250503014225408](./LLM-MLSys/image-20250503014225408.png)

* **IOå¤æ‚åº¦å¯¹æ¯”ï¼š**
  * ![image-20250201022049588](./LLM-MLSys/image-20250201022049588.png)
  * For typical values of ğ‘‘ (64-128) and ğ‘€ (around 100KB), ğ‘‘2 is many times smaller than ğ‘€,
  * $$N^2d^2M^{-1}=(Nd)*N/(M/d)$$
* ![image-20250201024230130](./LLM-MLSys/image-20250201024230130.png)

* Evaluation
  * Path-X and Path-256: åƒç´ é¢„æµ‹ï¼Œä¸¤ä¸ªé»‘ç‚¹æ˜¯å¦ç›¸è¿
    * 256*256
    * Block-sparse FlashAttention: 64k seq len
  * æ€§èƒ½ç›¸æ¯”å…¶å®ƒtransformer![image-20250201025905877](./LLM-MLSys/image-20250201025905877.png)

##### Online Softmax

> ã€ŠFrom Online Softmax to FlashAttentionã€‹ã€ã€ŠOnline normalizer calculation for softmaxã€‹

* (Safe) Softmax

  * é—®é¢˜ï¼šSRAMå­˜ä¸ä¸‹N^2çš„logitï¼Œå› æ­¤**need to access Q and K three times**
  * **3 read + 1 store per element**
  * ![image-20250503020010235](./LLM-MLSys/image-20250503020010235.png)

* Online Softmax
  * $$ \sum_{j} \left( \exp(l_j - m_{\text{new}}) \right) = \exp(m - m_{\text{new}}) \sum_{j} \left( \exp(l_j - m) \right) $$ 
    * can also do this for partial sum $\to$ do summing and max in one go 
  
  * **2 read + 1 store per element**
  * ç†è§£ï¼šdi'æ˜¯æ³¨æ„åŠ›æƒé‡çš„ç´¯ç§¯å’Œ
  * ![image-20250201033508793](./LLM-MLSys/image-20250201033508793.png)
  
  * ![image-20250201160812250](./LLM-MLSys/image-20250201160812250.png)
  

#### FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning

> æ ¸å¿ƒï¼šå…ˆéå†Qå†éå†KV
>
> - FAv1è¿˜æ˜¯æ²¡æœ‰åŠæ³•æ¥è¿‘GEMM,ä»…èƒ½è¾¾åˆ°ç¡¬ä»¶FLOPSçš„ç†è®ºä¸Šé™çš„30%,ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚
> - ä¸»è¦åŸå› æ˜¯FAåœ¨GPUä¸Šçº¿ç¨‹å—å’Œwarpä¹‹é—´çš„å·¥ä½œåˆ†é…ä¸ä½³,å¯¼è‡´ä¸èƒ½å‘æŒ¥ç¡¬ä»¶æ•ˆç‡ã€‚
> - é€šè¿‡ä¸‰ä¸ªåšæ³•æ¥è§£å†³ä¸Šè¿°çš„é—®é¢˜
>   - **æ”¹å†™ç®—æ³•ï¼Œå‡å°‘éçŸ©é˜µä¹˜æ³•çš„FLOPS**
>     - éçŸ©é˜µä¹˜åœ¨æ€»è®¡ç®—é‡ä¸­å æ¯”è¾ƒå°‘ï¼Œä½†è€—æ—¶å¾ˆé«˜ï¼ŒGPUå¯¹çŸ©é˜µä¹˜åšäº†å¾ˆå¤šä¼˜åŒ–é€šå¸¸èƒ½å¿«16Xï¼Œå› æ­¤å‡å°‘éçŸ©é˜µä¹˜çš„è®¡ç®—é‡éå¸¸é‡è¦ã€‚
>   - **æ›´å¥½çš„å¹¶è¡Œæ€§**ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå¤´ä¹Ÿå¯ä»¥åœ¨å¤šä¸ªçº¿ç¨‹å—ä¹‹é—´å¹¶è¡Œè®¡ç®—ã€‚
>     - é™¤äº†batchå’Œheadç»´åº¦ï¼Œåºåˆ—é•¿åº¦ä¹Ÿè¦æ”¯æŒå¹¶è¡ŒåŒ–ï¼Œè¿™æ ·èƒ½æé«˜GPUå ç”¨ç‡ã€‚
>   - çº¿ç¨‹å—å†…éƒ¨ï¼Œé€šè¿‡åˆç†çš„ç¼–æ’warpæ¥å‡å°‘å…±äº«å†…å­˜ä¸å¿…è¦çš„è®¿é—®ä»¥åŠé€šä¿¡ã€‚
> - ç»è¿‡ä¸Šé¢ä¸‰ä¸ªæ”¹é€ ç‚¹ï¼Œæ€§èƒ½ä¸Šv2æ¯”v1æå‡æ¥2å€ï¼Œæ•ˆç‡æ¥è¿‘GEMM,è¾¾åˆ°ç†è®ºFLOPSçš„70%
> - ä½¿ç”¨äº†cutlass



![image-20250511165841859](./LLM-MLSys/image-20250511165841859.png)

* è®¡ç®—é‡æ’åºï¼šå°†å¤–å¾ªç¯æ”¹ä¸ºéå†Qçš„å—å†…å¾ªç¯éå†K,Vçš„å—ï¼Œæé«˜äº†æ•°æ®å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§ã€‚
  * Qæ¯”KVåœ¨SRAMå¯ä»¥é©»ç•™æ›´é•¿çš„æ—¶é—´ï¼Œç¼“å­˜çš„å­˜æ´»æ—¶é—´æ›´é•¿ï¼Œæ›´èƒ½å‡å°‘HBMçš„è®¿é—®æ¬¡æ•°ã€‚
* å½’ä¸€åŒ–æ“ä½œï¼šæ”¾åœ¨å¾ªç¯å¤–ï¼Œ GPU çº¿ç¨‹ä¹‹é—´çš„å¹¶è¡Œæ€§å¾—åˆ°äº†æå¤§çš„æå‡ã€‚

* å¯¹äºå‰å‘è®¡ç®—:
  1. æ”¹æˆäº†å…ˆéå†Qåœ¨éå†kvï¼Œè¿™æ ·åšåˆ™æ˜¯åœ¨ä½¿å…¶åœ¨è¡Œå—å®ç°äº†å¹¶è¡Œã€‚ æ¯ä¸ªè¡Œå—åˆ†é…ç»™ä¸€ä¸ªGPUçš„thread blockï¼Œä»å…±äº«å†…å­˜çš„è§†è§’çœ‹Så¯ä»¥è¢«æ›´å¥½çš„å¤ç”¨ï¼Œä»å¹¶è¡Œåº¦ä¸Šå°†ï¼Œå¯ä»¥åˆ©ç”¨åºåˆ—ç»´åº¦çš„å¹¶è¡Œæ€§æ¥åŠ é€Ÿè®¡ç®—ã€‚
  2. å¯¹äºæ¯ä¸ªworkå†…éƒ¨çš„warpï¼Œå°½å¯èƒ½çš„å‡å°‘å…¶å¯¹å…±äº«å†…å­˜çš„è¯»å†™å¯ä»¥è·å¾—å¯è§‚çš„åŠ é€Ÿï¼Œfa1 æ˜¯å…±äº«kvï¼Œä½†æ˜¯qéœ€è¦è¢«æ‰€æœ‰warpè®¿é—®å’Œè¯»å†™ï¼Œä¸»è¦æ˜¯é¢‘ç¹çš„æ›´æ–°å…±äº«å†…å­˜ã€‚
  3. è€Œ fa2ï¼Œå…±äº«äº†qï¼Œæ¯ä¸ªwarpè¯»å–è‡ªå·±åˆ†å—å†…çš„kvï¼Œä¸éœ€è¦æ›´æ–°ä¸é€šä¿¡ï¼Œè·å¾—äº†åŠ é€Ÿã€‚
* å¯¹äºåå‘è®¡ç®—:
  1. è¿˜æ˜¯æŒ‰ç…§åˆ—å—è¿›è¡Œå¹¶è¡Œï¼Œè¿™æ ·å¹¶è¡Œçš„ç»„å—ä¹‹é—´æœ‰æœ€å°çš„é€šä¿¡è¡Œä¸ºï¼Œå¦åˆ™dkå’Œdvä¹Ÿè¦å…±äº«é€šä¿¡äº†ã€‚
  2. å°½ç®¡æŒ‰åˆ—åˆ†å—åï¼Œdqï¼Œdkï¼Œdvä¹‹é—´çš„ç›¸äº’ä¾èµ–å¾ˆå¤æ‚ï¼Œä½†é¿å…åˆ‡kï¼Œä¹Ÿä¾æ—§å¯ä»¥å‡å°‘warpå†…éƒ¨çš„å…±äº«å†…å­˜è¯»å†™çš„å¼€é”€ã€‚

#### FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision

**æ´å¯Ÿ**:  æ²¡æœ‰å……åˆ†åˆ©ç”¨æœ€æ–°ç¡¬ä»¶ä¸­çš„æ–°åŠŸèƒ½(tensor core ä¸ TMA)

1. å¼‚æ­¥åŒ–: åˆ©ç”¨ä¸“ç”¨warpé‡å è®¡ç®—ï¼Œmatmulå’Œsoftmaxã€‚
2. ä½ç²¾åº¦: åº”ç”¨FP8é‡åŒ–ï¼Œæ›´å¥½çš„åˆ©ç”¨tensor coreç‰¹æ€§ã€‚

æŒ‘æˆ˜: é‡å†™FA2æ¥é€‚é…å¼‚æ„ç¡¬ä»¶ï¼Œæœ€å°åŒ–FP8/4çš„é‡åŒ–è¯¯å·®ã€‚

**æ–¹æ¡ˆ**: 

1. ä¸“ç”¨warpå¼‚æ­¥åŒ–: é€šè¿‡æ‹†è§£ç”Ÿäº§è€…/æ¶ˆè´¹è€…warpæ¨¡å¼ï¼Œç§»åŠ¨æ•°æ®æ¥å®ç°æŒ‡ä»¤ä¸è®¿å­˜çš„é‡å 
2. éšè—softmaxè®¡ç®—:é€šè¿‡ä¼˜åŒ–ä¾èµ–å…³ç³»ï¼Œå°†éGEMMçš„è®¡ç®—éšè—åœ¨GEMMçš„å¯å¼‚æ­¥åŒ–é˜¶æ®µ
3. å—é‡åŒ–/éç›¸å¹²å¤„ç†:  è¡¥å¿FP8é‡åŒ–é€ æˆçš„ç²¾åº¦æŸå¤±ã€‚

* Tritonå®ç°ï¼šæ˜¾å­˜ä¸Šå®ç°ringbuffer

### Decodingä¼˜åŒ–

* Speculative Decoding, Lookahead Decoding, Flash-Decoding, Flash-decoding++, Deja Vu, Atom, Continunous Batchingï¼ŒPrefill-Decode Disaggregating

#### Speculative Decoding

> Draft modelã€large model

ã€ŠSpeculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generationã€‹

ã€ŠFast Inference from Transformers via Speculative Decodingã€‹

#### Flash-Decoding

> æ€è·¯ï¼šparallelize KVè®¡ç®—ï¼Œç”¨æ»¡GPU

* è§£å†³çš„é—®é¢˜ï¼š
  * **FlashAttention is Sub-optimal for Long-Context Inference**
    * parallelizes across blocks of queries and batch size only, and does not manage to occupy the entire GPU during token-by-token decoding.

* https://crfm.stanford.edu/2023/10/12/flashdecoding.html æœ‰åŠ¨ç”»

### MoE æ¨ç† â€”â€” Expert Parallelism

* Seedï¼šhttps://arxiv.org/abs/2504.02263

### Long-Contextä¼˜åŒ–

#### Ring Attention â€”â€” Sequence Parallel Attention Across Devices

> GPU Mode Lecture 13 https://www.youtube.com/watch?v=ws7angQYIxI

* æ˜¾å­˜ï¼šflash-attnçš„æ˜¾å­˜éšseq-lençº¿æ€§å¢é•¿
  * flash-attnå°†æ˜¾å­˜ä»O(s^2)é™åˆ°äº†O(s)
  * ![image-20250512023151453](./LLM-MLSys/image-20250512023151453.png)

* é•¿ä¸Šä¸‹æ–‡FLOPS
  * ![image-20250512024143990](./LLM-MLSys/image-20250512024143990.png)

* blockwise attn
  * åŠ¨ç”»ï¼šhttps://www.youtube.com/watch?v=JhR_xo9S0_E
  * ![image-20250513215122201](./LLM-MLSys/image-20250513215122201.png)

* SP
  * å‚è€ƒã€ŒMLSys.md â€”â€”å¹¶è¡Œè®­ç»ƒ â€”â€” SPã€

* ring-attn

  * ![image-20250513225747367](./LLM-MLSys/image-20250513225747367.png)
  * ![image-20250514001654067](./LLM-MLSys/image-20250514001654067.png)

  * ring attentionçš„é—®é¢˜ï¼šidle worker
    * ![image-20250514002313692](./LLM-MLSys/image-20250514002313692.png)
    * ![image-20250514002501925](./LLM-MLSys/image-20250514002501925.png)

#### Striped Attention (Reorder QKV)

![image-20250514003125607](./LLM-MLSys/image-20250514003125607.png)

![image-20250514003221884](./LLM-MLSys/image-20250514003221884.png)

## æ¨ç†æ¡†æ¶

* MLLMæ¨ç†
  * SGLang
  * LMDeploy
  * vLLM

### SGLang

* Intro
  * known for its almost [zero-overhead batch scheduler](https://lmsys.org/blog/2024-12-04-sglang-v0-4/) and fast [constrained decoding](https://lmsys.org/blog/2024-02-05-compressed-fsm/)

### ollama

* æ›´é€‚åˆæœ¬åœ°å®éªŒ
* [ollama deepseek-r1](https://ollama.com/library/deepseek-r1:8b)
* open-webui ç‰ˆæœ¬ï¼šdyrnq/open-webui:latest

## æ¨¡å‹è®­ç»ƒ

> å¹¶è¡Œè®­ç»ƒå‚è€ƒ MLSys.md

### å¤§è§„æ¨¡é›†ç¾¤

- [Meta LIama 3](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)ï¼Œ16k GPU å¹¶è¡Œè®­ç»ƒï¼ŒèƒŒé ï¼ˆç‹¬ç«‹çš„ï¼‰ä¸¤ä¸ªè§„æ¨¡è¾¾24K çš„ H100 é›†ç¾¤ï¼Œåˆ†åˆ«åŸºäº RoCE å’Œ IB æ„å»ºå•é“¾è·¯å¸¦å®½400Gbpsçš„èŠ‚ç‚¹äº’è”ã€‚
- [Google Gemini 1.5](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)ï¼ŒåŸºäºæ•°åä¸ª 4k TPUv4 Pod å¹¶è¡Œè®­ç»ƒï¼ŒPod å†…éƒ¨ 3D-Torus ICI äº’è”ï¼Œå•é“¾è·¯å¸¦å®½ 800Gbpsã€‚
  - TPUæœ‰SuperPodå¤§è§„æ¨¡ICIçš„ä¼˜åŠ¿
- [å­—èŠ‚ MegaScale](https://arxiv.org/abs/2402.15627)ï¼Œ12k GPU å¹¶è¡Œè®­ç»ƒã€‚

### Ckpt

* å­—èŠ‚Ckpt https://mp.weixin.qq.com/s/4pIAZqH01Ib_OGGGD9OWQg
  * ByteCheckpoint ï¼Œä¸€ä¸ª PyTorch åŸç”Ÿï¼Œå…¼å®¹å¤šä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ Checkpoint çš„é«˜æ•ˆè¯»å†™å’Œè‡ªåŠ¨é‡æ–°åˆ‡åˆ†çš„å¤§æ¨¡å‹ Checkpointing ç³»ç»Ÿã€‚

## è½¯ç¡¬ååŒ

### [Trends in Deep Learning Hardware: Bill Dally (NVIDIA)](https://www.youtube.com/watch?v=kLiwvnr4L80)

### DeepSeek-V3 çš„ç¡¬ä»¶ç•…æƒ³

* the **SMs** primarily perform the following tasks for **all-to-all communication:** ï¼ˆ 20/132 SMs for H800ï¼‰
  â€¢ Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB
  traffic destined for multiple GPUs within the same node from a single GPU.
  â€¢ Transporting data between RDMA buffers (registered GPU memory regions) and in-
  put/output buffers.
  â€¢ Executing reduce operations for all-to-all combine.
  â€¢ Managing fine-grained memory layout during chunked data transferring to multiple
  experts across the IB and NVLink domain.
  * æœŸæœ›ç”¨ç±»ä¼¼ NVIDIA SHARP Graham et al. (2016). æ¥åš
  * aim for this hardware to unify the IB (scale-out) and NVLink
    (scale-up) networks from the perspective of the computation units
* ScaleUPå’ŒScaleOutè¯­ä¹‰çš„èåˆæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„å·¥ä½œ, å‡†ç¡®çš„æ¥è¯´åœ¨ScaleOutä½¿ç”¨RDMAå°±æ˜¯ä¸€ä¸ªé”™è¯¯, å¹¶ä¸”æƒ³ç®€å•çš„åœ¨ScaleUPä½¿ç”¨RDMAä¹Ÿæ˜¯ä¸€ä¸ªé”™è¯¯.
  * [ã€ŠHotChip2024åè®°: è°ˆè°ˆåŠ é€Ÿå™¨äº’è”åŠScaleUPä¸ºä»€ä¹ˆä¸èƒ½ç”¨RDMAã€‹](https://mp.weixin.qq.com/s?__biz=MzUxNzQ5MTExNw==&mid=2247492300&idx=1&sn=8a239883c831233e7e06659ec3425ea2&scene=21#wechat_redirect)

### Fire-Flyer AI-HPC: **A Cost-Effective** Software-Hardware Co-Design for Deep Learning

> https://blog.csdn.net/m0_59163425/article/details/143349082

* ä½¿ç”¨äº†Pcleæ¥å£çš„A100èŠ¯ç‰‡ï¼ˆä¾¿å®œç‰ˆæœ¬ï¼Œè€Œéæ›´æ˜‚è´µçš„NVIDIA DGXï¼‰ï¼Œæ¯”åŸæ¥AIè®­ç»ƒçš„ä¸“ç”¨èŠ¯ç‰‡ç›´æ¥å°‘äº†ä¸€åŠçš„æˆæœ¬ã€‚åœ¨10,000 GPUé›†ç¾¤ä¸Šï¼Œå®ç°äº†DGX-A100 80%çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½50%æˆæœ¬å’Œ40%èƒ½è€—ï¼Œè¯æ˜äº†è¯¥è®¾è®¡çš„æˆæœ¬æ•ˆç›Šã€‚
* æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬ï¼š
  * è‡ªç ”**HFReduce é€šä¿¡åº“**æå‡ AllReduce æ•ˆç‡ï¼Œé€šè¿‡ CPU å¼‚æ­¥å¤„ç†å‡å°‘ PCIe å¸¦å®½å ç”¨ï¼›
  * ä¼˜åŒ–**HaiScale æ¡†æ¶**æ”¯æŒæ•°æ®ã€æµæ°´çº¿ã€å¼ é‡å¹¶è¡Œç­‰å¤šç§å¹¶è¡Œç­–ç•¥ï¼›
  * è®¾è®¡**ä¸¤å±‚ Fat-Tree ç½‘ç»œ**æ•´åˆè®¡ç®—ä¸å­˜å‚¨æµé‡ï¼Œé€šè¿‡ 3FS åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå®ç° 8TB/s è¯»å–ååé‡ï¼›HAI å¹³å°æä¾›ä»»åŠ¡è°ƒåº¦ä¸æ•…éšœæ¢å¤ï¼Œä¿éšœå¤§è§„æ¨¡é›†ç¾¤ç¨³å®šæ€§ã€‚
* HF Reduce
  * **å¼‚æ­¥æ¢¯åº¦èšåˆ**ï¼šé€šè¿‡ CPU é¢„å¤„ç†æ¢¯åº¦ï¼ˆD2H ä¼ è¾“ + èŠ‚ç‚¹å†… Reduceï¼‰ï¼Œå†ç» IB ç½‘ç»œè·¨èŠ‚ç‚¹ AllReduceï¼Œè¾ƒ NCCL æå‡ 2-3 å€å¸¦å®½åˆ©ç”¨ç‡ï¼ˆå›¾ 7aï¼‰ã€‚
  * **NVLink å¢å¼º**ï¼šé›†æˆ NVLink æ¡¥æ¥åï¼Œè·¨åŒºé€šä¿¡å¸¦å®½çªç ´ 10GB/sï¼ˆå›¾ 7bï¼‰ï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œé«˜æ•ˆè®¡ç®—ã€‚
  * ![image-20250502121007854](./LLM-MLSys/image-20250502121007854.png)
* **HaiScale è®­ç»ƒæ¡†æ¶**ï¼š
  - å¤šå¹¶è¡Œç­–ç•¥
    - æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ï¼šå¼‚æ­¥ AllReduce é‡å è®¡ç®—é€šä¿¡ï¼ŒVGG16 è®­ç»ƒæ—¶é—´è¾ƒ PyTorch DDP å‡åŠï¼ˆå›¾ 8aï¼‰ã€‚
    - æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ï¼šé€šè¿‡èŠ‚ç‚¹å†… GPU åˆ†å±ä¸åŒ DP ç»„ï¼Œå‡å°‘ç½‘ç»œæ‹¥å¡ï¼ŒLLaMA-13B è®­ç»ƒå¹¶è¡Œæ•ˆç‡è¾¾ 91%ï¼ˆå›¾ 9aï¼‰ã€‚
  - **FSDP ä¼˜åŒ–**ï¼šå†…å­˜ç®¡ç†æ›´é«˜æ•ˆï¼ŒGPT2-Medium è®­ç»ƒå¹¶è¡Œ scalability è¾¾ 95%ï¼ˆå›¾ 8bï¼‰ã€‚
* **3FS åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ**ï¼š
  - **ç¡¬ä»¶é…ç½®**ï¼š180 èŠ‚ç‚¹ Ã—16 NVMe SSDï¼Œæä¾›**8TB/s è¯»å–ååé‡**ä¸ 20PiB å­˜å‚¨å®¹é‡ã€‚
  - æŠ€æœ¯äº®ç‚¹
    - é“¾å¼å¤åˆ¶ï¼ˆCRAQï¼‰ä¿è¯æ•°æ®ä¸€è‡´æ€§ï¼Œè¯·æ±‚ - å‘é€æ§åˆ¶æœºåˆ¶é¿å…ç½‘ç»œæ‹¥å¡ã€‚
    - é›†æˆ 3FS-KV æ”¯æŒé”®å€¼å­˜å‚¨ï¼Œé™ä½ LLM æœåŠ¡æˆæœ¬ä¸€ä¸ªæ•°é‡çº§ã€‚
* **HAI å¹³å°**ï¼š
  - **æ—¶é—´å…±äº«è°ƒåº¦**ï¼šæŒ‰èŠ‚ç‚¹ç²’åº¦åˆ†é…èµ„æºï¼Œåˆ©ç”¨ç‡è¾¾ 99%ï¼Œæ”¯æŒä»»åŠ¡æ–­ç‚¹ç»­ä¼ ã€‚
  - **æ•…éšœæ¢å¤**ï¼šCheckpoint Manager æ¯ 5 åˆ†é’Ÿå¼‚æ­¥ä¿å­˜ï¼Œä»…ä¸¢å¤±æœ€æ–° 5 åˆ†é’Ÿæ•°æ®ï¼›Validator å·¥å…·å‘¨æ£€ç¡¬ä»¶çŠ¶æ€ï¼Œæå‰è¯†åˆ« GPU Xid é”™è¯¯ï¼ˆè¡¨ VIï¼‰ã€‚

### å…¶å®ƒ

MTP ~ [**Zen5çš„2-Ahead Branch Predictor**](https://chipsandcheese.com/p/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks)

## Vision Model æ¨ç†

* Swinï¼šmicrosoft/swinv2-large-patch4-window12-192-22k
  * pytorchåŸºç¡€ä½¿ç”¨
  * 1*V100
    * Batch_size=4, qps=32ï¼š æ˜¾å­˜ç“¶é¢ˆ 24G/32Gï¼Œ150W/300W
    * Batch_size=8, qps=20ï¼š æ˜¾å­˜ç“¶é¢ˆ 26G/32Gï¼Œ115W/300W
    * Batch_size=2, qps=27ï¼š æ˜¾å­˜ç“¶é¢ˆ 30G/32Gï¼ŒW/300W
  * æ³¨ï¼šqpså·²ç»è€ƒè™‘äº†batch_size

* Dinov2
  * Batch_size=4, qps=50: æ˜¾å­˜ 14Gï¼Œ120W

## åˆ†å¸ƒå¼ Agent

* [èš‚èš Ray](https://mp.weixin.qq.com/s/TFxzMJyQVoffV4SpiTh9AQ?open_in_browser=true)

  * è§£å†³çš„é—®é¢˜ï¼šagentçš„è´Ÿè½½ã€å½¢å¼å¤šæ ·ï¼Œä»POCåˆ°ä¸Šçº¿çš„gap
  * Ray-Agent ï¼ˆragentï¼‰
    * ä¸»è¦è€ƒè™‘ç‚¹å¦‚ä¸‹ï¼šâ‘ è¯¥æ¡†æ¶éœ€æä¾› Agent çš„ APIï¼›â‘¡åˆ©ç”¨ Ray å®ç°ä»æœ¬åœ°ä»£ç åˆ°æ”¯æŒå¼‚æ„èµ„æºçš„åˆ†å¸ƒå¼ä»£ç çš„æ‰©å±•ï¼›â‘¢åœ¨å¤š Agent åœºæ™¯ä¸­ï¼Œæ¯ä¸ª Agent éƒ½æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è¿›ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¡†æ¶æ¥åè°ƒè¿™äº›è¿›ç¨‹ï¼Œå³æ‰€è°“çš„ environmentï¼›â‘£è¦å…¼å®¹ä¸åŒçš„åº“ï¼Œå¦‚ MetaGPT å’Œ AutoGenï¼›â‘¤å¸Œæœ›åˆ©ç”¨ Ray çš„æ²™ç®±ï¼ˆsandboxï¼‰ã€æ‰¹å¤„ç†èƒ½åŠ›å’Œè·¨æºè°ƒåº¦åŠŸèƒ½ã€‚

  * ![image-20250228002001015](./LLM-MLSys/image-20250228002001015.png)

  * ![image-20250228002616025](./LLM-MLSys/image-20250228002616025.png)

  * ![image-20250228003128059](./LLM-MLSys/image-20250228003128059.png)
  * ![image-20250228003143293](./LLM-MLSys/image-20250228003143293.png)

  * ![image-20250228005839274](./LLM-MLSys/image-20250228005839274.png)

  * ![image-20250228010735262](./LLM-MLSys/image-20250228010735262.png)
  * æœªæ¥æœŸæœ›ï¼š
    * Agent Mesh/Agent Protocol
    * ç¦»åœ¨çº¿ä¸€ä½“æ¶æ„ï¼šå¯ä»¥ç”¨ Ray Data pipeline å®Œæˆç¦»çº¿å·¥ä½œ

## LLMOps

[Observability in LLMOps pipeline - Different Levels of Scale](https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline)