[toc]

# 深度学习推荐系统 —— 王喆

## 算法心得

* 加特征：某些维度上高低估的情况，新增特征之前计算calibration
* 分析全链路，链路瓶颈在哪
  * 比如保送精排有收益说明召回粗排不准
* 是否有信息增益

### 搜广推的差异

> 王喆：排得更好VS估得更准VS搜的更全
>
> 「推荐、广告、搜索」算法间到底有什么区别？https://zhuanlan.zhihu.com/p/430431149

* 问题定义

  * 搜索：搜索的经典场景是用户主动输入查询query，较明确地表达需求，然后搜索引擎从数据库中检索得到topK最优结果，最后展现给用户，这是一个用户主动获取信息的过程。

  * 推荐：推荐一般不存在用户主动提供query，而是系统根据其用户画像（性别、职业...）以及历史行为（浏览、点击、收藏...）等，排出最可能使用户消费的内容展现出来，这是一个用户被动获取信息的过程。

  * 广告：一般广告的参与有三个角色：广告主、平台和用户。与搜推的为用户找信息的过程不同，广告则是为信息找人，目的也非常直接，纯粹就是为公司增加收入，可以说是离 最近的业务了。

* 业务目标

  - 搜索：相比与推荐和广告，搜索在某种意义上是存在『正确答案』的，即用户是带着明确目的来完成这次行为的。
    - 所以搜索第一目标就是相关性，围绕『搜索词』展开优化，将"正确答案"展现给用户；
    - [推荐算法](https://zhida.zhihu.com/search?content_id=183780223&content_type=Article&match_order=2&q=推荐算法&zhida_source=entity)强调的个性化永远只是搜索算法的补充。“围绕着搜索词的信息高效获取问题“才是搜索算法想解决的根本问题。
    - 其次目标才是CTR/CVR/GMV等。随着相关商品量和算法的发展，现在也越来越强调类似推荐的 **个性化/千X千面** 。

  - 推荐：推荐算法的预估目标就不尽相同，视频类更多倾向于预测观看时长，新闻类预测CTR，电商类预估客单价等等这些 **跟用户参与度最相关的业务指标** 。
    - 只有用户的参与度高了，才能让广告系统有更多的inventory，进而增加公司营收。

  - 广告：各大公司广告算法的预估目标非常统一，就是 **预估CTR和CVR**，增加收入

* 评价指标：

  * 搜索：**非常看重能否把这些正确答案给召回回来**，针对召回率、MAP、NDCG这些指标进行优化
  * 推荐：由于推荐系统的推荐场景是生成一个列表，所以更加关注item间的相对位置，因此评估阶段更倾向于用AUC，gAUC，MAP这些指标作为评价标准。
  * 广告：因为CPC和CPA计价仍是效果类广告系统的主流计价方式。所以只有预估出CTR和CVR，才能反向推导出流量的价值，并进一步给出合理的出价。所以针对这样的目标，广告算法非常看重把**预估偏差**当作首要的评价指标。

* 算法设计

  * 搜索：**强调搜索词的关键性，以及对搜索词的理解**
    * 搜索词与其他特征组成的交叉特征，组合特征，以及模型中的交叉部分是异常重要的
    * 一定程度上要抑制个性化的需求，更多把质量和权威性放在更重要的位置
  * 推荐：list level，page level
    * 对推荐内容的**多样性，新鲜度**有更高的要求，这就让探索与利用，强化学习等一些列方法在推荐场景下更受重视。
  * 广告：对calibration方法的严苛要求，就算模型训练的过程中存在偏差，比如使用了负采样、weighted sampling等方式改变了数据原始分布，也要根据正确的后验概率在各个维度上矫正模型输出。此外，因为广告是很少以列表的形式连续呈现的，要对每一条广告的CTR，CVR都估的准，**广告算法大都是point wise的训练方式**。

* 辅助策略和算法上的区别

  * 搜索：NLP技术，对大量内容进行预处理，embedding化
  * 推荐：照顾用户的长期兴趣，需要一些补充策略做出一些看似“非最优”的选择，**比如探索性的尝试一些长尾内容**
  * 广告：pacing，bidding，budget control，ads allocation

* 模型结构差异

  * 搜索：搜索词和item之间天然是一个双塔结构，因此在模型构建的时候各种交叉特征，模型中的各种交叉结构往往是搜索类模型的重点。
  * 推荐：如果不抓住用户兴趣的连续变化，是很难做好推荐模型的，因此利用sequential model来模拟用户兴趣变化往往是有收益的
  * 广告：兴趣是不那么连贯的，因此容易造成sequential model的失效，attention机制可能会更加重要一些
    * 解释了为什么DIN简单实用

* 系统痛点

  * 搜索：往往把重心放在**搜索词和item的内容理解上**，只要能做好这一点，模型结构本身反而不是改进的关键点了，但是在多模态的时代，图片、视频内容的理解往往是制约搜索效果的痛点。
  * 推荐：**问题往往卡在长期利益与短期利益的平衡上**，在模型结构红利消失殆尽的今天，如何破局是推荐算法工程师们做梦都在想的问题。
  * 广告：**各模块协同工作找到平台全局利润最大化方法的难度非常大**，系统异常复杂到难以掌控的地步

## chpt1 互联网的增长引擎——推荐系统

概念：

* UGC(User Generated Content)，典型如Youtube、Tiktok
* CVR (Conversion Rate)、观看时长(Youtube Recommendations)

推荐系统架构：数据与模型

* 物品信息、用户信息、场景信息
* 数据离线批处理、实时流处理

## chpt2 前深度学习时代——推荐系统的进化之路

1. 演化关系图  p13
2. 协同过滤(CF)

[一篇详细的介绍文章](https://zhuanlan.zhihu.com/p/80069337)

UserCF (1994)
* 用户相似度计算：余弦相似度/皮尔逊相关系数/引入物品平均分
* 最终结果的排序：按相似度加权
* 特点：社交特性更强
* 缺点：
  * 用户太多，在线存储系统存不下矩阵。 
  * 不适用于正反馈获取困难的应用场景（酒店预定、大件商品购买），用户历史数据向量稀疏

UserCF 和 ItemCF 的对比
* UserCF 适合用在个性化需求不强，热点很明显的领域，比如新闻，电影推荐
* ItemCF 适合用在个性化需求比较强，长尾比较长的领域，比如书，电商的推荐

协同过滤的缺点
* 推荐系统的头部效应较明显，处理稀疏向量的能力弱 => MF用更稠密的隐向量表示用户和物品
* 无法引入场景信息和更精细的用户/物品信息 => LR模型、机器学习模型



3. 矩阵分解(MF)

* 用户和物品的隐向量通过分解协同过滤生成的共现矩阵得到
* 矩阵分解的方法：特征值分解（方阵，不适用）；SVD（计算量大、要求稠密）；梯度下降法
* 矩阵分解可加入偏差向量
* 优点：
  * 泛化能力强
  * 空间复杂度低
  * 更好的扩展性和灵活性
* 缺点
  * 同样不方便加入场景信息和更精细的用户/物品信息



4. 逻辑回归 (LR)

将推荐问题转换为一个CTR预估问题

流行原因：1）数学含义上的支撑 2）可解释性强 3）工程化的需要



5. 从FM到FFM——自动特征交叉的解决方案

* Poly2——特征交叉的开始
* FM模型——隐向量特征交叉 (2012-2014)
  * 参数减少到n*k，减小训练开销，增大收敛可能
* FFM模型——引入特征域的概念 (2015)
  * Features are empirically categorized into several groups
  * Within each group, features are further identified as user-side and ad-side features
  * For each group, user-side and ad-features are sum-pooled, respectively, followed by element-wise multiplication.
  * Limitations
    * Feature-level interactions are lost due to sum-pooling
    * Using element-wise multiplication of sum-pooled embeddings leads to high dimensional output.



6. GBDT+LR——特征工程模型化的开端

组合模型，一定程度上解决特征交叉的问题

优点：e2e特征工程模型化

缺点：容易过拟合；丢失大量特征的数值信息



7. LS-PLM —— alibaba曾经的主流推荐模型

也称为MLR(Mix Logistic Regression)，在逻辑回归的基础上加入聚类的思想，其灵感来自对广告推荐领域样本特点的观察

* 超参数：分片数m，阿里的经验值为12

优势：

* 端到端的非线性学习能力
* 模型稀疏性强



引申：

* L1范数比L2范数更容易产生稀疏解，因为加入正则化项的损失函数最小值更容易在参数空间的顶点处取得，对应稀疏参数

## chpt3 浪潮之巅——深度学习在推荐系统中的应用

1. 演化关系图：p51

2. [AutoRec](https://zhuanlan.zhihu.com/p/159087297) —— 单隐层神经网络推荐模型

结合了auto-encoder和协同过滤的思想，本质上是训练auto-encoder保存推荐系统的泛化信息，输入一个“不完整”的、“真实”的评分条目列表，得到对应这个条目列表的相似条目列表

* I-AutoRec, U-AutoRec

3. Deep Crossing模型 —— 经典的深度学习架构

* 如何解决稀疏特征向量稠密化的问题？
  * 除数值类特征，进入Embedding层
* 如何解决特征自动交叉的问题？
  * 无人工特征交叉，由 **残差神经网络** 加强提取非线性特征和组合特征信息的能力
* 如何在输出层中达成问题设定的优化目标？
  * 最后一层逻辑回归

4. NeuralCF模型——CF与深度学习的结合

* 用神经网络替代CF最后一层打分的点积操作
* NeuralCF混合模型：concat原始NeuralCF模型的输出和以element-wise product为互操作的广义矩阵分解模型
* 局限性和CF一致：没引入其他类型的特征、具体的互操作有待探索



5. PNN模型——加强特征交叉能力

Product层：线性操作 + 乘积操作
* 乘积操作 = 内积(IPNN)/外积(OPNN)
* 叠加外积互操作矩阵：本质上是让所有embedding通过一个平均池化层后，再进行外积互操作
  * [embedding的意义](https://www.zhihu.com/question/374835153/answer/1042845667)
  * 简单的sumpooling会忽略一些有价值的信息



6. Wide&Deep模型——记忆能力和泛化能力的综合

具体参考[MLSys.md](https://github.com/huangrt01/CS-Notes/blob/master/Notes/Output/MLSys.md)



7. FM与深度学习模型的结合

* FNN——用FM的隐向量完成Embedding层初始化
  * 背景：Embedding层收敛速度慢 <= 参数多、输入向量稀疏
* DeepFM——用FM代替Wide部分
* NFM——FM的神经网络化尝试
  * 在Embedding层和多层神经网络之间加入**特征交叉池化层**
  * NFM和FM的关系：FM是一种浅层的线性模型, 可以看作是不带隐层的NFM
  * 相比Wide&Deep, Deep&Cross用concat的方法连接特征，NFM方法更侧重特征之间的深度交互



8. 注意力机制在推荐模型中的应用

* AFM——引入注意力机制的FM
  * 在NFM的Pair-wise Interaction Lyaer和池化层之间加入注意力网络
  * 注意力网络 = 全连接层 + softmax，网络输入是 element-wise 交叉的特征vector



#### (DIN) Deep Interest Network for Click-Through Rate Prediction ——引入注意力机制的深度学习网络

  * 核心动机是常见的 Embedding&MLP 模型，user 特征的表达能力受限于 fixed-length vector。user特征表达能力难以通过简单的翻倍方式来提升（将多个域的user特征连接起来），考虑到线上压力以及过拟合的风险，需要寻求算法上的突破 => weighted sum-pooling
  * Attention 技术：NLP、搜索领域，通常用来刻画context（比如用target词加权句子、用target广告加权最近的query）。本文创新性地用广告特征来加权user序列特征
  * 应用于淘宝的电商广告推荐场景，感觉在电商场景，推荐和广告天然结合地紧密（广告就是推荐的商品）；短视频场景，两者更割裂一些，并且用户兴趣更长期
      * 电商场景，搜索个性化的必要性也更强，和推荐区分不严格
    * 此外，搜索广告场景先通过相关性做召回，严格意义上进入推荐系统的候选数量少
  * 用户侧的embedding是对每次行为的embedding通过注意力加权得到，注意力权重受广告特征影响
    * 广告侧：论文中有 goods id, shop id, cate id；实践中可以pooling ad id, category id, position encoding (特征抽取时做分钟的sqrt，相比 log 对长期历史行为有区分度)
    * 商铺id只和用户历史行为中的商铺id序列发生作用，商品id也如此 <=> 注意力轻重更应该由同类信息相关性决定
  * $\textbf{V}_u=f(\textbf{V}_a)=\sum_{i=1}^N\omega_i·\textbf{V}_i=\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a)·\textbf{V}$ 
      * 注意力激活单元：元素减操作的embedding（原论文中是取外积向量）, concat两个原输入embedding
      * $\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a) \neq 1$ ，描述 an approximation of the intensity of activated user interests to some degree
  * Training Techniques
    * Mini-batch Aware Regularization：引入L2范数，通过“只对输入中有的sparse fc算”减少运算量
    * Data Adaptive Activation Function：PReLu -> Dice
  * Experiments
    * 新的AUC计算方式：GAUC (averaging AUC over users)
    * Regularization 实验：Mini-Batch Aware L2 > Occurrency Filter > DropOut > Regularization in DiFacto > Base

#### DIEN——序列模型与推荐系统的结合

* 尝试刻画 latent interest 而非用 behaviour 描述 interest

* 兴趣进化网络
  * 行为序列层：普通的Embedding层
  * 兴趣抽取层：GRU
    * auxiliary loss: extra supervision information, uses consecutive behavior to supervise the learning of hidden state at each step. which makes hidden state expressive enough to represent latent interest. 算法实现上，是让第i次的 hidden state 向量更接近第 i+1 次的正样本 item 向量
  * 兴趣进化层：(attentional update gate) AUGRU，引入注意力机制，与 DIN 相似，更有针对性地模拟与目标广告相关的兴趣进化路径
    * 文章中讨论了 attention 结合 GRU 的几种方法：AIGRU、AGRU、AUGRU

#### (DSIN) Deep Session Interest Network for Click-Through Rate Prediction

* multiple sessions：30 mins gap
* session内用户兴趣抽取：multi self-attention mechanism(AutoInt) with bias encoding
* session间兴趣进化：Bi-LSTM
* 聚合兴趣预估ctr：local activation unit

#### UIC+MIMN

[从阿里的User Interest Center看模型线上实时serving方法](https://zhuanlan.zhihu.com/p/111929212)

* system: UIC

* algorithm: MIMN = improved NTM with two designs of memory utilization regularization and memory induction unit

3.REALTIME CTR PREDICTION SYSTEM

- 电商领域，特征中 90% 的规模是 user behaviour features
- long sequence 架构上的挑战主要在于 storage 和 latency

4.MULTI-CHANNEL USER INTEREST MEMORY NETWORK

- long sequence 算法上的挑战：RNN 结构并不最适合刻画 long sequence（hidden state 偏向于刻画 predicting target 而非 history；存储所有历史信息的方案较冗余）

=> MIMN 

- NTM
- memory utilization regularization: to increase the expressive ability of memory tensor in UIC by increasing the utilization of memory
  - 解决 NTM 的 memory 被热点行为 dominate 的问题：1）LRU 将信息写进不同的 slot，有利有弊；2）Memory Utilization Regularization 
- memory induction unit: to help capture high-order information
  - 继续用GRU进化兴趣，MIMN的特点，兴趣进化是 multi-channel 的（channel间共享GRU的参数）

#### SIM

Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction

Search-based Interest Model (SIM) -> Sub user Behavior Sequence (SBS)

- General Search Unit (GSU)
  - soft-search and hard-search
- Exact Search Unit (ESU)
  - multi-attention; encode temporal distance information

training方案：

* ESU: multi-head attention

* GSU: soft-search 联合 ESU 学习，auxiliary CTR model 学习 behavior embedding

serving方案：

* hard-search: user behavior tree (UBT), uid->category id->behavior items

there indeed exists massive noise in original long-term behavior sequences which may undermine long-term user interest learning. 信息并非越多越好，本文也和 Youtube 论文[2] 一样，体现了过滤噪音信息的思想

#### MIND (Multi-Interest Network with Dynamic routing), CIKM 2019

* 与 match 特征的关系：本文是把 match 做到模型里，如果模型能学到 历史 item 到 interest 的转换关系，就相当于是有 match 能力了（match 的 key 我感觉就像是 interest）

* 本质上是捕捉用户兴趣做召回，利用胶囊网络去“压缩”序列特征成兴趣特征，multi-interest embedding 尝试克服 user embedding 的维数局限性
  * multi-interest extractor layer: soft-cluster user interests

* label-aware attention: to help learn a user representation with multiple vectors.

* Related Work
  * User Representation: sequence model; from word embedding
  * Capsule Network. The concept of "Capsule", a small group of neurons assembled to output a whole vector, is firstly proposed by Hinton [13] at 2011. 
    Instead of backpropagation, dynamic routing [21] is used to learn the weights on the connections between cap-sules, which is improved by utilizing Expectation-Maximization algorithm [14] to overcome several deficiencies and achieves better accuracy.

* Method
  * 3.3 Multi-Interest Extractor Layer
    * 由序列特征生成 user embedding，是基于胶囊网络的 user tower，生成 
      multiple representation vectors
  * Dynamic Routing
    * In a nutshell, capsule is a new kind of neuron represented by one vector instead of one scalar used in ordinary neural networks.
    * dynamic routing：胶囊网络中利用 bilinear 矩阵，学习low-level到high-level表征关系
    * 最后一层用 a non-linear "squash" function 处理
  * B2I dynamic routing
    * learn interest capsules from behavior capsules.
    * Shared bilinear mapping matrix：设计的一方面因素是因为 user behaviour 是变长的
    * Randomly initialized routing logits
    * Dynamic interest number: 根据历史序列长度动态调整兴趣数量

  * 3.4 Label-aware Attention Layer
    * 训练时用
    * Q: 这个 dynamic routing 怎么训练？

* Experiments
  * 实验指标 HitRate
  * hard attention scheme 效果最好

### explore-exploit问题 

* [专治选择困难症——bandit算法](https://zhuanlan.zhihu.com/p/21388070)
  * Thompson sampling算法
    * `choice = numpy.argmax(pymc.rbeta(1 + self.wins, 1 + self.trials - self.wins))`
  * UCB算法：Upper Confidence Bound
  * Epsilon-Greedy算法：有点类似模拟退火
    * 一定的概率做纯随机决策
  * 总结：
    * UCB算法和Thompson采样算法显著优秀一些，两者都是给explore bonus，给出explore bonus的方式不同：
      * UCB是频率学派，假定了大数定律收益收敛，显式的通过试验次数给出的置信度上界
      * Thompson是靠分布刻画不确定性
    * 至于你实际上要选哪一种bandit算法，你可以选一种bandit算法来选bandit算法。。。

#### 强化学习与推荐系统的结合

* 强化学习：“行动—反馈—状态更新”循环
  * DQN
    * 任何深度学习模型都可作为智能体的推荐模型
  * DRN
    * 模型微更新：竞争梯度下降算法（随机扰动、探索网络）
    * 模型主更新：利用历史数据重 train
* 意义：变静态为动态，增强模型学习的实时性
  * “重量”与“实时”的折中

## 传统搜索

* 历史：https://www.vantagediscovery.com/post/ecommerce-search-transcended-for-the-ai-age
  * PageRank
  * As search engines become more personalized and user-focused, traditional ecommerce SEO tactics based on keyword optimization and backlinking are giving way to more sophisticated strategies that prioritize user intent and experience. 
* 传统的Indexing技术
  * [Suffix tree](https://en.wikipedia.org/wiki/Suffix_tree) 
    * Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [trie](https://en.wikipedia.org/wiki/Trie). Tries support [extendible hashing](https://en.wikipedia.org/wiki/Extendible_hashing), which is important for search engine indexing.[[8\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-8) Used for searching for patterns in [DNA](https://en.wikipedia.org/wiki/DNA) sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.[[9\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-Gus97-9) An alternate representation is a [suffix array](https://en.wikipedia.org/wiki/Suffix_array), which is considered to require less virtual memory and supports data compression such as the [BWT](https://en.wikipedia.org/wiki/Burrows–Wheeler_transform) algorithm.
  * [Inverted index](https://en.wikipedia.org/wiki/Inverted_index)
    * Stores a list of occurrences of each atomic search criterion,[[10\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-10) typically in the form of a [hash table](https://en.wikipedia.org/wiki/Hash_table) or [binary tree](https://en.wikipedia.org/wiki/Binary_tree).[[11\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-11)[[12\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-12)
  * [Citation index](https://en.wikipedia.org/wiki/Citation_index)
    * Stores citations or hyperlinks between documents to support citation analysis, a subject of [bibliometrics](https://en.wikipedia.org/wiki/Bibliometrics).
  * [*n*-gram index](https://en.wikipedia.org/wiki/N-gram)
    * Stores sequences of length of data to support other types of retrieval or [text mining](https://en.wikipedia.org/wiki/Text_mining).[[13\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-13)
  * [Document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix)
    * Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix).
* faceted search
  * https://www.sparq.ai/blogs/ecommerce-faceted-search
* TF-IDF
  * Term Frequency–Inverse Document Frequency
  * 指标定义：TF-IDF = TF * IDF。其中，TF（Term Frequency）指的是词频，即某个词在文本中出现的次数。IDF（Inverse Document Frequency）指的是逆文档频率，即该词在整个语料库中出现的文档数的倒数。
  * 应用场景：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
* beam search 
  * https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24
  * Beam search is an algorithm used in many NLP and speech recognition models as a final decision making layer to choose the best output given target variables like maximum probability or next output character.   
  * 核心在于LM generate output的地方
  * Greedy Search: A Naïve Approach
  * Beam Search: **Using Conditional Probability** ,多次计算Decoder
    * The algorithm can take any number of N best alternatives through a hyperparameter know as Beam width


![image-20241005231947598](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/translation.png)

### ElasticSearch

* Intro

  * https://www.elastic.co/guide/en/elasticsearch/reference/8.15/release-highlights.html

  * ES基于Lucene引擎，其核心是基于关键词的倒排索引，关键组件包括分词、倒排索引、相关性计算等。其中相关性计算通常采用TF-IDF和BM25等基于词频的算法

  * denormalization
    * inverted index
      * An index is a collection of documents.
      * index: <word -> [documents]>
      * document: <field -> [values]>
      * 用stopword处理


  * document oriented tool

* Usage

  * term关键字，不能用于text，要用于keywords类型

    * https://stackoverflow.com/questions/21933787/elasticsearch-not-returning-results-for-terms-query-against-string-property

  * phrase匹配

    * ```
      "query": {
          "multi_match": {
                  "query": "严有兵",
                  "fields": [
                      ...
                  ],
                  "operator": "OR",
                  "type": "phrase"
              }
        }
      ```

  * 前缀匹配 query_string

  * 分类汇总

    * HyperLogLog:  cardinality关键字

    * ```
      "aggs": {
          "count_unlisted_companies": {
            "cardinality": {
              "field": "company_id" // 假设这里有一个代表公司唯一标识的company_id字段
            }
          }
        }
      ```

    * ```
      "aggs": {
          "count_active_companies": {
            "value_count": {
              "field": "_id"
            }
          }
        }
      ```

### Evaluation

* Recall & NDCG
  * ndcg影响用户价值
  * recall影响商家价值

* F-score: 精确率和召回率的调和平均数
* GSB：Good-Same-Bad

### Case Analysis

* 点击和相关性逆序比较大的case

### Case Examples

- “母亲节给妈妈买什么”
- “一个有趣的夜晚外出的衬衫”
- running shoes
- casual red Nike sneakers for summer
  - sparse: 'red', 'Nike', and 'sneakers'
  - dense: 'casual' and 'summer'
- recipes for a 6-year-old's birthday party
- recipes for a family dinner
- graduation garden party
  - floral sundress
  - wide-brim sunhat
- skin protection for outdoor concert
  - sunshine savior
- I need a stylish, comfortable dress for a summer wedding
- gear for a weekend camping trip
- eco-friendly sneakers
- what to wear to a beach party
- long floral dress with short sleeves.
- 菠萝
  - -X-> 马可菠萝火腿肠
- 生姜
  - -X-> 生姜洗发水
- 蒙牛牛奶
- 口条=猪舌
- 角瓜=茭瓜=西葫芦
- Redmi
- 烧烤（泛意图）
  - -> 烤羊肉串、烤羊腿
- 康师傅方便面
  - 品牌：康师傅
  - 商品：【袋装面、桶装面、泡面】
  - 类目：粮油调味
- 品牌搜索
  - 安慕希
- 基础单词
  - 苹果
  - 鱼
  - 水
    - -X-> 柔肤水
    - -X-> 丰水梨
- 同义词搜索
  - 圣女果
- 纠错
  - 平果、pingguo 和 pinguo

![9236e387-c3c1-4c80-b3a5-699c71e9299e](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/9236e387-c3c1-4c80-b3a5-699c71e9299e-3938325-3938327.png)

##### 对话式

- a camping backpack for an upcoming trip
  - the material that the backpack is made of, its volume, its straps, and its storage compartments.
- 更贵的xxx
  - 需要retrieve商品信息





## 电商搜索

### 业务理解

* 商品搜索中的用户强意图场景，召回率/MAP/NDCG指标要求高，因为不能让一些商品永远没有曝光的机会 —— 第四范式
* 商品搜索对个性化的要求高于网页/视频/文字搜索，比如搜索的时候，不同的人消费能力的不同，那么排序时，需要考虑用户的消费能力，返回合适价格的商品
* 电商场景出于商家生态考虑，商品覆盖率要求高

### 关键词提取和生成

> [电商搜索全链路（PART II）Query理解](https://mp.weixin.qq.com/s/GrMItUHW8Szghmveejn9XA)

![图片](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/640-20241011183258573)

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/78aa0a537b0122edf97ec9a6d01a4fbf.png)

* Query预处理
  * 运营审核干预
  * 归一化：包括大小写转换、繁简体转换、全半角转换、符号表情移除等
  * 长度截断：对超长的query进行截断
* Query分词
  * 目前业界中大部分搜索系统中的分词模块都会有专门的基础中台部门来迭代优化，亦或直接使用开源的分词工具（譬如JieBa、HanLP、PyLTP、LAC等）
  * Review of Chinese Word Segmentation Studies: *https://manu44.magtech.com.cn/Jwk_infotech_wk3/CN/Y2020/V4/I2/3/1*
  * NLP分词算法深度综述: *https://zhuanlan.zhihu.com/p/50444885*

```python
# 提取名词
values = [token.word for token in jieba.posseg.cut(query)
            if token.flag in {'n', 'nr', 'ns', 'nt', 'nz'}]
```





> Query改写

- Query纠错：技术方案主要可以分为pipeline和end2end两种类型

  - Pipeline错误检测：识别输入句子中错误词的位置。主要方法有以下几种：

  - - 基于词典：对query切分后，检查各个词是否在维护的自定义词表或挖掘积累的常见纠错pair中；
    - 基于语言模型：统计大规模语料的n-gram信息，频率小于一定阈值的即认为是错误词；
    - 基于序列标注：通过模型（bi-LSTM-CRF、BERT-CRF等）来学习错误词的开始和结束位置，'0' 表示无错误，'1' 表示错误；

  - Pipeline错误纠正：定位到错词后，进行错词的纠正。首先采用多种策略（编辑距离、HMM模型、训练深度模型挖掘等）进行纠错候选召回，然后对该候选集合进行排序得到最终的正确query。

  - End2End：

    - 字节AI Lab的Soft-Mask BERT
    - 蚂蚁金服SpellGCN
    - 腾讯 PLOME

  - 业界案例：在实际应用场景中，会存在很多论文未涉及的问题

    - [百度：中文纠错技术](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247488610&idx=1&sn=c8793392f789ba5c39a9e8a4d7c6beac&scene=21#wechat_redirect)
    - [哈工大讯飞文本纠错系统](http://cogskl.iflytek.com/archives/1306)
    - [平安寿险AI：文本纠错技术](https://zhuanlan.zhihu.com/p/159101860)
    - [阿里：语音对话中的纠错系统](https://mp.weixin.qq.com/s?__biz=MzA3MTQ0NTUyMw==&mid=2247484572&idx=1&sn=de6d707458e05bec4d53c4e4427da0e2&scene=21#wechat_redirect)
    - [小爱：基于BERT的ASR纠错](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247503412&idx=1&sn=75ef312902713d3766a43a6c71e1024e&scene=21#wechat_redirect)
    - [滴滴：语音交互自然语言理解探索与实践](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247529750&idx=2&sn=dbf897c5cb112fb87b6a1d9a37804548&scene=21#wechat_redirect)
    - [流利说：自动语法纠错](https://mp.weixin.qq.com/s?__biz=MzI0NjIzNDkwOA==&mid=2247484827&idx=1&sn=137c9b927a9d77af73825eb24abb5c8f&scene=21#wechat_redirect)

![图片](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/640-20241011184242866)

- Query归一：目标是将长尾冷门的query/词语归一到热门标准query
  - 涉及的主要技术是同义词挖掘及语义实体对齐。具体实现上有很多方式，譬如：
    - 从知识库或者结构化数据构造规则模板来挖掘；
    - 利用丰富的行为数据，结合无监督词向量，来挖掘语义相似词；
    - 通过深度匹配模型、文本生成模型seq2seq等先挖掘出语义表达相近的query-query、item-item或query-item短语对，然后再将语义相近的query/item短语对进行语义对齐；
- Query扩展：根据粒度的不同分为Term粒度和Query粒度两种
  - 美团方案：
    - 首先离线通过用户搜索日志、翻译（词对齐等）、图方法（协同过滤、graph embedding等）、词向量Embedding等方法挖掘得到千万级别的候选语料；
    - 但一般上述挖掘语料质量不够高，又设计了基于BERT的语义判别模型进一步提高改写pair对的准确率；
    - 在线的目标是进一步提高改写的效果，设计了高精度的词典改写、较高精度的模型改写（基于SMT统计翻译模型和XGBoost排序模型）、覆盖长尾Query的基于强化学习方法优化的NMT模型、针对商户搜索的向量化召回四种线上方案。
  - 其它方案：
    - [丁香园：搜索中的Query扩展技术](https://zhuanlan.zhihu.com/p/138551957)
    - [丁香园：搜索中的Query扩展技术(二)](https://zhuanlan.zhihu.com/p/296504323)
    - [Query 理解和语义召回在知乎搜索中的应用](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247496409&idx=1&sn=7b2f5984d71454e1a2812321f6018cf8&scene=21#wechat_redirect)
    - [美团搜索中查询改写技术的探索与实践](https://tech.meituan.com/2022/02/17/exploration-and-practice-of-query-rewriting-in-meituan-search.htm)

### Query Rewrite

#### Literature Review

* Pseudo-Relevance Feed- back (PRF)
* Document Expansion
* 数据集 Evaluation：https://github.com/amazon-science/esci-data

#### Large Language Model based Long-tail Query Rewriting in Taobao Search

* BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries
  * multi-instruction supervised fine tuning (SFT)
    * based on rejection sampling and auxiliary tasks mixing to fine-tune LLM
  * offline feedback
  * objective alignment.
  * beam search to generate multiple candidate rewrites

* 现有查询改写方法的局限
  - 基于嵌入的检索范式结果难解释
  - “查询改写 & 精确匹配” 范式中判别式方法难以控制语义范围和确保相关性
  - 生成式方法受限于模型规模对长尾查询理解不足，基于大语言模型的改写方法缺乏微调与目标对齐

![image-20241117235808683](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241117235808683.png)

* 多指令 SFT
  - 收集改写相关任务数据微调大语言模型，包括:
  - 构建查询改写数据集（经两轮拒绝采样提升质量并结合辅助任务数据）
  - 利用辅助任务数据集（质量分类、产品标题预测、思维链任务）增强模型对长尾查询的理解

* Evaluation: 利用taobao rele score function，定义hit rate

#### Query Expansion by Prompting Large Language Models

* Intro
  * PRF-based approaches assume that the top retrieved documents are relevant to the query
  * we rely on the knowledge inherent in the LLM.
* ![image-20241114182225681](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241114182225681.png)

* 结论：
  * PRF可以增强排序

#### Query2doc: Query Expansion with Large Language Models

* 检索sparse：重复5遍再相连
* 检索dense：用[SEP]相连

### 实体识别

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/ab517b8391561f900d538776c1bc0381.png)

* 领域知识积累
  * e.g.
    * 口条=猪舌
    * 角瓜=茭瓜=西葫芦
    * Redmi
  * 词库挖掘
    * 同义词挖掘
      * 基于word2vec共现关系（噪声大）
      * 百科爬取
      * 运营提供
      * 现有词库
    * 上位词挖掘
      * 类目作为上位词
      * 爬取类目体系
  * 商品知识图谱构建
    * 知识图谱其实是做了一个非个性化全局的知识构建，通过商品库去分析静态概率，最后根据用户点击行为会做一些动态调整，调整完的知识图谱再用在后面的排序上。
    * ![image-20241011154227917](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241011154227917.png)
  * LLM都能搞定

### 意图识别

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/a0dd83557a74b8d07f3bed5e4a6fd0ef.png)

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/43f0a0f7c0b801a7be62446738bf1b6a.png)

* FastText分类器 https://fasttext.cc/

### 召回

* ES

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/d350bbd78199bc8b214e81bc6a387820.png)

### 粗排

* 当召回数量较大时，粗排截断
* 相关性、时间、热度、销量、好评数和收藏数等等特征，训练出简单的模型，做一些粗排的排序，进行截断
  * 找出核心的特征，做加权平均也可以。

### Rerank

> [搜推广生死判官：重排技术发展](https://mp.weixin.qq.com/s/oSxtpVuoTFQGsVxbZelGQg)

* LR、GBDT

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/690ce3a3bbc3f407bc453ebc35ceb18b.png)

### 模型微调

[“阿里灵杰”问天引擎电商搜索算法赛--季军经验分享](https://mp.weixin.qq.com/s/sfFZpttGqf_f4sBFpQIiyg)

* ![image-20241010195711640](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241010195711640.png)

* ![image-20241010195813065](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241010195813065.png)

* 领域数据后训练
  * 动机：大量corpus未曝光，未经过训练
  * 预训练模型：Chinese-roberta-wwm-ext, Whole World MLM
* 召回任务微调
  * 数据构造：
    * 正样本：标注document
    * 负样本：
      * in-batch-negative: batch内其它query的标注document
      * random-negative：随机在corpus采样非标注document
        * 动机：如果仅使用in-batch-negative，会影响非标注document的学习过程

![image-20241010202306817](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241010202306817.png)

* 精排任务微调
  * 均值变换：相似程度较高的召回，区分度增强，增加梯度

![image-20241010202519864](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241010202519864.png)

![image-20241010202709678](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241010202709678.png)

![image-20241010202740700](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E7%8E%8B%E5%96%86/image-20241010202740700.png)

#### 各种Tricks

* **数据增强**

  - **训练bart生成模型，使用document生成query构造伪标签**（还可以生成时随机替换一些阿拉伯数字、拼音），同时使用一定规则修正生成的伪标签query

  - 反向翻译（不确定是否有效）

  - **把query中长度小于4的拿出来，加入词表构建新词**

  - 把document分词后随机抽几个拼接作为新doc（不确定是否有效）

* **召回模型：**

  - 降维时可以使用无监督PCA作为降维矩阵初始化

  - 训练交互式精排模型去蒸馏非交互式召回模型

  - 难负例构造：使用训练好的召回模型+faiss召回，取相似度中等水平（30左右）的document作为难负例

  - infoNCE loss的温度：可以首先设定一个比较小的，然后慢慢变大，但不能太大

  - 集成使用提交的线上得分作为系数加权求和embedding（不确定是否有效）

* **精排模型：**
  - **可以使用ES引擎先召回query对应top-k doc，然后经过规则过滤（字面相似度高，向量余弦相似度不低）出过滤doc，然后把query和这些doc的向量表示按照一定规则相加，作为新query向量表示**

* **召回模型融合：**
  - **不能直接取均值，而是使用stacking的方法，把多个模型的输出向量使用一个训练后的FFN网络融合。**



## chpt4 Embedding 技术在推荐系统中的应用

1. 什么是Embedding
* 定义：用低维稠密向量“表示”一个对象 
  *	向量“表示”对象特征
  *	向量距离“表示”相似性
  *	具有本体论哲学层面上的意义
* 意义
  * 稀疏特征向量转稠密
  * 本身表达能力强，可引入任何信息进行编码(Graph Embedding)
  * 对物品、用户相似度的计算是常用的推荐系统召回层技术



2. Word2vec——经典的Embedding方法

* CBOW模型和Skip-gram模型
  * 经验上，Skip-gram模型效果更好，输入是本体，输出是周边的词
* 模型训练过程
  * 输入向量的权重矩阵 = 词向量查找表
  * 负采样方法减轻训练负担
    * $E=-log\sigma({v^{'}_{w_o}}^Th)-\sum_{w_j\in W_{neg}}log\sigma({v^{'}_{w_j}}^Th)$
    * 还有Hierarchical softmax方法加速训练



3. Item2Vec——Word2vec在推荐系统领域的推广



4. Graph Embedding——引入更多结构信息的图嵌入技术

### Graph Service

* 基于Euler改造
  * 支持全图节点遍历，支持按时间戳采样

* 高性能采样
  * 节点生成前缀和，二分查找随机数
  * 全局点采样：shard有权（shard上所有节点的权重），shard内再次按权
* Graph Embedding on GPU: sample和worker分离
* GE应用
  * 预训练：利用uid与author间的finish关系，构造双向异构图。边权重用finish视频数量，点权重用finish视频总数。
    * 不用gid而用author是为了减小图结构随时间的变化
    * 正负例构造：正例是user及其finish的节点，负例随机采样
  * end2end training
    * 利用uid和cid之间的click关系，边权重是click次数，点权重是click总数
    * 采样：有放回按权采样
    * 优点：训练目标和LTR任务一致
    * 缺点：1.全局图的拟合能力不如业务图 2.end2end结构更自由，可以用node2vec等
  * Finetune BERT/ResNet
    * cid和cid的co-click，边权重用交并比，点权重用click总数，点特征用广告id类泛化特征

## chpt6 深度学习推荐系统的工程实现

1. 推荐系统的数据流

四种架构：批处理、流计算、Lambda、Kappa

* Lambda: 流计算以增量计算为主，批处理进行全量计算；利用离线层数据对实时流数据进行校验和检错

2. Spark MLlib

3. Parameter Server

* Spark MLlib: 同步阻断式
* Parameter Server: 异步非阻断式

两者区别在于模型参数的分发是否同步

6. 工程与理论之间的均衡

* end2end：强调模型一致性的收益
* two stages：强调模型实时性的收益

## Potpourri

* 多样性探索：消重、打散、部分流量未知兴趣

* 评价指标：[AUC和GAUC](https://www.jianshu.com/p/03a11a083a6d)、[AUC不等于线上效果可能的原因](线下AUC提升为什么不能带来线上效果提升? - 萧瑟的文章 - 知乎 https://zhuanlan.zhihu.com/p/58152702)

* 边缘计算

  * [EdgeRec：揭秘边缘计算在淘宝推荐系统的重要实践](https://developer.aliyun.com/article/742144)

* 弹性近线计算

  * [百度信息流和搜索业务中的弹性近线计算探索与应用](https://mp.weixin.qq.com/s/53KLAPphK9t4G3q-78S9mg)

  * 弹性近线计算系统主要包括几个子系统：

    * 触发控制系统：主要负责根据业务参数，控制近线计算的触发，达到削峰填谷的目的。

    * 动态算力和动态调参系统：它相当于弹性近线计算系统的大脑，根据集群的资源情况，分配近线计算算力；再根据算力情况，计算控制参数，从而控制跟算力匹配的负载。

    * 历史数据中心：保存近线计算历史的计算记录。可以根据资源的情况，复用历史计算结果，来调节对算力的使用。

    * 业务近线计算 & 控制系统：这个主要是和业务接入近线计算相关的一些架构机制设计，比如说输入输出缓存的读写，计算的拆包 / 并包等等，业务计算与失败信号的反馈等等。
      * 实践下来，通过错峰调度，预估资源需求并提前分配计算资源是比较有效的提升算力的办法，可以理解是如果在资源已经紧张的时候，再进行近线计算模块的调度，新的近线计算模块的算力消耗有概率本身就造成局部热点，导致扩展资源被回收，造成调度失败

    * 业务在线接入：部分主要是业务接入近线计算系统上的一些设计。这块主要考虑的是如何高效的接入业务，避免业务接入过高的架构人力成本。

  * 应用场景：

    * Feed 在线 & 近线混合计算架构：将多个推荐算法服务的召回结果进行缓存，再放到近线系统来进行综合打分之后生成一个用户对应的近线计算候选集，作为一路新的召回。之后在线请求来了之后，使用这路新的召回结果，进行轻量的在线计算之后就可以返回给用户，这样就把召回层初步排序打分的计算规模提升了 1 个数量级。
    * 搜索结果排序：



[Inside TikTok's Algorithm: A WSJ Video Investigation](https://www.wsj.com/articles/tiktok-algorithm-video-investigation-11626877477)

