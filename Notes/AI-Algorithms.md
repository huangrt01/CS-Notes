# AI-Algorithms

[toc]

## å†å²å‘å±•

* è®¡ç®—æ™ºèƒ½ -> æ„ŸçŸ¥æ™ºèƒ½ -> é€šç”¨æ™ºèƒ½
* AGI
  * å¤´éƒ¨å…¬å¸é¢„æµ‹3-5å¹´ï¼Œè¿™ä¸ªè·¨åº¦çš„æ•…äº‹æ˜¯å¥½æ•…äº‹
* Note
  * GPT-3.5ç›¸æ¯”äºGPT-3ï¼Œå‚æ•°é‡å˜åŒ–ä¸å¤§ï¼Œæ•ˆæœå·®è·å¾ˆå¤§ï¼Œè¿™æ˜¯ç”±äºå¾®è°ƒæŠ€æœ¯

## Scaling Law

|            | 1ä»£  | 2ä»£  | 3ä»£  | 4ä»£                                                          |
| :--------- | :--- | :--- | :--- | :----------------------------------------------------------- |
| GPT ç³»åˆ—   | 117M | 1.5B | 175B | 1.7T as [rumoured](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/) |
| LIama ç³»åˆ— | 65B  | 70B  | 70B  | (3.1ä»£ï¼‰405B                                                 |

| æ¨¡å‹ | Gopher | LaMDA | Chinchilla | PaLM |
| :--- | :----- | :---- | :--------- | :--- |
| å‚æ•° | 280B   | 127B  | 79B        | 540B |

* Scaling Law: https://arxiv.org/abs/2001.08361
* emergent ability
  * [How much bigger can/should LLMs become?](https://cmte.ieee.org/futuredirections/2023/04/24/how-much-bigger-can-should-llms-become/)
  * https://arxiv.org/abs/2206.07682
  * 100TB=50000Billion
* [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
  * Google - Chinchilla
  * æ ¸å¿ƒæ€è·¯ï¼š
    * $$L(N,D)=AN^{-\alpha}+BD^{\beta}+E$$
      * Chinchilla-optimal
      * $$D/N \approx 20$$
    * æ¨¡å‹undertrainedï¼Œnå’Œdéœ€è¦ä¸€èµ·å¢é•¿
  * function formå¾ˆé‡è¦

![æˆªå±2023-11-19 05.00.35](./AI-Algorithms/emergent-1.png)

![image-20231119050100619](./AI-Algorithms/emergent-2.png)

![image-20231119050435217](./AI-Algorithms/history-1.png)

![Compute required for training LLMs](./AI-Algorithms/Compute-for-Training-LLMs-GPT3-paper-672x385.jpg)

* é‡åŒ–scaling lawï¼Œå‚è€ƒScaling Laws for Precision



## Literature Review

> from InteRecAgent

* Survey
  * ã€ŠPretrain, prompt, and predict: A systematic survey of
    prompting methods in natural language processingã€‹

* LLM capability
  * in-context learning
    * ã€Šfew-shot learnersã€‹
  * instruction following
  * planning and reasoning

* alignment
  * ã€ŠTraining language models to follow instructions with human feedbackã€‹

* leverage LLMs as autonomous agents
  * equipped LLMs with an external memory
  * CoT and ReActï¼špropose to enhance planning by step-wise reasoning;
  * ToT and GoTï¼šintroduce multi-path reasoning to ensure consistency and correctness
  * Self-Refine and Reflexionï¼šlead the LLMs to reflect on errors
  * To possess domain-specific skillsï¼Œguiding LLMs to use external tools
    * such as a web search engine
    * mathematical tool
    * code interpreters
    * visual models
    * recommender systems

> from MindMap

* LLMåº”ç”¨äºç”Ÿäº§çš„å±€é™æ€§
  * Inflexibility. 
    * The pre-trained LLMs possess outdated knowledge and are inflexible to parameter updating. Fine-tuning LLMs can be tricky be-
      cause either collecting high-quality instruction
      data and building the training pipeline can be
      costly (Cao et al., 2023), or continually fine-
      tuning LLMs renders a risk of catastrophic for-
      getting (Razdaibiedina et al., 2022).
  * Hallucination. 
    * LLMs are notoriously known to produce hallucinations with plausible-sounding
      but wrong outputs (Ji et al., 2023), which causes
      serious concerns for high-stake applications such
      as medical diagnosis.
  * Transparency. 
    * LLMs are also criticized for their
      lack of transparency due to the black-box na-
      ture (Danilevsky et al., 2020). The knowledge
      is implicitly stored in LLMâ€™s parameters, thus
      infeasible to be validated. Also, the inference
      process in deep neural networks remains elusive
      to be interpretable
* CoTã€ToT
  * æŒ–æ˜LLMçš„implicitçŸ¥è¯†
  * ç›¸åº”åœ°ï¼ŒMindMapåŒæ—¶æŒ–æ˜explicit and implicitçŸ¥è¯†

### ç”Ÿæˆå¼æ¨¡å‹

* åˆ¤åˆ«å¼ï¼Œå³æ ¹æ®ç”¨æˆ·å’Œç‰©å“çš„ç‰¹å¾é¢„æµ‹ç”¨æˆ·ä¸ç‰©å“äº¤äº’çš„**æ¡ä»¶**æ¦‚ç‡ï¼›è€Œç”Ÿæˆå¼æ¨¡å‹é¢„æµ‹çš„æ˜¯**è”åˆæ¦‚ç‡**ï¼š

  * åˆ¤åˆ«å¼å…³æ³¨é¢„æµ‹ç»™å®šæ¡ä»¶ä¸‹çš„ç»“æœï¼Œç”Ÿæˆå¼ç†è§£æ‰€æœ‰å˜é‡å¦‚ä½•å…±åŒå‡ºç°
  * æ–‡æœ¬ç”Ÿæˆï¼šChatGPTï¼ŒGemini

  - å›¾ç‰‡ç”Ÿæˆï¼šStable Diffusionï¼ŒDALL-E

  - è§†é¢‘ç”Ÿæˆï¼šSoraï¼ŒKling



## Intro

* å¤§æ¨¡å‹æœ€é‡è¦çš„æ¼”è¿›æ–¹å‘ï¼š
  * ä¸€ã€ä¸–ç•ŒçŸ¥è¯†æ–¹é¢å¦‚ä½•æœ‰æ•ˆæ¶ˆé™¤å¹»è§‰
    * éšç€æ•°æ®è§„æ¨¡å¢å¤§ï¼Œé‡åˆ°çš„æ–°çŸ¥è¯†æ¯”ä¾‹å°±è¶Šä½ï¼Œåœ¨ä¸–ç•ŒçŸ¥è¯†æ–¹é¢å°±ä½“ç°å‡ºScaling lawçš„å‡ç¼“ç°è±¡ã€‚
  * äºŒã€å¦‚ä½•å¤§å¹…æå‡å¤æ‚é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚
    * é€»è¾‘æ¨ç†ç›¸å…³æ•°æ®æ¯”ä¾‹ä½ï¼Œæ›´æ…¢ã€‚
    * ç°åœ¨ä¸ºäº†æé«˜æ¨¡å‹é€»è¾‘èƒ½åŠ›ï¼Œå¾€å¾€åœ¨é¢„è®­ç»ƒé˜¶æ®µå’ŒPost-trainingé˜¶æ®µï¼Œå¤§å¹…å¢åŠ é€»è¾‘æ¨ç†æ•°æ®å æ¯”çš„åŸå› ï¼Œä¸”æ˜¯æœ‰æˆæ•ˆçš„ã€‚
  * è¯­è¨€èƒ½åŠ›å·²ä¸æ˜¯é—®é¢˜ã€‚

* [Yann LeCunæ¼”è®²â€œäººç±»æ°´å¹³çš„AIâ€@ Husdonè®ºå› 2024.10](https://www.bilibili.com/video/BV1b1ycYTECU)
  * ä»‹ç»äº†ç®—æ³•æ²¿è¢­
  * Moravec's Paradox: AIåšä¸åˆ°ä¸€äº›äººç±»å¾ˆå®¹æ˜“åšçš„äº‹æƒ…

![image-20241019021542281](./AI-Algorithms/image-20241019021542281.png)

### Tokenization è¯å…ƒåŒ–

* tokenæ˜¯LLMè®­ç»ƒæ¨ç†çš„æœ€å°å•å…ƒï¼Œç”±tokenizeræ¨¡å‹å°†æ–‡æœ¬åˆ‡æˆtoken
  * å¯èƒ½æ˜¯ 1/3 ä¸ªæ±‰å­—ï¼ˆå› ä¸ºæ±‰å­—çš„UTF-8ç¼–ç æ˜¯ä¸‰ä¸ªå­—èŠ‚ï¼Œå–ä¸€ä¸ªå­—èŠ‚ï¼‰ã€ä¸€ä¸ªæ±‰å­—ã€åŠä¸ªå•è¯ç­‰
  * å’Œæ¨¡å‹è®¾è®¡æœ‰å…³ï¼š
    * å¤šè¯­è¨€å¤§æ¨¡å‹ï¼šæ±‰å­—æ‹†å¼€
    * ä¸­æ–‡å¤§æ¨¡å‹æ¯”å¦‚ChatGLMï¼šä¸€ä¸ªæ±‰å­—å¤§æ¦‚1 token
    * OpenAIçš„å®˜ç½‘ä¸Šï¼Œ1 Tokenså¤§æ¦‚æ˜¯0.75ä¸ªè‹±æ–‡å•è¯ä¸Šä¸‹ï¼ˆ0.5ä¸ªæ±‰å­—ä¸Šä¸‹ï¼‰
  * å’Œæ¶ˆè€—ç®—åŠ›æœ‰å…³
    * ->ä¸­æ–‡å¤§æ¨¡å‹æ›´ä¾¿å®œ
  * e.g.
    * encoding = encod + ing
    * encoded = encod + ed
    * subword = sub + word
* Tiktoken
  * ä¸ºä»€ä¹ˆç”¨å­è¯ï¼šå‡å°‘è¯è¡¨çš„æ•°é‡
    * æ±‰å­—æœ‰10ä¸‡ä¸ª


```
å¦‚æœè¾“å…¥å†…å®¹æ˜¯ï¼šæµ·å—éº’éºŸç“œ<br/>
  æµ·, unicode:28023, utf8:b'\xe6\xb5\xb7'<br/>
  å—, unicode:21335, utf8:b'\xe5\x8d\x97'<br/>
  éº’, unicode:40594, utf8:b'\xe9\xba\x92'<br/>
  éºŸ, unicode:40607, utf8:b'\xe9\xba\x9f'<br/>
  ç“œ, unicode:29916, utf8:b'\xe7\x93\x9c'<br/><br/>
  
é€šè¿‡tiktokenå¤„ç†ä¹‹åå¾—åˆ°çš„Tokenåºåˆ—æ˜¯ï¼šï¼ˆå…±11ä¸ªTokenï¼‰<br/>
  b'\xe6\xb5\xb7'<br/>
  b'\xe5\x8d\x97'<br/>
  b'\xe9'<br/>
  b'\xba'<br/>
  b'\x92'<br/>
  b'\xe9'<br/>
  b'\xba'<br/>
  b'\x9f'<br/>
  b'\xe7'<br/>
  b'\x93'<br/>
  b'\x9c'<br/><br/>
```



* https://huggingface.co/docs/transformers/en/tokenizer_summary
  * Byte-level BPE
  * GPT-2 has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.



## Attention Is All You Need

> Paper
>
> ç¡¬æ ¸è¯¾å ‚ï¼šChatGPTçš„è®¾è®¡å’Œå®ç° https://hardcore-tech.feishu.cn/wiki/DtO3wHVzEiOUdNk0r3cc8BY8nef

### ä»è¿‡å»çš„NLPæŠ€æœ¯åˆ° Transformer

* ä»¥RNNä¸ºæ ¸å¿ƒçš„Encoder Decoderæœ‰ä»¥ä¸‹å‡ ä¸ªé‡è¦çš„é—®é¢˜
  * ä¿¡æ¯ä¸¢å¤±ï¼šæ¯æ¬¡ä¼ é€’ä¹˜äº†ç³»æ•°ï¼Œä¸¢å¤±å‰é¢çš„ä¿¡æ¯
  * æ— æ³•å¤„ç†è¾ƒé•¿å¥å­ï¼šRNN å¯¹é•¿æœŸåºåˆ—ä¾èµ–å…³ç³»ä¸ç¨³å®šï¼ŒLSTM/GRU è™½ä¸€å®šç¨‹åº¦å…‹æœé•¿æœŸä¾èµ–é—®é¢˜ï¼Œä½†æ— æ³•æ•è·å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
    * the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.
    * RNNæ˜¯sequence-alignedå®ç°
  * ä¸èƒ½å¹¶è¡Œè®¡ç®—ï¼Œå¯¹GPUä¸å‹å¥½
* ä»¥ä¸Šé—®é¢˜ï¼Œå¯¹**ä»åºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹**å¾ˆé‡è¦

> Transformer çš„ç›®æ ‡æ˜¯ **è®¾è®¡å…¨æ–°çš„ã€å¹¶è¡Œçš„ã€é•¿æœŸä¾èµ–ç¨³å®šä¸”èƒ½æ•è·å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€å¤„ç†å¯å˜é•¿åº¦åºåˆ—çš„ç¥ç»ç½‘ç»œæ¶æ„**ã€‚

![image-20241216030117146](./AI-Algorithms/image-20241216030117146.png)

* N-gram word2vecæ¨¡å‹æ³›åŒ–æ€§å·®
  * -> å¤§åŠ›å‡ºå¥‡è¿¹ï¼Œå¯¹å…¨å±€åšattention



* seq2seqæ¨¡å‹çš„æ—©æœŸæ¢ç´¢
  * https://arxiv.org/abs/1609.08144
  * additive attn: https://arxiv.org/abs/1703.03906



### Intro

* Intro
  * connect the encoder and decoder through an attention mechanism. 
  * Encoder: æ˜ å°„åˆ°å¦ä¸€ä¸ªè¯­ä¹‰ç©ºé—´
  * Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
  
* å…¬å¼
  * multi-head self-attention (MSA) + multi-layer perceptron (MLP) blocks
  * ![image-20241213200148729](./AI-Algorithms/image-20241213200148729.png)
  
* æ¨¡å‹ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ
  * è¿‡Nä¸ªæ³¨æ„åŠ›å±‚ï¼Œå†è¿‡ä¸€ä¸ªfull connection
  * $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    * normalizationï¼š$$d_k$$æ˜¯head dimï¼ˆæœ€åä¸€ç»´ï¼‰
  * æ®‹å·®ç½‘ç»œ
* æ¨¡å‹å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ
  * è¯åµŒå…¥å‘é‡
    * learnable?
  * å°†è¯åµŒå…¥å‘é‡è½¬åŒ–ä¸ºqã€kã€vå‘é‡çš„ä¸‰ä¸ªçŸ©é˜µå’Œbias
    * çº¿æ€§å˜æ¢çŸ©é˜µ $$W^Qã€W^Kã€W^V$$
    * ç†è§£Qã€Kã€Vï¼šKåå‘å…´è¶£å’Œæ‘˜è¦ï¼›Våå‘åŸå§‹ä¿¡æ¯
* æ¨¡å‹è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ
  * å…¨è¿æ¥å±‚çš„ç»“æœï¼Œä¸€ä¸ªé•¿åº¦ä¸ºå…¨éƒ¨è¯æ±‡æ•°é‡çš„å‘é‡
  * å¦‚ä½•å¢å¼ºéšæœºæ€§ï¼š
    * top-ké‡‡æ ·

### Encoder & Decoder

> ä¸€äº›æ€è€ƒï¼š
>
> * åœ¨åŒç­‰å‚æ•°ä¸‹è®¤ä¸º Decoder - Only æ¶æ„æ¯” Encoder - Decoder æ¶æ„æ›´å¤æ‚ï¼Œå› å…¶æ³¨æ„åŠ›å±‚è¾“å…¥ä¿¡æ¯æ›´å¤šå¯¹æ¨¡å‹èƒ½åŠ›æŒ‘æˆ˜æ›´å¤§ï¼Œè¿™ç§è§‚ç‚¹æœ‰ä¸€å®šåˆç†æ€§
> * Decoder - Only æ¶æ„åœ¨ç†è®ºä¸Šå¦‚æœæ¨¡å‹èƒ½åŠ›è¶³å¤Ÿå¼ºå¤§ï¼Œç¡®å®æœ‰å¤„ç†é•¿åºåˆ—å¹¶é¿å…æ˜æ˜¾ä¿¡æ¯ä¸¢å¤±çš„æ½œåŠ›

* encoderç”¨äºåˆ†æï¼Œdecoderç”¨äºç”Ÿæˆ
* ![image-20250203160834537](./AI-Algorithms/image-20250203160834537.png)
* Encoder Only & Decoder Only & encoder-decoder
  * Decoder Onlyï¼šå°†è¾“å…¥æ‹¼èµ·æ¥ï¼Œä½œä¸ºprompt
    * ç›¸æ¯”åŸå§‹transformerï¼Œå»é™¤äº†ï¼šencoderã€decoderä¸­å’Œencoderç›¸è¿çš„MSA
    * è½¬æ¢æˆäº†ã€Œç»­å†™ä»»åŠ¡ã€ï¼Œå¤§éƒ¨åˆ†LLMä½¿ç”¨è¿™ç§æ¶æ„
    * *Decoder*-*Only*æ¨¡å‹åœ¨å‚æ•°æ•ˆç‡ä¸Šé€šå¸¸ä¼˜äº*Encoder*-*Decoder*æ¨¡å‹ï¼Œå› ä¸ºå®ƒä¸éœ€è¦åŒæ—¶è®­ç»ƒä¸¤ä¸ªæ¨¡å—

### Encoder

#### å¤šå¤´è‡ªæ³¨æ„åŠ›

> * ä»æ¨¡å‹å¤æ‚åº¦çš„è§’åº¦ï¼šå‡è®¾è¶…å‚å¯¹æ•ˆæœè´¡çŒ®ç›¸åŒï¼Œä¼˜å…ˆè®©æ¨¡å‹æ›´å¤æ‚ï¼Œåˆ©äºScalableï¼Œæ›´æ™šé‡åˆ°å¤©èŠ±æ¿
>
>   - â€œå¤šå¤´â€æ¯”â€œå•å¤´â€å¤æ‚ï¼Œæ˜¯æ¯”è¾ƒç›´è§‚çš„
>
>   - â€œå¤šå¤´â€ v.s. overlapï¼Œç±»ä¼¼äºæ¨èç³»ç»Ÿä¸­çš„share embï¼Œä¸ç¡®å®šå“ªç§â€œæ›´å¤æ‚â€
>     - overlapåˆ©äºæå–ç‰¹å¾çš„å±€éƒ¨ç»†èŠ‚ï¼Œå¯¹äºè¯­è¨€ç±»æ¨¡å‹ï¼Œä¸çŸ¥é“æœ‰æ²¡æœ‰ç”¨
>
> - åŠ¨æœºï¼šç¼“è§£å…¨å±€attentionçš„ä¿¡æ¯ä¸¢å¤±

* The Transformer follows this overall architecture using **stacked self-attention and point-wise**, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1
  * å·¦è¾¹encoderï¼Œå³è¾¹decoder
    * Encoder: è‡ªæ³¨æ„åŠ›
    * Decoderï¼šQç”¨outputs embeddingåšmasked attentionåçš„ç»“æœï¼ŒKã€Vç”¨encoderç»“æœ
    * è¡¨å¾å‘é‡512ç»´
  * è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼šQï¼ˆè¾“å…¥çŸ©é˜µï¼‰ã€Kï¼ˆå­—å…¸ï¼‰ã€V
    * ç”¨1/(dk)^(1/2) scaleäº†ä¸€ä¸‹QKçš„ä¹˜æ³•ï¼Œå¯èƒ½æ˜¯ä¸ºäº†é˜²æ­¢gradientå¤ªå°
      * Dot productçš„ç»“æœæ–¹å·®æ¯”additive attentionçš„æ–¹å·®å¤§
      * https://arxiv.org/abs/1703.03906
* Multi-head attention: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
  * å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti - Head Attentionï¼‰çš„è®¡ç®—è¡¨è¾¾å¼ä¸ºï¼š $$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W_O $$
    * å…¶ä¸­æ¯ä¸ªå¤´çš„è®¡ç®—å…¬å¼ä¸ºï¼š $$ \text{head}_i = \text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i}) $$
  * è‡ªæ³¨æ„åŠ›å’ŒCNNçš„è¾¨æ https://www.mstx.cn/pytorch/209.html
    * ç›¸ä¼¼æ€§ï¼šä¿¡æ¯æå–æœºåˆ¶ã€å¹¶è¡Œã€å¯å †å 
    * åŒºåˆ«ï¼šæ„Ÿå—é‡çš„å›ºå®šæ€§å’Œçµæ´»æ€§ã€å±€éƒ¨æ€§å’Œå…¨å±€æ€§ã€è®¡ç®—å¤æ‚åº¦ã€ç©ºé—´ç»´åº¦ä¸åºåˆ—ç»´åº¦


![image-20231025203852956](./AI-Algorithms/multi-head-attention.png)

![image-20241216030854804](./AI-Algorithms/image-20241216030854804.png)

![image-20241216030905082](./AI-Algorithms/image-20241216030905082.png)

* è‡ªæ³¨æ„åŠ›ï¼š
  * æœ¬è´¨ä¸Šæ˜¯ä¿¡æ¯çš„èšåˆ
  * è®¡ç®—å¤æ‚åº¦ï¼šO(N^2)
  * ç»å…¸çš„transformerï¼š6å±‚
  * GPT: 12å±‚
  * GPT-2: 32å±‚
  * GPT-3: 96å±‚
  * GPT-4ã€llama3ï¼š120å±‚
  * Qã€Kã€Vä¸åŒçš„ç†è§£ï¼šQã€Kã€Vé€šè¿‡ä¸åŒçš„çº¿æ€§å˜æ¢ä»åŒä¸€è¾“å…¥åºåˆ—ä¸­ç”Ÿæˆï¼Œå„è‡ªæ‰¿æ‹…ç€ä¸åŒçš„ä½œç”¨ï¼Œå…±åŒå®ç°äº†æ¨¡å‹å¯¹åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´å…³ç³»çš„æ•æ‰ã€‚

#### MLP

* æå‡ç½‘ç»œè¡¨è¾¾åŠ›
* éçº¿æ€§
* èåˆå¤šå¤´ç‰¹å¾

#### Layer Normalization

å¯¹seq_lenç»´åº¦ä¸Šçš„æ¯ä¸€ä¸ªembeddingï¼ˆ768ç»´ï¼‰åšLN

##### Pre-LN

* å°†å½’ä¸€åŒ–å±‚æ”¾åœ¨å­å±‚ï¼ˆAttention æˆ– FFNï¼‰ ä¹‹å‰ çš„ç»“æ„è¢«ç§°ä¸º Pre-LN (Pre-Layer Normalization) ã€‚

  ä¸»è¦åŸå› å’Œä¼˜ç‚¹å¦‚ä¸‹ï¼š

  1. è®­ç»ƒç¨³å®šæ€§ ï¼šè¿™æ˜¯é‡‡ç”¨ Pre-LN æœ€ä¸»è¦çš„åŸå› ã€‚åœ¨åŸå§‹çš„ Post-LN ç»“æ„ (Input -> Sublayer -> Add -> LayerNorm) ä¸­ï¼Œéšç€ç½‘ç»œå±‚æ•°åŠ æ·±ï¼Œæ¯ä¸€å±‚çš„è¾“å‡ºåœ¨ç´¯åŠ ï¼ˆAdd æ“ä½œï¼‰åæ‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶å‡ºç°å‰§çƒˆå˜åŒ–ï¼ˆæ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼‰ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®šï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹å¾ˆæ·±çš„æ—¶å€™ã€‚Pre-LN ç»“æ„ (Input -> LayerNorm -> Sublayer -> Add) é€šè¿‡åœ¨æ¯ä¸ªå­å±‚çš„è¾“å…¥å¤„è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¨³å®šäº†ä¼ é€’ç»™å­å±‚çš„æ¿€æ´»å€¼èŒƒå›´ï¼Œä»è€Œä¹Ÿç¨³å®šäº†åå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦æµã€‚è¿™ä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ å¹³æ»‘ï¼Œä¸æ˜“å‘æ•£ã€‚
  2. å‡å°‘å¯¹å­¦ä¹ ç‡ Warmup çš„ä¾èµ–
  3. æ›´å¿«çš„æ”¶æ•›ï¼ˆæœ‰æ—¶ï¼‰

### Decoder

#### å› æœæ©ç æœºåˆ¶

* masked multi-head attention

  * ä¿è¯è¾“å‡ºå¯¹è¾“å…¥çš„æ„ŸçŸ¥åºåˆ—ä¸ä¼šè¶…å‡ºé•¿åº¦ï¼šé˜²æ­¢åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ï¼Œç¡®ä¿é¢„æµ‹æ˜¯åŸºäºä¹‹å‰çš„è¾“å‡º

  * å¯¹ QK^T åšmask
  * æ³¨æ„åŠ›çŸ©é˜µï¼šä¸‹ä¸‰è§’éé›¶
  * A = A + Mï¼ŒMçš„æŸäº›å…ƒç´ æ˜¯ $$-\infty$$

#### äº¤å‰å¤šå¤´æ³¨æ„åŠ›å±‚

* Qæ¥è‡ªDecoderï¼šè€ƒè™‘å·²ç»ç”Ÿæˆçš„å†…å®¹
* Kã€Væ¥è‡ªEncoderï¼šè€ƒè™‘ä¸Šä¸‹æ–‡
  * **ä¼ ç»Ÿ Transformer Decoder çš„å±€é™æ€§**ï¼šä¼ ç»Ÿ Transformer Decoder ä¸»è¦ä¾é è¾“å…¥çš„ Kã€V ä¸ Q è®¡ç®—æ³¨æ„åŠ›ï¼Œè¿›è€Œç”Ÿæˆè¾“å‡ºã€‚å½“è¾“å…¥çŸ­ï¼ŒKã€V æä¾›çš„ä¿¡æ¯ä¸è¶³ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯èšç„¦çš„èŒƒå›´çª„ï¼Œè§£ç å™¨éš¾ä»¥è·³å‡ºæœ‰é™ä¿¡æ¯çš„é™åˆ¶ï¼Œå¯¼è‡´é¢„æµ‹ç»“æœå•ä¸€ã€‚

#### KV Cacheçš„å¯è¡Œæ€§

* èƒ½å¦ç›´æ¥æ›´æ–°å†å² KVï¼Ÿ
  * ç†è®ºä¸Šï¼Œä½ å¯ä»¥è®¾è®¡ä¸€ç§æœºåˆ¶ï¼Œåœ¨ç”Ÿæˆç¬¬ t ä¸ª token æ—¶ï¼Œä¸ä»…è®¡ç®— Q_t , K_t , V_t ï¼Œè¿˜å»ä¿®æ”¹ç¼“å­˜ä¸­ K_1...K_{t-1} å’Œ V_1...V_{t-1} çš„å€¼ã€‚
* ä¸ºä»€ä¹ˆé€šå¸¸ä¸è¿™æ ·åšï¼Ÿ

1. ç ´å KV Cache çš„æ ¸å¿ƒä¼˜åŠ¿ ï¼šå¦‚æœæ¯ä¸€æ­¥éƒ½è¦æ›´æ–°æ‰€æœ‰å†å² K/Vï¼Œæ¨ç†æˆæœ¬å°†æ€¥å‰§å¢åŠ ï¼Œä» O(N)ï¼ˆN ä¸ºåºåˆ—é•¿åº¦ï¼Œä½¿ç”¨ Cacheï¼‰å˜å› O(N^2)ï¼ˆæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æˆ–æ›´æ–°æ‰€æœ‰å†å² K/Vï¼‰ï¼Œå¤±å»äº† Transformer æ¨ç†æ•ˆç‡çš„å…³é”®ä¼˜åŒ–ã€‚
2. æ”¹å˜äº†æ³¨æ„åŠ›æœºåˆ¶çš„å«ä¹‰ ï¼šæ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å‡è®¾ä¸€ä¸ª token çš„ K å’Œ V ä»£è¡¨å…¶åœ¨ é‚£ä¸ªæ—¶é—´ç‚¹ çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚åŸºäº æœªæ¥ çš„ token æ¥ä¿®æ”¹ è¿‡å» token çš„ K/V è¡¨ç¤ºï¼Œæ”¹å˜äº†è¿™ç§å‰å‘å› æœå…³ç³»ï¼Œä½¿å¾—æ¨¡å‹ç»“æ„å’Œä¿¡æ¯æµå˜å¾—å¤æ‚ã€‚è¿™æ›´åƒæ˜¯åŒå‘æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰åœ¨ç¼–ç æ•´ä¸ªåºåˆ—æ—¶åšçš„äº‹æƒ…ï¼Œè€Œä¸æ˜¯è‡ªå›å½’ç”Ÿæˆæ¨¡å‹é€è¯ç”Ÿæˆæ—¶çš„å·¥ä½œæ–¹å¼ã€‚
3. å®ç°å¤æ‚ä¸”æ”¶ç›Šä¸æ˜ç¡® ï¼šè®¾è®¡ä¸€ä¸ªæœ‰æ•ˆä¸”ç¨³å®šçš„æ›´æ–°å†å² K/V çš„æœºåˆ¶ä¼šéå¸¸å¤æ‚ï¼Œå¹¶ä¸”ä¸æ¸…æ¥šè¿™æ ·åšæ˜¯å¦èƒ½å¸¦æ¥è¶³å¤Ÿçš„å¥½å¤„æ¥æŠµæ¶ˆå·¨å¤§çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§å¢åŠ ã€‚





### Position Encoding

https://arxiv.org/pdf/1705.03122

* ä¸ºä»€ä¹ˆå¼•å…¥ï¼Ÿ
  * MSAçš„è®¡ç®—ï¼Œæ”¹å˜Qå’ŒKçš„è¯å…ƒä½ç½®ï¼Œè®¡ç®—ç»“æœä¸å˜

* ç»å¯¹ä½ç½®ç¼–ç ï¼š
  * Convolutional Sequence to Sequence Learning
  * æ­£å¼¦-ä½™å¼¦ç¼–ç 
* ç›¸å¯¹ä½ç½®ç¼–ç ï¼š
  * ä½œç”¨äºè‡ªæ³¨æ„åŠ›æœºåˆ¶

#### æ­£å¼¦-ä½™å¼¦ç¼–ç 

åœ¨Transformeræ¶æ„ä¸­ï¼Œç”±äºå…¶æ ¸å¿ƒç»„ä»¶ï¼ˆå¦‚å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰æœ¬èº«ä¸å…·å¤‡æ•æ‰åºåˆ—ä¸­å…ƒç´ ä½ç½®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæ‰€ä»¥éœ€è¦é¢å¤–çš„ä½ç½®ç¼–ç æ¥ä¸ºæ¨¡å‹æä¾›ä½ç½®ä¿¡æ¯ã€‚æ­£å¼¦ - ä½™å¼¦ç¼–ç é€šè¿‡ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°æ¥ç”Ÿæˆä½ç½®ç¼–ç å‘é‡ï¼Œå…¶åŸºæœ¬æ€æƒ³æ˜¯åˆ©ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦æ³¢æ¥è¡¨ç¤ºä¸åŒçš„ä½ç½®ã€‚

å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º $L$ã€ç»´åº¦ä¸º $d$ çš„åºåˆ—ï¼Œä½ç½®ç¼–ç  $PE$ æ˜¯ä¸€ä¸ª $L\times d$ çš„çŸ©é˜µï¼Œå…¶ä¸­ç¬¬ $pos$ ä¸ªä½ç½®ã€ç¬¬ $i$ ä¸ªç»´åº¦çš„ç¼–ç å€¼è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

å½“ $i$ ä¸ºå¶æ•°æ—¶ï¼š
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$$

å½“ $i$ ä¸ºå¥‡æ•°æ—¶ï¼š
$$PE_{(pos, 2i + 1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$$

å…¶ä¸­ï¼Œ$pos$ è¡¨ç¤ºä½ç½®ï¼ˆèŒƒå›´ä» 0 åˆ° $L - 1$ï¼‰ï¼Œ$i$ è¡¨ç¤ºç»´åº¦ï¼ˆèŒƒå›´ä» 0 åˆ° $\frac{d}{2}-1$ï¼‰ï¼Œ$d$ æ˜¯ä½ç½®ç¼–ç å‘é‡çš„ç»´åº¦ã€‚

- **æä¾›ä½ç½®ä¿¡æ¯**ï¼šé€šè¿‡æ­£å¼¦ - ä½™å¼¦ç¼–ç ï¼Œæ¨¡å‹èƒ½å¤ŸåŒºåˆ†åºåˆ—ä¸­ä¸åŒä½ç½®çš„å…ƒç´ ï¼Œä»è€Œå­¦ä¹ åˆ°å…ƒç´ ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚è¿™å¯¹äºå¤„ç†åºåˆ—æ•°æ®ï¼ˆå¦‚è‡ªç„¶è¯­è¨€ã€æ—¶é—´åºåˆ—ç­‰ï¼‰è‡³å…³é‡è¦ï¼Œå› ä¸ºå…ƒç´ çš„é¡ºåºå¾€å¾€æºå¸¦äº†é‡è¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚
- **çº¿æ€§å¯å­¦ä¹ **ï¼šæ­£å¼¦ - ä½™å¼¦ç¼–ç å…·æœ‰ä¸€å®šçš„çº¿æ€§ç‰¹æ€§ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢æ¥å­¦ä¹ ä½ç½®ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚
- **å¤–æ¨æ€§**ï¼šç”±äºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å‘¨æœŸæ€§ï¼Œæ­£å¼¦ - ä½™å¼¦ç¼–ç å…·æœ‰è¾ƒå¥½çš„å¤–æ¨æ€§ï¼Œå³æ¨¡å‹å¯ä»¥å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ã€‚
  - may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.


ä¼˜ç‚¹

- **æ— éœ€å­¦ä¹ **
- **ç›¸å¯¹ä½ç½®ä¿¡æ¯**ï¼šæ¯”å¦‚Båœ¨Aåé¢Nä¸ªä½ç½®ï¼ŒCåœ¨Båé¢Nä¸ªä½ç½®
- **è®¡ç®—é«˜æ•ˆ**

ç¼ºç‚¹

- **å›ºå®šæ¨¡å¼**
- **ç¼ºä¹è¯­ä¹‰ä¿¡æ¯**ï¼šåªæä¾›äº†ä½ç½®ä¿¡æ¯ï¼Œä¸åŒ…å«å…ƒç´ çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¹äºä¸€äº›éœ€è¦ç»“åˆä½ç½®å’Œè¯­ä¹‰ä¿¡æ¯çš„ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦ä¸å…¶ä»–ç¼–ç æ–¹å¼ç»“åˆä½¿ç”¨ã€‚

### æ˜ å°„åˆ°é¢„æµ‹ç©ºé—´

* çº¿æ€§å±‚ã€è¯è¡¨æ•°é‡
  * **éšè—å±‚çš„æ˜ å°„ç›®çš„**ï¼šç¥ç»ç½‘ç»œä¸­æ‰€æœ‰éšè—å±‚çš„æ˜ å°„ï¼Œæœ¬è´¨æ˜¯ä¸æ–­åœ¨å¤šä¸ªè¶…ç©ºé—´ä¸­å¯¹ç‰¹å¾è¿›è¡Œç›¸äº’æ˜ å°„ã€‚
  * **è¡¨ç¤ºå­¦ä¹ å‡è®¾**ï¼šè¡¨ç¤ºå­¦ä¹ å‡å®šå­˜åœ¨ä¸€ä¸ªè¶…ç©ºé—´ï¼Œåœ¨è¿™ä¸ªè¶…ç©ºé—´é‡Œèƒ½å¤Ÿæ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢ï¼Œå°†ç©ºé—´ä¸­çš„ç›®æ ‡å‘é‡åŒºåˆ†å¼€æ¥ã€‚
* share the same weight matrix between the two embedding layers and the pre-softmax linear transformation
  * https://arxiv.org/abs/1608.05859

### ç®—æ³•æŠ€å·§

* Label Smoothing
  * During training, we employed label smoothing of value Ïµls=0.1*Ïµ**l**s*=0.1 [(cite)](https://arxiv.org/abs/1512.00567). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.
  * label [2,1,0,3,3]
    * ![image-20250207130816676](./AI-Algorithms/image-20250207130816676.png)

1.BPE/ Word-piece
https://github.com/rsennrich/subword-nmt
å‡å°‘æœªç™»å½•è¯ï¼ˆOOVï¼‰çš„é—®é¢˜

2.Shared Embeddings
https://arxiv.org/abs/1608.05859

3.Beam Search
https://github.com/OpenNMT/OpenNMT-py/

4.Model Averaging



### æ¨ç†

* æ¨ç†ï¼š
  * `<sos>`ï¼ˆstart of sentenceï¼‰
  * ä¸æ–­è¿‡decoder
  * ç›´åˆ°ç”Ÿæˆeos
* æ¨ç†å’Œè®­ç»ƒçš„åŒºåˆ«
  * æ¨ç†é˜¶æ®µçš„æ“ä½œå’Œè®­ç»ƒé˜¶æ®µçš„è§£ç å™¨æ“ä½œç±»ä¼¼ï¼Œä½†æ˜¯è®­ç»ƒé˜¶æ®µæœ‰ç›®æ ‡åºåˆ—çš„çœŸå®å€¼ä½œä¸ºè¾“å…¥æ¥è®¡ç®—æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­è®­ç»ƒï¼Œè€Œæ¨ç†é˜¶æ®µæ˜¯æ ¹æ®ä¹‹å‰ç”Ÿæˆçš„å•è¯ä¸æ–­ç”Ÿæˆæ–°çš„å•è¯ã€‚
  * åœ¨è®­ç»ƒæ—¶ï¼Œè§£ç å™¨çš„è¾“å…¥æ˜¯å·²çŸ¥çš„ç›®æ ‡åºåˆ—ï¼Œåœ¨æ¨ç†æ—¶ï¼Œè§£ç å™¨çš„è¾“å…¥æ˜¯é€æ­¥ç”Ÿæˆçš„å•è¯åºåˆ—ã€‚

### å±€é™æ€§

* over-smoothing https://arxiv.org/abs/2202.08625
  * æ·±å±‚tokené—´ç›¸ä¼¼åº¦å¢åŠ 
  * è‡ªå›å½’+casual maskçš„éå¯¹ç§°å¯ä»¥ç¼“è§£

### Implementation

* The Annotated Transformer https://nlp.seas.harvard.edu/annotated-transformer
  * https://github.com/harvardnlp/annotated-transformer/


### å®éªŒ

![image-20250205164614941](./AI-Algorithms/image-20250205164614941.png)

### transformerå¤–çš„ç›¸å…³æ¨¡å‹ç»“æ„

| æ¶æ„        | è®¾è®¡è€…                                               | ç‰¹ç‚¹                                     | é“¾æ¥                                                         |
| ----------- | ---------------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------ |
| Transformer | Google                                               | æœ€æµè¡Œï¼Œå‡ ä¹æ‰€æœ‰å¤§æ¨¡å‹éƒ½ç”¨å®ƒ             | [OpenAI çš„ä»£ç ](https://github.com/openai/finetune-transformer-lm/blob/master/train.py) |
| RWKV        | [PENG Bo](https://www.zhihu.com/people/bopengbopeng) | å¯å¹¶è¡Œè®­ç»ƒï¼Œæ¨ç†æ€§èƒ½æä½³ï¼Œé€‚åˆåœ¨ç«¯ä¾§ä½¿ç”¨ | [å®˜ç½‘](https://www.rwkv.com/)ã€[RWKV 5 è®­ç»ƒä»£ç ](https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v5) |
| Mamba       | CMU & Princeton University                           | æ€§èƒ½æ›´ä½³ï¼Œå°¤å…¶é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆ             | [GitHub](https://github.com/state-spaces/mamba)              |

* ç›®å‰åªæœ‰ transformer è¢«è¯æ˜äº†ç¬¦åˆ scaling-lawã€‚
  * æ”¶æ•ˆç”šå¾®
  * è¿™äº›æ–°æ¡†æ¶ï¼Œä¸»è¦ç”¨åœ¨ç«¯ä¾§å¤§æ¨¡å‹
  * å¤§å…¬å¸è¿½æ±‚æ•ˆæœæè‡´çš„å¥½
* RWKVã€Mambaï¼šçº¿æ€§transformer
  * mambaï¼šé€‰æ‹©æ€§SSMæ¶æ„
* MoEæ··åˆä¸“å®¶æ¨¡å‹ï¼š
  * é—¨æ§ç½‘ç»œ+ä¸“å®¶ç½‘ç»œ
  * GPT-3 1750äº¿å‚æ•°
  * GPT-4 1.8ä¸‡äº¿å‚æ•°
    * 16ä¸ªä¸“å®¶ç½‘ç»œ
    * è¿è¡Œæ—¶åªè·‘2ä¸ªä¸“å®¶ç½‘ç»œ
    * ç›¸æ¯”GPT-3.5æ›´åƒäººè„‘
* Additive Attention https://arxiv.org/abs/1409.0473

## Bert

> å®Œå½¢å¡«ç©ºçš„è®­ç»ƒéš¾åº¦æ¯”NTPå°

* Transformer å…·æœ‰ field reduce èƒ½åŠ›ï¼Œå°† N ä¸ª token reduce æˆ M ä¸ª token
* [GELU](https://paperswithcode.com/method/gelu)
  * GELUs are used in [GPT-3](https://paperswithcode.com/method/gpt-3), [BERT](https://paperswithcode.com/method/bert), and most other Transformers.
* Layer Normalization
  * The LayerNorm operator was first introduced in [BA2016]() as a way to **improve the performance of sequential models (e.g., Transformers) or neural networks with small batch size**
  * å¯¹æ¯”layernormå’ŒBN
    * LayerNorm åœ¨ç‰¹å¾ç»´åº¦ä¸Šå¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸ä¾èµ– batch sizeï¼Œè®­ç»ƒå’Œæ¨ç†è¡Œä¸ºä¸€è‡´ï¼Œå¸¸ç”¨äº RNNã€Transformer ç­‰åºåˆ—æ¨¡å‹ã€‚
    * BatchNorm åœ¨ batch ç»´åº¦ä¸Šå¯¹channelè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¯¹ batch size æ•æ„Ÿï¼Œè®­ç»ƒå’Œæ¨ç†è¡Œä¸ºä¸åŒï¼Œå¸¸ç”¨äº CNNã€‚

![image-20241019021744575](./AI-Algorithms/bert-5434356.png)

### Paper

* Intro
  * BERT: Bidirectional Encoder Representations from Transformers.
  * taskç±»å‹ï¼šsentence-level/paraphrasing/token-level
  * æ–¹æ³•ï¼šfeature-based and fine-tuning
    *  In previous work, both approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.
  * BERT addresses the previously mentioned uni-directional constraints by proposing a new pre-training objective:
    * the â€œmasked language model" (MLM)
    * â€œnext sentence predictionâ€ task

![image-20250102001058277](./AI-Algorithms/image-20250102001058277.png)

![image-20250102001246772](./AI-Algorithms/image-20250102001246772.png)

* è¶…å‚ï¼š
  * BERTBASE: L=12, H=768, A=12, Total Parameters=110M
  * BERTLARGE: L=24, H=1024, A=16, Total Parameters=340M
  * In all cases we set the feed-forward/ï¬lter size to be 4H
  * mask settingï¼š
    * mask 15%ï¼Œåªé¢„æµ‹maskedè¯
  * training
    * We train with batch size of 256 sequences (256
      sequences * 512 tokens = 128,000 tokens/batch)
      for 1,000,000 steps, which is approximately 40
      epochs over the 3.3 billion word corpus.
    * use Adam with learning rate of 1e-4, Î²1 = 0.9,
      Î²2 = 0.999, L2 weight decay of 0.01ï¼Œdropout 0.
  * å¾®è°ƒ
    * Batch size: 16, 32
    * Learning rate (Adam): 5e-5, 3e-5, 2e-5
    * Number of epochs: 3, 4

* æ¨¡å‹
  * Embåˆå§‹åŒ–ï¼šWe use WordPiece embeddings (Wu et al.,2016) with a 30,000 token vocabulary. We
    denote split word pieces with ##
  * è®¾è®¡æ€æƒ³ï¼š
    * maskedçš„åŠ¨æœºï¼šçœ‹åˆ°ä¸¤è¾¹ï¼Œä¸æ³„éœ²ä¿¡æ¯
  * é—®é¢˜1:è®­ç»ƒå’Œå¾®è°ƒä¸ä¸€è‡´
    * æ–¹æ¡ˆï¼š8:1:1
    * ![image-20250102001657033](./AI-Algorithms/image-20250102001657033.png)
  * é—®é¢˜2:æ¯ä¸ªbatchåªæœ‰15%çš„tokenè¢«é¢„æµ‹ï¼Œè®­ç»ƒä»£ä»·å¤§
    * æ•ˆæœæ”¶ç›Šæ›´é«˜
  * ä»»åŠ¡ç±»å‹2:next sentenceé¢„æµ‹ï¼Œä¸€åŠå¯¹ä¸€åŠ

* å’ŒGPTå¯¹æ¯”
  *  GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only in-
     troduced at fine-tuning time; BERT learns
     [SEP], [CLS] and sentence A/B embeddings during pre-training
  *  bertè®­ç»ƒè¯­æ–™å¤šã€batch sizeå¤§

### model finetune

* paper
  * squadä»»åŠ¡ï¼Œå­¦ä¸€ä¸ªstartå’Œend vectoré¢„æµ‹startå’Œendä½ç½®
  * CoNLL 2003 Named Entity Recognition (NER) dataset
  * swagä»»åŠ¡ï¼ŒNé€‰ä¸€
    * å­¦ä¸€ä¸ªV vector
    * ![image-20250102002146508](./AI-Algorithms/image-20250102002146508.png)

![image-20250102001936987](./AI-Algorithms/image-20250102001936987.png)

* model finetuneæ˜¯åŸºäºBERTé¢„è®­ç»ƒæ¨¡å‹å¼ºå¤§çš„é€šç”¨è¯­ä¹‰èƒ½åŠ›ï¼Œä½¿ç”¨å…·ä½“ä¸šåŠ¡åœºæ™¯çš„è®­ç»ƒæ•°æ®åšfinetuneï¼Œä»è€Œé’ˆå¯¹æ€§åœ°ä¿®æ­£ç½‘ç»œå‚æ•°ï¼Œæ˜¯å…¸å‹çš„åŒé˜¶æ®µæ–¹æ³•ã€‚ï¼ˆ[BERTåœ¨ç¾å›¢æœç´¢æ ¸å¿ƒæ’åºçš„æ¢ç´¢å’Œå®è·µ](https://zhuanlan.zhihu.com/p/158181085)ï¼‰
* åœ¨BERTé¢„è®­ç»ƒæ¨¡å‹ç»“æ„ç›¸å¯¹ç¨³å®šçš„æƒ…å†µä¸‹ï¼Œç®—æ³•å·¥ç¨‹å¸ˆåšæ–‡ç« çš„æ˜¯æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºã€‚é¦–å…ˆéœ€è¦äº†è§£BERTé¢„è®­ç»ƒæ—¶è¾“å…¥å’Œè¾“å‡ºçš„ç‰¹ç‚¹ï¼ŒBERTçš„è¾“å…¥æ˜¯è¯å‘é‡ã€æ®µå‘é‡ã€ä½ç½®å‘é‡çš„ç‰¹å¾èåˆï¼ˆembeddingç›¸åŠ æˆ–æ‹¼æ¥ï¼‰ï¼Œå¹¶ä¸”æœ‰[CLS]å¼€å¤´ç¬¦å’Œ[SEP]ç»“å°¾ç¬¦è¡¨ç¤ºå¥é—´å…³ç³»ï¼›è¾“å‡ºæ˜¯å„ä¸ªä½ç½®çš„è¡¨ç¤ºå‘é‡ã€‚finetuneçš„ä¸»è¦æ–¹æ³•æœ‰åŒå¥åˆ†ç±»ã€å•å¥åˆ†ç±»ã€é—®ç­”QAã€å•å¥æ ‡æ³¨ï¼ŒåŒºåˆ«åœ¨äºè¾“å…¥æ˜¯å•å¥/åŒå¥ï¼›éœ€è¦ç›‘ç£çš„è¾“å‡ºæ˜¯ å¼€å¤´ç¬¦è¡¨ç¤ºå‘é‡ä½œä¸ºåˆ†ç±»ä¿¡æ¯ æˆ– ç»“åˆåˆ†å‰²ç¬¦æˆªå–éƒ¨åˆ†è¾“å‡ºåšè‡ªç„¶è¯­è¨€é¢„æµ‹ã€‚
* æœç´¢ä¸­finetuneçš„åº”ç”¨ï¼šmodel finetuneåº”ç”¨äºquery-docè¯­ä¹‰åŒ¹é…ä»»åŠ¡ï¼Œå³æœç´¢ç›¸å…³æ€§é—®é¢˜å’ŒembeddingæœåŠ¡ã€‚åœ¨å¬å›andç²—æ’ä¹‹åï¼Œéœ€è¦ç”¨BERTç²¾æ’è¿”å›ä¸€ä¸ªç›¸å…³æ€§åˆ†æ•°ï¼Œè¿™ä¸€é—®é¢˜å’Œè¯­å¥åˆ†ç±»ä»»åŠ¡æœ‰ç›¸ä¼¼æ€§ã€‚æœç´¢finetuneçš„æ‰‹æ³•æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
  * å¹¿æ³›æŒ–æ˜æœ‰æ”¶ç›Šçš„finetuneç´ æï¼šæœ‰æ•ˆçš„åŒ…æ‹¬å‘å¸ƒå·embeddingã€æ–‡ç« æ‘˜è¦ã€ä½œè€…åï¼Œè®­ç»ƒæ‰‹æ®µåŒ…æ‹¬ç›´æ¥è¾“å…¥ã€é¢„å¤„ç†ã€‚model finetuneæ–¹æ³•èƒ½åœ¨æ ‡æ³¨æ•°æ®çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨æ›´å¤šçš„æŒ–æ˜æ•°æ®ä¼˜åŒ–æ¨¡å‹ã€‚
  * æ”¹é€ æ¨¡å‹è¾“å…¥orè¾“å‡º
    * æ¨¡å‹è¾“å…¥
      * ç®€å•çš„title+summary+username+queryæ‹¼æ¥
      * å¤šåŸŸåˆ†éš”ï¼šâ€œè€ƒè™‘åˆ°titleå’Œsummaryå¯¹äºqueryçš„ç›¸å…³æ€§æ˜¯ç±»ä¼¼çš„åˆ†å¸ƒï¼Œusernameå’Œqueryçš„ç›¸å…³æ€§å…³è”æ˜¯æ½œåœ¨çš„ã€‚æ‰€ä»¥ç»™user_nameå•ç‹¬è®¾äº†ä¸€ä¸ªåŸŸï¼Œç”¨sepåˆ†éš”â€
    * æ¨¡å‹è¾“å‡º
      * é—¨è¿‡æ»¤æœºåˆ¶ï¼Œç”¨æŸäº›è¡¨ç¤ºå‘é‡çš„ç›¸åº”åˆ†æ•°åŠ æƒCLSçš„è¯­å¥ç±»å‹è¾“å‡ºåˆ†
      * å¼•å…¥UEï¼Œç›´æ¥å’ŒCLSè¾“å‡ºå‘é‡concat
  * ç´ æçš„è¿›ä¸€æ­¥å¤„ç†ï¼Œå¼•å…¥æ— ç›‘ç£å­¦ä¹ 
    * åœ¨model finetuneçš„æœ‰ç›‘ç£è®­ç»ƒä¹‹å‰ï¼Œåˆ©ç”¨text rankç®—æ³•å¤„ç†finetuneç´ æï¼Œç›¸å½“äºåˆ©ç”¨æ— ç›‘ç£å­¦ä¹ æå‡äº†æŒ–æ˜æ•°æ® â€”â€” å–‚å…¥BERTçš„æ•°æ®çš„è´¨é‡ã€‚
    * æˆªæ–­æ‘˜è¦ï¼Œå®æµ‹æœ‰æ•ˆ
  * Bertè®­ç»ƒä»»åŠ¡çš„è®¾è®¡æ–¹å¼å¯¹æ¨¡å‹æ•ˆæœå½±å“å¤§
    * å°†finetuneè¿›ä¸€æ­¥åˆ†ä¸ºä¸¤é˜¶æ®µï¼ŒæŠŠè´¨é‡è¾ƒä½ã€æŒ–æ˜çš„æ•°æ®æ”¾åœ¨ç¬¬ä¸€é˜¶æ®µfinetuneï¼Œè´¨é‡é«˜çš„æ ‡æ³¨æ•°æ®æ”¾åœ¨ç¬¬äºŒé˜¶æ®µfinetuneï¼Œä¼˜åŒ–finetuneçš„æ•´ä½“æ•ˆæœã€‚
    * è¿™ç§é€’è¿›çš„è®­ç»ƒæŠ€å·§åœ¨BERTä¸­è¾ƒå¸¸è§ï¼Œè®ºæ–‡ä¸­ä¹Ÿæœ‰å°†é•¿åº¦è¾ƒçŸ­çš„å‘é‡æ”¾åœ¨ç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„æ–¹æ³•ã€‚

### å‘é‡é™ç»´

* å‘é‡ç™½åŒ–
  * https://arxiv.org/pdf/2103.15316

## GPT

* ç»´ç‰¹æ ¹æ–¯å¦ï¼šè¯­è¨€æ˜¯æ€æƒ³çš„è¾¹ç•Œ
  * NLPæ˜¯å®ç°AGIçš„å…³é”®
* ç›®æ ‡ï¼šå»ºè®¾NLPé¢†åŸŸçš„â€œé¢„è®­ç»ƒ+å¾®è°ƒâ€œçš„è®­ç»ƒèŒƒå¼
  * ä¸ºä»€ä¹ˆNLPçš„ç ”å‘æ•ˆç‡ä½ï¼Ÿ
    * è®­ç»ƒé€Ÿåº¦æ…¢ã€æˆæœ¬é«˜
    * ä»»åŠ¡ç§ç±»å¤šã€ç¹æ‚
      * æ‰€æœ‰NLPä»»åŠ¡éƒ½å¯ä»¥è½¬åŒ–ä¸ºè¯­è¨€æ¨¡å‹çš„é¢„æµ‹
      * ![image-20250205180037387](./AI-Algorithms/image-20250205180037387.png)
        * Entailmentï¼šæ–‡æœ¬è•´å«ä»»åŠ¡
    * è¯­æ–™å¤„ç†éš¾åº¦å¤§
    * é«˜è´¨é‡æ•°æ®ç¨€ç–
      * next token predictionä»»åŠ¡çš„æ³›åŒ–æ€§å·® --> Scaling Lawä¼˜åŒ–

* å¦‚ä½•Scaling Lawï¼Ÿ
  - ç®€åŒ–æ¨¡å‹ç»“æ„ï¼š
    - Decoder-Onlyæ¶æ„ï¼Œå»é™¤äº¤å‰æ³¨æ„åŠ›å±‚
      - 6ç¼–ç 6è§£ç  -> 12å±‚è§£ç å™¨ï¼Œè¶…å¤§å‚æ•°è§„æ¨¡
    - N-gramæ”¹å˜ä¸ºå¯¹å…¨å±€ä¸Šä¸‹æ–‡attention
  - å¤æ‚åŒ–æ¨¡å‹ç»“æ„ï¼š
    - multi head
    - å¢åŠ MLP
    - å¤šå±‚è§£ç 



* æ¨¡å‹ç»“æ„ï¼š
  * é¢„è®­ç»ƒLossï¼šå–å¯¹æ•°ï¼Œè§£å†³seq lenå¢åŠ ä¹‹åï¼Œæ¡ä»¶æ¦‚ç‡çš„ç›¸ä¹˜é—®é¢˜
  * å¾®è°ƒLossï¼š
    * ![image-20250205191932896](./AI-Algorithms/image-20250205191932896.png)







## GPT-2

* ç›®æ ‡ï¼šå¦‚æœä¸å¾®è°ƒäº†ï¼Œèƒ½ä¸èƒ½æœ‰æ›´å¥½çš„æ•ˆæœï¼Ÿ
  * ç¨€ç–è‡ªæ³¨æ„åŠ›æœºåˆ¶
  * å¢åŠ batch sizeåˆ°ç™¾ä¸‡ï¼Œå‡å°‘é€šä¿¡é‡
  * çˆ¬å–rabbit/wikipedia
* æ€è·¯ï¼šè¶³å¤Ÿå¤šçš„æ•°æ®ï¼Œæ¨¡å‹èƒ½å¤Ÿç†è§£ä»»åŠ¡æ„å›¾
  * promptçš„å‰èº«

![image-20241019021839037](./AI-Algorithms/image-20241019021839037.png)

* è‡ªå›å½’æ¶æ„
  * å±€é™æ€§ï¼šåªæ¥å—ç¦»æ•£æ ·æœ¬
  * ä¸€ä¸ªä¸€ä¸ªå­—è¾“å‡º



* TODO1: https://jalammar.github.io/illustrated-gpt2/
* https://github.com/openai/gpt-2

## GPT-3

* ç›®æ ‡ï¼šæ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œå­¦ä¹ 
  * ![image-20250205193747735](./AI-Algorithms/image-20250205193747735.png)

* Decoder
  * 12288ç»´
  * 96å±‚ï¼š
    * 12288 -> 128
    * 12288 -> 4*12288
    * Insightï¼š512ç»´å­˜ä¸ä¸‹96å±‚ä¿¡æ¯èšåˆï¼Œå› æ­¤ç”¨12288ç»´

|      | N layers | Dim   | Head | Dim per Head |
| ---- | -------- | ----- | ---- | ------------ |
| 1.3B | 24       | 2048  | 16   | 128          |
| 13B  | 40       | 5120  | 40   | 128          |
| 175B | 96       | 12288 | 96   | 128          |

## GPT-3.5 (ChatGPT)

* ç›®æ ‡ï¼šä¸äººç±»çš„æŒ‡ä»¤å¯¹é½
  * æ— æ³•å¯¹é½/ä¸å®‰å…¨

![image-20250205194626295](./AI-Algorithms/image-20250205194626295.png)

* å¯¹è¯å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼šhttps://openai.com/blog/chatgpt/
  * è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼šå¸®åŠ©èƒŒä¸‹æ¥äº‹ä»¶çŸ¥è¯†
  * å¤§è¯­è¨€æ¨¡å‹ï¼šç™¾äº¿å‚æ•°ä»¥ä¸Š
    * ä¸å¥½åšfinetuneï¼Œæˆæœ¬é«˜
    * ç”¨promptä½œä¸ºè¾“å…¥ï¼Œgenerated textä½œä¸ºè¾“å‡º
    * è¯­è¨€çŸ¥è¯† + äº‹ä»¶çŸ¥è¯†ï¼Œäº‹ä»¶çŸ¥è¯†æ›´éœ€è¦å¤§æ¨¡å‹

  * æœªæ¥ï¼šAGI(Artificial General Intelligence)ï¼›æ•™ä¼šå®ƒä½¿ç”¨å·¥å…·

* ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼š
  * In-Context Learning æƒ…æ™¯å­¦ä¹ 
    * åœ¨å‰å‘ä¸­å­¦ä¹ 
    * æ¶Œç°èƒ½åŠ›ï¼šç™¾äº¿å‚æ•°è§„æ¨¡ä¹‹åï¼Œèƒ½åŠ›çªç„¶æå‡ï¼Œæ”¹å˜ä¼ ç»Ÿå­¦ä¹ èŒƒå¼
    * å¤§å¹…é™ä½ä¸‹æ¸¸ä»»åŠ¡å¼€å‘æˆæœ¬
    * ã€ŠRethinking the Role of Demonstrations: What Makes In-Context Learning Work?ã€‹ --> éšæœºlabelä»å¯èƒ½æå‡æ•ˆæœ
  * Chain-of-Thought, CoT æ€ç»´é“¾
    * ã€ŠPAL: Program-aided Language Modelsã€‹ï¼Œè®©è¯­è¨€æ¨¡å‹ç”Ÿæˆèƒ½ç”±è®¡ç®—æ¨¡å‹æ‰§è¡Œçš„æè¿°ä»£ç 
    * åœ¨å¤§æ¨¡å‹ä¸­æ‰“ç ´scaling law
  * Learning from Natural Instructions è‡ªç„¶æŒ‡ä»¤å­¦ä¹ 
    * å¾ˆåƒæƒ…æ™¯å­¦ä¹ ï¼Œæ ·æœ¬é€æ¸ç®€åŒ–ï¼ˆè´Ÿä¾‹ä¸éœ€è¦suggestionï¼›ä¸éœ€è¦è´Ÿä¾‹ï¼‰
    * https://instructions.apps.allenai.org/
    * OpenAI: é€šè¿‡äººç±»åé¦ˆå¯¹é½äººç±»æŒ‡ä»¤
* **å¤§æ¨¡å‹å…·å¤‡äº†å¯¹çŸ¥è¯†çš„è·¨è¯­è¨€èƒ½åŠ›**
* RLHF
  * è§ã€æœ¬æ–‡æ¡£-finetuning-RLHFã€‘éƒ¨åˆ†
  * æƒ©ç½š1ï¼šè¿‡å¤§çš„æ¢¯åº¦/æ¦‚ç‡å€¼
  * æƒ©ç½š2ï¼šç¾éš¾æ€§é—å¿˜
* limitations
  * Correctness: æ¨¡å‹ä¸æ˜¯å…¨çŸ¥çš„ï¼Œä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´å…«é“
  * sensitive to rephrase
  * verbose
  * No asking clarifying questionsï¼Œè€Œæ˜¯çŒœ
  * it will sometimes respond to harmful instructions or exhibit biased behavior

* [Iterative deployment](https://openai.com/blog/language-model-safety-and-misuse/)
* Evaluation
  * Holistic Evaluation of Language Models


* Note
  * ç§‘æŠ€éƒ¨éƒ¨é•¿ç‹å¿—åˆšè¡¨ç¤ºï¼ŒChatGPTæœ‰å¾ˆå¥½çš„è®¡ç®—æ–¹æ³•ï¼ŒåŒæ ·ä¸€ç§åŸç†ï¼Œåœ¨äºåšå¾—å¥½ä¸å¥½ï¼›å°±åƒè¸¢è¶³çƒï¼Œéƒ½æ˜¯ç›˜å¸¦ã€å°„é—¨ï¼Œä½†æ˜¯è¦åšåˆ°åƒæ¢…è¥¿é‚£ä¹ˆå¥½ä¹Ÿä¸å®¹æ˜“ã€‚
  * å®¢è§‚é¢˜é«˜è€ƒ515åˆ†æ°´å¹³
* [ä¸“è®¿Altman](https://www.pingwest.com/a/285835)

  * **æ„Ÿæƒ³**ï¼šæœ‰å‡ ä¸ªç‚¹å€¼å¾—å…³æ³¨ï¼šaiè‡ªè¿è¡Œçš„èƒ½åŠ›ã€aiéšè—æ„å›¾çš„èƒ½åŠ›ã€aiä¸çœŸå®ç‰©è´¨ä¸–ç•Œæ¥å£çš„èƒ½åŠ›ã€aiè®¤è¯†åˆ°è‡ªå·±çš„ç°å®å¤„å¢ƒå¹¶å·®å¼‚åŒ–å¤„ç†çš„èƒ½åŠ›

    * å½“è¿™äº›èƒ½åŠ›å®Œå…¨å…·å¤‡ï¼Œå¯èƒ½AGIç¡®å®å¯ä»¥æ¯ç­äººç±»

  * å½“ä»–è§‚å¯Ÿæ¨¡å‹çš„éšè—å±‚æ—¶ï¼Œå‘ç°å®ƒæœ‰ä¸€ä¸ªä¸“é—¨çš„ç¥ç»å…ƒç”¨äºåˆ†æè¯„è®ºçš„æƒ…æ„Ÿã€‚ç¥ç»ç½‘ç»œä»¥å‰ä¹Ÿåšè¿‡æƒ…æ„Ÿåˆ†æï¼Œä½†å¿…é¡»æœ‰äººå‘Šè¯‰å®ƒä»¬è¿™æ ·åšï¼Œè€Œä¸”å¿…é¡»ä½¿ç”¨æ ¹æ®æƒ…æ„Ÿæ ‡è®°çš„æ•°æ®å¯¹å®ƒä»¬è¿›è¡Œä¸“é—¨çš„è®­ç»ƒã€‚è€Œè¿™ä¸ªç¥ç»ç½‘ç»œå·²ç»è‡ªè¡Œå¼€å‘å‡ºäº†è¿™ç§èƒ½åŠ›ã€‚
  * è¯­è¨€æ˜¯ä¸€ç§ç‰¹æ®Šçš„è¾“å…¥ï¼Œä¿¡æ¯é‡æä¸ºå¯†é›†
  * "å‡è®¾æˆ‘ä»¬çœŸçš„é€ å‡ºäº†è¿™ä¸ªäººå·¥æ™ºèƒ½ï¼Œå…¶ä»–ä¸€äº›äººä¹Ÿé€ å‡ºäº†"ã€‚ä»–è®¤ä¸ºï¼Œéšä¹‹è€Œæ¥çš„å˜é©å°†æ˜¯å†å²æ€§çš„ã€‚ä»–æè¿°äº†ä¸€ä¸ªå¼‚å¸¸ä¹Œæ‰˜é‚¦çš„æ„¿æ™¯ï¼ŒåŒ…æ‹¬é‡å¡‘é’¢ç­‹æ°´æ³¥çš„ä¸–ç•Œã€‚ä»–è¯´ï¼š"ä½¿ç”¨å¤ªé˜³èƒ½å‘ç”µçš„æœºå™¨äººå¯ä»¥å»å¼€é‡‡å’Œæç‚¼å®ƒä»¬éœ€è¦çš„æ‰€æœ‰çŸ¿ç‰©ï¼Œå¯ä»¥å®Œç¾åœ°å»ºé€ ä¸œè¥¿ï¼Œä¸éœ€è¦äººç±»åŠ³åŠ¨ã€‚"ä½ å¯ä»¥ä¸ 17 ç‰ˆ DALL-E å…±åŒè®¾è®¡ä½ æƒ³è¦çš„å®¶çš„æ ·å­ï¼Œ"Altmanè¯´ã€‚"æ¯ä¸ªäººéƒ½å°†æ‹¥æœ‰ç¾ä¸½çš„å®¶å›­ã€‚åœ¨ä¸æˆ‘çš„äº¤è°ˆä¸­ï¼Œä»¥åŠåœ¨å·¡å›æ¼”è®²æœŸé—´çš„èˆå°ä¸Šï¼Œä»–è¯´ä»–é¢„è§åˆ°äººç±»ç”Ÿæ´»çš„å‡ ä¹æ‰€æœ‰å…¶ä»–é¢†åŸŸéƒ½å°†å¾—åˆ°å·¨å¤§çš„æ”¹å–„ã€‚éŸ³ä¹å°†å¾—åˆ°æå‡ï¼ˆ"è‰ºæœ¯å®¶ä»¬å°†æ‹¥æœ‰æ›´å¥½çš„å·¥å…·"ï¼‰ï¼Œäººé™…å…³ç³»ï¼ˆäººå·¥æ™ºèƒ½å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ° "ç›¸äº’å¯¹å¾…"ï¼‰å’Œåœ°ç¼˜æ”¿æ²»ä¹Ÿå°†å¦‚æ­¤ï¼ˆ"æˆ‘ä»¬ç°åœ¨éå¸¸ä¸æ“…é•¿æ‰¾å‡ºåŒèµ¢çš„å¦¥åæ–¹æ¡ˆ"ï¼‰ã€‚
  * GPT-4å­¦ä¼šäº†â€œè¯´è°â€ï¼šéªŒè¯ç 

    * -> è®©GPT-4è®²è§£è‡ªå·±åšäº‹æƒ…çš„ç›®çš„ï¼Œå°†ä¸å†å¯é 
    * Sutskever è¯´ï¼Œä»–ä»¬å¯èƒ½ä¼šåœ¨å¼±å°çš„æ—¶å€™é‡‡å–ä¸€ç§è¡ŒåŠ¨ï¼Œè€Œåœ¨å¼ºå¤§çš„æ—¶å€™é‡‡å–å¦ä¸€ç§è¡ŒåŠ¨ã€‚æˆ‘ä»¬ç”šè‡³ä¸ä¼šæ„è¯†åˆ°ï¼Œæˆ‘ä»¬åˆ›é€ çš„ä¸œè¥¿å·²ç»å†³å®šæ€§åœ°è¶…è¶Šäº†æˆ‘ä»¬ï¼Œæˆ‘ä»¬ä¹Ÿä¸çŸ¥é“å®ƒæ‰“ç®—ç”¨è‡ªå·±çš„è¶…èƒ½åŠ›åšäº›ä»€ä¹ˆã€‚

## GPT-4

> * äº®ç‚¹ï¼š
>   * å¤šæ¨¡æ€
>   * å¤§é‡çš„RLHFï¼Œæœ€å®‰å…¨/å¯æ§çš„æ¨¡å‹
>   * åœ¨å°æ¨¡å‹ä¸Šåšæ¶ˆèå®éªŒï¼Œä»è€Œé¢„æµ‹å¤§æ¨¡å‹å®éªŒæ•ˆæœ
>   * ä¸“å®¶ç®—æ³•æŠ•ç¥¨

* GPT-4å¹•åçš„ç ”å‘å›¢é˜Ÿå¤§è‡´å¯åˆ†ä¸ºä¸ƒä¸ªéƒ¨åˆ†ï¼šé¢„è®­ç»ƒï¼ˆPretrainingï¼‰ã€é•¿ä¸Šä¸‹æ–‡ï¼ˆLong contextï¼‰ã€è§†è§‰ï¼ˆVisionï¼‰ã€å¼ºåŒ–å­¦ä¹ å’Œå¯¹é½ï¼ˆRL & alignmentï¼‰ã€è¯„ä¼°å’Œåˆ†æï¼ˆEvaluation & analysisï¼‰ã€éƒ¨ç½²ï¼ˆDeploymentï¼‰ä»¥åŠå…¶ä»–è´¡çŒ®è€…ï¼ˆAdditional contributionsï¼‰
* [GPT-4æŠ€æœ¯æŠ¥å‘Š](https://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA==&mid=2247484155&idx=1&sn=5ef0fcf20d4b87366269d3c0cf4312c0&scene=21#wechat_redirect)
  * 32kå¯¹åº”50é¡µçš„context
* [Language models can explain neurons in language models](https://openai.com/research/language-models-can-explain-neurons-in-language-models)
  * æ­¥éª¤ï¼š
    * GPT-4è§£é‡ŠæŸä¸ªGPT-2ç¥ç»å…ƒçš„è¡Œä¸º
    * ç”¨GPT-4æ¨¡æ‹Ÿè¿™ä¸€è¡Œä¸º
    * æ¯”è¾ƒå¹¶æ‰“åˆ†

  * OpenAI å…±è®© GPT-4 è§£é‡Šäº† GPT-2 ä¸­çš„ 307200 ä¸ªç¥ç»å…ƒï¼Œå…¶ä¸­å¤§å¤šæ•°è§£é‡Šçš„å¾—åˆ†å¾ˆä½ï¼Œåªæœ‰è¶…è¿‡ 1000 ä¸ªç¥ç»å…ƒçš„è§£é‡Šå¾—åˆ†é«˜äº 0.8ã€‚
  * ä¸‰ç§æé«˜è§£é‡Šå¾—åˆ†çš„æ–¹æ³•ï¼š
    - å¯¹è§£é‡Šè¿›è¡Œè¿­ä»£ï¼Œé€šè¿‡è®© GPT-4 æƒ³å‡ºå¯èƒ½çš„åä¾‹ï¼Œæ ¹æ®å…¶æ¿€æ´»æƒ…å†µä¿®æ”¹è§£é‡Šæ¥æé«˜åˆ†æ•°ã€‚
    - ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹æ¥è¿›è¡Œè§£é‡Šï¼Œå¹³å‡å¾—åˆ†ä¹Ÿä¼šä¸Šå‡ã€‚
    - è°ƒæ•´è¢«è§£é‡Šæ¨¡å‹çš„ç»“æ„ï¼Œç”¨ä¸åŒçš„æ¿€æ´»å‡½æ•°è®­ç»ƒæ¨¡å‹ã€‚
  * https://github.com/openai/automated-interpretability
  * ä¼ ç»Ÿçš„è§†è§‰è§£é‡Šæ–¹æ³•ä¸èƒ½scale well
    * https://openai.com/research/microscope
    * https://distill.pub/2020/circuits/curve-detectors/

## LLAMA 3

* Intro
  * uses RMSNorm [ZS19], SwiGLU [Sha20], rotary embedding [SAL+24], and removes all biases
* https://hasgeek.com/simrathanspal/the-llama3-guide/sub
* https://ai.meta.com/blog/meta-llama-3/

## DeepSeek-V3

> DeepSeek-V3 Technical Report

* DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total
  parameters with 37B activated for each token
  * å…³é”®æŠ€æœ¯
    * Multi-head Latent Attention (MLA)
    * DeepSeekMoE architectures
    * an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training
      objective for stronger performance
    * fp8 training
    * DualPipeï¼šovercome the communication bottleneck in cross-node MoE training
    * cross-node all-to-all communication kernels
    * æ˜¾å­˜ä¼˜åŒ–
    * MTP
  * æ•°æ®é‡ï¼š14T tokens
  * è®­ç»ƒæˆæœ¬ï¼š
    * 2.788M H800 GPU hours for its full training
    * 558ä¸‡åˆ€
  * è®­ç»ƒæµç¨‹ï¼š
    * pretrain 14T tokens
    * a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K.
    * post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)

![image-20250501010935207](./AI-Algorithms/image-20250501010935207.png)

### MLA

* The core of MLA is the **low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference**
  * ä» ht åˆ° ctKVï¼Œè¿›è¡Œä¸€æ¬¡ä½ç§©å˜æ¢



![image-20250501011323591](./AI-Algorithms/image-20250501011323591.png)

### DeepSeekMoE

* shared experts
  * ![image-20250501014015449](./AI-Algorithms/image-20250501014015449.png)

* Auxiliary-Loss-Free Load Balancing
  * æ¯ä¸ªstepè¿›è¡Œç­–ç•¥è°ƒèŠ‚
  * ![image-20250501014407504](./AI-Algorithms/image-20250501014407504.png)
* Complementary Sequence-Wise Auxiliary Loss.
  * ![image-20250501021522251](./AI-Algorithms/image-20250501021522251.png)

* Node-Limited Routing.
  * è‡³å¤šM nodesï¼Œæ¯ä¸ªnodeé€‰ Kr/M ä¸ªä¸“å®¶

### MTP

> Gloeckle et al. (2024)

* Different from Gloeckle et al. (2024), which parallelly predicts ğ· additional tokens using independent
  output heads, we sequentially predict additional tokens and keep the complete causal chain at
  each prediction depth.

![image-20250501023813072](./AI-Algorithms/image-20250501023813072.png)

* Our principle of maintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its primary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we
utilize MTP to improve training.

* the acceptance rate of the second token prediction ranges between 85% and 90%

### DualPipe + Efficient communication kernels

* On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajb-
  handari et al., 2020).
  * ![image-20250501025054344](./AI-Algorithms/image-20250501025054344.png)
  * ![image-20250501025258887](./AI-Algorithms/image-20250501025258887.png)

* customize efficient cross-node all-to-all communication kernels (including dispatching and combining) to conserve the number of SMs dedicated to communication.
  * In detail, we employ the **warp specialization technique** (Bauer et al., 2014) and partition
    20 SMs into 10 communication channels. 
  * During the dispatching process, (1) IB sending, (2)
    IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The
    number of warps allocated to each communication task is dynamically adjusted according to the
    actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending,
    (2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also
    handled by dynamically adjusted warps.
  * In addition, both dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and the interference to other SMs.

### Fp8-Trainingã€æ¨ç†éƒ¨ç½²

å‚è€ƒå…¶å®ƒç¬”è®°

### ç¡¬ä»¶è®¨è®º

* the **SMs** primarily perform the following tasks for **all-to-all communication:** ï¼ˆ 20/132 SMs for H800ï¼‰
  â€¢ Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB
  traffic destined for multiple GPUs within the same node from a single GPU.
  â€¢ Transporting data between RDMA buffers (registered GPU memory regions) and in-
  put/output buffers.
  â€¢ Executing reduce operations for all-to-all combine.
  â€¢ Managing fine-grained memory layout during chunked data transferring to multiple
  experts across the IB and NVLink domain.
  * æœŸæœ›ç”¨ç±»ä¼¼ NVIDIA SHARP Graham et al. (2016). æ¥åš
  * aim for this hardware to unify the IB (scale-out) and NVLink
    (scale-up) networks from the perspective of the computation units

### Pretraining

* data
  * Inspired by Ding et al. (2024), we implement the document
    packing method for data integrity but do not incorporate cross-sample attention masking during
    training
  * Fill-in-Middle (FIM) strategy does not compromise the next-token prediction capability while
    enabling the model to accurately predict middle text based on contextual cues
    * ![image-20250501215109853](./AI-Algorithms/image-20250501215109853.png)
  * The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended
    vocabulary of 128K tokens.
    * the new pretokenizer introduces tokens that combine punctuations and line breaks. However,
      this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes
      multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts.
      To address this issue, we randomly split a certain proportion of such combined tokens during
      training, which exposes the model to a wider array of special cases and mitigates this bias.

* model
  * We set the number of Transformer layers to 61 and the hidden
    dimension to 7168. All learnable parameters are randomly initialized with a standard deviation
    of 0.006.
  * In MLA, we set the number of attention heads ğ‘›â„ to 128 and the per-head dimension ğ‘‘â„
    to 128. The KV compression dimension ğ‘‘ğ‘ is set to 512, and the query compression dimension ğ‘‘â€²ğ‘
    is set to 1536. For the decoupled queries and key, we set the per-head dimension ğ‘‘ğ‘…â„ to 64. We
    **substitute all FFNs except for the first three layers with MoE layers**. Each MoE layer consists of 1 shared expert and 256 routed experts, where the intermediate hidden dimension of each expert
    is 2048. Among the routed experts, 8 experts will be activated for each token, and each token
    will be ensured to be sent to at most 4 nodes. The multi-token prediction depth ğ· is set to 1, i.e.,
    besides the exact next token, each token will predict one additional token. As DeepSeek-V2,
    DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors,
    and multiplies additional scaling factors at the width bottlenecks. Under this configuration,
    DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token.
  * 4.3. Long Context Extension

* evaluation
  * MTPæå‡æ•ˆæœ
  * auxiliary-loss-free balancing strategyæå‡æ•ˆæœ

### Post-Training

#### SFT

* RL training phase
  * R1ç”Ÿæˆreasoning data
    * <problem, original response>, <system prompt, problem, R1 response>.
  * Non-Reasoning Data.
    * For non-reasoning data, such as creative writing, role-play, and sim-
      ple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human
      annotators to verify the accuracy and correctness of the data.
* SFT Settingsï¼šWe fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the
  cosine decay learning rate scheduling that starts at 5 Ã— 10âˆ’6 and gradually decreases to 1 Ã— 10âˆ’6.
  During training, **each single sequence is packed from multiple samples**. However, we adopt a
  sample masking strategy to ensure that these examples remain isolated and mutually invisible.

#### RL

* Rule-Based RM.
* Model-Based RM.
  * The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its
    reliability, we construct preference data that not only provides the final reward but also includes
    the chain-of-thought leading to the reward.

#### å…¶å®ƒ

* Distillation from DeepSeek-R1

* Self-Rewarding

  

## Datasets and Evaluation

### Intro

* å°æ•°æ®é›†ï¼šå¿«é€ŸéªŒè¯æ”¶æ•›æ€§

![20250402-184503](./AI-Algorithms/20250402-184503.jpeg)

### Datasets

* 100B tokenï¼šCommon Crawlæ•°æ®é›†
* hellaswag, a commonsense sentence completion task
* wikitext, a next token/byte prediction task, and a few question-answering tasks such as arc, openbookqa, and piqa.
  * For wikitext, **perplexity** refers to the inverse of how well the model can predict the next word or byte (lower is better), and **bits_per_byte** refers to how many bits are needed to predict the next byte (lower is also better here). For all other tasks, **acc_norm** refers to the accuracy normalized by the byte-length of the target string.
* Dolmaï¼š3T token https://huggingface.co/datasets/allenai/dolma


### Evaluation

* lm-evaluation-harness: https://github.com/EleutherAI/lm-evaluation-harness

## MoE

> æ€è·¯ï¼šæ‰©å‚æ•°é‡ï¼Œä¿Flopsä¸å˜

* Intro
  * https://huggingface.co/blog/moe

### SparseMoE

* æ¯ä¸ªtokenåˆ†é…åˆ°Gateåˆ†æ•°æœ€é«˜çš„kä¸ªExpertsä¸Šè¿›è¡Œè®¡ç®—
* é—®é¢˜ï¼š
  * load balance
  * è®¿å­˜boundï¼šExpert parallelism

#### Load Balance

* For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance.ã€deepseek-v3ã€‘
  * Auxiliary-Loss-Free Load Balancing.
    * æ¯ä¸ªstepè¿›è¡Œç­–ç•¥è°ƒèŠ‚
    * ![image-20250501014407504](./AI-Algorithms/image-20250501014407504.png)

### SoftMoE

> google paper

* å¯¹äºè¾“å…¥çš„$$N$$ä¸ª tokens é€šè¿‡çº¿æ€§ç»„åˆï¼ˆDispatchï¼‰å¾—åˆ°$$S$$ä¸ª slotï¼Œç”±$$E$$ä¸ª Expert å‡åŒ€å¤„ç†$$S$$ä¸ª slot åå†æ˜ å°„å›ï¼ˆCombineï¼‰$$N$$ä¸ª tokensï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥çœ‹ä½œæ˜¯æŸç§Merge Tokensçš„æ€æƒ³ã€‚å½“$$S<N$$å¯æ˜¾è‘—å‡å°‘ FLOPSï¼ŒåŒæ—¶å¯ä»¥é€šè¿‡ Expert çš„æ•°ç›®æ¥æ§åˆ¶å‚æ•°é‡ã€‚
  * S == E æ—¶ï¼Œç†è§£ä¸º Merge Tokens

### HardMoE

* N == Sï¼Œä¸å†å¯¹è¾“å…¥tokensè¿›è¡Œdispatchï¼ŒPertokensFFN
  * æ ¹æ®è¯­ä¹‰ä¿¡æ¯åˆ†é…token



## MLLM(Multimodal LLM)

### Intro

* Modal: å›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬

* MLLM = LLM + æ¥æ”¶ã€æ¨ç†å¤šæ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›

  * å¬é›¨å£°ï¼Œåˆ¤æ–­è·¯é¢æƒ…å†µï¼Œä»Šå¤©æ˜¯å¦é€‚åˆå‡ºé—¨
  * æ¦‚å¿µï¼šå•æ¨¡æ€ã€å¤šæ¨¡æ€ã€è·¨æ¨¡æ€ã€å¤šæ¨¡æ€è¯­è¨€å¤§æ¨¡å‹
  * å•æ¨¡æ€
    * ![image-20241124014848392](./AI-Algorithms/image-20241124014848392.png)
    * LVM
  * è·¨æ¨¡æ€ï¼š
    * éŸ³é¢‘->è§†è§‰ï¼šæ•°å­—äºº
      * èš‚èšEchomimicï¼šå®æ—¶æ¸²æŸ“å£æ’­
      * å¿«æ‰‹ï¼šLivePortrait
        * éäººã€å¡é€šï¼Œéƒ½èƒ½é©±åŠ¨
      * SadTalker paper/code
      * æµ™å¤§ã€å­—èŠ‚ Real3d-portrait
      * ani-portrait
      * facebook researchï¼šaudio2photoreal
    * æ–‡æœ¬->éŸ³é¢‘ï¼š
      * TTSã€éŸ³è‰²å…‹éš†ã€å°‘æ ·æœ¬ï¼šGPT-SoVITS
        * æƒ…æ„Ÿè‰²å½©ã€è¯­è°ƒï¼Œä¸€èˆ¬
      * ChatTTS
        * æœ‰æƒ…æ„Ÿè‰²å½©
      
      * SUNOï¼šéŸ³ä¹ç”Ÿæˆ
      * å¼€æºå·¥å…·
        * Metaï¼šaudiodraft
        * stable-audio-open-1.0
  
  * å¤šæ¨¡æ€æ¨¡å‹
    * ![image-20241207210031737](./AI-Algorithms/image-20241207210031737.png)

### Literature Review

* Vision Transformers       [Beyond the CLS Token: Image Reranking using Pretrained Vision Transformers]
  * Vision Transformers (ViT) [9], directly applied transformer architectures from NLP to image classification. 
  * To improve the training efficiency of ViT, DeiT [28] introduced token-based distillation with Convolutional Neural Networks (CNNs) as the teacher.
  * combine CNNs and ViT
    * PVT [30] introduced the pyramid structure into ViT, which generates
      multi-scale feature for dense prediction tasks.
    * CvT [33] leveraged convolutional patch embedding and convolutional attention projection to combine the best aspects of both CNNs and transformers.
    * The Swin Transformer [18] introduced a shifted window scheme to limit
      self-attention within windows while allowing interaction between windows.



### å¤šæ¨¡æ€å¤§æ¨¡å‹å†å²å‘å±•

#### ViTæ¨¡å‹ï¼Œå›¾åƒè¡¨ç¤ºçš„tokenåŒ–

##### ViT

![image-20241207210214783](./AI-Algorithms/image-20241207210214783.png)

![image-20241207210250921](./AI-Algorithms/image-20241207210250921.png)

#####  [ViT-MAE] Vision Transformer based on Masked Autoencoding  (Kaiming He) 

* In the input image, 75% patches are randomly masked; the encoder module of ViT only takes unmasked patches as input, and produces an embedding. This embedding is then concatenated with learnable masked image patch encoding.
* ![img](./AI-Algorithms/figure_6-1.png)



##### Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

* key differences between language and vision data is of **the variation in scale between image features and language tokens.**
  * visual featureçš„å°ºåº¦æ›´ç»†ï¼› nlp tokençš„å°ºåº¦å›ºå®š
* ![img](./AI-Algorithms/figure_7.png)

* SWIN is a hierarchical transformer which addresses this problem of scale variation by computing transformer representation with shifted windows. The idea is to further divide usual image patches of input image to even smaller patches. These smaller non overlapping patches are then presented to attention layers.

* The output from these attention layers are then **concatenated in pairs** to combine attention output the two higher level patches, this concatenated output is presented to next set of attention modules.
* This hierarchical propagation through attention layers, allows transformer to **pay attention to smaller scale features and deal with variation in scales for image data.** 
  * brings greater efï¬ciency by lim-
    iting self-attention computation to non-overlapping local
    windows while also allowing for cross-window connection.
  * è§£å†³transformerå¤æ‚åº¦O(N^2)çš„é—®é¢˜

![image-20241218022713658](./AI-Algorithms/image-20241218022713658.png)

![image-20241218023502807](./AI-Algorithms/image-20241218023502807.png)

![image-20241218023820301](./AI-Algorithms/image-20241218023820301.png)

* Efï¬cient batch computation for shifted conï¬guration
  * Cyclic shift
* å…¶å®ƒ
  * relative position bias
  * Table 5 ç ”ç©¶äº† Real speed of different self-attention computation meth-
    ods and implementations on a V100 GPU





##### SWIN v.s ViT

* https://www.reddit.com/r/MachineLearning/comments/1b3bhbd/d_why_is_vit_more_commonly_used_than_swin/
  * vitçš„scalingæ›´å¥½
* https://stuartfeeser.com/blogs/ai-engineers/swin-vs-vit/index.html
  * å¢å¤§patchæ•°é‡Næ—¶ï¼Œswinæ•ˆç‡æ›´é«˜ï¼Œvit O(N^2), swin O(N)
  * swinå¯¹ç»†èŠ‚æ•æ‰æ›´å¥½ï¼Œæ›´é€‚åˆåšdense vision tasksï¼ˆè¯­ä¹‰åˆ†å‰²ã€å®ä½“æ£€æµ‹ï¼‰

#### åŸºäºtransformerçš„å›¾åƒ-æ–‡æœ¬è”åˆå»ºæ¨¡

![image-20241207210505919](./AI-Algorithms/image-20241207210505919.png)

* BEit
  * ![img](./AI-Algorithms/figure_5.png)

#### å¤§è§„æ¨¡å›¾æ–‡Tokenå¯¹é½æ¨¡å‹ CLIP

![image-20241207210538634](./AI-Algorithms/image-20241207210538634.png)

![image-20241207210618154](./AI-Algorithms/image-20241207210618154.png)

#### å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹

* GPT-4v

  * éµå¾ªæ–‡å­—æŒ‡ä»¤

  * ç†è§£è§†è§‰æŒ‡å‘å’Œå‚è€ƒ
  * æ”¯æŒè§†è§‰+æ–‡æœ¬è”åˆæç¤º
  * few-shot
  * è§†è§‰è®¤çŸ¥èƒ½åŠ›å¼º

  * æ—¶åºè§†è§‰ä¿¡å·ç†è§£

* Geminiï¼šåŸç”Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹

![image-20241207211915709](./AI-Algorithms/image-20241207211915709.png)

* GPT-4o
  * GPT 4oæœ¬è´¨ä¸Šæ˜¯è¦æ¢ç´¢ä¸åŒæ¨¡æ€ç›¸äº’èåˆçš„å¤§ä¸€ç»Ÿæ¨¡å‹åº”è¯¥æ€ä¹ˆåšçš„é—®é¢˜ï¼Œå¯¹äºæå‡å¤§æ¨¡å‹çš„æ™ºåŠ›æ°´å¹³ä¼°è®¡å¸®åŠ©ä¸å¤§





### Embedding Model

https://ezml.io/blog/beyond-clip-the-future-of-multimodal-retrieval-with-visualized-bge-vista-and-magiclens

#### CLIP

**What is CLIP?**

CLIP, developed by OpenAI, is a model designed to understand and relate images and text through contrastive learning. It learns to match images with their corresponding text descriptions and to differentiate these pairs from mismatches, enabling it to perform various tasks, from image classification to zero-shot learning.

**How Does CLIP Work?**

- **Contrastive Learning:** CLIP is trained on a vast dataset of image-text pairs, learning to create a shared embedding space where both images and texts are represented as vectors. The model maximizes the similarity of correct image-text pairs and minimizes it for incorrect pairs.
- **Joint Embedding Space:** CLIPâ€™s ability to create a joint embedding space for images and text allows it to generalize across different tasks and domains.

**Limitations of CLIP**

- **Fine-Grained Visual Understanding:** CLIP struggles with fine-grained visual details due to its broad learning approach. It can miss subtle distinctions within images that are critical for certain tasks.
- **Imprecise Multimodal Alignment:** The alignment between text and images can be imprecise, especially when dealing with complex or nuanced relationships.
- **Retrieval Performance Variability:** CLIP's performance can vary depending on the specificity of the query and the image, sometimes leading to suboptimal results.

#### CoCa

https://research.google/blog/image-text-pre-training-with-contrastive-captioners/



#### Visualized BGE (Bootstrapped Grid Embedding)

**How Does Visualized BGE Work?**

- **Grid-Based Embeddings:** Unlike CLIP, which processes entire images, Visualized BGE (specifically the BGE-Visualized-M3 variant) breaks down images into grids and embeds each segment separately. This grid-based approach allows the model to capture more localized and detailed visual information.
- **Bootstrapping:** Visualized BGE uses a bootstrapping process where the model iteratively refines its understanding of the imageâ€™s content. This iterative training enhances the model's ability to differentiate between subtle visual details.
- **Leveraging Stable Diffusion:** The training process of Visualized BGE, especially in its M3 variant, incorporates techniques similar to stable diffusion to generate edited images. These variations expose the model to a diverse set of images, thereby improving its ability to recognize and embed fine-grained details across various scenarios.

**Prominent Example - BGE-Visualized-M3**

The **BGE-Visualized-M3** model is a prominent example of the Visualized BGE architecture. It supports multiple retrieval functionalities such as:

- **Dense Retrieval:** Standard dense retrieval, commonly seen in text embeddings.
- **Multi-Vector Retrieval:** Fine-grained interactions between multiple vectors.
- **Sparse Retrieval:** Term-based retrieval with enhanced importance assigned to certain terms.

**Advantages of Visualized BGE**

- **Fine-Grained Detail Recognition:** The grid-based embedding method enhances the modelâ€™s ability to recognize and differentiate fine details within images.
- **Improved Retrieval Accuracy:** The detailed focus leads to more accurate retrieval results, particularly in scenarios where specific visual features are critical.
- **Complex Image Handling:** Visualized BGE, especially in its BGE-Visualized-M3 variant, excels in understanding complex images with multiple elements, where generalist models like CLIP might struggle.

#### VISTA (Visualized Text Embedding for Universal Multimodal Retrieval)

![img](./AI-Algorithms/rygUM4x9yYMvOzaCGkxrVuR0.png)

**What is VISTA?**

VISTA (Visualized Text Embedding for Universal Multimodal Retrieval) takes the advancements of Visualized BGE even further by enhancing the integration of text and image data. VISTA introduces a sophisticated method of embedding text in a way that is deeply integrated with visual data, making it a versatile model for a broad range of multimodal tasks.

**How Does VISTA Work?**

- **ViT and Text Tokenization:** VISTA uses a Vision Transformer (ViT) as an image tokenizer, feeding the visual tokens into a pre-trained text encoder. This allows the model to handle images, text, and multimodal data seamlessly.
- **In-Depth Fusion:** VISTA creates a deeply fused multimodal representation by concatenating the visual tokens from the ViT encoder with the text tokens and processing this interleaved sequence through a frozen text encoder. This ensures that the text embedding capabilities are preserved while enhancing image-text alignment.
- **Two-Stage Training Process:** VISTA employs a two-stage training process. In the first stage, it performs cross-modal training using massive weakly labeled data, aligning visual tokens with the text encoder. In the second stage, VISTA fine-tunes this alignment with high-quality composed image-text datasets, significantly improving the model's ability to handle complex multimodal tasks.

**Improvements Over CLIP**

- **Unified Embedding Space:** Unlike CLIP, which handles text and image embeddings separately, VISTA creates a unified embedding space that ensures better integration and alignment of text and image data.
- **Versatility:** VISTAâ€™s architecture allows it to excel across a broader range of multimodal retrieval tasks, from simple image-text matching to complex multimodal document retrieval.
- **Enhanced Detail and Context Understanding:** By deeply integrating visual and textual data, VISTA can better understand and retrieve information based on nuanced and detailed queries.

#### MagicLens by Google 

![img](./AI-Algorithms/ZlUMrMOnFObZ7sRbqFe7d8QYZcI.png)

**What is MagicLens?**

MagicLens is a cutting-edge, self-supervised image retrieval model designed to handle **open-ended instructions** for image search. Unlike traditional models that focus on visual similarities, MagicLens allows users to express complex search intents through natural language, retrieving images based on diverse semantic relations beyond mere visual features.

**How Does MagicLens Work?**

- **Training on Web Data:** MagicLens is trained on **36.7 million image triplets** (query image, instruction, target image) mined from naturally occurring web image pairs. These pairs contain implicit relations (e.g., â€œinside view of,â€ â€œdifferent angleâ€), which are made explicit using large multimodal models (LMMs) and large language models (LLMs).

- **Self-Supervised Learning:** The model generates diverse instructions using foundation models (PaLM and PaLI) and learns to align image-text pairs via contrastive learning, allowing it to support open-ended, complex queries.
- **Dual-Encoder Architecture:** A dual-encoder system processes the query image and integrates the instruction into the target image retrieval, making the system highly efficient for diverse retrieval tasks.

**Key Innovations:**

- **Beyond Visual Similarity:** MagicLens excels at retrieving images based on **non-visual relations**, such as context, object-specific queries, or semantic differences (e.g., â€œdifferent product angleâ€ or â€œrelated landmarksâ€).
- **Efficient Model Size:** Despite being **50x smaller** than previous state-of-the-art models, MagicLens achieves superior performance across various image retrieval benchmarks.
- **Real-Time and Accurate Retrieval:** MagicLens allows for **interactive, real-time search** and refines results based on user feedback, making it adaptable to dynamic retrieval tasks.

**Why Itâ€™s an Advancement:**

MagicLens moves beyond the visual similarity limitations of CLIP and Visualized BGE, supporting **open-ended, natural language-driven searches**. It represents a significant leap in the ability to handle complex, contextually rich image queries, making it highly effective and scalable for modern multimodal search applications.



### Data Prepare

![image-20241207212813240](./AI-Algorithms/image-20241207212813240.png)

* Trick
  * imageæ”¾åœ¨promptç»“å°¾ï¼Œæ¯”è¾ƒå°‘å—æ–‡æœ¬ä¿¡æ¯å¹²æ‰°

![image-20241207212906560](./AI-Algorithms/image-20241207212906560.png)

* prompt

![image-20241207215105555](./AI-Algorithms/image-20241207215105555.png)

### Training - Llava

#### Intro

* æ¨¡å‹ï¼š
  * ViTçš„å€’æ•°ç¬¬äºŒå±‚é™¤cls tokenå¤–çš„image token

* ç»†èŠ‚ï¼š
  * å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œ--num_train_epochs=1ï¼Œä¸€èˆ¬æ˜¯ä»å¤´è®­ç»ƒ

![image-20241207212730097](./AI-Algorithms/image-20241207212730097.png)

![image-20241207213002988](./AI-Algorithms/image-20241207213002988.png)

#### ç®—æ³•è¿­ä»£

* æ”¹è¿›Visual Encoder
  * ![image-20241207215512977](./AI-Algorithms/image-20241207215512977.png)
  * ![image-20241207215556328](./AI-Algorithms/image-20241207215556328.png)
  * ![image-20241207215612284](./AI-Algorithms/image-20241207215612284.png)
  * ![image-20241207225347798](./AI-Algorithms/image-20241207225347798.png)
* æ”¹è¿›Projection Layer
  * loraæ€æƒ³ã€æ”¹è¿›æ–‡æœ¬èƒ½åŠ›
  * ![image-20241207230013814](./AI-Algorithms/image-20241207230013814.png)


#### è§†é¢‘ã€è¯­éŸ³è¾“å…¥

![image-20241207230602139](./AI-Algorithms/image-20241207230602139.png)



#### åŸç”ŸMLLM

* Next-GPTè®­ç»ƒ
  * é˜¶æ®µä¸€ï¼šæ›´æ–°input projection layer 
  * é˜¶æ®µäºŒï¼šdecoderæ®µè¾“å‡ºç»“æœä¸æŒ‡ä»¤å¯¹é½ï¼Œåªæ›´æ–°output projection layer
  * é˜¶æ®µä¸‰ï¼š

![image-20241207231036385](./AI-Algorithms/image-20241207231036385.png)

![image-20241207231155505](./AI-Algorithms/image-20241207231155505.png)





![image-20241207230626127](./AI-Algorithms/image-20241207230626127.png)



![image-20241207230702605](./AI-Algorithms/image-20241207230702605.png)

![image-20241207230732346](./AI-Algorithms/image-20241207230732346.png)

![image-20241207230840157](./AI-Algorithms/image-20241207230840157.png)

![image-20241207230853889](./AI-Algorithms/image-20241207230853889.png)

![image-20241207230909253](./AI-Algorithms/image-20241207230909253.png)

![image-20241207230919593](./AI-Algorithms/image-20241207230919593.png)

![image-20241207230938271](./AI-Algorithms/image-20241207230938271.png)

![image-20241207230956810](./AI-Algorithms/image-20241207230956810.png)

![image-20241207231244953](./AI-Algorithms/image-20241207231244953.png)

### å¼€æºé¡¹ç›®

![image-20241207230532748](./AI-Algorithms/image-20241207230532748.png)

![image-20241207230211854](./AI-Algorithms/image-20241207230211854.png)

![image-20241207230235619](./AI-Algorithms/image-20241207230235619.png)  



### Evaluation

* MMEè¯„æµ‹é›†
  * https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation
* ![image-20241207212642821](./AI-Algorithms/image-20241207212642821.png)

### åº”ç”¨äº åˆ‡å›¾ã€ç‰©ä½“åŒ¹é…

#### Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation

> æŠ€æœ¯å…³é”®ç‚¹å’Œç»“è®ºï¼š
>
> * åˆ©ç”¨ViT patch embeddingï¼Œä½œä¸ºå›¾åƒçš„å±€éƒ¨ç‰¹å¾
> * é€šè¿‡learned mappingï¼Œå°†å›¾åƒçš„å±€éƒ¨ç‰¹å¾å’Œclip category embeddingå¯¹é½ï¼Œåšå®ä½“åˆ†å‰²

* æ ¸å¿ƒæ€è·¯ï¼šé€šè¿‡learned mappingï¼Œå¯¹vit patch embeddingå’Œclip category embeddingå¯¹é½ï¼Œåšå®ä½“åˆ†å‰²

![image-20241213203708145](./AI-Algorithms/image-20241213203708145.png)

* ç®—æ³•ï¼š
  * warp text embedding
  * Dinov2:
    * Nä¸ªattention mapï¼ˆpatchç»´åº¦ï¼‰
    * Nä¸ªweighted visual embedding
    * Nä¸ªç›¸ä¼¼åº¦åˆ†æ•°
  * å¯¹æ¯”å­¦ä¹ ï¼šæœ€ç›¸ä¼¼çš„weighted visual embedding <-> text embedding
  * Identifying Background Regions



![image-20241214121142744](./AI-Algorithms/image-20241214121142744.png)

![image-20241214121543467](./AI-Algorithms/image-20241214121543467.png)

#### [todo] OmniGlue: Generalizable Feature Matching with Foundation Model Guidance

> - æŠ€æœ¯å…³é”®ç‚¹å’Œç»“è®ºï¼ˆä»…ç•¥è¯»ï¼‰ï¼š
>   - Googleçš„CVé¢†åŸŸSOTA paperï¼ŒåŸºäºæ›´å¼ºçš„Foundation Modelåšä¼˜åŒ–
>   - é’ˆå¯¹å›¾åƒFeature Matchingçš„åœºæ™¯ï¼ŒDIMLæŠ€æœ¯ï¼Œç”¨optimal transportåšæ’åº



### Applications

* å·¥ä¸š
* åŒ»ç–—
* è§†è§‰å†…å®¹è®¤çŸ¥ä¸ç¼–è¾‘
* å…·èº«æ™ºèƒ½
* æ–°ä¸€ä»£äººæœºäº¤äº’



* å¤šæ¨¡æ€Agent
  * CogAgent
    * å›´ç»•GUIçš„èƒ½åŠ›å¼ºåŒ–ï¼šè§£æå’Œç›®æ ‡å®šä½èƒ½åŠ›

* Llavaè¡ç”Ÿçš„åº”ç”¨
  * å›¾è¡¨é—®ç­”ç”Ÿæˆï¼šChartLlama-code

![image-20241207213052536](./AI-Algorithms/image-20241207213052536.png)





### å¤šæ¨¡æ€

#### [Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://arxiv.org/pdf/2303.13835)

* Introï¼š
  * ç»“è®ºæ˜¯ï¼šMoRec is already comparable to its IDRec counterpart with an expensive end-to-end training method, **even for warm item recommendation**
  * https://github.com/westlake-repl/IDvs.MoRec
  * Q(i): Equipped with strong modality encoders (ME), can
    MoRec be comparable to or even surpass IDRec in regular, especially in warm-start item recommendation scenario?
    * two-tower based DSSM [24, 50] and session-based SASRec [25])ï¼Œå…¬å¹³çš„å®éªŒsettingå¯¹æ¯”
  * Q(ii): If Q(i) is yes, can the recent technical advances devel-
    oped in NLP and CV fields translate into accuracy improve- ment in MoRec when using text and visual features? 
  * Q(iii): Are the representations learned by these founda-
    tion models as general as claimed? How can we effectively use item modality representations derived from an NLP or CV encoder network?

* ç®—æ³•ï¼š
  * Userè¡¨å¾ï¼šUser Embã€User BHVã€User Profile
  * Itemè¡¨å¾ï¼šItem Embã€æ¨¡æ€Emb
  * åŸºäºDSSMå’ŒSASRECç ”ç©¶IDRecå’ŒMoRec
    * SASRec is a well-known se- quential recommendation model based on multi-head self-attention (MHSA) [59] which describes a user by her interacted item ID sequence.
* ç»“è®ºï¼š
  * seq2seqè®­ç»ƒ + SASRECç›¸æ¯”åŒå¡”ï¼Œæ›´èƒ½å‘æŒ¥MoRecçš„èƒ½åŠ›
  * E2Eè®­ç»ƒæ•ˆæœæ¯”two stageå¥½å¾ˆå¤š
    * â€œå”¯ä¸€The good thingâ€ is that by proper adaption (i.e., TS-DNN), TS-based MoRec have some potential to compete with E2E MoRec for text recommendation in the future (16.66 vs 18.23).
    * representation fea- tures are not universal enough, at least for item recommendation.

![image-20241003233046500](./AI-Algorithms/morec.png)

* å…³äºTraining Costï¼š
  * the best MoRec (with SASRec as user encoder and Swin-B as ME) takes an astonishing more than 100x compute and training time than IDRec
  * inference timeå·®ä¸å¤š
  * ä¼˜åŒ–æ€è·¯ï¼š
    * åªfinetune top-layer
* å…¶å®ƒç®—æ³•ç›¸å…³ï¼š
  * extra pre-trainingï¼šåœ¨e2e morecçš„åŸºç¡€ä¸Šï¼Œæ¯”è¾ƒéš¾åšæ•ˆæœ
  * Combing ID & modality featuresï¼šæ•ˆæœå·®
  * it is sometimes necessary to set different learning rate for item ME and other modules. This may be because item ME has been pre-trained on NLP and CV datasets before, and its learning stride may be different from other modules trained from scratch.

#### Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights

* Intro
  * Text-based collaborative filtering (TCF)
  * We examine whether these extremely large LMs could enable a universal item representation for the recommendation task.

* ç®—æ³•ï¼š
  * lossï¼šeither be a pairwise BPR [38] loss or a cross-entropy classification loss [54].

* ç»“è®º
  * Q1: How does the recommender systemâ€™s performance respond to the continuous increase in the item encoderâ€™s size? Is the performance limits attainable at the scale of hundreds of billions? 
    * sasrecæ•ˆæœå¥½äºDSSM
    * the TCF model with a 175B parameter LM may not have reached its performance ceiling
  * Q2: Can super-large LMs, such as GPT-3 with 175-billion parameters, generate universal text representations?
    * even the item representation learned by an extremely large LM (e.g., GPT-3) may not result in a universal representation, at least not for the text
    * ![image-20241006172858506](./AI-Algorithms/tcf-result.png)
    * Finetune LMæ•ˆæœå¥½ï¼ˆtop two layersï¼‰![image-20241006173055402](./AI-Algorithms/image-20241006173055402.png)
  * Q3: Can recommender models with a 175-billion parameter LM as the item encoder easily beat the simplest ID embedding based models (IDCF), especially for warm item recommendation?
    * ![image-20241006173158353](./AI-Algorithms/tcf-result2.png)
  * Q4: How close is the TCF paradigm to a universal recommender model?
    * while TCF models with large LMs do exhibit a certain degree of transfer learning capability, they still fall significantly short of being a universal recommender model, as we had initially envisioned
    * Table 3
    * For a universal recommender system model, not only should item representations be transferable, **but also the matching relationship between users and items needs to be transferable.** However, the matching relationship is closely related to the exposure strategy of the specific recommender system.
  * Q5: Will the classic TCF paradigm be replaced by a recent prompt engineering based rec- ommendation method that utilizes ChatGPT (called ChatGPT4Rec)?

![image-20241006171904133](./AI-Algorithms/TCF.png)

* å…¶å®ƒï¼š
  * appendixæœ‰sasrecåœ¨ä¸åŒæ•°æ®é›†çš„è®­ç»ƒè¶…å‚



## è§†é¢‘ç®—æ³•

### Intro

* æŠ€æœ¯æŠ¥å‘Šï¼šhttps://openai.com/research/video-generation-models-as-world-simulators

* [ç‰©ç†æ”¹å˜å›¾åƒç”Ÿæˆï¼šæ‰©æ•£æ¨¡å‹å¯å‘äºçƒ­åŠ›å­¦ï¼Œæ¯”å®ƒé€Ÿåº¦å¿«10å€çš„æŒ‘æˆ˜è€…æ¥è‡ªç”µåŠ¨åŠ›å­¦](https://zhuanlan.zhihu.com/p/599013984)

* VideoPoet

![image-20241207231303500](./AI-Algorithms/image-20241207231303500.png)

* [ä¸€é”¤é™ç»´ï¼è§£å¯†OpenAIè¶…çº§è§†é¢‘æ¨¡å‹SoraæŠ€æœ¯æŠ¥å‘Šï¼Œè™šæ‹Ÿä¸–ç•Œæ¶Œç°äº†](https://mp.weixin.qq.com/s/ODsebK3fEc-adRDwRVDhQA?poc_token=HMxd12WjhN3a1nz74MaIrMjep8dIn2Cj_NTdFwef)
  * æ‰©å±•è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è§„æ¨¡ï¼Œæ˜¯æ„å»ºæ¨¡æ‹Ÿç‰©ç†ä¸–ç•Œé€šç”¨æ¨¡æ‹Ÿå™¨çš„éå¸¸æœ‰å¸Œæœ›çš„æ–¹å‘
  * patch
    * ä»å®è§‚è§’åº¦æ¥çœ‹ï¼Œç ”ç©¶è€…é¦–å…ˆå°†è§†é¢‘å‹ç¼©åˆ°ä¸€ä¸ªä½ç»´æ½œç©ºé—´ä¸­ï¼ŒéšåæŠŠè¿™ç§è¡¨å¾åˆ†è§£ä¸ºæ—¶ç©ºpatchï¼Œè¿™æ ·å°±å®ç°äº†ä»è§†é¢‘åˆ°patchçš„è½¬æ¢ã€‚
    * åœ¨æ¨ç†æ—¶ï¼Œå¯ä»¥é€šè¿‡åœ¨ä¸€ä¸ªåˆé€‚å¤§å°çš„ç½‘æ ¼ä¸­é€‚å½“æ’åˆ—éšæœºåˆå§‹åŒ–çš„patchï¼Œä»è€Œæ§åˆ¶ç”Ÿæˆè§†é¢‘çš„å¤§å°ã€‚
  * è®­ç»ƒæŠ€å·§
    * ç›´æ¥åœ¨è§†é¢‘åŸå§‹æ¯”ä¾‹ä¸Šè®­ç»ƒ
    * ç ”ç©¶è€…é‡‡ç”¨äº†DALLÂ·E 3ä¸­çš„é‡æ–°æ ‡æ³¨æŠ€æœ¯ï¼Œåº”ç”¨åœ¨äº†è§†é¢‘ä¸Šã€‚
      * é¦–å…ˆï¼Œç ”ç©¶è€…è®­ç»ƒäº†ä¸€ä¸ªèƒ½ç”Ÿæˆè¯¦ç»†æè¿°çš„æ ‡æ³¨æ¨¡å‹ï¼Œç„¶åç”¨å®ƒä¸ºè®­ç»ƒé›†ä¸­çš„æ‰€æœ‰è§†é¢‘ï¼Œç”Ÿæˆæ–‡æœ¬è¯´æ˜ã€‚
      * ä»–ä»¬å‘ç°ï¼Œä½¿ç”¨è¯¦ç»†çš„è§†é¢‘è¯´æ˜è¿›è¡Œè®­ç»ƒï¼Œä¸ä»…èƒ½æé«˜æ–‡æœ¬çš„å‡†ç¡®æ€§ï¼Œè¿˜èƒ½æå‡è§†é¢‘çš„æ•´ä½“è´¨é‡ã€‚
      * ç±»ä¼¼äºDALLÂ·E 3ï¼Œç ”ç©¶è€…ä¹Ÿä½¿ç”¨äº†GPTï¼ŒæŠŠç”¨æˆ·çš„ç®€çŸ­æç¤ºè½¬åŒ–ä¸ºè¯¦ç»†çš„è¯´æ˜ï¼Œç„¶åè¿™äº›è¯´æ˜ä¼šè¢«è¾“å…¥åˆ°è§†é¢‘æ¨¡å‹ä¸­ã€‚

  * ç”Ÿæˆçš„è§†é¢‘ç‰¹ç‚¹ï¼š
    * å¤šç§è¾“å…¥å½¢å¼ã€å¤šè§†é¢‘é—´è¿‡æ¸¡ã€äººå’Œç‰©çš„ç‰¹å¾

### è§†é¢‘æŠ½å…³é”®å¸§

#### Literature Review

* æ–¹æ³•ï¼š
  * uniform sampling based,
  * clustering based,
    * VSUMM [4], SGC [5], GMC [6] used k-means, minimum spanning tree, and graph modularity
    * ç¼ºç‚¹æ˜¯å¿½ç•¥äº†temporal sequences
  * comparison based,
    * VSUKFE [7] and DiffHist [8]
    * æ ¹æ®é˜ˆå€¼å¯¹æ¯”
  * shot based approaches
    * drawing only one frame
      from each shot is insufficient to fully describe videosâ€™ visual
      contents;
    * using traditional features for boundary
      detection might be inaccurate for shot segmentations.

#### Large Model based Sequential Keyframe Extraction for Video Summarization

* åˆ‡ç‰‡ï¼ˆshotï¼‰ï¼šTransNetV2
* å¸§ç†è§£ï¼šCLIP
* æ¯ä¸ªshotå†…çš„frameèšç±»
  * è¿­ä»£å‡ºk_maxä¸ªèšç±»ä¸­å¿ƒ
    * $$k_{max}=\sqrt{N}$$
  * æœ€å¤§åŒ–SC(silhouette coefficient)ï¼Œåˆå¹¶èšç±»ä¸­å¿ƒ
    * èšç±»ä¸­å¿ƒåˆå¹¶åˆ°2ä¸ªï¼Œé€‰æ‹©SCæœ€å¤§çš„ä¸€ä¸ªèšç±» ï¼ˆç±»æ¯”äºç­›æ‰ä¸€åŠèšç±»å¯¹åº”çš„å¸§ï¼Œå¹¶é€‰æ‹©èšç±»æ•ˆæœæ›´å¥½çš„ä¸€ä¸ªä¸­å¿ƒï¼‰
  * Redundancy Elimination
    * å…ˆåŸºäºcolor histogramå»é™¤solid-color or uninformative frames
    * å†åŸºäºcolor histogramè¿­ä»£å»é™¤ç›¸ä¼¼å¸§

![image-20250109174239161](./AI-Algorithms/image-20250109174239161.png)

![image-20250109181815631](./AI-Algorithms/image-20250109181815631.png)

* benchmarkæ„å»º
  * äººå·¥æ‰“åˆ†ï¼Œå–å±€éƒ¨æå€¼ç‚¹ä½œä¸ºå…³é”®å¸§

#### An effective Key Frame Extraction technique based on Feature Fusion and Fuzzy-C means clustering with Artificial Hummingbird

- https://www.nature.com/articles/s41598-024-75923-y
- å’Œ LMSKE çš„å·®å¼‚ï¼ˆäºŒè€…å‡ä¸ºä¸€ä¸ªhybridæ–¹æ¡ˆï¼‰ï¼š
  - å…ˆåˆ©ç”¨ ã€é¢œè‰²é€šé“ç›¸å…³æ€§ã€ç›´æ–¹å›¾å·®å¼‚ã€äº’ä¿¡æ¯ã€æƒ¯æ€§çŸ©ã€‘ç­›é€‰å…³é”®å¸§å†åšèšç±»
    - LMSKEï¼šshotåˆ‡åˆ† -> èšç±»(åˆ©ç”¨å¤šæ¨¡æ€Embedding) -> ç­›é€‰(é¢œè‰²é€šé“)
    - è¯¥paperï¼šç­›é€‰(å¤šç§ç‰¹å¾) -> èšç±»(åˆ©ç”¨HSV)
  - èšç±»ç®—æ³•çš„æ”¹è¿›ï¼šArtificial Hummingbirdã€Fuzzy C-means Clustering
- ä¼˜åŠ£åŠ¿åˆ†æï¼šç›¸æ¯”LMSKEï¼Œå®æ—¶æ€§æ›´å¥½ã€è§†é¢‘å›¾ç‰‡è¯­ä¹‰ä¿¡æ¯çš„åˆ©ç”¨æ›´å°‘

## OpenAI o1

> o1æœ¬è´¨ä¸Šæ˜¯åœ¨æ¢ç´¢å¤§æ¨¡å‹åœ¨AGIè·¯ä¸Šèƒ½èµ°å¤šè¿œã€å¤©èŠ±æ¿åœ¨å“ªé‡Œçš„é—®é¢˜

* [å¦‚ä½•ç†è§£OpenAI o1](https://mp.weixin.qq.com/s/QdVSq8q7wLWtPakdZdqidA)

  * æå‡LLMæ¨¡å‹è®¤çŸ¥èƒ½åŠ›çš„æ ¸å¿ƒåœ¨äºå¤æ‚é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚

    * LLMçš„é€»è¾‘æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼Œåˆ™èƒ½è§£é”æ›´å¤šå¤æ‚åº”ç”¨ï¼Œå¤§æ¨¡å‹åº”ç”¨çš„å¤©èŠ±æ¿å°±è¶Šé«˜
    * o1æ¨¡å‹èƒ½åŠ›è¶Šå¼ºï¼Œåˆ™å¯ä»¥åå“ºåŸºåº§æ¨¡å‹

  * o1çš„åšæ³•æœ¬è´¨ä¸Šæ˜¯CoTçš„è‡ªåŠ¨åŒ–orå†…åŒ–ã€‚

    * rlæœç´¢COTçš„å†³ç­–ç©ºé—´
    * é—®é¢˜è¶Šå¤æ‚ï¼Œéšè—çš„COT tokenæ¶ˆè€—è¶Šå¤§

    * å¤§éƒ¨åˆ†é€»è¾‘æ¨ç†æ•°æ®çš„å½¢å¼æ˜¯<é—®é¢˜ï¼Œæ­£ç¡®ç­”æ¡ˆ>ï¼Œç¼ºäº†ä¸­é—´çš„è¯¦ç»†æ¨ç†æ­¥éª¤ï¼Œè€Œo1æœ¬è´¨ä¸Šæ˜¯è®©å¤§æ¨¡å‹å­¦ä¼šè‡ªåŠ¨å¯»æ‰¾ä»é—®é¢˜åˆ°æ­£ç¡®ç­”æ¡ˆçš„ä¸­é—´æ­¥éª¤ï¼Œä»¥æ­¤æ¥å¢å¼ºå¤æ‚é—®é¢˜çš„è§£å†³èƒ½åŠ›ã€‚

  * RLçš„scaling lawæœ¬è´¨ä¸Šæ˜¯COTå†³ç­–æ ‘æœç´¢çš„scaling law

  * Agentæ— æ³•å®ç”¨åŒ–çš„ä¸»è¦åŸå› å°±åœ¨äºåŸºåº§æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ä¸å¤Ÿå¼ºã€‚

    * é€šè¿‡åŸºåº§æ¨¡å‹PlanæŠŠä¸€ä¸ªå¤æ‚ä»»åŠ¡åˆ†è§£ä¸º10ä¸ªæ­¥éª¤ï¼Œå“ªæ€•å•ä¸ªæ­¥éª¤çš„æ­£ç¡®ç‡é«˜è¾¾95%ï¼Œè¦æƒ³æœ€åæŠŠä»»åŠ¡åšå¯¹ï¼Œ10ä¸ªç¯èŠ‚çš„å‡†ç¡®ç‡è¿ä¹˜ä¸‹æ¥ï¼Œæœ€ç»ˆçš„æ­£ç¡®ç‡åªæœ‰59%

  * OpenAIæƒ³åšçš„æ–¹å‘å¤ªå¤šï¼Œèµ„æºåˆ†æ•£å¯¼è‡´åˆ†åˆ°å…·ä½“ä¸€ä¸ªæ–¹å‘çš„èµ„æºä¸å¤Ÿç”¨ï¼Œæ‰€ä»¥è¶Šå¾€åå‘å±•â€œæœŸè´§çŠ¶æ€â€çš„æ–¹å‘è¶Šå¤šï¼Œä¹Ÿè®©äººè§‰å¾—å°½æ˜¾ç–²æ€ã€‚

## AGI

### Lecun

> LeCun: https://www.bilibili.com/video/BV1b1ycYTECU
>
> è§†é¢‘å…¶ä¸­ä¸€ä¸ªæ ¸å¿ƒæ€æƒ³æ˜¯â€œé¢„æµ‹èƒ½åŠ›çš„æœ¬è´¨æ˜¯æˆ‘ä»¬æ‰¾åˆ°æˆ‘ä»¬è§‚å¯Ÿçš„äº‹ç‰©çš„è‰¯å¥½è¡¨å¾â€ï¼Œäº‹å®ä¸Šç°åœ¨äººç±»åšæœºå™¨å­¦ä¹ çš„å·¥ä½œå¤§éƒ¨åˆ†æ˜¯åœ¨ å¯»æ‰¾è¡¨å¾ã€ä¼˜åŒ–è¡¨å¾ã€‚
>
> æœ€è¿‘ä¸€æ®µæ—¶é—´ä¼´éšLLMå‡ºç°ï¼ŒæŠ€æœ¯é¢†åŸŸçš„å‘å±•ä¸å¤–ä¹è¿™ä¸¤ç§ï¼š1ï¼‰åˆ©ç”¨LLMå­¦åˆ°çš„è¡¨å¾å»åšä¸€äº›äº‹æƒ…ï¼›2ï¼‰è®©LLMå­¦ä¼šæ›´å¤šè¡¨å¾ã€‚

* Lecunçš„Insightï¼šéœ€è¦è§†è§‰ä¿¡æ¯è®­ç»ƒ
  * åé©³â€œè§†è§‰ä¿¡æ¯å†—ä½™â€
    * è§†ç¥ç»çº¤ç»´ 1byte/s å·²ç»ç›¸æ¯”è§†ç½‘è†œå…‰ä¼ æ„Ÿå™¨æœ‰1/100çš„å‹ç¼©æ¯”äº†
      * 6000w-1e8å…‰ä¼ æ„Ÿå™¨
      * 100wç¥ç»çº¤ç»´
    * self-supervised learningéœ€è¦å†—ä½™ä¿¡æ¯æ‰èƒ½å­¦å¥½
      * é«˜åº¦å‹ç¼©==éšæœº -> å­¦ä¸å¥½

![image-20241019022443123](./AI-Algorithms/image-20241019022443123.png)

* Objective-Driven AI
  * è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜ï¼Œè®©å†³ç­–outputæ¥è¿‘objectiveï¼Œéœ€è¦å…ˆä¼˜åŒ–perception
  * optimization-based AI
    * æœ‰zero-shotèƒ½åŠ›
    * search/plan

![image-20241019023330137](./AI-Algorithms/image-20241019023330137.png)

![image-20241019163724135](./AI-Algorithms/image-20241019163724135.png)

* ç³»ç»Ÿ
  * Model Predictive Controlï¼ˆMPCï¼‰
    * using gradient-based method, graph search, MCTS, DP, ...
  * åˆ†å±‚çš„planningï¼Œworld modelé¢„ä¼°çº§è”

* è®­ç»ƒï¼š
  * è§‚å¯Ÿå©´å„¿å¯¹ä¸–ç•Œæ¨¡å‹çš„è®¤çŸ¥è·¯å¾„ï¼Œå¯ä»¥å¯å‘å„ç§å±æ€§çš„è®¤çŸ¥é¡ºåºå’Œéš¾åº¦ï¼ˆæ¯”å¦‚å¯¹é‡åŠ›çš„è®¤çŸ¥ï¼‰
  * generative + self-supervisedè¡Œä¸é€š

![image-20241019165227218](./AI-Algorithms/image-20241019165227218.png)

* Joint Embedding Predictive Architecture
  * é¢„æµ‹èƒ½åŠ›çš„æœ¬è´¨æ˜¯æˆ‘ä»¬æ‰¾åˆ°æˆ‘ä»¬è§‚å¯Ÿçš„äº‹ç‰©çš„è‰¯å¥½è¡¨å¾
    * e.g. ç”µå•†åœºæ™¯ä¸‹çš„ç±»ç›®ä½“ç³»ï¼Œç±»ç›®æ˜¯å¯¹å•†å“çš„å‘ä¸Šä¸€å±‚çš„æŠ½è±¡è¡¨å¾

![image-20241019165308598](./AI-Algorithms/image-20241019165308598.png)

![image-20241019165600928](./AI-Algorithms/image-20241019165600928.png)

![image-20241019171905634](./AI-Algorithms/image-20241019171905634.png)

![image-20241019172914244](./AI-Algorithms/image-20241019172914244.png)

* VICReg
  * å…ˆæ‰©ç»´å†æ­£åˆ™åŒ–

![image-20241019173438149](./AI-Algorithms/image-20241019173438149.png)

* Video-JEPA
  * è’¸é¦é˜²æ­¢collapse

![image-20241019173516379](./AI-Algorithms/image-20241019173516379.png)

### å…¶å®ƒ

* è±†åŒ…å¤§æ¨¡å‹è§†è§‰ https://zhuanlan.zhihu.com/p/5761953085

  * å°½ç®¡Scalingåœ¨Soraä¸Šå–å¾—æˆåŠŸï¼Œä½†ä¸è¶³ä»¥ä½¿è§†é¢‘ç”Ÿæˆæ¨¡å‹çœŸæ­£ç†è§£å¹¶æ³›åŒ–åº”ç”¨åŸºæœ¬çš„ç‰©ç†å®šå¾‹ã€‚
    * æ¨¡å‹ä»…åœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒå†…è¡¨ç°è‰¯å¥½ï¼Œåˆ†å¸ƒå¤–è¡¨ç°è¾ƒå·®ï¼Œä¸è¿‡Scalingå¯¹ç»„åˆæ³›åŒ–ï¼ˆéœ€ç»„åˆè®­ç»ƒæ—¶å·²ç†Ÿæ‚‰çš„æ¦‚å¿µæˆ–å¯¹è±¡ï¼‰æœ‰æ•ˆï¼›
    * æ¨¡å‹æ— æ³•æŠ½è±¡å‡ºä¸€èˆ¬è§„åˆ™ï¼Œè€Œæ˜¯è¯•å›¾æ¨¡ä»¿æœ€æ¥è¿‘çš„è®­ç»ƒç¤ºä¾‹ï¼›
    * å½“æ¨¡å‹å‚è€ƒè®­ç»ƒç¤ºä¾‹æ—¶ï¼Œç”šè‡³å­˜åœ¨é¡ºåºåå¥½ï¼šé¢œè‰² > å¤§å° > é€Ÿåº¦ > å½¢çŠ¶ï¼›

  * è®­ç»ƒæ•°æ®åˆ†å¸ƒå†…ï¼ˆin-distributionï¼‰ï¼šè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œ**è¡¨ç°è‰¯å¥½**ï¼›
  * è®­ç»ƒæ•°æ®åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰ï¼šæ¨¡å‹åœ¨é¢å¯¹ä»æœªè§è¿‡çš„æ–°åœºæ™¯æ—¶ï¼Œæ˜¯å¦èƒ½å¤Ÿå°†å·²å­¦è¿‡çš„ç‰©ç†å®šå¾‹åº”ç”¨åˆ°æœªçŸ¥çš„æƒ…å¢ƒï¼Œ**è¡¨ç°ä¸ä½³**ï¼›
  * ç»„åˆæ³›åŒ–ï¼ˆcombinatorial generalizationï¼‰ï¼šä»‹äºå‰ä¸¤è€…ä¹‹é—´ï¼Œè®­ç»ƒæ•°æ®å·²åŒ…å«äº†æ‰€æœ‰æ¦‚å¿µæˆ–ç‰©ä½“ï¼Œä½†è¿™äº›æ¦‚å¿µã€ç‰©ä½“å¹¶æœªä»¥æ‰€æœ‰å¯èƒ½çš„ç»„åˆæˆ–æ›´å¤æ‚çš„å½¢å¼å‡ºç°ï¼Œ**Scalingæœ‰æ•ˆ**ï¼›
  * è§†é¢‘æ¨¡å‹å…·æœ‰**ä¸‰ç§åŸºæœ¬ç»„åˆæ¨¡å¼**ï¼Œåˆ†åˆ«ä¸ºï¼š
    - å±æ€§ç»„åˆ
    - ç©ºé—´ç»„åˆï¼ˆå¤šä¸ªç‰©ä½“ä¸åŒè¿åŠ¨çŠ¶æ€ï¼‰
    - æ—¶é—´ç»„åˆï¼ˆä¸åŒçš„æ—¶é—´ç‚¹å¤šä¸ªç‰©ä½“çš„ä¸åŒçŠ¶æ€ï¼‰

  * è§†é¢‘ç”Ÿæˆçš„Scaling Law**åº”å½“ä¾§é‡äºå¢åŠ ç»„åˆå¤šæ ·æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‰©å¤§æ•°æ®é‡**ã€‚

## In-context Learning

https://ai.stanford.edu/blog/understanding-incontext/

## CoT ç›¸å…³æŠ€æœ¯

* [OpenAIç ”ç©¶å‘˜ã€æ€ç»´æ ‘ä½œè€…å§šé¡ºé›¨ä¸“è®¿ï¼šäººç”Ÿæ˜¯ä¸€åœºæ— é™æµæ¸¸æˆä¸¨ç‹¬å®¶](https://mp.weixin.qq.com/s/MdPI-X1HvRxFuX_Z0Ju_ug)
  * è®¸å¤šè®¡ç®—æœ¬è´¨ä¸Šå°±æ˜¯å»è®¡ç®—ä¸‹ä¸€ä¸ªtokenï¼Œnext token predictionå¼€å§‹æˆä¸ºä¸€ä¸ªæ–°çš„è®¡ç®—ã€‚é‚£ä¹ˆé’ˆå¯¹è®¡ç®—å¤æ‚æ€§ï¼Œä¼ ç»Ÿçš„è¯­è¨€å¦‚ä½•åœ¨æ–°æ¡†æ¶ä¸‹é€‚ç”¨ï¼Œè¿˜æœ‰å¾ˆå¤šé—®é¢˜éœ€è¦å»è§£å†³
  * Open-endedness
    * è¯­è¨€æ¸¸æˆä¹‹æ‰€ä»¥å’Œå…¶ä»–æ¸¸æˆåŒºåˆ«å¾ˆå¤§ï¼Œå°±æ˜¯å› ä¸ºè¯­è¨€çš„å¼€æ”¾æ€§ï¼Œå³open-endednessã€‚æ—¢ç„¶è¿™æ ·ï¼Œé‚£ä¹ˆå®ƒæœ¬è´¨ä¸Šåº”è¯¥æœ‰ä¸€ä¸ªgenerative solutionï¼Œè€Œä¸æ˜¯ä¸€ä¸ªdiscriminative solutionã€‚æ‰€ä»¥ä»æˆ‘ç¬¬ä¸€ä¸ªå·¥ä½œå¼€å§‹ï¼Œæˆ‘å°±ä¸€ç›´åœ¨åšautoregressive language model (GPT-2)
    * ä»å“²å­¦çš„è§’åº¦æ¥çœ‹ï¼Œäººç”Ÿå°±æ˜¯ä¸€ä¸ªæ— é™æµæ¸¸æˆï¼ŒæŸç§ç¨‹åº¦ä¸Šæ¥è¯´ï¼Œæ›´åƒä¸€ä¸ªæ–‡å­—æ¸¸æˆï¼Œè€Œä¸æ˜¯ç”µå­æ¸¸æˆã€‚æ¯å¤©ä½ éƒ½æœ‰å¾ˆå¤šé€‰æ‹©ï¼Œä»ç¨‹åº¦ä¸Šè¯´æ˜¯éå¸¸high levelã€ open endedçš„ã€‚
  * ReAct
    * è¿™ç¯‡è®ºæ–‡çš„æœ¬è´¨æ˜¯Agentä¸ä»…ä»…æœ‰environment actionï¼Œä¹Ÿæœ‰thinking actionã€‚
    * ä¸»è¦çš„æ€è·¯æ˜¯ï¼Œåœ¨ç©æ–‡å­—æ¸¸æˆçš„æ—¶å€™ï¼Œä¸ºä»€ä¹ˆæœºå™¨å¾ˆç¬¨ï¼Œè€Œäººå¾ˆèªæ˜ï¼Œæ˜¯å› ä¸ºäººç±»æœ‰æ€è€ƒçš„èƒ½åŠ›ã€‚å½“æ—¶æˆ‘åœ¨åšReActçš„æ—¶å€™ï¼Œæœ€åˆçš„æƒ³æ³•æ˜¯ï¼Œå¦‚æœæˆ‘èƒ½å¤Ÿè®©æœºå™¨æ¨¡ä»¿äººï¼Œä¸ä»…ä»…æ˜¯æ¨¡ä»¿äººçš„æ´»åŠ¨ï¼Œä¹Ÿæ¨¡ä»¿äººæ€ä¹ˆæ€è€ƒï¼Œæ˜¯ä¸æ˜¯å°±å¯ä»¥æ³›åŒ–å¾—æ›´å¥½ã€‚å…·ä½“æ¯”å¦‚äººçœ‹åˆ°äº†ä¸€ä¸ªåŸå ¡ï¼Œäººçš„é€‰æ‹©æ˜¯èµ°å‘ç¬¬ä¸‰ä¸ªé—¨ï¼Œå¦‚æœä½ åªå»æ¨¡ä»¿è¿™æ ·çš„Mappingï¼Œå¾ˆå¤šæ—¶å€™æ˜¯å¾ˆéš¾å»æ³›åŒ–çš„ã€‚ä½†æ˜¯å¦‚æœèƒ½å¤Ÿè®©å®ƒåŒæ—¶å»æ¨¡ä»¿äººçš„æ€è€ƒè¿‡ç¨‹ï¼Œé‚£å¯èƒ½å°±æ˜¯ä¸€ä¸ªéå¸¸è‡ªç„¶çš„ã€å¯ä»¥æ³›åŒ–çš„ä¸€ä¸ªç†ç”±ã€‚æ¯”å¦‚äººå¯èƒ½ä¼šæƒ³ï¼Œç°åœ¨å‘¨å›´å¾ˆé»‘æš—è€Œä¸”æœ‰å¥‡æ€ªçš„å«å£°ï¼Œå¯èƒ½æœ‰å±é™©éœ€è¦ç¯ã€‚ç¯åœ¨ç¬¬ä¸€ä¸ªæˆ¿é—´ï¼Œä½†æ˜¯ç¬¬ä¸€ä¸ªæˆ¿é—´çš„é’¥åŒ™åœ¨ç¬¬ä¸‰ä¸ªæˆ¿é—´ï¼Œæ‰€ä»¥æˆ‘å¾—å…ˆå»ç¬¬ä¸‰ä¸ªæˆ¿é—´ã€‚
  * CoTçš„æ‰©å±•
    * ä»æŸç§ç¨‹åº¦ä¸Šæ¥è¯´ï¼ŒReActå’ŒTree of Thoughtså…¶å®ç›¸å½“äºæ˜¯CoTçš„ä¸¤ä¸ªæ–¹å‘çš„æ‰©å±•ã€‚ä¸€ä¸ªæ–¹å‘æ˜¯è¦å’Œå¤–éƒ¨ä¸–ç•Œå‘ç”Ÿè”ç³»ï¼Œå¦ä¸€ä¸ªæ–¹å‘æ˜¯å†…éƒ¨çš„æ€è€ƒï¼Œå¦‚ä½•ä»ä¸€ä¸ªçº¿æ€§è¿‡ç¨‹å˜æˆä¸€ä¸ªéçº¿æ€§ï¼Œä¹Ÿå°±æ˜¯æ›´åŠ é€šå¾€ system 2çš„ä¸€ä¸ªè¿‡ç¨‹ã€‚
  * èº«è¾¹å¤ªå¤šèªæ˜çš„äººï¼Œä½†ä½ å‘ç°è‡ªå·±å¹¶ä¸æ¯”ä»–ä»¬å·®ã€‚åšç ”ç©¶éå¸¸é‡è¦çš„å› ç´ å°±æ˜¯ä¿¡å¿ƒï¼Œå¦‚æœä½ ä¸ç›¸ä¿¡èƒ½åšå‡ºéå¸¸å¥½çš„ç ”ç©¶ï¼Œé‚£ä½ æ˜¯ä¸å¯èƒ½åšå‡ºæ¥å¥½çš„ç ”ç©¶çš„ã€‚

## Finetuning

### Intro

* finetune v.s. from scratch
* å¦‚ä½•åšfinetune
  * åŸºåº§æ¨¡å‹é€‰å‹
* å…¨å‚æ•°finetuneå’Œå°å‚æ•°é‡finetune
  * å°å‚æ•°é‡finetune
    * Adapters
    * Prompt-tuning v1/v2
    * LoRA

* finetuneéœ€æ±‚
  * OpenAI: 1.3wæ¡SFT prompt
  * embeddingï¼šè‡³å°‘10wæ¡æ•°æ®ï¼Œç›¸ä¼¼æ€§å’ŒåŒä¹‰æ€§
* alpaca

![image-20231025213448602](./AI-Algorithms/alpaca.png)

### Literature Review

* finetuningåˆ†ç±»
  * fullï¼šTraining Language Models to Follow Instructions with Human Feedback
    * aligned with human preferences with instruction-tuning

  * é«˜æ•ˆçš„ï¼šLoRA: Low-Rank Adaptation of Large Language Models

* Pre-trained LLMs can be adapted to domain tasks with further fine-tuning
  * ã€ŠLarge language models encode clinical knowledgeã€‹

* fine-tuned LLMs fail to learn from examples
  * DAIL-SQL

### RLHF

* Reinforcement Learning from Human Feedback (RLHF), using the same methods as [InstructGPT](https://openai.com/blog/instruction-following/), but with slight differences in the data collection setup
  * RLHFçš„blogä»‹ç»ï¼šhttps://huggingface.co/blog/rlhf
    * supervised fine-tuning: human AI trainers provided conversations in which they played both sidesâ€”the user and an AI assistant
  * æ­¥éª¤ï¼š
    * é¢„è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ (LM) ï¼›
    * èšåˆé—®ç­”æ•°æ®å¹¶è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ (Reward Modelï¼ŒRM) ï¼›
    * ç”¨å¼ºåŒ–å­¦ä¹  (RL) æ–¹å¼å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ã€‚
      * é•¿æœŸä»¥æ¥ï¼Œå‡ºäºå·¥ç¨‹å’Œç®—æ³•åŸå› ï¼Œäººä»¬è®¤ä¸ºç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒ LM æ˜¯ä¸å¯èƒ½çš„ã€‚è€Œç›®å‰å¤šä¸ªç»„ç»‡æ‰¾åˆ°çš„å¯è¡Œæ–¹æ¡ˆæ˜¯ä½¿ç”¨ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹  (Policy Gradient RL) ç®—æ³•ã€è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (Proximal Policy Optimizationï¼ŒPPO) å¾®è°ƒåˆå§‹ LM çš„éƒ¨åˆ†æˆ–å…¨éƒ¨å‚æ•°ã€‚å› ä¸ºå¾®è°ƒæ•´ä¸ª 10Bï½100B+ å‚æ•°çš„æˆæœ¬è¿‡é«˜ (ç›¸å…³å·¥ä½œå‚è€ƒä½ç§©é€‚åº” LoRA å’Œ DeepMind çš„ Sparrow LM)
  * reward model: äººå·¥æ‰“åˆ†
    * äººå·¥å†™ç­”æ¡ˆ -> äººå·¥é€‰ç­”æ¡ˆ -> æœºå™¨é€‰ç­”æ¡ˆ
    * prompt dataset
    * fine-tune the model using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/)
    * ä¸€äº›å·§å¦™çš„æ‰“åˆ†æ–¹å¼ï¼š
      * å®¢æœç‚¹æŒ‰é’®ï¼Œé€‰å–aiç­”æ¡ˆï¼Œä¹Ÿæ˜¯finetuneè¿‡ç¨‹
      * redditå¸–å­ä¸­çš„æœ€é«˜åˆ†

![img](./AI-Algorithms/ChatGPT_Diagram.svg)

* 

### LoRA

![image-20231026212212239](./AI-Algorithms/LoRA.png)



https://github.com/huggingface/peft



### Instruction tuning

#### Literature Review

* Zero-shot training of retrievers.
  * å…‹æœæ²¡è§è¿‡çš„ä»»åŠ¡çš„queryéš¾ç‚¹
    * æ— ç›‘ç£ï¼šleveraging another model to automatically generate
      training data (Wang et al., 2022a).[TRwI]
    * ç”Ÿæˆlabelï¼ˆtemplate-based)ï¼šDai et al. (2022) use task-speciï¬c tem-
      plates and few-shot samples to automatically gen-
      erate in-domain training queries given randomly
      sampled documents from the target corpus using
      FLAN (Wei et al., 2022a)..[TRwI]

* Instruction Tuning

  * Weiet al., 2022a; Sanh et al., 2022; Ouyang et al., 2022;
    Min et al., 2022; Wang et al., 2022b; Mishra et al.,
    2022; Chung et al., 2022 .[TRwI]
  * ç¼ºå°‘æŒ‡ä»¤tuningçš„retrieval[TRwI]
    * ç¼ºå°‘æ ‡æ³¨æ•°æ®é›†
    * llmç”Ÿæˆæµ·é‡embçš„æˆæœ¬é«˜ 
    * Retrieval with descriptionsçš„è·¯çº¿ï¼šæ•ˆæœä¸€èˆ¬

  * dataset scaleæå‡instructionçš„æ³›åŒ–èƒ½åŠ›
    * Recent work (Wang et al., 2022b; Chung et al., 2022)
      show that scaling up the number of the training
      datasets improves LLMsâ€™ ability to adapt to new
      task via instructions. We open-source our instruc-
      tion data and call for community efforts to collect
      more retrieval tasks and human-written instructions
      as in instruction-following for LMs (Wang et al.,
      2022b; Bach et al., 2022), to investigate whether
      further increasing the number of the datasets (e.g.,
      more than 100 datasets) improves zero-shot and
      cross-task retrieval. [TRwI]

#### Task-aware Retrieval with Instructions

> https://github.com/facebookresearch/tart

* Intro
  * ![image-20241210014430460](./AI-Algorithms/image-20241210014430460.png)
  * In summary, our contributions are as follows:
    * Retrieval with instructions, a new formulation
      to model usersâ€™ intent explicitly (Section 3).
    * BERRI, a new large-scale collection of approximately 40 retrieval datasets in diverse domains with instructions (Section 4).
    * TART, a task-aware retriever trained on
      BERRI that advances state of the art on zero-
      shot and cross-task retrieval (Section 5).
* æ•°æ®
  * berri æ•°æ®é›†
    * intent domain unit
    * ![image-20241210015507819](./AI-Algorithms/image-20241210015507819.png)
    * https://huggingface.co/datasets/sentence-transformers/embedding-training-data
  * ERRI (Bank of Explicit RetRieval Instructions), a collection of
    approximately 40 retrieval datasets with diverse in-
    structions in a unified format, covering 10 diverse
    domains. Each task has on average 3.5 diverse
    instructions annotated by experts, 
  * éš¾è´Ÿä¾‹ï¼š![image-20241210015627115](./AI-Algorithms/image-20241210015627115.png)
    * We mine hard negative documents dHD us-
      ing an off-the-shelf retriever and then **filter out**
      **false negative documents using an off-the-shelf**
      **reranker**, following Qu et al. (2021).
      * ms-marco-MiniLM-L-12-v27
* æ¨¡å‹
  * dual-encoderï¼Œinstructionå’Œqueryç›¸è¿
    * The bi-encoder architecture is
      known to be less expressive since it only has
      limited interactions between queries and docu-
      ments (Khattab and Zaharia, 2020), especially
      when the training data is limited (HofstÃ¤tter et al.,
      2021). 
  * cross-encoderåšrank
    * To address this issue, we also explore a
      cross-encoder architecture (Nogueira and Cho,
      2019), which computes the relevance between
      a query and each document by jointly encoding
      them with cross-attention.
* Training
  * ç”¨cross-encoder rank modelæ›´å‡†ç¡®åœ°æŒ–æ˜hard negativeï¼Œç»™dual modelå­¦ä¹ 
  * ![image-20241210024754923](./AI-Algorithms/image-20241210024754923.png)
* è¯„ä¼°
  * è¯„æµ‹æ•°æ®é›†ï¼šbeirã€lotte-pooled
  * a new evaluation setup, X2-Retrieval
    * closed performance and pooled performance
* ç»“è®ºï¼š
  * ![image-20241210030107766](./AI-Algorithms/image-20241210030107766.png)
  * ![image-20241210030310460](./AI-Algorithms/image-20241210030310460.png)
  * 8.2 Dataset Scale
  * dual modelæ•ˆæœä¸€èˆ¬(110Mï¼Œtable-3)ï¼ŒçŒœæµ‹éœ€è¦å‚æ•°é‡æ¯”è¾ƒå¤§æˆ–è€…cross-encoderæ‰èƒ½å­¦å¥½



### Alignment

https://github.com/tatsu-lab/stanford_alpaca

æŒ‡ä»¤å¾®è°ƒæ˜¯ä»€ä¹ˆ? - superpengçš„å›ç­” - çŸ¥ä¹
https://www.zhihu.com/question/603488576/answer/3178990801

* æŒ‡ä»¤å¾®è°ƒæ˜¯ä¸€ç§ç‰¹å®šçš„å¾®è°ƒæ–¹å¼ï¼Œåœ¨ä¸åŒçš„è®ºæ–‡ä¸­ä»¥ä¸åŒçš„æ–¹å¼å¼•å…¥ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­çš„ç¤ºä¾‹å…·æœ‰é¢å¤–çš„ç»“æ„ï¼ŒåµŒå…¥åˆ°æ¨¡å‹æç¤ºä¸­ã€‚
  * å…ˆæ— ç›‘ç£è®­ç»ƒï¼Œå†ç”¨æœ‰ç›‘ç£çš„â€œæŒ‡ä»¤-å›ç­”â€œé¢„æ–™
  * æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ¥æ”¶ä¸€å¯¹è¾“å…¥å’Œè¾“å‡ºï¼Œæè¿°å¼•å¯¼æ¨¡å‹çš„ä»»åŠ¡ã€‚
* æ ¸å¿ƒæ€è·¯ï¼šè§£å†³â€œå›ç­”é—®é¢˜â€ä¸â€œæ¥è¯â€çš„å·®å¼‚
* Noteï¼š
  * æ•°æ®è·å–æ˜‚è´µï¼ˆRLHFäººå·¥æ‰“åˆ†çš„æˆæœ¬æ¯”äººå·¥å†™æ•…äº‹è¦ä½ï¼‰
  * å¯¹å¼€æ”¾æ€§é—®é¢˜æ•ˆæœä¸å¥½ï¼ˆwrite a story about ...ï¼‰

### SFT

### FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt

* Incremental Pre-training å¢é‡é¢„è®­ç»ƒ
  * å›¾åƒå’Œæ‰«ææ–‡æ¡£
    * å­˜å‚¨å¤§é‡é¢†åŸŸæ ‡å‡†æ–‡æ¡£ä¿¡æ¯ï¼Œä½¿ç”¨ OCR æŠ€æœ¯å¤„ç†ã€‚å› æ–‡æ¡£å¯èƒ½è¶…æ¨¡å‹è®­ç»ƒåºåˆ—é•¿åº¦ï¼ŒæŒ‰ç« èŠ‚æ‹†åˆ†ï¼Œä¸ºé˜²æè¿°å†²çªï¼Œç»™æ•°æ®ç« èŠ‚æ·»åŠ å‰ç¼€ï¼ˆé€šè¿‡ UIE æ¨¡å‹æå–æ–‡æ¡£åï¼Œå¯å‘å¼ç”Ÿæˆæ–¹æ³•æ„å»ºå‰ç¼€ï¼‰ã€‚åŒæ—¶ç”¨ BERT å’Œ GPT - 2 è®¡ç®—æ–‡æœ¬ç« èŠ‚ä¸­å¥å­çš„å›°æƒ‘åº¦ï¼Œæ’é™¤é«˜å›°æƒ‘åº¦å¥å­ã€‚
  * ç»“æ„åŒ–çŸ¥è¯†
    * å­˜åœ¨äºç§æœ‰ç»“æ„åŒ–æ•°æ®åº“ï¼Œç”±äººå·¥è¾“å…¥çš„è¡¨æ ¼ç»„æˆã€‚åˆ›å»º Datav1 å’Œ Datav2 ä¸¤ä¸ªç‰ˆæœ¬ç”¨äºå¢é‡é¢„è®­ç»ƒã€‚Datav1 å»é™¤æœºå¯†éšç§ä¿¡æ¯åç”¨å­—å…¸æ„å»ºæ•°æ®ï¼Œä»¥ â€œæµ‹è¯•é¡¹ç›®â€ ä¸ºé”®ï¼Œå¯¹åº”å¤šä¸ªå…·ä½“æµ‹è¯•é¡¹ç›®çš„è¡¨æ ¼ï¼ˆmarkdown æ ¼å¼ï¼‰ä¸ºå€¼ï¼›Datav2 é‡‡ç”¨æ–°æ–¹æ³•åºåˆ—åŒ–ï¼Œå»é™¤æœºå¯†éšç§ä¿¡æ¯ååˆå¹¶éƒ¨åˆ†æ— å•ç‹¬æ„ä¹‰çš„å­—æ®µï¼Œè¾“å…¥ ChatGPT æŒ‰è§„åˆ™éšæœºç”Ÿæˆæ–‡æœ¬ã€‚
  * å…¶ä»–ç±»å‹æ•°æ®
    * åŒ…æ‹¬é£Ÿå“æ£€æµ‹å­—å…¸ã€ä¸­å›½é£Ÿå“æ£€æµ‹æ•™ç¨‹å’Œç ”ç©¶è®ºæ–‡ã€é£Ÿå“æƒ…æ„Ÿæ•°æ®ã€é£Ÿå“å®‰å…¨ç›¸å…³æ³•å¾‹ã€é£Ÿå“å®‰å…¨ç›¸å…³è€ƒé¢˜ç­‰ï¼Œé€‰æ‹© Chinese - LLaMA2 - 13B ä¸ºåŸºç¡€æ¨¡å‹ï¼Œç”¨ LoRA æ–¹æ³•è¿›è¡Œå¢é‡é¢„è®­ç»ƒã€‚

* Instruction Fine-tuning

  - æ•°æ®é›†æ„å»º
    - é€šè¿‡ä¸¤ç§æ–¹å¼æ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚ä¸€æ˜¯ä»é£Ÿå“è®ºå›é€‰å–ç›¸å…³ä¸»é¢˜ï¼ŒæŠ“å–å¤§é‡é—®ç­”å¯¹ï¼Œä¼˜å…ˆé€‰æ‹©å‘å¸–é¢‘ç‡é«˜çš„ç”¨æˆ·ä»¥ç¡®ä¿é«˜è´¨é‡ç­”æ¡ˆï¼›äºŒæ˜¯ä¸é£Ÿå“æ£€æµ‹é¢†åŸŸä¸“å®¶åˆä½œè®¾è®¡ 100 ä¸ªé«˜è´¨é‡ç§å­æŒ‡ä»¤ï¼Œç”¨ evol - instruct æ–¹æ³•æ‰©å±•å’Œå¤šæ ·åŒ–ã€‚

  - è®­ç»ƒè¿‡ç¨‹
    - ç”¨ LoRA æ–¹æ³•å¯¹ Chinese - LLaMA2 - 13B çš„æŒ‡ä»¤è¿›è¡Œå¾®è°ƒã€‚



## Long-Context é•¿ä¸Šä¸‹æ–‡

* æ—©æœŸGPTçš„ä¸Šä¸‹æ–‡åªæœ‰4K
* è¶…å¤§çš„ä¸Šä¸‹æ–‡çª—å£=è¶…é•¿çš„çŸ­æœŸè®°å¿†
* 128K Token = 124K Input Token + 4096 Output Token

### â€œTrain Short, Test Longâ€, Positional Embedding

* TSTLæŒ‡çš„æ˜¯ä¸€ç§è®­ç»ƒå’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–å…¶ä»–åºåˆ—å¤„ç†æ¨¡å‹çš„æ–¹æ³•å’ŒæœŸæœ›èƒ½åŠ›ã€‚å…·ä½“å«ä¹‰å¦‚ä¸‹ï¼š

  * Train Short (çŸ­åºåˆ—è®­ç»ƒ) ï¼šåœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œä¸»è¦ä½¿ç”¨ç›¸å¯¹è¾ƒçŸ­çš„æ–‡æœ¬åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º 512 æˆ– 1024 ä¸ª tokenï¼‰è¿›è¡Œè®­ç»ƒã€‚è¿™æ ·åšå¯ä»¥ï¼š

    - èŠ‚çœè®¡ç®—èµ„æº ï¼šå¤„ç†çŸ­åºåˆ—éœ€è¦æ›´å°‘çš„å†…å­˜å’Œè®¡ç®—æ—¶é—´ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

    - åˆ©ç”¨ç°æœ‰æ•°æ® ï¼šå¾ˆå¤šç°æœ‰çš„è®­ç»ƒæ•°æ®é›†å¯èƒ½åŒ…å«å¤§é‡ä¸­çŸ­é•¿åº¦çš„æ–‡æœ¬ã€‚

  * Test Long (é•¿åºåˆ—æµ‹è¯•/æ¨ç†) ï¼šåœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåï¼ŒæœŸæœ›å®ƒèƒ½å¤Ÿåœ¨å¤„ç†æ¯”è®­ç»ƒæ—¶æ‰€è§è¿‡çš„åºåˆ— æ›´é•¿ çš„æ–‡æœ¬æ—¶ï¼Œä¾ç„¶ä¿æŒè‰¯å¥½çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåœ¨ 1024 token é•¿åº¦ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå¸Œæœ›å®ƒåœ¨å¤„ç† 2048ã€4096 ç”šè‡³æ›´é•¿ token çš„è¾“å…¥æ—¶ï¼Œä¹Ÿèƒ½ç†è§£ä¸Šä¸‹æ–‡ã€ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶ä¸”ä¸ä¼šå‡ºç°æ€§èƒ½æ€¥å‰§ä¸‹é™æˆ–å´©æºƒçš„æƒ…å†µã€‚
  * ä¼ ç»Ÿçš„ç»å¯¹ä½ç½®ç¼–ç ï¼ˆå¦‚ Transformer åŸå§‹è®ºæ–‡ä¸­çš„æ­£å¼¦/ä½™å¼¦ç¼–ç æˆ–å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥ï¼‰åœ¨ TSTL æ–¹é¢è¡¨ç°ä¸ä½³ã€‚å› ä¸ºå®ƒä»¬è¦ä¹ˆä¸ºæ¯ä¸ªç»å¯¹ä½ç½®å­¦ä¹ ä¸€ä¸ªç‰¹å®šçš„åµŒå…¥å‘é‡ï¼Œè¦ä¹ˆå…¶ç¼–ç æ–¹å¼åœ¨è¶…è¿‡è®­ç»ƒé•¿åº¦åæ— æ³•è‡ªç„¶å¤–æ¨ã€‚å½“é‡åˆ°æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—æ—¶ï¼Œæ¨¡å‹æ²¡æœ‰è§è¿‡è¿™äº›æ–°ä½ç½®çš„ç¼–ç ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

#### Alibi

https://arxiv.org/abs/2108.12409

- å®ƒä¸ç›´æ¥å‘è¯åµŒå…¥æ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œè€Œæ˜¯åœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶ï¼Œç»™æ¯ä¸ª query-key å¯¹æ·»åŠ ä¸€ä¸ª ä¸å®ƒä»¬ä¹‹é—´è·ç¦»æˆæ­£æ¯”çš„æƒ©ç½šé¡¹ï¼ˆbiasï¼‰ ã€‚
- è¿™ä¸ªæƒ©ç½šæ˜¯ ç›¸å¯¹çš„ ã€ å±€éƒ¨çš„ ï¼Œå¹¶ä¸”æ˜¯ éå­¦ä¹  çš„ï¼ˆæˆ–è€…è¯´ï¼Œå…¶æ–œç‡æ˜¯å›ºå®šçš„ï¼ŒæŒ‰æ³¨æ„åŠ›å¤´åˆ†é…ï¼‰ã€‚
- å› ä¸ºæƒ©ç½šåªä¾èµ–äºç›¸å¯¹è·ç¦»ï¼Œè€Œä¸æ˜¯ç»å¯¹ä½ç½®ç¼–å·ï¼Œæ‰€ä»¥å½“åºåˆ—å˜é•¿æ—¶ï¼Œè¿™ç§ç›¸å¯¹è·ç¦»çš„æƒ©ç½šæœºåˆ¶ä»ç„¶æœ‰æ•ˆã€‚æ¨¡å‹è‡ªç„¶åœ°å€¾å‘äºå…³æ³¨æ›´è¿‘çš„ tokenï¼Œè¿™ç§å€¾å‘æ€§ä¸ä¾èµ–äºåºåˆ—çš„æ€»é•¿åº¦ã€‚å› æ­¤ï¼ŒAlibi è¡¨ç°å‡ºå¾ˆå¥½çš„é•¿åº¦å¤–æ¨èƒ½åŠ›ã€‚

#### RoPE

https://arxiv.org/abs/2104.09864

- å®ƒé€šè¿‡å°†ä½ç½®ä¿¡æ¯ç¼–ç ä¸º æ—‹è½¬çŸ©é˜µ ï¼Œå¹¶åº”ç”¨äº query å’Œ key å‘é‡ã€‚
- ä¸¤ä¸ª token ä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°ä¾èµ–äºå®ƒä»¬å‘é‡çš„ç‚¹ç§¯ï¼Œè€Œ RoPE çš„è®¾è®¡ä½¿å¾—è¿™ä¸ªç‚¹ç§¯ä¸»è¦å–å†³äºå®ƒä»¬çš„ ç›¸å¯¹ä½ç½® ï¼ˆé€šè¿‡æ—‹è½¬è§’åº¦çš„å·®å€¼ä½“ç°ï¼‰ã€‚
- è™½ç„¶ RoPE ç¼–ç çš„æ˜¯ç»å¯¹ä½ç½®ï¼ˆé€šè¿‡æ—‹è½¬è§’åº¦ï¼‰ï¼Œä½†å…¶æ ¸å¿ƒæœºåˆ¶ä½¿å¾—ç›¸å¯¹ä½ç½®ä¿¡æ¯å¾—ä»¥ä¿ç•™å’Œåˆ©ç”¨ã€‚è¿™ç§åŸºäºæ—‹è½¬çš„ç›¸å¯¹ä½ç½®ç¼–ç æ–¹å¼ï¼Œç›¸æ¯”äºå­¦ä¹ ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå…·æœ‰æ›´å¥½çš„å¤–æ¨æ€§ï¼Œå› ä¸ºå®ƒä¸ä¾èµ–äºä¸ºè®­ç»ƒé•¿åº¦å†…çš„æ¯ä¸ªç»å¯¹ä½ç½®åˆ†é…ç‰¹å®šç¼–ç ã€‚

#### å…¶å®ƒ

[LongRoPE](https://arxiv.org/abs/2402.13753), [NTK-RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/), [ReRoPE](https://github.com/bojone/rerope?tab=readme-ov-file),



## Interpretability

* Intro
  * å…³äºå¯è§£é‡Šæ€§ï¼Œè¯™è°çš„ä¸¾ä¾‹ï¼Œé’å°‘å¹´åœ¨æƒ³ä»€ä¹ˆæ— æ³•ç†è§£ï¼Œæœ‰äº›ä¸œè¥¿å°±æ˜¯å¾ˆéš¾ç†è§£ï¼Œä½†ä»–çœŸå®å­˜åœ¨å¹¶workï¼Œé’å°‘å¹´ä¹Ÿæ˜¯äºº

* sparse autoencoders (SAEs) , Anthropic's paper https://transformer-circuits.pub/2024/scaling-monosemanticity/

* Interpretabilityåœ¨ç”µå•†åœºæ™¯çš„æ½œåœ¨åº”ç”¨ https://www.vantagediscovery.com/post/the-future-of-e-commerce-is-ai-powered-and-interpretable

  * **Hyper-Personalized Product Discovery**
    * Scenario: An e-commerce platform wants to move beyond basic recommendation algorithms and create truly personalized shopping experiences that resonate with individual customers. They need to understand not just what products customers interact with, but the underlying reasons and preferences driving their choices.
    * Solution: By using SAEs to analyze the LLM activations associated with user browsing behavior, purchase history, and product interactions (e.g., reviews, ratings, wishlists), the platform can extract nuanced features representing individual preferences and decision criteria. For example, features might emerge for "aesthetic style," "sustainability concerns," "value for money," "brand affinity," or specific functional requirements.
    * ![image-20241009174406858](./AI-Algorithms/interpretability1.png)

  * **Optimized Merchandising and Assortment**
    * Scenario: A retailer using an e-commerce platform wants to make data-driven decisions about inventory management, product assortment, and merchandising strategies. They need to understand which products are resonating with customers, which attributes are driving demand, and how to optimize pricing and promotions for maximum profitability.
    * Solution: By applying SAEs to analyze the LLM activations linked to product sales data, customer reviews, and market trends, the platform can identify crucial features influencing purchasing decisions. These might include features like "price sensitivity," "seasonal demand," "regional preferences," or "emerging trends" related to specific product attributes.
  * **Enhanced Explainable Search**
    * Scenario: A customer searches for "running shoes" but is dissatisfied with the results, feeling they are too generic.
    * Solution: The platform could use SAEs to analyze the search query's representation in the LLM's activation space. By identifying the activated features, they could provide an explanation to the customer, like "We are showing you popular running shoes based on your location and browsing history." Additionally, the platform could offer "steering" options based on other relevant features. For example, they could suggest refining the search by "cushioning," "terrain," or "price range."

## å¹»è§‰

ã€ŠLost in the middle: How language models use long contextsã€‹



- è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­å…³äºå¹»è§‰ç ”ç©¶çš„ç»¼è¿°ï¼šhttps://arxiv.org/abs/2202.03629
- è¯­è¨€æ¨¡å‹å‡ºç°çš„å¹»è§‰æ˜¯å¦‚ä½•æ»šé›ªçƒçš„ï¼šhttps://arxiv.org/abs/2305.13534
- ChatGPT åœ¨æ¨ç†ã€å¹»è§‰å’Œäº¤äº’æ€§ä¸Šçš„è¯„ä¼°ï¼šhttps://arxiv.org/abs/2302.04023
- å¯¹æ¯”å­¦ä¹ å‡å°‘å¯¹è¯ä¸­çš„å¹»è§‰ï¼šhttps://arxiv.org/abs/2212.10400
- è‡ªæ´½æ€§æé«˜äº†è¯­è¨€æ¨¡å‹çš„æ€ç»´é“¾æ¨ç†èƒ½åŠ›ï¼šhttps://arxiv.org/abs/2203.11171
- ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹çš„é»‘ç›’å¹»è§‰æ£€æµ‹ï¼šhttps://arxiv.org/abs/2303.08896

## å®‰å…¨ & ä¼¦ç†

> ä»…ä¸€å¤©å°±è¢«å¤–åª’å°æ€ å‰è°·æ­ŒCEOåˆ°åº•è¯´äº†... https://v.douyin.com/iBttgjpb/

### AIæˆ˜äº‰

* ç¾å›½ç™½é¹¤è®¡åˆ’crane war
  * æœºå™¨äºº/æ— äººæœºæ‘§æ¯æ•´ä¸ªå†›é˜Ÿç†è®ºï¼ˆå¦å…‹ã€ç‚®å…µã€è¿«å‡»ç‚®ï¼‰ï¼Œè®©åœ°é¢è¿›æ”»æˆä¸ºä¸å¯èƒ½
  * ç¾å›½èƒ½æºä¸è¶³ï¼ŒåŠ æ‹¿å¤§å‘ç”µï¼Œé˜¿æ‹‰ä¼¯æŠ•èµ„

### AIå®‰å…¨

* å…³é”®é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸€ä¸ªå­¦ä¹ äº†çš„ç³»ç»Ÿä¸­æ£€æµ‹å±é™©ï¼ˆæ¯”å¦‚æ··åˆæŸäº›åŒ–åˆç‰©ï¼‰ï¼Œå¹¶ä¸”ä½ æ— æ³•ç›´æ¥è¯¢é—®å®ƒè¿™äº›å†…å®¹
  * è§£å†³æ–¹æ¡ˆï¼šè®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼Œè¶…è¿‡äº†å‘æ”¿åºœæŠ¥å‘Š

### AIæ”¿æ²»

* å¯¹æ°‘ä¸»çš„å¨èƒ-è™šå‡ä¿¡æ¯-aigc
  * å°è¯•è§£å†³tiktoké—®é¢˜ï¼šå¹³ç­‰æ—¶é—´è§„åˆ™ï¼ˆæ€»ç»Ÿå€™é€‰äººçš„å†…å®¹æ’­å‡ºæ—¶é—´å¹³ç­‰ï¼‰
* å»ä¸­å¿ƒåŒ–çš„æ€æƒ³æ„å»ºæœªæ¥çš„AIå®‰å…¨ï¼šhttps://mp.weixin.qq.com/s/K1gbW1aIkwl8aLzkD9nYnQ
  * æ¯”ç‰¹å¸ï¼šæ”»å‡»æ”¶ç›Šè¿œå°äºæ”»å‡»æˆæœ¬
  * ä»¥ç”Ÿæ€è‘—ç§°çš„å…¬é“¾ä»¥å¤ªåŠï¼šè™½ç„¶ç§˜é’¥ä¹Ÿæ˜¯å‡ åä½ï¼Œä½†æ˜¯ç³»ç»Ÿå°±å¤ªå¤æ‚äº†ï¼Œå„ç§äºŒå±‚æŠ€æœ¯ã€è·¨é“¾æ¡¥ç­‰å¸¦æ¥äº†å¾ˆå¤šæ¼æ´ï¼Œä»¥è‡³äºç½‘ç»œæ”»å‡»ä¸æ–­ï¼Œå°±æ˜¯å› ä¸ºæ”»å‡»æ”¶ç›Šå¤§äºæ”»å‡»æˆæœ¬
  * æ–¹æ¡ˆï¼šç¡®æƒï¼Œå®åï¼Œç«äº‰

### AIä¼¦ç†

* ç®—æ³•æ¼”å˜åˆ°æœ€åä¼šæ‰©å¤§â€œout of rageâ€ï¼Œå› ä¸ºå†²çªå¸¦æ¥æµé‡
* å…³äºä¸¢å¤±å·¥ä½œï¼šéœ€è¦é«˜ç­‰æ•™è‚²çš„å·¥ä½œæ²¡äº‹ï¼Œå› ä¸ºè¿™äº›äººä¼šå’Œç³»ç»Ÿåä½œ

## RAG

### Intro

* RAGï¼ˆRetrieval Augmented Generationï¼‰é¡¾åæ€ä¹‰ï¼Œé€šè¿‡***\*æ£€ç´¢\****çš„æ–¹æ³•æ¥å¢å¼º***\*ç”Ÿæˆæ¨¡å‹\****çš„èƒ½åŠ›ã€‚

![image-20240923003438170](./AI-Algorithms/rag.png)

* æ­å»ºè¿‡ç¨‹ï¼š
  * æ–‡æ¡£åŠ è½½ï¼Œå¹¶æŒ‰ä¸€å®šæ¡ä»¶**åˆ‡å‰²**æˆç‰‡æ®µ
  * å°†åˆ‡å‰²çš„æ–‡æœ¬ç‰‡æ®µçŒå…¥**æ£€ç´¢å¼•æ“**
  * å°è£…**æ£€ç´¢æ¥å£**
  * æ„å»º**è°ƒç”¨æµç¨‹**ï¼šQuery -> æ£€ç´¢ -> Prompt -> LLM -> å›å¤
* ç¦»çº¿æ­¥éª¤ï¼š
  1. æ–‡æ¡£åŠ è½½
  2. æ–‡æ¡£åˆ‡åˆ†
  3. å‘é‡åŒ–
  4. çŒå…¥å‘é‡æ•°æ®åº“

- åœ¨çº¿æ­¥éª¤ï¼š
  1. è·å¾—ç”¨æˆ·é—®é¢˜
  2. ç”¨æˆ·é—®é¢˜å‘é‡åŒ–
  3. æ£€ç´¢å‘é‡æ•°æ®åº“
  4. å°†æ£€ç´¢ç»“æœå’Œç”¨æˆ·é—®é¢˜å¡«å…¥ Prompt æ¨¡ç‰ˆ
  5. ç”¨æœ€ç»ˆè·å¾—çš„ Prompt è°ƒç”¨ LLM
  6. ç”± LLM ç”Ÿæˆå›å¤

### Literature Review

> LightRAG 5.2

#### LLM + Graphs

* GNNs as Prefixï¼š
  * (GNNs) are utilized as the initial processing layer for graph data, generating structure-aware tokens that LLMs can use during inference
  * GraphGPTã€LLaGA
* LLMs as Prefix
  * GALMã€OFA
* LLMs-Graphs Integration
  * focuses on achieving a seamless interaction between LLMs and graph data, employing techniques such as fusion training and GNN alignment
  * developing LLM-based agents capable of engaging with graph information directly

> HybridRAG

#### KG

* knowledge extraction
  * The main tasks in this step are entity recognition, relationship extraction, and co-reference resolution. 
* knowledge improvement
  *  KG completion technique infers missing entities and relationships within the graph using methods such as link prediction and entity resolution. 
  *  Link prediction predicts the existence and type of a relation between two entities
     based on the graph structure and features
  *  entity resolution matches and merges different representations of the same entity
     from different sources
* knowledge adaptation

> Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering: Related Work

#### KGQA: Question answering (QA) with knowledge graphs (KGs)

* retrieval-based
  * utilize relation extraction [19] or distributed representations [5] to derive answers from KGs, but they face difficulties with questions involving multi- ple entities.
* template-based
  * depend on manually-created templates for encoding complex queries, yet are limited by the scope of available templates [16].
* semantic parsing-based methods
  * map text to logical forms containing predicates from KGs [4] [14] [21]
* Evaluation
  * Mean Reciprocal Rank (MRR)
    * MRR gauges the average inverse rank of the initial correct response
  * recall@K
    * recall@K determines the likelihood of a relevant itemâ€™s appearance within the top K selections
  * NDCG@K
    * NDCG@K appraises the rank quality by considering both position and pertinence of items.
  * For question-answering performance, we juxtaposed the "golden" solutions against the generated responses, utilizing metrics such as BLEU [11], ROUGE [9], and METEOR [3] scores.

#### LLM4KGQA

* [7] provide a comprehensive review of this integration, categorizing the roles of LLMs as Predictors, Encoders, and Aligners
* For graph-based reasoning, Think-on-Graph [15] and Reasoning-on-Graph [10] enhance LLMsâ€™ reasoning abilities by integrating KGs. 
* Yang et al. [20] propose augmenting LLMsâ€™ factual reasoning across various training phases using KGs. 
* For LLM-based question answering, Wen et al.â€™s Mindmap [18] and Qi et al. [13] employ KGs to boost LLM inference capabilities in specialized domains such as medicine and food. These contributions underscore the increasing efficacy of LLM and KG combinations in enhancing information retrieval and reasoning tasks.

> MindMap

#### LLM + KG

> MindMap

* èå…¥è®­ç»ƒï¼šKGs emerged as a promising complement to the drawbacks of LLMs
  (Pan et al., 2023). 
  * For instance, KG triples were
    added to the training of LLMs (Zhang et al., 2019b)ã€Sun et al., 2021
  * KG encoders were entangled with LLM layers
    for joint inference and optimization on graph and
    text data (Zhang et al., 2022). 
  * applying KG prediction tasks, e.g., link prediction, as additional supervision (Ya-
    sunaga et al., 2022)
* synergistic inference of KGs and fixed LLMs
  * 22å¹´å·¦å³ï¼Œå¾ˆå¤šå·¥ä½œæŒ–æ˜GNNã€Graph Encoderã€added interactions between text tokens and KG
    entities in the intermediate layers of LLMs (Zhang et al., 2022; Yao et al., 2023b)ï¼Œåæ¥æ‰è½¬å‘**prompting fixed pre-trained LLMs with graphical inputs**
  * Retrieval-Augmented LLM Inference
    * ã€ŠKnowledge-augmented language model prompting
      for zero-shot knowledge graph question answering.ã€‹ å¿½ç•¥äº†å›¾ç»“æ„ä¿¡æ¯
  * Graph Mining with LLMs
    * å®ä½“/å…³ç³»è¯†åˆ«ã€å›¾summary
      * prompting LLMs for KG entity linking prediction (Choudhary and Reddy, 2023; Sun et al., 2023), graph mining (Guo et al., 2023), and KG question answering (Baek et al., 2023)
      * ã€ŠGPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarkingã€‹
      * ã€ŠExploring the potential of large language models (llms) in learning on
        graphs.ã€‹
      * ã€ŠComplex logical reasoning over knowledge graphs
        using large language modelsã€‹
      * å±€é™æ€§ï¼š rely heavily on the factual correctness of the KG and ignore the situation where
        the KG does not match the question
    * complex reasoning across multiple evidence graphs grounded on KGs
      * MindMap



### å…³é”®å­—æ£€ç´¢

* Elastic Search
  * Elasticsearchï¼ˆç®€ç§°ESï¼‰æ˜¯ä¸€ä¸ªå¹¿æ³›åº”ç”¨çš„å¼€æºæœç´¢å¼•æ“: https://www.elastic.co/
  * å…³äºESçš„å®‰è£…ã€éƒ¨ç½²ç­‰çŸ¥è¯†ï¼Œç½‘ä¸Šå¯ä»¥æ‰¾åˆ°å¤§é‡èµ„æ–™ï¼Œä¾‹å¦‚: https://juejin.cn/post/7104875268166123528
  * å…³äºç»å…¸ä¿¡æ¯æ£€ç´¢æŠ€æœ¯çš„æ›´å¤šç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒ: https://nlp.stanford.edu/IR-book/information-retrieval-book.html
* **å…³é”®å­—æ£€ç´¢çš„å±€é™æ€§**
  * åŒä¸€ä¸ªè¯­ä¹‰ï¼Œç”¨è¯ä¸åŒï¼Œå¯èƒ½å¯¼è‡´æ£€ç´¢ä¸åˆ°æœ‰æ•ˆçš„ç»“æœ

### å‘é‡åº“å’Œå‘é‡æ£€ç´¢

* Text Embeddings

  * **è¯­ä¹‰ç›¸ä¼¼åº¦**ï¼šå‘é‡ä¹‹é—´è·ç¦»
    * æ¬§æ°è·ç¦»
    * ä½™å¼¦è·ç¦»

* å‘é‡æ•°æ®åº“

  * ä¸ä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“æ˜¯äº’è¡¥çš„

* ä¸»æµå‘é‡æ•°æ®åº“æ€§èƒ½å¯¹æ¯”ï¼š

  * FAISS: Meta å¼€æºçš„å‘é‡æ£€ç´¢å¼•æ“ https://github.com/facebookresearch/faiss

  - Pinecone: å•†ç”¨å‘é‡æ•°æ®åº“ï¼Œåªæœ‰äº‘æœåŠ¡ https://www.pinecone.io/

  * **Milvus**: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://milvus.io/
    * æ€§èƒ½ä¼˜åŒ–è¾ƒå¤š
  * Weaviate: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://weaviate.io/
  * Qdrant: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://qdrant.tech/
  * PGVector: Postgres çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/pgvector/pgvector
  * RediSearch: Redis çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/RediSearch/RediSearch
  * ElasticSearch ä¹Ÿæ”¯æŒå‘é‡æ£€ç´¢ https://www.elastic.co/enterprise-search/vector-search

![vectordb](./AI-Algorithms/vectordb.png)

* pgvector
  * PostgreSQLé‡Œé¢çš„ä¸€ä¸ªvector searchçš„æ’ä»¶
  * ç¼ºç‚¹ï¼š
    * å‘é‡ç»´åº¦æœ€å¤§åªæ”¯æŒ2000ç»´ï¼Œè€Œç°åœ¨å¾ˆå¤šæ–°çš„æ¨¡å‹ç”Ÿæˆçš„å‘é‡è¿œè¿œè¶…è¿‡2000ç»´ï¼Œå¯èƒ½è¾¾åˆ°4096ç»´ä»¥ä¸Šï¼ˆå’Œé‡‡ç”¨äº†PostgreSQLåº•å±‚å­˜å‚¨æœ‰å…³ï¼‰
    * å¤„ç†å¤æ‚åº”ç”¨åœºæ™¯æ—¶èƒ½åŠ›éå¸¸å¼±ã€‚è¿™é‡Œçš„å¤æ‚åœºæ™¯æŒ‡çš„æ˜¯ä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“ä¸­çš„æ“ä½œï¼Œå¦‚filterã€joinå’Œwhereç­‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœéœ€è¦å°†ä¸¤å¼ è¡¨è¿›è¡Œjoinç„¶åå†è¿›è¡Œå‘é‡æœç´¢ï¼Œpgvectorå¤„ç†è¿™ç§å…³ç³»å‹æ“ä½œçš„èƒ½åŠ›å¾ˆå·®ã€‚
* PGVector.rs
  * ä¸»è¦è®ºç‚¹ï¼švectoræ˜¯ä¸€ç§æ–°çš„data typeï¼Œè€Œä¸æ˜¯æ–°çš„indexingæ„å»ºæ–¹å¼
  * åŸºäºå…³ç³»å‹æ•°æ®åº“æ¥æ”¯æŒå‘é‡æœç´¢ï¼Œè€Œä¸æ˜¯å¼€å‘ä¸€ä¸ªæ–°çš„specialized vector DB
  * å¤æ‚åœºæ™¯ï¼šå…³ç³»å‹æ•°æ®åº“ä¸­çš„è¡¨ä¸è¡¨ä¹‹é—´çš„å¤æ‚æŸ¥è¯¢æ“ä½œã€‚
    * ä¾‹å¦‚ï¼Œæ”¯ä»˜å®çš„ä¸šåŠ¡å¯èƒ½æ¶‰åŠå‡ åå¼ è¡¨ï¼Œéœ€è¦å¾ˆå¤šjoinå’Œwhereè¯­å¥æ¥å®ç°ã€‚è¿™ç§å¤æ‚çš„å…³ç³»å‹æ•°æ®åº“æŸ¥è¯¢éœ€æ±‚æ˜¯ç‹¬ç«‹çš„vector DBæ— æ³•æ»¡è¶³çš„ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸åªåšå‘é‡æœç´¢ï¼Œæ²¡æœ‰å¤§é‡çš„è¡¨ä¸è¡¨ä¹‹é—´çš„æ“ä½œã€‚
  * å¯¹äºé‚£äº›ä¸“æ³¨å‘é‡æœç´¢çš„åº”ç”¨ï¼Œç‹¬ç«‹çš„vector DBç¡®å®å¯èƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚å®ƒä»¬æœ‰æ›´å¥½çš„æ‰©å±•èƒ½åŠ›ï¼Œèƒ½æ›´å¥½åœ°æ»¡è¶³è¿™ç±»éœ€æ±‚ã€‚å› æ­¤ï¼Œè¿™ä¸¤ç§åœºæ™¯å¹¶ä¸å†²çªï¼Œå…·ä½“é€‰æ‹©å–å†³äºä¸šåŠ¡éœ€æ±‚ã€‚å¦‚æœä¸šåŠ¡éœ€è¦å¤„ç†å¤æ‚çš„å…³ç³»å‹æ•°æ®åº“æŸ¥è¯¢ï¼Œæˆ‘ä»¬çš„pgvecto.rsä¼šæ›´é€‚åˆï¼Œè€Œå¦‚æœä¸šåŠ¡é‡å¿ƒåœ¨å‘é‡æœç´¢ï¼Œç‹¬ç«‹çš„vector DBå¯èƒ½æ›´æœ‰ä¼˜åŠ¿ã€‚

* turbopuffer
  * ä¸“é—¨åšå¤šç§Ÿæˆ·åœºæ™¯ï¼Œè¿™ä¸€å•ç‚¹å·®å¼‚åŒ–è®©å®ƒçš„å•†ä¸šåŒ–è¿›ç¨‹éå¸¸é¡ºåˆ©ã€‚å®ƒé’ˆå¯¹æœ‰å¤šç§Ÿæˆ·éœ€æ±‚çš„å®¢æˆ·ï¼ˆæ¯”å¦‚Notionè¿™æ ·çš„åº”ç”¨ï¼‰æä¾›æ•°æ®åº“æœåŠ¡ã€‚

### Embeddingæ¨¡å‹

* å‘é‡æ¨¡å‹æ€ä¹ˆè®­ç»ƒï¼š

  * æ„å»ºç›¸å…³ï¼ˆæ­£ä¾‹ï¼‰ä¸ä¸ç›¸å…³ï¼ˆè´Ÿä¾‹ï¼‰çš„å¥å­å¯¹å„¿æ ·æœ¬

  * è®­ç»ƒåŒå¡”å¼æ¨¡å‹ï¼Œè®©æ­£ä¾‹é—´çš„è·ç¦»å°ï¼Œè´Ÿä¾‹é—´çš„è·ç¦»å¤§

  * https://www.sbert.net/

* OpenAI æ–°å‘å¸ƒçš„ä¸¤ä¸ª Embedding æ¨¡å‹
  * text-embedding-3-largeã€text-embedding-3-small
  * ç‰¹ç‚¹ï¼š**è¶Šå¤§è¶Šå‡†ã€è¶Šå°è¶Šå¿«**
    * æ”¯æŒè‡ªå®šä¹‰çš„ç¼©çŸ­å‘é‡ç»´åº¦ï¼Œä»è€Œåœ¨å‡ ä¹ä¸å½±å“æœ€ç»ˆæ•ˆæœçš„æƒ…å†µä¸‹é™ä½å‘é‡æ£€ç´¢ä¸ç›¸ä¼¼åº¦è®¡ç®—çš„å¤æ‚åº¦
    * è®¡ç®—æ—¶ç”¨å‰Nç»´
  * å¯å˜é•¿åº¦çš„ Embedding æŠ€æœ¯ï¼š
    * https://arxiv.org/abs/2205.13147 Matryoshka Representation Learning
  * ![mteb](./AI-Algorithms/mteb.png)

* å¼€æºåº“ï¼š
  * https://github.com/FlagOpen/FlagEmbedding

* Noteï¼š
  * å¯èƒ½æ”¯æŒè·¨è¯­è¨€

### ç®—æ³•è¿›é˜¶

*  æ–‡æœ¬åˆ†å‰²çš„ç²’åº¦
   * ç¼ºé™·
     * ç²’åº¦å¤ªå¤§å¯èƒ½å¯¼è‡´æ£€ç´¢ä¸ç²¾å‡†ï¼Œç²’åº¦å¤ªå°å¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸å…¨é¢
       * æ¯”å¦‚åˆ‡åˆ†è‡ªç„¶æ®µï¼Œç²’åº¦å¤ªå¤§
     * é—®é¢˜çš„ç­”æ¡ˆå¯èƒ½è·¨è¶Šä¸¤ä¸ªç‰‡æ®µ
   * æ”¹è¿›: æŒ‰ä¸€å®šç²’åº¦ï¼Œéƒ¨åˆ†é‡å å¼çš„åˆ‡å‰²æ–‡æœ¬ï¼Œä½¿ä¸Šä¸‹æ–‡æ›´å®Œæ•´

*  æ£€ç´¢åæ’åº
   * é—®é¢˜: æœ‰æ—¶ï¼Œæœ€åˆé€‚çš„ç­”æ¡ˆä¸ä¸€å®šæ’åœ¨æ£€ç´¢çš„æœ€å‰é¢
   * æ–¹æ¡ˆ:
     * æ£€ç´¢æ—¶è¿‡æ‹›å›ä¸€éƒ¨åˆ†æ–‡æœ¬
     * é€šè¿‡ä¸€ä¸ªæ’åºæ¨¡å‹å¯¹ query å’Œ document é‡æ–°æ‰“åˆ†æ’åº
   * ä¸€äº› Rerank çš„ API æœåŠ¡
     * [Cohere Rerank](https://cohere.com/rerank)ï¼šæ”¯æŒå¤šè¯­è¨€
     * [Jina Rerank](https://jina.ai/reranker/)ï¼šç›®å‰åªæ”¯æŒè‹±æ–‡

![sbert-rerank](./AI-Algorithms/sbert-rerank.png)

* **æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰**
  * å‚è€ƒ ã€ŒLLM + Searchã€
  * å¾ˆå¤šå‘é‡æ•°æ®åº“éƒ½æ”¯æŒæ··åˆæ£€ç´¢ï¼Œæ¯”å¦‚ [Weaviate](https://weaviate.io/blog/hybrid-search-explained)ã€[Pinecone](https://www.pinecone.io/learn/hybrid-search-intro/) ç­‰ã€‚ä¹Ÿå¯ä»¥æ ¹æ®ä¸Šè¿°åŸç†è‡ªå·±å®ç°ã€‚

* RAG Fusion

![rag-fusion](./AI-Algorithms/rag-fusion.jpeg)

*  [query rewriting and query expansion](https://www.google.com/search/howsearchworks/how-search-works/ranking-results/#meaning)
*  PDFä¸­çš„è¡¨æ ¼å¦‚ä½•å¤„ç†
   * TableTransformeræ¨¡å‹ + GPT-4V
     * TableTransformeræ‰¾åˆ°è¡¨æ ¼
     * ç”¨ GPT-4 Vision ç”Ÿæˆè¡¨æ ¼ï¼ˆå›¾åƒï¼‰æè¿°ï¼Œå¹¶å‘é‡åŒ–ç”¨äºæ£€ç´¢
   * ä¸€äº›é¢å‘ RAG çš„æ–‡æ¡£è§£æè¾…åŠ©å·¥å…·

     - [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/): PDF æ–‡ä»¶å¤„ç†åŸºç¡€åº“ï¼Œå¸¦æœ‰åŸºäºè§„åˆ™çš„è¡¨æ ¼ä¸å›¾åƒæŠ½å–ï¼ˆä¸å‡†ï¼‰
     - [RAGFlow](https://github.com/infiniflow/ragflow): ä¸€æ¬¾åŸºäºæ·±åº¦æ–‡æ¡£ç†è§£æ„å»ºçš„å¼€æº RAG å¼•æ“ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
     - [Unstructured.io](https://unstructured.io/): ä¸€ä¸ªå¼€æº+SaaSå½¢å¼çš„æ–‡æ¡£è§£æåº“ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
     - [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/)ï¼šä»˜è´¹ API æœåŠ¡ï¼Œç”± LlamaIndex å®˜æ–¹æä¾›ï¼Œè§£æä¸ä¿è¯100%å‡†ç¡®ï¼Œå®æµ‹å¶æœ‰æ–‡å­—ä¸¢å¤±æˆ–é”™ä½å‘ç”Ÿ
     - [Mathpix](https://mathpix.com/)ï¼šä»˜è´¹ API æœåŠ¡ï¼Œæ•ˆæœè¾ƒå¥½ï¼Œå¯è§£ææ®µè½ç»“æ„ã€è¡¨æ ¼ã€å…¬å¼ç­‰ï¼Œè´µï¼


![table_rag](./AI-Algorithms/table_rag.png)

![https://storage.googleapis.com/gweb-cloudblog-publish/images/15._document_processing.max-1100x1100.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/15._document_processing.max-1100x1100.png)



### Rank

#### Literature Review

* rank
  * encoders of T5-based instruction-following pretrained mod-
    els, namely T0-3B (Sanh et al., 2022) and FLAN-
    T5 (Chung et al., 2022), empirically leads to supe-
    rior performance as found in prior work (Sachan
    et al., 2022). We follow the EncT5 approach (Liu
    et al., 2021) and prepended each sequence with a
    start-of-sequence token. The token representation
    is then fed to a newly initialized feed-forward net-
    work. Unlike MonoT5 (Nogueira et al., 2020), we
    use their encoders only to reduce parameters and
    improve inference-time efficiency [Task-aware Retrieval with Instructions]

### GraphRAG

> [Graph Retrieval-Augmented Generation: A Survey è®ºæ–‡è§£è¯»](https://mp.weixin.qq.com/s/Dx8pYhmbrhtRMXNez_GOmw)

* Intro
  * åˆ©ç”¨äº†å®ä½“ä¹‹é—´çš„ç»“æ„ä¿¡æ¯ï¼Œå®ç°äº†æ›´ç²¾ç¡®ã€å…¨é¢çš„æ£€ç´¢ï¼Œæ•æ‰äº†å…³ç³»çŸ¥è¯†ï¼Œä¿ƒè¿›äº†æ›´å‡†ç¡®ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”
  * Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation
* éš¾ç‚¹ï¼š
  * **å¿½è§†å…³ç³»ï¼š**ä¼ ç»ŸRAGæ–¹æ³•ä¸»è¦åŸºäºæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè€Œå¿½è§†äº†æ–‡æœ¬ä¹‹é—´çš„ç»“æ„å…³ç³»ã€‚ä¾‹å¦‚ï¼Œåœ¨å¼•ç”¨ç½‘ç»œä¸­ï¼Œä¼ ç»ŸRAGæ–¹æ³•å¯èƒ½æ— æ³•æ•æ‰åˆ°è®ºæ–‡ä¹‹é—´çš„å¼•ç”¨å…³ç³»ã€‚
  * **å†—ä½™ä¿¡æ¯ï¼š**RAGé€šå¸¸ä»¥æ–‡æœ¬ç‰‡æ®µçš„å½¢å¼æä¾›ä¿¡æ¯ï¼Œå½“è¿™äº›ç‰‡æ®µæ‹¼æ¥åœ¨ä¸€èµ·ä½œä¸ºæç¤ºæ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œå‡ºç°â€œlost in the middleâ€çš„é—®é¢˜ã€‚
  * **ç¼ºä¹å…¨å±€ä¿¡æ¯ï¼š**RAGåªèƒ½æ£€ç´¢åˆ°æ–‡æ¡£çš„å­é›†ï¼Œè€Œæ— æ³•å…¨é¢ç†è§£å…¨å±€ä¿¡æ¯ï¼Œè¿™åœ¨æŸ¥è¯¢èšç„¦æ‘˜è¦ï¼ˆQFSï¼‰ç­‰ä»»åŠ¡ä¸­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚

![image-20241020235306018](./AI-Algorithms/image-20241020235306018.png)

* GraphRAGçš„æ€è·¯ï¼š
  * GraphRAGçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†çŸ¥è¯†å›¾è°±ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå¦‚èŠ‚ç‚¹ã€ä¸‰å…ƒç»„ã€è·¯å¾„æˆ–å­å›¾ï¼‰ä¸LLMsçš„è¾“å‡ºç›¸ç»“åˆï¼Œä»¥æä¾›æ›´å‡†ç¡®å’Œä¸°å¯Œçš„ç”Ÿæˆç»“æœã€‚
  * ä½¿ç”¨ç»“æ„åŒ–çŸ¥è¯†å›¾è°±æ¥æ›´æœ‰æ•ˆåœ°å¤„ç†å†—ä½™ä¿¡æ¯å’Œå…¨å±€ä¿¡æ¯çš„é—®é¢˜ï¼Œæ›´æ–¹ä¾¿åœ°è¿›è¡Œä¿¡æ¯çš„æ£€ç´¢å’Œèšåˆ

![image-20241020235459558](./AI-Algorithms/image-20241020235459558.png)

* Preliminaries

  * Text-Attributed Graphs (TAGs)
    * ![image-20241021001256375](./AI-Algorithms/TAG.png)
  * GNN
    * ![image-20241021001339780](./AI-Algorithms/GNN.png)

* Graph-Based Indexing

  * æ•°æ®
    * å¼€æ”¾çŸ¥è¯†å›¾è°±ï¼šå…¬å¼€å¯ç”¨çš„çŸ¥è¯†å›¾è°±ï¼Œä¸€èˆ¬ä¸»è¦åŒ…æ‹¬ä¸‰ç±»ï¼šç™¾ç§‘çŸ¥è¯†å›¾è°±ï¼ˆå¦‚WikiDataï¼‰ã€å¸¸è¯†çŸ¥è¯†å›¾è°±ï¼ˆConceptNetï¼‰ä»¥åŠé¢†åŸŸçŸ¥è¯†å›¾è°±ã€‚
    * è‡ªæ„å»ºå›¾æ•°æ®ï¼šè¿™äº›æ˜¯ç ”ç©¶äººå‘˜æ ¹æ®ç‰¹å®šä»»åŠ¡éœ€æ±‚æ„å»ºçš„è‡ªå®šä¹‰å›¾æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå¯èƒ½ä»æ–‡æ¡£ã€è¡¨æ ¼æˆ–å…¶ä»–æ•°æ®åº“ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œå¹¶å°†å®ƒä»¬ç»„ç»‡æˆå›¾ç»“æ„ã€‚
  * ç´¢å¼•
    * å›¾ç´¢å¼•ï¼šå›¾ç´¢å¼•ä¿ç•™äº†å›¾çš„å®Œæ•´ç»“æ„ï¼Œä½¿èŠ‚ç‚¹å’Œè¾¹çš„è®¿é—®å˜å¾—å®¹æ˜“ã€‚åœ¨åç»­çš„GraphRAGè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ç»å…¸çš„å›¾æœç´¢ç®—æ³•ï¼ˆå¦‚BFSå’Œæœ€çŸ­è·¯å¾„ç®—æ³•ï¼‰æ¥å¿«é€Ÿæ£€ç´¢ä¿¡æ¯ã€‚
    * æ–‡æœ¬ç´¢å¼•ï¼šè¿™ç§æ–¹æ³•å°†å›¾æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œä»¥ä¾¿ä½¿ç”¨å„ç§æ–‡æœ¬æ£€ç´¢æŠ€æœ¯ï¼ˆå¦‚ç¨€ç–æ£€ç´¢å’Œå¯†é›†æ£€ç´¢ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚
    * å‘é‡æ£€ç´¢ï¼šè¿™ç§æ–¹æ³•å°†å›¾æ•°æ®è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œä»¥åˆ©ç”¨é«˜æ•ˆçš„å‘é‡æœç´¢ç®—æ³•ï¼ˆå¦‚å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰è¿›è¡Œå¿«é€Ÿæ£€ç´¢ã€‚

* Graph-Guided Retrieval

  * ![image-20241021001832040](./AI-Algorithms/graph-retrieval.png)

  * **æ£€ç´¢å™¨çš„é€‰æ‹©ï¼š**åœ¨å›¾æ£€ç´¢ä¸­ï¼Œé€‰æ‹©é€‚å½“çš„æ£€ç´¢å™¨æ˜¯è‡³å…³é‡è¦çš„ã€‚ç ”ç©¶äººå‘˜å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚å’Œæ•°æ®ç±»å‹é€‰æ‹©ä»¥ä¸‹ç±»å‹çš„æ£€ç´¢å™¨ã€‚
    * éå‚æ•°åŒ–æ£€ç´¢å™¨ï¼šåŸºäºä¼ ç»Ÿçš„å›¾æœç´¢ç®—æ³•ï¼ˆå¦‚BFSå’ŒDFSï¼‰ï¼Œä¸ä¾èµ–äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€‚ç”¨äºé«˜æ•ˆçš„å¤§è§„æ¨¡æ•°æ®æ£€ç´¢ã€‚
    * è¯­è¨€æ¨¡å‹æ£€ç´¢å™¨ï¼šåŸºäºè¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTã€GPTç­‰ï¼‰ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤„ç†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
    * å›¾ç¥ç»ç½‘ç»œæ£€ç´¢å™¨ï¼šåŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆå¦‚GCNã€GATç­‰ï¼‰ï¼Œåˆ©ç”¨å…¶å¯¹å›¾ç»“æ„æ•°æ®çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œé€‚ç”¨äºå¤„ç†å¤æ‚çš„å›¾ç»“æ„æ•°æ®ã€‚
  * Retrieval Paradigm
    * Once Retrieval
    * **Iterative Retrieval**
      * **Non-Adaptive Retrieval**
      * **Adaptive Retrieval.**
    * **Multi-Stage Retrieval.**

* Graph-Enhanced Generation
  * ![å›¾ç‰‡](./AI-Algorithms/640-20241021002249376)



* è®­ç»ƒ
  * Retrieverè®­ç»ƒ
    * Training-Free
    * Training-Based
  * Generatorè®­ç»ƒ
    * Training-Free
    * SFT
    * GNN

* åº”ç”¨
  * ä¸‹æ¸¸ä»»åŠ¡ï¼šé—®ç­”ï¼ˆçŸ¥è¯†åº“é—®ç­”ã€å¸¸è¯†é—®ç­”ï¼‰ã€ä¿¡æ¯æŠ½å–ï¼ˆå®ä½“é“¾æ¥ã€å…³ç³»æŠ½å–ï¼‰ã€äº‹å®éªŒè¯ã€é“¾æ¥é¢„æµ‹ã€å¯¹è¯ç³»ç»Ÿã€æ¨èç³»ç»Ÿç­‰ã€‚
  * åº”ç”¨é¢†åŸŸï¼šGraphRAGçš„åº”ç”¨é¢†åŸŸä¸»è¦åŒ…æ‹¬ï¼šç”µå•†ã€ç”Ÿç‰©åŒ»ç–—ã€å­¦æœ¯ã€æ–‡çŒ®å­¦ã€æ³•å¾‹
    * ç”µå•†ï¼š
      * RETE: Retrieval-Enhanced Temporal Event Forecasting on **Unified Query Product Evolutionary Graph.**
        * auto-regressive
      * Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering

* å¼€æºä»£ç 
  * å¾®è½¯GraphRAGï¼š[GitHub - microsoft/graphrag: A modular graph-based Retrieval-Augmented Generation (RAG) system](https://github.com/microsoft/graphrag)
  * èš‚èšGraphRAGï¼šhttps://github.com/eosphoros-ai/DB-GPTNeo4j 
  * NallMï¼šhttps://github.com/neo4j/NaLLMNeo4j 
  * LLM Graph Builderï¼šhttps://github.com/neo4j-labs/llm-graph-builderNebulaGraph 
  * GraphRAGï¼šhttps://www.nebula-graph.io/posts/graph-RAG



### LightRAG

> https://github.com/HKUDS/LightRAG
>
> [ä»åŸç†ã€æœ¬åœ°Qwen2.5-3Bæ¨¡å‹éƒ¨ç½²åˆ°æºç è§£è¯»ï¼Œå…¨æµç¨‹è§£æLightRAG](https://www.bilibili.com/video/BV1CwCRYGE6J)
>
> * æ€è·¯ï¼š
>
>   - æ•°æ®å¢å¼ºï¼šLLM
>
>   - å‰ªæï¼š
>     - LLM realtime update Graphï¼šå›¾èŠ‚ç‚¹/è¾¹å»é‡
>     - high-level concept / low-level entity

* Intro
  * **incorporates graph structures into text indexing** and retrieval processes
  * a **dual-level retrieval** system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery
  * an incremental update algorithm that ensures the timely integration of new data

![image-20241021170751318](./AI-Algorithms/lightrag-example.png)

* RAGçš„è®¾è®¡
  * Comprehensive Information Retrieval: The indexing function Ï†(Â·) must be adept at extracting global information, as this is crucial for enhancing the modelâ€™s ability to answer queries effectively.
  * Efficient and Low-Cost Retrieval: The indexed data structure DË† must enable rapid and cost- efficient retrieval to effectively handle a high volume of queries.
  * Fast Adaptation to Data Changes: The ability to swiftly and efficiently adjust the data structure to incorporate new information from the external knowledge base, is crucial for ensuring that the system remains current and relevant in an ever-changing information landscape.

![image-20241021142447180](./AI-Algorithms/lightrag.png)

* Framework
  * we leverage LLMs to identify and extract various entities (e.g., names, dates, locations, and events) along with the relationships between them.
  * Graph-Based Text Indexing
  * DUAL-LEVEL RETRIEVAL PARADIGM
* Graph-Based Text Indexing
  * Extracting Entities and Relationships. R(Â·)
  * LLM Profiling for Key-Value Pair Generation. P(Â·)
    * Entities use their names as the sole index key,
    * whereas relations may have multiple index keys derived from LLM enhancements that include global themes from connected entities.
  * Deduplication to Optimize Graph Operations. D(Â·)
* ä¸¤è·¯å¬å› DUAL-LEVEL RETRIEVAL PARADIGM
  - Specific Queries -> Low-Level Retrieval
    - â€œWho wrote â€™Pride and Prejudiceâ€™?â€
    - -> å¬å›title
  - Abstract Queries -> High-Level Retrieval
    - â€œHow does artificial intelligence influence modern education?â€
    - -> å¬å›å…³ç³»
  - Integrating Graph and Vectors for Efficient Retrieval.
    - Query Keyword Extraction: 
      - local query keywords k(l) and global query keywords k(g).
    - Keyword Matchingï¼š
      - match local query keywords with candidate entities and global query keywords with relations linked to global keys
    - Incorporating High-Order Relatedness.
      - åŸºäºå‰é¢å·²å¬å›çš„èŠ‚ç‚¹å’Œè¾¹ï¼Œå†å¤šä¸€è·³

* Evaluation

  * åŸºçº¿ï¼š
    * Naive RAG
    * RQ-RAGï¼šThese sub-queries are designed to enhance search accuracy by utilizing explicit techniques such as rewriting, decomposition, and disambiguation
    * GraphRAG:
      * It generates corresponding descriptions for these elements, aggregates nodes into communities, and produces a community report to capture global information
  * **LightRAGåšå•ä¸€é¢†åŸŸçš„ä»»åŠ¡æ¯”GraphRAGå¼º**
    * ![img_v3_02fs_6682e564-a869-4d15-a5c3-8fb11492dbeg](./AI-Algorithms/img_v3_02fs_6682e564-a869-4d15-a5c3-8fb11492dbeg.jpg)

  * ç»“è®ºï¼š
    * The Superiority of Graph-enhanced RAG Systems in Large-Scale Corpora
    * Enhancing Response Diversity with LightRAG
    * LightRAGâ€™s Superiority over GraphRAG
      * **Enhanced Response Variety**: By integrating low-level retrieval of specific entities with high-level retrieval of broader topics, LightRAG boosts response diversity. This dual-level mechanism effectively addresses both detailed and abstract queries, ensuring a thorough grasp of information.
      * **Complex Query Handling**: This approach is especially valuable in scenarios requiring diverse perspectives. By accessing both specific details and overarching themes, LightRAG adeptly responds to complex queries involving interconnected topics, providing contextually relevant answers.
    * å¯¹high/low level retrievalçš„åˆ†æï¼š
      * å»æ‰Highï¼šit struggles to gather information for complex queries that demand comprehensive insights
    * Semantic Graph Excels in RAG.
      * We eliminated the use of original text in our retrieval process. Surprisingly, the resulting variant, -Origin, does not exhibit significant performance declines across all four datasets. In some cases, this variant even shows improvements (e.g. in Agriculture and Mix). We attribute this phenomenon to the effective extraction of key information during the graph-based indexing process, which provides sufficient context for answering queries. Additionally, the original text often contains irrelevant information that can introduce noise in the response.
      * å¯å‘ï¼šä¿¡æ¯å¹¶ä¸æ˜¯è¶Šå¤šè¶Šå¥½ -> å¯¹rerankçš„å¯å‘

* Prompts
  * Prompts for Graph Generationï¼š7.3.1 
  * Prompts for Query Generationï¼š7.3.2
  * Prompts for Keyword Extractionï¼š7.3.3
  * Prompts for RAG Evaluation



### LLM4KGQA

> KGQA: Knowledge Graph Question Answering

#### FinDKG

* æŠ½å–KGçš„prompt

![image-20241027014446582](./AI-Algorithms/image-20241027014446582.png)

* åŠ¨æ€å›¾
  * GNNï¼Œæ—¶åºä¿¡æ¯å»ºæ¨¡

#### HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction

* Intro
  * KGï¼šå°†æ–‡æ¡£è§†ä¸ºä¸¤ä¸ªå®ä½“å’Œå…³ç³»çš„triplet
  * å½“å‰ RAG æŠ€æœ¯åŒ…æ‹¬åŸºäºå‘é‡æ•°æ®åº“çš„ VectorRAG å’ŒåŸºäºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„ GraphRAGï¼Œå„æœ‰å±€é™ï¼Œå¦‚ VectorRAG å¯¹é‡‘èæ–‡æ¡£çš„æ®µè½åˆ†å—å‡è®¾ä¸åˆç†ï¼ŒGraphRAG åœ¨æŠ½è±¡é—®ç­”ä»»åŠ¡æˆ–é—®é¢˜æœªæåŠæ˜ç¡®å®ä½“æ—¶è¡¨ç°ä¸ä½³ã€‚
* KGæ„å»º
  * each triplet is represented as **a nested list [â€™hâ€™, â€™typeâ€™, â€™râ€™, â€™oâ€™, â€™typeâ€™, â€™metadataâ€™]**,
    * â€™hâ€™ and â€™oâ€™ denote the head and object entities respectively,
    * â€™typeâ€™ specifies the entity category,
    * â€™râ€™ represents the relationship,
    * â€™metadataâ€™ encapsulates additional contextual information.
    * This format allows for a rich, multidimensional representation of information, facilitating
      more nuanced downstream analysis.
  * å°‘äº4 word
  * å®ä½“æ¶ˆé‡
  * å®ç°ï¼šNetworkxEntityGraph
* è¯„ä¼°
  * faithfulness, answer relevance, and context relevance      ï¼ˆHybridRAGï¼‰
    * ä½¿ç”¨ RAGAS æ¡†æ¶



#### Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering [SIGIR 2024]

* Intro
  * intra-issue structure and inter-issue relations
  * è¿‡å¾€å·¥ä½œçš„ Limitations
    * Limitation 1 - Compromised Retrieval Accuracy from Ignoring Structures
    * Limitation 2 - Reduced Answer Quality from Segmentation
* æ„å›¾è¯†åˆ«
  * 3.2.1 intentè¯†åˆ«ï¼Œã€‚è¯†åˆ«é™ˆè¿°å¥å’Œç–‘é—®å¥åŒºåˆ«ä¸å¤§ æ ¸å¿ƒæ˜¯è¯†åˆ«å¯¹è±¡ï¼Œå› æ­¤ç”¨ä¸€ä¸ªtemplateè¯†åˆ«kåˆ°vçš„æ˜ å°„
* çŸ¥è¯†å›¾è°±æ„å»º
  * æ˜¾å¼å’Œéšå¼å»ºç«‹ticketä¹‹é—´å…³ç³»
    * æ˜¾å¼ï¼šå·²æœ‰æ•°æ®
    * éšå¼ï¼štitle embeddingï¼Œä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé˜ˆå€¼

* Embedding-based Retrieval of Sub-graphs. (3.2.2)
  * EBR-based ticket identification step
    * è®¡ç®—ticketçš„ç›¸å…³æ€§ï¼šæ¶‰åŠå¤šä¸ªentityï¼Œæ¯ä¸ªentityç®—ç›¸å…³æ€§ç„¶ååˆ†æ•°ç›¸åŠ å¬å›
    * å¼•ç”³ï¼šå›¾çš„äºŒè·³é—®é¢˜
  * LLM-driven subgraph extraction step
    * ä»å·¥å•ä¸­æŸ¥æ‰¾æƒ³è¦çš„å±æ€§

#### MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models

> https://github.com/wyl-willing/MindMap
>
> æ€è·¯å¾ˆæ¸…æ™°ï¼šæ—¢åˆ©ç”¨KGåŠ å¼ºå¬å›ç‡å’Œç²¾å‡†åº¦ï¼Œåˆèå…¥GoTæŒ–æ˜LLMçš„å†…åœ¨çŸ¥è¯†

![image-20241027022219991](./AI-Algorithms/image-20241027022219991.png)

![image-20241027023313029](./AI-Algorithms/image-20241027023313029.png)

![image-20241027045058720](./AI-Algorithms/image-20241027045058720.png)

* Evidence graph mining
  * å®ä½“è¯†åˆ«ï¼š
    * **Promptï¼šTable 9 of Appendix D.**
    * BERT similarity to match entities and keywords
  * Evidence Sub-graphs Exploration
    * åŸºäºæå–çš„å®ä½“ä»æº KG æ„å»ºè¯æ®å­å›¾ï¼ŒåŒ…æ‹¬åŸºäºè·¯å¾„çš„æ¢ç´¢å’ŒåŸºäºé‚»å±…çš„æ¢ç´¢ä¸¤ç§æ–¹æ³•ï¼Œå¹¶å¯¹ç”Ÿæˆçš„å­å›¾è¿›è¡Œä¿®å‰ª
    * ç®—æ³•è§Appendix E
    * Path-based
    * Neighbor-based
      * ä¸€è·³å¿…åŠ 
      * äºŒè·³æ ¹æ®å’Œqueryçš„ç›¸å…³æ€§åŠ 
* Evidence graph aggregation
  * ä»å‰é¢æ­¥éª¤ä¸­æå–è‡³å°‘ k ä¸ªåŸºäºè·¯å¾„å’Œ k ä¸ªåŸºäºé‚»å±…çš„è¯æ®å­å›¾ï¼Œå°†æ¯ä¸ªå­å›¾æ ¼å¼åŒ–ä¸ºå®ä½“é“¾å¹¶è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œå®šä¹‰ä¸ºæ¨ç†å›¾ã€‚
  * é¡ºå¸¦èƒ½è§£å†³å®ä½“é‡å¤çš„é—®é¢˜
* LLM reasoning on the mind map
  * ç›¸æ¯”æ¥è¯´ï¼Œä»¥å‰çš„LLM4KGï¼š they do not think
    on multiple evidence KG sub-graphs with multi-
    thought in LLM, and without backtracking evi-
    dence sources

* Evaluation
  * hallucination quantificationï¼šå¼•å…¥æŒ‡æ ‡å®šä¹‰
  * train a keyword extraction model(NER-MT5) based on mT5-large
  * "combine with the knowledge you already haveâ€œ èƒ½æå‡æ•ˆæœ





* Prompt

å®ä½“æŠ½å–

```
template = """
There are some samples:
\n\n
### Instruction:\nâ€™Learn to extract entities from the following
medical questions.â€™\n\n### Input:\n
<CLS>Doctor, I have been having discomfort and dryness in my vagina
for a while now. I also experience pain during sex. What could be
the problem and what tests do I need?<SEP>The extracted entities
are\n\n ### Output:
<CLS>Doctor, I have been having discomfort and dryness in my vagina
for a while now. I also experience pain during sex. What could be
the problem and what tests do I need?<SEP>The extracted entities
are Vaginal pain, Vaginal dryness, Pain during intercourse<EOS>
\n\n
Instruction:\nâ€™Learn to extract entities from the following medical
answers.â€™\n\n### Input:\n
<CLS>Okay, based on your symptoms, we need to perform some diagnostic
procedures to confirm the diagnosis. We may need to do a CAT scan
of your head and an Influenzavirus antibody assay to rule out any
other conditions. Additionally, we may need to evaluate you
further and consider other respiratory therapy or physical therapy
exercises to help you feel better.<SEP>The extracted entities are
\n\n ### Output:
<CLS>Okay, based on your symptoms, we need to perform some diagnostic
procedures to confirm the diagnosis. We may need to do a CAT scan
of your head and an Influenzavirus antibody assay to rule out any
other conditions. Additionally, we may need to evaluate you
further and consider other respiratory therapy or physical therapy
exercises to help you feel better.<SEP>The extracted entities are
CAT scan of head (Head ct), Influenzavirus antibody assay,
Physical therapy exercises; manipulation; and other procedures,
Other respiratory therapy<EOS>
\n\n
Try to output:
### Instruction:\nâ€™Learn to extract entities from the following
medical questions.â€™\n\n### Input:\n
<CLS>{input}<SEP>The extracted entities are\n\n ### Output:
"""
```

ç”Ÿæˆç­”æ¡ˆã€GoT

```Python
SystemMessage(content= """You are an excellent AI doctor, and you can diagnose diseases and recommend medications based on the symptoms in the conversation."""),
HumanMessage(content"""Patient input:"""+ Question),
AIMessage(content=f """Combine the knowledge you already have, you have some extra medical knowledge information in the following:\n\n ### """+ path_reasoning_graph + """\n\n###""" + neighbor_reasoning_path),
HumanMessage(content="""What disease does the patient have? What tests should patient take to confirm the diagnosis? What recommened medications can cure the disease? Think step by step.\n\n\n
Output1: The answer includes disease and tests and recommened medications.\n\n
Output2: Show me inference process as a string about extract what knowledge from which Path-based Evidence or Neighor-based Evidence, and in the end infer what result. \n Transport the inference process into the
following format:\n Path-based Evidence number('entity name'->'relation name'->...)->Path-based Evidence number('entity name'->'relation name'->...)->Neighbor-based Evidence number('entity name'->'relation name'->...)-
>Neighbor-based Evidence number('entity name'->'relation name'->...)->result number('entity name')->Path-based Evidence number('entity name'->'relation name'->...)->Neighbor-based Evidence number('entity name'->'relation
name'->...). \n\n
Output3: Draw a decision tree. The entity or relation in single quotes in the inference process is added as a node with the source of evidence, which is followed by the entity in parentheses.\n\n
There is a sample:\n ... """)
```

ä¼ ç»ŸRAG

```
template = """
You are an excellent AI doctor, and you can diagnose diseases and Patient input:\n conversation.\n\n recommend medications based on the symptoms in the
{question}
\n\n
You have some medical knowledge information in the following:
{instruction}
What disease does the patient have? What tests should patient \n\n
take to confirm the diagnosis? What recommened medications can
cure the disease?
"""
```

LLM Evaluation

* â€œIf they are the same, output "2". Try to output "1" or "0"â€

```
def prompt_comparation(reference,output1,output2): template = """
Reference: {reference} \n\n
output1: {output1}
\n\n
output2: {output2}
\n\n
According to the facts of disease diagnosis and drug and tests recommendation in reference output, which output is better match. If the output1 is better match, output â€™1â€™. If the
output2 is better match, output â€™0â€™. If they are same match,
output â€™2â€™.
"""
prompt = template.format(reference=reference, output1=output1,
output2=output2)
response = openai.ChatCompletion.create( messages=[ model="gpt-4", {"role": "user", "content": prompt} {"role": "system", "content": """You are an excellent AI doctor."""},
]
response_of_comparation = response.choices[0].message.content return response_of_comparation
```

### ä¸šåŠ¡åœºæ™¯

* åœºæ™¯ä¸€ï¼šåˆä½œä¼™ä¼´è¯„ä¼°
  - â€œå“ªäº›ä¼ä¸šæœ€é€‚åˆæˆä¸ºæˆ‘ä»¬çš„æˆ˜ç•¥åˆä½œä¼™ä¼´ï¼Ÿâ€
  - å¯¹è¯å¼æœæ¨ --> è¿½é—®ç”¨æˆ·ä¼ä¸š
  - çŸ¥è¯†å›¾è°± --> ä¸šåŠ¡é¢†åŸŸã€æŠ€æœ¯ä¼˜åŠ¿ã€å¸‚åœºå®šä½ã€ä¿¡ç”¨è¯„çº§ã€çŸ¥è¯†äº§æƒæƒ…å†µã€è¯‰è®¼è®°å½•
  - å¯»æ±‚ä¸šåŠ¡ã€æŠ€æœ¯èƒ½åŠ›çš„äº’è¡¥æ€§ (å·¥ç¨‹æ–½å·¥ + è®¾è®¡è§„åˆ’)
* åœºæ™¯äºŒï¼šå¸‚åœºè¶‹åŠ¿æ´å¯Ÿ
  - â€œæœªæ¥å“ªäº›è¡Œä¸šé¢†åŸŸå¯èƒ½å‡ºç°çˆ†å‘å¼å¢é•¿ï¼Œæˆ‘ä»¬ä¼ä¸šè¯¥å¦‚ä½•æå‰å¸ƒå±€ï¼Ÿâ€
  - å¯¹è¯å¼æœæ¨ --> è¿½é—®ç”¨æˆ·è¡Œä¸š
  - çŸ¥è¯†å›¾è°± --> æ³¨å†Œæ•°é‡ã€èµ„æœ¬æŠ•å…¥ã€æ–°å¢ä¸“åˆ©æ•°é‡
  - å¯»æ‰¾ä¸åŒè¡Œä¸šä¹‹é—´çš„å…³è”èŠ‚ç‚¹
* åœºæ™¯ä¸‰ï¼šæ½œåœ¨é¡¹ç›®é¢„æµ‹
  - â€œæœªæ¥å“ªäº›é¡¹ç›®æœ€æœ‰å¯èƒ½é€‚åˆæˆ‘ä»¬ä¼ä¸šå‚ä¸æŠ•æ ‡ï¼Ÿâ€
  - å¯¹è¯å¼æœæ¨ --> è¿½é—®ç”¨æˆ·æŠ€æœ¯ä¼˜åŠ¿
  - çŸ¥è¯†å›¾è°± --> é¢†åŸŸæ‹›æŠ•æ ‡é¡¹ç›®æ•°é‡å¢é•¿è¶‹åŠ¿ã€æ”¿ç­–æ³•è§„ã€è¡Œä¸šåŠ¨æ€
  - ä¸ºç”¨æˆ·æä¾›æ½œåœ¨é¡¹ç›®æ¸…å•







### ç«å“

![image-20241007224527684](./AI-Algorithms/pai-rag.png)



## Multi-modal Search

### Intro

### Literature Review

* DML is to learn image embeddings to reflect the seman-
  tics among samples. [BtCT]
  * loss functions [6, 14, 20, 25]
  * sampling strategies [23, 32, 36]. 
  * interpret the decision made by the models. Inspired by DIML [35], [BtCT] leverage the spatial structure for improved and interpretable metric learning.

* Image Search [BtCT]
  * In [10], image descriptors generated by vision transformers are used for the image retrieval task. Although improvements over CNNs are reported, it is not clear why vision transformers perform better. 
    * uses transformersâ€™ class token only, 
  * [BtCT] : **consider both CLS token and patch tokens for image retrieval** to improve interpretability and accuracy.

* Optimal Transport for Feature Matching [BtCT]
  * Similar to image retrieval, inputs to feature matching are image pairs. The goal of feature matching is to establish pointwise correspondence using local features.
  * Recently, methods combining the attention mechanism with CNNs features are the state of the art. 
    * Given keypoint descriptors, SuperGlue [24] uses
      a graph neural network and attention layers to solve an assignment problem.
    * In [17], an Optimal Transport (OT) layer is adopted to obtain the semantic correspondence.
    * Matching quality is improved by suppressing one-to-many matchings. LoFTR [27] proposes a two-stage method using coarse and fine level features with optimal transport.
    * Given the feature maps of two images, COTR [13] concatenate and feed feature maps to a transformer with query point as input. The output is further fed into a decoder to infer the correspondence.
  * Among these approaches, we find two common differences with image retrieval. * 
    * First, all methods require CNNs backbone for feature extraction.
    * Second, feature matching heavily depends on datasets with dense feature correspondence for training. Examples are ScanNet[8] and MegaDepth [16].
    * In our work, unlike feature matching, optimal transport is exploited within a metric learning framework, in which only image level labels are available.

* Interpretable Deep Vision Models [BtCT]
  * For vision transformers, a common class-agnostic method to understand its predictions is to consider the attentions as relevancy scores.
  * Instead of taking a single attention layer, attention rollout [1] proposed to combine all attention maps in a linear way and to reassign all attention scores.
  * ã€ŠTowards interpretable deep metric learning with structural matchingã€‹

* Transformers for high-resolution images [PEaLF]
  * [50] designed a pyramidal architecture and addresses
    complexity by gradually reducing the spatial resolution of keys and values.
  * lowering spatial resolution at each layer for efficient computations
    * [17] utilized pooling to reduce the resolution
      across the spatial and temporal dimensions,
    * [27] used local attention with
      shifted windows and patch merging.
    * XCiT [1] proposed to replace the
      quadratic self-attention operation with a â€œtransposedâ€ attention operation be-
      tween channels which they call â€œcross-covariance attentionâ€ (XCA).
      * ç‰¹ç‚¹æ˜¯æ·±å±‚ç½‘ç»œä¹Ÿä¿ç•™å±€éƒ¨ç‰¹å¾





### é¢„å¤„ç†

* ç‰©ä½“æ£€æµ‹
* ç±»ç›®é¢„æµ‹
* Query Expansion
  * random crop

### è§†è§‰ç‰¹å¾

#### Intro

* Global featureå’Œlocal feature
  * global feature
    * compact representation
    * can be learned so that it is in-
      variant to viewpoint and illumination
    * the risk of losing information about
      the spatial arrangement of visual elements

![image-20241215014023835](./AI-Algorithms/image-20241215014023835.png)

#### å„ç±»backbone

[SIFT Meets CNN: A Decade Survey of Instance Retrieval](https://arxiv.org/pdf/1608.01807.pdf)

![275f8067-4c5a-42ba-ae58-66b6f7c93067](./AI-Algorithms/275f8067-4c5a-42ba-ae58-66b6f7c93067.png)



* [Image Similarity for Brand Detection: A Comparison Between BEiT, SWIN and ViT-MAE](https://bolster.ai/blog/image-similarity-beit-swin-vit-mae)
  * åœºæ™¯ï¼šbrandè¯†åˆ«ï¼Œè¯†åˆ«phishing attack.
  * ç®—æ³•æ–¹æ¡ˆè®¨è®ºï¼š
    * CNNåˆ†ç±»ï¼šlabelè´¨é‡è¦æ±‚é«˜ã€æ•°æ®åˆ†å¸ƒå‡åŒ€æ€§è¦æ±‚é«˜
  * å†å²æ–¹æ¡ˆï¼š
    * CV Hashing: Secure Hashing Algorithm or SHA-> a 64 bit hexadecimal encodingï¼Œåªèƒ½å¬å›åŸå›¾ã€‚ã€‚ã€‚
  * Embedding Model
    * BERT Pre-training of image transformers (BEiT)
    * SWIN
    * ViT-MAE
    * ç»“è®ºï¼šSWINæœ€å¼º

* [å›½æ——è¯†åˆ« - Build an AI Image Similarity Search with Transformers â€” ViT, CLIP, DINO-v2, and BLIP-2](https://medium.com/@tapanbabbar/build-an-image-similarity-search-with-transformers-vit-clip-efficientnet-dino-v2-and-blip-2-5040d1848c00)
  * **CLIP** (Contrastive Language-Image Pre-training): Built by OpenAI, it learns to match images with text. Not a bad choice for our similarity search.
  * **ViT** (Vision Transformer): ViT revolutionizes image processing by treating images as sequences, similar to how Transformers handle text.
  * **BLIP**: A vision-language model with a focus on aligning visual and textual content.
  * **EfficientNet**: Known for its efficiency, this model is great for image recognition tasks.
  * **DINO**: A self-supervised transformer model that excels at learning features from images.
  * **VGG16**: A classic convolutional neural network (CNN) thatâ€™s been around for years and still holds its own in image recognition tasks.
  * Caution: Contextual Bias in Similarity Models

* https://github.com/huggingface/notebooks/blob/main/examples/image_similarity.ipynb
  * å®Œæ•´çš„torch demo

#### æå– Embedding

- CNNs such as ResNet
  - å¯¹æœ€åä¸€å±‚å·ç§¯å±‚çš„feature mapsï¼Œåšglobal average pooling and fully connected layer 
- Vision Transformers such as ViT
  - Hidden Stateçš„ç¬¬ä¸€ä¸ªEmbeddingï¼Œå¯¹åº”äº CLS Token çš„ Embedding
- SwinV2
  - æ‰€æœ‰Hidden Stateçš„ avg pooling ç»“æœ

#### Beyond the CLS Token: Image Reranking using Pretrained Vision Transformers

> * æŠ€æœ¯å…³é”®ç‚¹å’Œç»“è®ºï¼š
>   - vit/swin/dinov2çš„patch embeddingä½œä¸ºå›¾åƒå±€éƒ¨ç‰¹å¾
>     - swinæœ€å¼º
>   - å¼•å…¥DIMLæŠ€æœ¯ï¼Œç”¨optimal transportåšæ’åº
>   - æŠ€æœ¯é€‚ç”¨åœºæ™¯ï¼š
>     - åœ¨CvTï¼ˆvit + convolutionï¼‰ä¸Šï¼Œè¿™ä¸ªæŠ€æœ¯æ•ˆæœå¥½
>     - Visual Place Recognitionè¯„æµ‹ï¼Œè¿™ä¸ªæŠ€æœ¯æ•ˆæœå¾ˆå¥½

* Intro
  * exploit a pretrained model for optimal spatial weights
    assigned to local patch tokens.
  * local patch similarity equipped with
    an optimal transport solver could improve image retrieval accuracy compared to the one using global similarity only
  * Apart from the added interpretability, leveraging local feature maps does not require extra learning at all. 
    * In CNNs, patch level features are available before the aggregation of global
      average pooling and projection of fully connected layers.
    * For ViT, local patch tokens are trained together with a special CLS token. All tokens interact each other with self-attentions. To adapt a permutation-invariant transformer to work on images, position embeddings are added to the patch embedding.
  
* Related Work
  * Deep metric learning (DML)  
  * ResNet
    * Hierarchical design
    * translation invariance
    * local receptive field
  * **Towards interpretable deep metric learning with structural matching**

* DML
  * For CNNs such as ResNet, f is obtained by global average pooling and fully connected layer on the feature maps of the final convolutional layer.

* DIML
  * ![image-20241213195211466](./AI-Algorithms/image-20241213195211466.png)

* ViT with Convolutions
  * åŠ¨æœºï¼šFor structural similarity learning, good properties of the representation should be locally smooth and semantically discriminative. Comparing to ResNet and vanilla ViT, we hypothesize that the introduction of convolution to ViT satisfies the two requirements.
  * ç»“è®ºï¼šComparing to ResNet and vanilla ViT, we
    hypothesize that the introduction of convolution to ViT satisfies the two requirements.
    * semantic intra-class features are correlated
    * inter-class semantics are distinguished.

* Structural Metric Learning using Transformers

  * ![image-20241213200708432](./AI-Algorithms/image-20241213200708432.png)

  * Relevancy Score as Marginal Distribution
    * Cross-correlation is proposed in [35]
    * Aggregated attention is obtained by multiplying attention maps from all attention layers.
      * It was originally used for the purpose of interpreting transformers classification [1].
      * In our method, the relevancy map is used to guide the optimal transport optimization for structural similarity.
      * The relevancy map can be obtained by a forward pass of transformers
      * it is theoretically applicable to almost all the transformers architectures [1] that use global attentions such as DeiT and CvT.
    * ![image-20241213201553244](./AI-Algorithms/image-20241213201553244.png)

  * OTçš„æ”¹è¿›ï¼šâ€œpartialâ€ï¼Œç¼“è§£è§†è§’/Scaleå·®å¼‚çš„å½±å“
    * ![image-20241213201914520](./AI-Algorithms/image-20241213201914520.png)

* ç»“è®ºï¼š
  * Swinæœ€å¼º
  * åœ¨CvTä¸Šï¼Œè¿™ä¸ªæŠ€æœ¯æ•ˆæœå¥½
  * ![image-20241213202153167](./AI-Algorithms/image-20241213202153167.png)
  * Visual Place Recognitionè¯„æµ‹ï¼Œè¿™ä¸ªæŠ€æœ¯æ•ˆæœå¾ˆå¥½

#### Patch Embedding as Local Features: Unifying Deep Local and Global Features Via Vision Transformer for Image Retrieval

> https://github.com/PXThanhLam/ViTGaL
>
> - æŠ€æœ¯å…³é”®ç‚¹å’Œç»“è®ºï¼š
>   - vitçš„patch embeddingå¯ä½œä¸ºå›¾åƒå±€éƒ¨ç‰¹å¾
>   - å…ˆé€šè¿‡å›¾åƒå…¨å±€ç‰¹å¾åšå¬å›ï¼Œå†åŸºäºå›¾åƒå±€éƒ¨ç‰¹å¾åšRankï¼Œæ•ˆæœè¾ƒå¥½
>   - multi-astrousï¼Œpatch embeddingé€šè¿‡ç©ºæ´å·ç§¯ï¼Œæœ‰æ•ˆæœæå‡
>   - ç”¨ a small autoencoder (AE) åšç‰¹å¾é™ç»´

* ViTGaL
  * Vision Transformer based Global and Local features (ViT-
    GaL). 
  * add a multi-atrous convolution to the output of the
    transformer encoder layer of ViTs to simulate the image pyramid used in
    standard image retrieval algorithms.
  * use class attention to aggregate the token embeddings output from the multi-atrous layer to get both global and local features.

* Intro

  * **ViTçš„æ·±å±‚patch embeddingï¼Œå…·å¤‡å±€éƒ¨ç‰¹å¾**
    * a recent study [39] found that spatial information from the input is
      preserved in ViT even as the final layer.
    * using patch embeddings from the final layer of ViT yields the best result
    * Vit embeddingçš„å¯è§†åŒ–
      * Peeling Back the Layers: Interpreting the Storytelling of ViT https://mp.weixin.qq.com/s/gzTRfu3SU1_6ZJsH2ngduA
        * æ³¨æ„åŠ›å‘é‡çš„L2èŒƒæ•°ï¼ˆé•¿åº¦è§†ä¸ºä¿¡æ¯é‡çš„åº¦é‡ï¼‰åœ¨ç½‘ç»œçš„ä¸åŒå±‚å‘ˆç°å‡ºä¸€ç§æŠ›ç‰©çº¿å‹çš„å˜åŒ–è¶‹åŠ¿ï¼šå…ˆä¸Šå‡ï¼Œè¾¾åˆ°å³°å€¼ååˆä¸‹é™ã€‚
        * è¿™ä¼¼ä¹æš—ç¤ºç€è§†è§‰ç†è§£åœ¨ä¸­å±‚è¾¾åˆ°äº†ä¸€ä¸ªé¥±å’ŒçŠ¶æ€ï¼Œä¹‹åéƒ¨åˆ†éæ ¸å¿ƒä¿¡æ¯è¢«èˆå¼ƒï¼Œæœ€ç»ˆæç‚¼å‡ºé«˜åº¦å‡ç»ƒçš„è¯­ä¹‰è¡¨å¾
  * ViTçš„é—®é¢˜
    * æ™¯è‰²è¯†åˆ«ï¼Œé«˜ç²¾åº¦å›¾ç‰‡å¾ˆé‡è¦ï¼Œæ¨¡å‹éš¾è®­ç»ƒ
      * æ–¹æ¡ˆ1ï¼šThe dominant approach is reducing the
        spatial dimension of input resolutions at every block of layers, similar to CNN
        [27,26,50]
      * æ–¹æ¡ˆ2ï¼šXCiT [1] replaced a self-attention between tokens with a
        â€œtransposedâ€ attention between channels which they call â€œcross-covariance attentionâ€ (XCA).
  
  * image pyramid
    * we proposed to simulate an image pyramid with multi-atrous convolutions [10]

![image-20241215020433590](./AI-Algorithms/image-20241215020433590.png)

* æ¨¡å‹
  * merge all the attention scores in different attention
    heads in the class attention layer and extract associated patch embeddings with
    the top scores.
    * class attention layer. This layer is identical to the
      transformer encoder block used in ViT, except the self-attention operation is
      only calculated between the cls token embedding (treated as a query) and the
      token embeddings of image patches (treated as keys and values).

* Local featureé™ç»´
  * [23] shows that
    whitening down weights co-occurrences of local features, which is generally ben-
    eficial for retrieval applications.
  * using a small autoencoder (AE) module [21] following
    the state-of-the-art dimensionality reduction method used in [9]
    * use the attention scores from the autoencoder network as key point detection scores to extract top local descriptors
    * For local features matching, we use RANSAC [18] with an affine model

* ç»“è®ºï¼š
  * multi-atrousæ•ˆæœå¥½ï¼Œä¸éœ€è¦multi-scale
  * rerankæ•ˆæœå¥½



### ç«å“

* Aliyun
  * https://help.aliyun.com/zh/image-search/developer-reference/api-searchbypic?spm=a2c4g.11186623.help-menu-66413.d_4_3_1_3.7538364fjOQka0&scm=20140722.H_202282._.OR_help-V_1

* Googleï¼šhttps://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search
  * https://ai-demos.dev/demos/matching-engine
  * https://atlas.nomic.ai/map/vertexAI-mercari å¯è§†åŒ–
  * ![image-20241221224534885](./AI-Algorithms/image-20241221224534885.png)



### Cases

* ç”µå•†

  * *"cups with dancing people"*

  * *"handmade accessories with black and white beads"*

  * *"Cups in the Google logo colors"*

  * *"Shirts that says my birthday"*

* è‡ªåŠ¨é©¾é©¶
  * "a crossing road with red lights on and pedestrians are standing,"
  * "a crushed car stopping in the middle of the freeway ahead" 
* å®‰é˜²
  * a person trying to open the doors,
  * water is flooding in the factory
  * the machines are on fire.

## LLM4Rec

### Intro

* https://github.com/WLiK/LLM4Rec-Awesome-Papers
* [LLM+Recommendationå¤§æ¨¡å‹æ¨èè¿‘æœŸè¿›å±•|å«WWW, SIGIR, AAAIç­‰é¡¶ä¼šæ–‡ç« ](https://mp.weixin.qq.com/s/m8DMgSt_r-HVNHHzA8ceVw)
* KDD 2024 å·¥ä¸šç•Œæœå¹¿æ¨å·¥ä½œæ•´ç† https://mp.weixin.qq.com/s/io8bZRMTmt9rQ2pRh1T2pQ
* ä¸€ç¯‡ä¸­æ–‡ç§‘æ™®æ–‡ç« ï¼šhttps://36kr.com/p/2805108795192961
  * LLM MLSysæ¯”ä¼ ç»ŸRecSysæ›´é€šç”¨
    * ä¼ ç»ŸRecSysæ¶‰åŠçš„ä¸­é—´ä»¶æ›´å¤šã€æ›´é‡
    * Langchainçš„è°ƒç”¨æµç¨‹é€šç”¨æ€§å¼º
  * AI Paaså¼•é¢†æ¨èç³»ç»ŸSaasç”±ç®—æ³•ä¸»å¯¼åˆ°å·¥ç¨‹ä¸»å¯¼çš„è½¬å‹

![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2024%2F0604%2F94c56fc3j00sejlo6001bd200u000klg00hx00ca.jpg&thumbnail=660x2147483647&quality=80&type=jpg)

![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2024%2F0604%2Fa2c9deb7j00sejlo7002rd200u000npg00id00ei.jpg&thumbnail=660x2147483647&quality=80&type=jpg)

![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2024%2F0604%2Ff9887823j00sejlog005cd200u000i6g00hx00au.jpg&thumbnail=660x2147483647&quality=80&type=jpg)



#### [é©¬åšé‘« é˜¿é‡ŒM6å›¢é˜Ÿ MLNLP2023å¤§æ¨¡å‹ä¸æ¨èç³»ç»Ÿè®ºå›](https://www.bilibili.com/video/BV17u4y1N7zY)

* Qwen LLMä»‹ç»

  * 7Bå¼€æºæ¨¡å‹
  * ReAct prompting
    * æŠ€å·§ï¼šå¤„ç†å¤šè½®é—®ç­”ä¸Šä¸‹æ–‡ï¼Œå°†ReAct promptingè´´åœ¨å€’æ•°ç¬¬äºŒä¸ªå›ç­”å‰é¢ï¼Œè€Œä¸æ˜¯æœ€åä¸€ä¸ªé—®é¢˜å‰ï¼Œæœ‰åŠ©äºæ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡
  * å¦‚ä½•è®­ç»ƒAgentèƒ½åŠ›
    * AI aligns AI
      * Step 0: å°‘é‡é«˜è´¨é‡æ•°æ®ï¼Œç”¨ä½œSFTå’Œin-context examples
      * Step 1: self-instructï¼Œå³é€šè¿‡in-context promptç”Ÿæˆæ›´å¤šæ ·æœ¬
      * Step 2: è§„åˆ™å¤„ç†ï¼Œäººå·¥å®¡æ ¸ï¼Œï¼ˆå¯é€‰ï¼‰äººå·¥æ”¹æ­£
      * Step 3: å¾ªç¯ Step 1-2

* LLMåœ¨RecSysçš„ä»·å€¼ â€”â€” ä»ç”Ÿäº§è€…ã€å¹³å°ã€æ¶ˆè´¹è€…è§†è§’

  * ![image-20240719185430334](./AI-Algorithms/llm-rec-text.png)

  * LLM + RecSys æœ‰ç›Šäºå†…å®¹ç”Ÿäº§è€…

    * æ ¸å¿ƒè¯‰æ±‚ï¼šæµé‡ï¼Œå°¤å…¶æ˜¯æ–°å†…å®¹çš„å†·å¯åŠ¨
    * ![image-20240719185656541](./AI-Algorithms/llm-recsys-1.png)

    * aliç»“æœï¼šå°æ¨¡å‹ã€å°‘é‡æ ·æœ¬ã€å†·å¯åŠ¨ä¼˜ç§€

  * LLM + RecSysæœ‰ç›Šäºæ¨èå¹³å°

    * æ ¸å¿ƒè¯‰æ±‚ï¼šç®—æ³•æ”¯æŒå¹³å°çš„è¿è¥ä¼åˆ’
    * æ—¶äº‹çƒ­ç‚¹ï¼šåŠ å¿«å¯¹äº‹ä»¶çš„å“åº”é€Ÿåº¦
      * å¿«ä¸­ç§‹äº† -> æ¨èæœˆé¥¼ï¼Œæ— éœ€ä»è¡Œä¸ºå­¦ä¹ 
    * äººå·¥å¹²é¢„ï¼šåŸºäºLLMçš„customized instruction/system prompt

  * LLM + RecSysæœ‰ç›Šäºå†…å®¹æ¶ˆè´¹è€…

    * æ¨èç†ç”±
    * ç”¨æˆ·åé¦ˆ
    * å¯¼è´­ã€å¯¹è¯å¼æ¨è

* RecSyså¯¹LLMçš„æŒ‘æˆ˜

  * æ¨ç†æˆæœ¬
    * ç¡¬ä»¶
    * å·¥ç¨‹ï¼šç¼“å­˜ï¼›æŠ•æœºé‡‡æ ·
    * ç®—æ³•ï¼šå¤§æ¨¡å‹+å°æ¨¡å‹+è§„åˆ™ï¼›æ”¾å¼ƒé€šç”¨ï¼›Linear Attn
      * e.g. GPTåšæ•°å­¦é¢˜éå¸¸æ¶ˆè€—tokenï¼ˆCoTï¼‰ï¼Œä½†è°ƒç”¨å·¥å…·å¾ˆè½»é‡
    * UI/UXï¼šå¥½çš„äº§å“è®¾è®¡èƒ½é¿å¼€ç¼ºé™·
      * e.g. chatGPTæµå¼è¾“å‡ºï¼Œå·§å¦™åœ°è®©ç”Ÿæˆé•¿æ–‡çš„è€—æ—¶å¯æ¥å—

* æ¨¡å‹ç»“æ„å°è¯•ï¼š

  * å»¶è¿Ÿäº¤äº’ï¼ˆlate interactionï¼‰
    * æŠŠæµ…å±‚çš„cross attentionå¹²æ‰ï¼Œé«˜å±‚å†è¿›è¡Œtransformerçš„äº¤äº’

![image-20240719191437165](./AI-Algorithms/llm-recsys-2.png)

* æŒ‘æˆ˜ï¼šæ¨¡å‹æ›´æ–° - RecSysæ¯å¤©éƒ½æœ‰æ–°å†…å®¹
  * æ£€ç´¢å¢å¼ºRAGçš„éš¾ç‚¹1ï¼šæ–°çŸ¥è¯†æ˜“æ£€ç´¢ï¼Œæ–°å¸¸è¯†éš¾æ£€ç´¢
    * e.g. æœ‰æ²¡æœ‰è¶…è¿‡GPT-4çš„å¤§æ¨¡å‹
  * RAGçš„éš¾ç‚¹2ï¼šæ£€ç´¢ç®—æ³•çš„ç²¾åº¦ã€LLMçš„é•¿åºåˆ—æ”¯æŒ
  * é¢„æµ‹ï¼š1-2å¹´å†…ä¼šå‡ºç°LLMçš„online learning

![image-20240719191754039](./AI-Algorithms/llm-recsys3.png)





### ç”µå•† LLM4Rec

#### Amazon:  [åŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œæ¨èç³»ç»Ÿæ„å»ºç”µå•†æ™ºèƒ½å¯¼è´­æœºå™¨äºº](https://aws.amazon.com/cn/blogs/china/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system/)

* åŸºäº Amazon SageMakerã€Amazon OpenSearchã€AWS Lambdaã€Amazon Personalize å’Œ Amazon API Gateway ç­‰åŸºç¡€äº‘æœåŠ¡ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ã€å¼€æºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨æ¡†æ¶ langchain ä»¥åŠå¼€æºå‰ç«¯æ¶æ„ Stramlit
* åŠŸèƒ½ï¼šæ™ºèƒ½é—®è¯¢ã€å•†å“æ¨èã€å•†å“ä¸ªæ€§åŒ–è¥é”€æ–‡æ¡ˆ
  * å¤šè½®å¯¹è¯ï¼šæŒ–æ˜ç”¨æˆ·éœ€æ±‚ï¼Œå•†å“çš„å“ç‰Œã€ä»·æ ¼ã€æè´¨ã€ç”¨é€”ã€ä½¿ç”¨åœºæ™¯ç­‰è§’åº¦
* æ¡†æ¶ï¼š
  * dynamodbå­˜å‚¨â€œç”¨æˆ·åŒsessionçš„å¯¹è¯è®°å½•â€ï¼ˆç±»ä¼¼OpenAIçš„threadæ¦‚å¿µï¼‰
* æµ‹è¯•é›†ï¼šhttps://github.com/aws-samples/retail-demo-store
  * 2000 å¤šä¸ªè™šæ‹Ÿå•†å“æ•°æ®ã€6000 å¤šä¸ªè™šæ‹Ÿé¡¾å®¢æ•°æ®å’Œ 2 ä¸‡å¤šæ¡è™šæ‹Ÿäº¤äº’ä¿¡æ¯

![build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system1](./AI-Algorithms/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system1.png)

![img](https://s3.cn-north-1.amazonaws.com.cn/awschinablog/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system3.png)

![img](https://s3.cn-north-1.amazonaws.com.cn/awschinablog/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system7.png)

![img](https://s3.cn-north-1.amazonaws.com.cn/awschinablog/build-an-e-commerce-intelligent-shopping-guide-robot-based-on-large-language-model-and-recommendation-system8.png)

#### é˜¿é‡Œ[LLMåœ¨ç”µå•†æ¨èç³»ç»Ÿçš„æ¢ç´¢ä¸å®è·µ](https://www.53ai.com/news/qianyanjishu/357.html)ã€LLM4RECç»¼è¿°

> LLM+RSã€LLM As RS
>
> åŸºäºLLMçŸ¥è¯†èƒ½åŠ›çš„ç±»ç›®æ­é…æ¨è

* å¯¹æ¯”RecSyså’ŒLLMï¼š
  * å‰è€…æ˜¯ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„ç³»ç»Ÿï¼Œä¾èµ–ç”µå•†IDä½“ç³»æ¥å»ºæ¨¡ç”¨æˆ·æˆ–ç‰©å“ï¼Œç¼ºä¹è¯­ä¹‰å’Œå¤–éƒ¨çŸ¥è¯†ä¿¡æ¯ï¼Œå­˜åœ¨ä¿¡æ¯èŒ§æˆ¿ã€å†·å¯åŠ¨ã€å¤šæ ·æ€§ä¸è¶³ã€æ— æ³•è·¨åŸŸæ¨èç­‰é—®é¢˜ï¼›
  * åè€…ç¼ºä¹æ¨èé¢†åŸŸå†…çš„ä¸“æœ‰æ•°æ®ä¿¡æ¯ï¼Œä¸å…·å¤‡ä¼ ç»Ÿæ¨èæ¨¡å‹çš„åºåˆ—å¤„ç†å’Œè®°å¿†èƒ½åŠ›ï¼ŒåŒæ—¶è®¡ç®—å¤æ‚åº¦é«˜ã€è®­ç»ƒå’Œæ¨ç†æˆæœ¬å¤§ã€‚

* ä¸¤ç§èŒƒå¼ï¼šLLM+RSï¼›LLM as RS
* LLM + RS
  * LLM Embedding: U-BERT[2]å¯¹ç”¨æˆ·è¯„è®ºå†…å®¹è¿›è¡Œç¼–ç æ¥å¢å¼ºç”¨æˆ·çš„ä¸ªæ€§åŒ–å‘é‡è¡¨å¾ï¼Œæœ€ç»ˆå¾—åˆ°ç¨ å¯†çš„embeddingå‘é‡ï¼›UniSRec[3]é€šè¿‡å¯¹å•†å“title/ç”¨æˆ·è¡Œä¸ºåºåˆ—è¿›è¡Œç¼–ç ï¼Œæ¥è¾¾æˆè·¨åŸŸåºåˆ—æ¨èçš„ç›®æ ‡ã€‚
  * LLM Summary:
    * ç”Ÿæˆå¼æ–°é—»æ¨èæ¡†æ¶GENRE[5]
    * GPT4Rec[6]å°†LLMæ¨¡å‹ç”¨äºç”¨æˆ·æ„å›¾ç†è§£ï¼Œæ ¹æ®ç”¨æˆ·çš„è¡Œä¸ºå†å²ï¼Œè¿›è¡Œå…´è¶£æŠ½å–å¹¶ç”Ÿæˆä¸­é—´çš„è¯­ä¹‰queryï¼Œç”¨äºåç»­çš„æ¨èå¬å›ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå…¶ä¸»è¦åˆ†ä¸ºä¸¤æ­¥ï¼šé¦–å…ˆæ ¹æ®ç”¨æˆ·å†å²äº¤äº’çš„å•†å“å’Œå®ƒä»¬å¯¹åº”çš„æ ‡é¢˜ï¼Œé€šè¿‡promptæ ¼å¼åŒ–åï¼Œä½¿ç”¨GPT2æ¥ç”Ÿæˆå¯ä»¥è¡¨å¾ç”¨æˆ·å¤šä¸ªå…´è¶£çš„â€œsearch queryâ€ã€‚ç„¶åå°†GPT2ç”Ÿæˆçš„queryæä¾›ç»™æœç´¢å¼•æ“ï¼Œä»¥æ£€ç´¢è¦æ¨èçš„å•†å“ï¼Œä»è€Œæé«˜æ¨èå¬å›çš„ç›¸å…³æ€§å’Œå¤šæ ·æ€§ã€‚

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkRGJlYzFpYTRhckl1N2ROcVVNNjFNTlhZZm03cU4wbTJtUEo5YWF1aWFxZ1A0TXY1TUJ3MzhkeXcvNjQwP3d4X2ZtdD1wbmc=)

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkaFJpYWljVVZINWJ5eDJpY1hMQzR2R0xXaFdxbkV0TERERFRNb1I2NkVDQ2c0R21XZ2dYb0N3YVlBLzY0MD93eF9mbXQ9cG5n)

* LLM As RS
  * LLM As Ranker
    * æ­¤ç±»å·¥ä½œ[7] [8]å°†æ¨èé—®é¢˜å½¢å¼åŒ–ä¸ºç»™å®šæ¡ä»¶çš„æ’åºä»»åŠ¡ï¼Œå…¶ä¸­ç”¨æˆ·çš„å†å²äº¤äº’ä½œä¸ºæ¡ä»¶ï¼Œæ¨èç³»ç»Ÿå¬å›å¾—åˆ°çš„å•†å“ä½œä¸ºå€™é€‰ã€‚é€šè¿‡è®¾è®¡åˆé€‚çš„promptæ¨¡ç‰ˆï¼Œç»“åˆæ¡ä»¶ã€å€™é€‰ã€æ’åºæŒ‡ä»¤ï¼Œä½¿å¾—LLMä¸ºå€™é€‰çš„å•†å“è¿›è¡Œæ‰“åˆ†æˆ–è€…æ’åºã€‚
    * å®éªŒè¯æ˜ï¼ŒLLMåœ¨Zero-Shotåœºæ™¯å…·æœ‰è¾ƒå¥½çš„é›¶æ ·æœ¬æ’åºèƒ½åŠ›ï¼Œä½†åœ¨æ’åºæ—¶ä¸å¯é¿å…åœ°æœ‰position biaså’Œpopularity biasé—®é¢˜ã€‚

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkNzI3aWFxb1ZXZnBNZHN3SWVmU2ljWjF2SGpVMlU3dk5nSjFFWUhNbjNpY1BTZVZqaWFUakVWZ3NkZy82NDA/d3hfZm10PXBuZw==)



* ç®—æ³•æ–¹æ¡ˆï¼šå—é™äºLLMæ¨¡å‹æå¤§çš„æ¨ç†è€—æ—¶ï¼Œæ— æ³•æ»¡è¶³åœ¨çº¿æ¨èç³»ç»Ÿæ¯«ç§’çº§çš„æ—¶å»¶é™åˆ¶ï¼ŒçŸ­æœŸå†…ä¸å…·å¤‡å°†LLMæ¨¡å‹ç”¨äºåœ¨çº¿æ¨ç†çš„æ¡ä»¶ã€‚äºæ˜¯æˆ‘ä»¬æ›´å¤šåœ°é‡‡ç”¨"LLM + æ¨è"çš„æ–¹å¼ï¼Œå»åˆ©ç”¨å¤§æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæé«˜æ¨èæ¨¡å‹å¯¹å•†å“ä¿¡æ¯ã€ä¸Šä¸‹æ–‡ã€ç”¨æˆ·è¡Œä¸ºåºåˆ—çš„çŸ¥è¯†è¡¨è¾¾ï¼ŒåŒ…æ‹¬ï¼š
  * å€ŸåŠ©LLMé€šç”¨çŸ¥è¯†ä¿¡æ¯ï¼Œæ„å»ºç±»ç›®æ­é…ä½“ç³»ï¼Œå¼•å…¥æ¨èç³»ç»Ÿåœ¨æ¨èå¬å›ä¾§å¼•å…¥æ­é…I2Iã€æ’åºä¾§è¿›è¡Œç±»ç›®å…´è¶£æ‰©å±•å»ºæ¨¡ï¼Œæé«˜æ¨èçš„å¤šæ ·æ€§ã€‚
  * å€ŸåŠ©LLMæ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œè¾…åŠ©å•†å“/ç”¨æˆ·ç†è§£ã€‚
    * æˆ‘ä»¬ä½¿ç”¨LLMå°†ç”µå•†Itemå†—ä½™æ–‡æœ¬ä¿¡æ¯è¿›è¡Œå»å™ªæçº¯å’Œæ”¹å†™ï¼›
    * ç»“åˆç”¨æˆ·è¡Œä¸ºåºåˆ—ã€ä¸Šä¸‹æ–‡ä»¥åŠç”¨æˆ·ç”»åƒï¼Œè¿›è¡Œç”¨æˆ·è¡Œä¸ºsumarryæ€»ç»“ã€‚å¹¶é€šè¿‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå°†æ–‡æœ¬çŸ¥è¯†ç»“æœè¿›è¡Œembeddingå‘é‡åŒ–è¡¨å¾ï¼Œä¸ä¼ ç»Ÿçš„æ¨èæ¨¡å‹è¿›è¡ŒçŸ¥è¯†æ„ŸçŸ¥åµŒå…¥ï¼Œæé«˜æ¨¡å‹çš„çŸ¥è¯†è¡¨è¾¾ã€‚

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkSk1icEM1aWJER1FhUjdBN29udG5aZVhyTkt6T0hoSUgxQjJ3ZUFWTjJJTDhKdTE3NXk4NHRLdy82NDA/d3hfZm10PXBuZw==)

* åŸºäºLLMçŸ¥è¯†èƒ½åŠ›çš„ç±»ç›®æ­é…æ¨è
  * ç»è¿‡å¤šå¹´çš„æ²‰æ·€ï¼Œç”µå•†å¹³å°å·²ç»æ‹¥æœ‰äº†ä¸€å¥—å®Œæ•´çš„ç±»ç›®ä½“ç³»ã€‚è¿™å¥—ç±»ç›®ä½“ç³»é€šå¸¸é‡‡ç”¨æ ‘çŠ¶ç»“æ„ï¼Œé€šè¿‡å±‚å±‚æ‹†è§£ï¼Œæœ€ç»ˆå°†ä¸€ä¸ªå•†å“æ˜ å°„åˆ°ä¸€ä¸ªæœ«çº§ç±»ç›®ï¼Œä¸åŒæœ«çº§ç±»ç›®ä¹‹é—´ç›¸å¯¹ç‹¬ç«‹ã€‚ç°æœ‰çš„ç±»ç›®ä½“ç³»æ— æ³•ä½“ç°å‡ºè¿™ç±»ç›®ä¹‹é—´å­˜åœ¨çš„æ­é…ä¿¡æ¯ï¼Œç¼ºä¹è·¨ç±»ç›®çš„æ­é…å…³ç³»è¡¨è¾¾ã€‚
  * åŒæ—¶ï¼Œç›¸è¾ƒäºå“ç‰Œå’Œå–å®¶ï¼Œç±»ç›®å¯ä»¥æ›´åŠ æ˜¾å¼åœ°ä¸ç”¨æˆ·å…´è¶£è¿›è¡Œèšåˆå’Œæ˜ å°„ã€‚åœ¨æ¨èåœºæ™¯ä¹‹ä¸­ï¼Œç»™ç”¨æˆ·å‡†ç¡®åœ°æ¨èç›¸å…³å•†å“çš„åŒæ—¶ï¼Œå¦‚æœèƒ½å¤ŸæŒ–æ˜ä¸åŒå…´è¶£ä¹‹é—´çš„éšè—å…³ç³»ï¼ŒåŸºäºæ­é…è¿›è¡Œå‘æ•£æ¨èï¼Œå°†ç»™ç”¨æˆ·å¸¦æ¥æ–°çš„æƒŠå–œæ„Ÿã€å®ç°ç”¨æˆ·éœ€æ±‚å’Œå…´è¶£çš„æ‰©å±•ã€‚
  * ç±»ç›®ä½“ç³»ï¼šä¼‘é—²è£¤å’Œè¡¬è¡«åˆ†åˆ«å±äºä¸€çº§ç±»ç›®ï¼ˆç”·è£…ï¼‰ä¸‹é¢çš„ä¸åŒäºŒçº§ç±»ç›®ï¼Œè€Œç”·å¸†å¸ƒé‹åˆæŒ‚è½½åœ¨å¦ä¸€ä¸ªä¸€çº§ç±»ç›®ï¼ˆæµè¡Œç”·é‹ï¼‰ä¸Š
  * ä¼ ç»Ÿçš„ç±»ç›®å…³ç³»æŒ–æ˜å¾€å¾€åŸºäºçŸ¥è¯†å›¾è°±ï¼Œé‡‡ç”¨è·ç¦»åº¦é‡ã€èšç±»ã€è¡Œä¸šè§„åˆ™ã€ååŒè¿‡æ»¤ç­‰æ–¹æ³•ã€‚è¿™äº›å·¥ä½œå¤§éƒ½éœ€è¦ç¹æ‚çš„æ•°æ®æ¸…æ´—ã€ç®—æ³•æŒ–æ˜å’Œè¡Œä¸šä¸“å®¶çŸ¥è¯†ã€‚LLMå¤§æ¨¡å‹çš„å‡ºç°ï¼Œè®©å¿«é€Ÿã€é«˜æ•ˆçš„ç”µå•†çŸ¥è¯†æ„å»ºå˜æˆäº†ç°å®ã€‚
  * Prompt:"1.ç”¨é€—å·åˆ†éš”,2.è¿”å›æ ¼å¼ä¸º'''ç±»ç›®1,ç±»ç›®2,ç±»ç›®3...''',3.ä¸åŒ…å«ã€cate_nameã€‘è¿™ä¸ªè¯,4.æ­é…ç±»ç›®ä¸°å¯Œ"
  * ç«™å†…ç±»ç›®IDæ˜ å°„ï¼šç”±äºLLMæ¨¡å‹è¿”å›çš„æ˜¯é€šç”¨çŸ¥è¯†ä¿¡æ¯ï¼Œå­˜åœ¨ä¸ç«™å†…çš„ç±»ç›®ä½“ç³»æ— æ³•å®Œå…¨å¯¹åº”çš„æƒ…å†µã€‚ä¸ºäº†ä¾¿äºåç»­æ¨èå„ä¸ªæ¨¡å—ä½¿ç”¨ï¼Œå…¼å®¹ç°æœ‰çš„ç”µå•†æ¨èé“¾è·¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†LLMæ­é…ç±»ç›®æ˜ å°„æˆç«™å†…ç±»ç›®IDã€‚ç«™å†…ç±»ç›®IDæ˜ å°„å¯ä»¥é‡‡ç”¨ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ï¼š
    * åŸºäºæ–‡æœ¬ç›¸å…³æ€§çš„å‘é‡å¬å›ã€‚å°†LLMæ­é…ç±»ç›®å’Œç«™å†…ç±»ç›®åˆ†åˆ«è¡¨å¾æˆæ–‡æœ¬embeddingå‘é‡ï¼Œç„¶åé€šè¿‡å‘é‡å¬å›çš„æ–¹å¼ï¼Œé€‰å–ä¸LLMæ­é…ç±»ç›®è·ç¦»ç©ºé—´æœ€è¿‘çš„topç«™å†…ç±»ç›®è¿›è¡Œæ˜ å°„ã€‚
    * åŸºäºç«™å†…åéªŒç»Ÿè®¡çš„query2cateæ˜ å°„ã€‚å°†æ­é…ç±»ç›®ä½œä¸ºqueryï¼Œæ ¹æ®ç”µå•†å¹³å°æœç´¢query2cateçš„ç»Ÿè®¡æ•°æ®ï¼Œä½¿ç”¨è¯¥queryä¸‹topçš„ç‚¹å‡»cateä½œä¸ºæ˜ å°„ç±»ç›®ï¼Œå®ç°LLMæ­é…åˆ°ç«™å†…IDçš„æ˜ å°„ã€‚
  * ç²¾æ’å…´è¶£æ‰©å±•

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkeWlhR2lhTlE3QzVVOWVkSGlhaE1EY0NOaWNWUTV6cUZQUTVrYWpZaWNoc2lhVU5KSXZKd1h5MUtKaWNhZy82NDA/d3hfZm10PXBuZw==)

![img](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=78909&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zM1AyRmRBbmp1aWNIaGVlM1hJQjFNZXNjZm84dGljdFhkVkIyMmVSWDJ2MjZzcEVub0JlWUI4Y0NIZ0x6eFFHRWxsQjZJSjgybGhzeW1OWTlmazdlQ0p3LzY0MD93eF9mbXQ9cG5n)



* **åŸºäºLLMæ–‡æœ¬èƒ½åŠ›çš„å•†å“è¯­ä¹‰è¡¨å¾**
  * å¯¹äºå•†å“ç±»ç›®ä»¥åŠå±æ€§ä¿¡æ¯ï¼Œé€šå¸¸å°†å…¶é€šè¿‡multi-hotçš„æ–¹å¼è¿›è¡Œç¼–ç è½¬åŒ–æˆç‰¹å¾å‘é‡ã€‚
    * å®¹æ˜“äº§ç”Ÿæ•°æ®ç¨€ç–é—®é¢˜ã€‚
  * å•†å“æ ‡é¢˜è¯­ä¹‰ä¸Šå¹¶ä¸è¿è´¯ï¼Œä¿¡æ¯å‡Œä¹±ï¼ˆåŒ…æ‹¬â€œçˆ†æ¬¾â€ã€â€œç‰¹ä»·â€ç­‰ï¼‰ï¼Œç›´æ¥è¿›è¡Œmutli-hotæˆ–è€…æ–‡æœ¬ç¼–ç éš¾ä»¥å¾—åˆ°å¾ˆå¥½çš„åµŒå…¥è¡¨ç¤ºã€‚
  * ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯å°†å¯¹å•†å“é›¶æ•£çš„ä¿¡æ¯è½¬æ¢æˆè¯­ä¹‰è¿è´¯çš„æ–‡æœ¬ï¼Œç„¶åé€šè¿‡pre-trainè¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ã€‚å¯¹æ­¤ï¼Œæˆ‘ä»¬å€ŸåŠ©LLMè•´å«çš„å¼ºå¤§çš„è¯­è¨€è¡¨è¾¾èƒ½åŠ›å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ä»å•†å“æ ‡é¢˜ä¸­æŠ½å–å‡ºå…³é”®ä¿¡æ¯ï¼Œä»è€Œå®ç°å¯¹å•†å“æ ‡é¢˜çš„æ­£åˆ™åŒ–ï¼Œå¾—åˆ°è¯­ä¹‰è¿è´¯çš„æ–‡æœ¬æè¿°ï¼Œå†å¯¹å…¶è¿›è¡Œç¼–ç ï¼Œä»è€Œä¸°å¯Œå•†å“çš„ç‰¹å¾ã€‚
  * Promptï¼šä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¹°å®¶ã€‚ç»™å®šå•†å“çš„æè¿°è¯ã€Aã€‘ä»¥åŠå„ç§å±æ€§ã€Bã€‘ï¼Œè¯·æ ¹æ®å…³é”®è¯å’Œå…³é”®å±æ€§æè¿°å‡ºå•†å“æ˜¯ä»€ä¹ˆã€‚è¦æ±‚æ˜¯åªéœ€è¦å›ç­”æ˜¯ä»€ä¹ˆï¼Œä¸è¦è¡¥å……å…¶ä»–å†…å®¹ï¼Œå°½é‡ä»Aå’ŒBä¸­é€‰å‡ºè¯è¯­è¿›è¡Œæè¿°ï¼Œå­—æ•°ä¸è¶…è¿‡40ï¼Œå›ç­”æ¨¡ç‰ˆä¸º:è¿™ä¸ªå•†å“æ˜¯...ã€‚æ¯”å¦‚å½“A=['giyo', 'å…¬è·¯', 'å±±åœ°è½¦', 'ä¸“ç”¨', 'è‡ªè¡Œè½¦', 'å•è½¦', 'ä¸“ä¸š', 'éª‘è¡Œ', 'æ‰‹å¥—', 'åŠæŒ‡', 'å¤å­£', 'ç”·', 'ç¡…èƒ¶', 'å‡éœ‡', 'å¥³']ï¼ŒB=['å°ºç ': 'XXL', 'ç±»ç›®': 'è‡ªè¡Œè½¦æ‰‹å¥—', 'é€‚ç”¨å¯¹è±¡': 'é€šç”¨', 'é¢œè‰²åˆ†ç±»': 'å¼§å…‰åŠæŒ‡-é»„è‰²-åŒé¢é€æ°”+GELç¡…èƒ¶+åŠ²åšæŒå«', 'ä¸Šå¸‚æ—¶é—´': '2016å¹´å¤å­£', 'è´§å·': '1183', 'å“ç‰Œ': 'GIYO/é›†ä¼˜', 'æ¬¾å¼': 'åŠæŒ‡æ‰‹å¥—']ï¼Œè¾“å‡ºï¼šè¿™ä¸ªå•†å“æ˜¯GIYOç‰Œçš„è‡ªè¡Œè½¦åŠæŒ‡æ‰‹å¥—ã€‚ç°åœ¨A=...,B=...
  * æŒ‡æ ‡ï¼šå¹³å‡å›°æƒ‘åº¦ https://zhuanlan.zhihu.com/p/114432097

* å•†å“è¯­ä¹‰å‘é‡-å¼•å…¥æ’åºæ¨¡å‹ï¼šä»…ä»…æ˜¯åŠ ç‰¹å¾
  * å€ŸåŠ©Modelscopeçš„CoROMæ¨¡å‹[15]ï¼Œæˆ‘ä»¬å¯¹æ­£åˆ™åŒ–åçš„å•†å“æ ‡é¢˜æ–‡æœ¬è¿›è¡Œäº†å‘é‡åŒ–æŠ½å–ï¼Œå¹¶ä½œä¸ºç‰¹å¾åŠ å…¥åŸºäºåŒå¡”ç»“æ„çš„DSSMç²—æ’æ¨¡å‹ä¸­[16]
    * https://www.modelscope.cn/models/damo/nlp_corom_sentence-embedding_chinese-base-ecom/summary
  * ç‰¹å¾é™ç»´æ–¹å¼æ˜¯BERT-whitening[18]

* æ›´å¤šæ–¹å‘ï¼š
  * å¤šæ¨¡æ€æ¨èï¼šåˆ©ç”¨å¤šæ¨¡æ€LLMå¤§æ¨¡å‹çš„å¤šæ¨¡æ€ä¿¡æ¯æŠ½å–å’Œè¡¨å¾èƒ½åŠ›ï¼Œæå–åŒ…æ‹¬å›¾ç‰‡ã€æ–‡æœ¬ã€è§†é¢‘å…³é”®å¸§ï¼Œè§†é¢‘è¯­éŸ³æ–‡å­—ç­‰ä¸åŒæ¨¡æ€çš„è¯­ä¹‰åŒ–ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ç¦»çº¿ç‰¹å¾å·¥ç¨‹è¿›è¡Œè¡¨å¾ï¼Œä½¿çº¿ä¸Šæ¨èæ¨¡å‹èƒ½å¤ŸçœŸæ­£å®Œæ•´åœ°æ„ŸçŸ¥åˆ°å„ç§ç”µå•†æ¨¡æ€ä¿¡æ¯ï¼Œå¹¶å®ç°å¯¹ç”¨æˆ·ä¸åŒä¿¡æ¯åå¥½å’Œæ„å›¾çš„ç†è§£ã€‚
  * LLMæ¨ç†åŠ é€Ÿï¼šç°é˜¶æ®µLLMå­˜åœ¨æ¨ç†æ—¶å»¶è¿‡é«˜çš„é—®é¢˜ï¼Œæ— æ³•æ»¡è¶³æ¨èç³»ç»Ÿæ•°åmsçº§åˆ«çš„rtè¦æ±‚ï¼Œæˆ‘ä»¬çš„LLMæ¢ç´¢ä¹Ÿæ­¢æ­¥äºç¦»çº¿ç‰¹å¾ç¼–ç é˜¶æ®µã€‚åç»­è€ƒè™‘é€šè¿‡è’¸é¦ã€å‰ªæã€é‡åŒ–ç­‰æ‰‹æ®µï¼Œç”¨ä¸€ä¸ªå°æ¨¡å‹è’¸é¦å‡ºLLMçš„éƒ¨åˆ†èƒ½åŠ›ï¼Œä»è€Œé™ä½æ¨ç†çš„å¤æ‚æ€§ï¼Œä½¿å…¶èƒ½çº¿ä¸Šservingã€‚
  * LLM as é‡æ’: åˆ©ç”¨LLMä¸°å¯Œçš„çŸ¥è¯†é¢†åŸŸæ‰©å±•èƒ½åŠ›ï¼Œåœ¨å•†å“å·²æœ‰ä¸°å¯Œçš„è¯­ä¹‰æ ‡ç­¾åŸºç¡€ä¸Šï¼Œç»“åˆç”¨æˆ·å†å²äº¤äº’å…´è¶£ã€é€‰æ‹©åå¥½ã€åºåˆ—ç”Ÿæˆè§„åˆ™ å’Œ prompt templateä¸ºç”¨æˆ·ä»topæ’åºé›†åˆä¸­é€‰å–åˆé€‚çš„å•†å“æˆ–è¯é¢˜ï¼Œç”Ÿæˆæ¨èåˆ—è¡¨ã€‚

#### é˜¿é‡Œäº‘-æ–½å…´-æ¨èæœç´¢æŠ€æœ¯çš„å‰æ²¿æ¢ç´¢

> https://github.com/alibaba/EasyRec/

![image-20241007223126666](./AI-Algorithms/pairec.png)

![image-20241007223250405](./AI-Algorithms/ali-ai.png)

![image-20241007223648967](./AI-Algorithms/easyrec.png)



![image-20241007223838777](./AI-Algorithms/pairec-opt.png)

![image-20241007224303869](./AI-Algorithms/ali-query-rewrite.png)

### é€šç”¨ LLM4Rec

> https://github.com/CHIANGEL/Awesome-LLM-for-RecSys
>
> https://github.com/WLiK/LLM4Rec-Awesome-Papers

#### Literature Review

* LLMå¢å¼ºæ•°æ® [hllm]
  * (Zhang et al. 2024a; Ren et al. 2024;
    Xi et al. 2023), such as summary of user behavior and item
    information expansion.
  * RLMRec (Ren et al. 2024) develops a user/item profiling paradigm em-
    powered by LLMs, and aligns the semantic space of LLMs
    with the representation space of collaborative relational sig-
    nals through a cross-view alignment framework.
  * LLMs are also employed to generate augmented training signals for
    coldstart items (Wang et al. 2024)

* LLMs as either feature encoders [9â€“24] [star]

  * ç›´æ¥ä½¿ç”¨
    * star
    * [15]
  * mapping continuous LLM
    embeddings into discrete tokens using vector quantization and
    training a subsequent generative model [12, 13, 21, 22];
  * training sequential models by initializing the embedding layer with
    LLM embeddings [9, 14, 24];
  * training models to directly compute the relevance between item and user embeddings (i.e., embeddings of user selected items) [10, 11, 16â€“20, 23].

* LLM as scoring and ranking functions [25â€“31]. [star]

  * generative selection prompting, instructing the LLM to choose the top k items in ranked order from a set of candidates [25, 27, 28]
  * lag behind the performance of fine-tuned models due to a lack of collaborative knowledge
  * fine-tuning the models with interaction data, though this approach is also costly [40â€“45].

* LLM as a Ranker for Information Retrieval.[star]

  * ä¼˜åŠ¿ï¼š
    * æ„å»ºç®€å•
    * é¡ºä¾¿å¾—åˆ°æ¨èè§£é‡Š
    * å°‘é‡äº¤äº’é‡çš„æ•°æ®ä¸‹ï¼Œæ•ˆæœå¥½äºä¼ ç»Ÿæ¨¡å‹
  * åŠ£åŠ¿ï¼š
    * å»¶æ—¶é«˜
    * æ­£å¸¸äº¤äº’é‡çš„æ•°æ®ä¸‹ï¼Œæ•ˆæœä¸€èˆ¬
    * æ•ˆæœæå‡å¾ˆéš¾
  * point-wise: LLMs directly evaluate relevance using numerical scores or binary judgments [48, 49]
    * capturing the relative importance of passages
  * pair-wise: LLMs express preferences between item pairs
    * effective but inefficient due to the high number ofcalls required [50]
  * List-wise: LLMs compare multiple passages simultaneously [51],
    * performance heavily relies on the modelâ€™s semantic prior and
      reasoning capabilities [50]
  * adapt the recommendation domain data into conversational
    formats (Bao et al. 2023; Friedman et al. 2023; Zhang
    et al. 2023; Yang et al. 2023; Zhai et al. 2023). [HLLM]

* LLMæ¥å—ID Featureä½œä¸ºè¾“å…¥ï¼Œå¹¶å»ºæ¨¡ [HLLM]

  * æ”¹è¿›å¤„ç†æ–‡æœ¬è¡Œä¸ºåºåˆ—è€—æ—¶é•¿çš„é—®é¢˜
  * LLaRA (Liao et al.2024) proposed a novel hybrid prompting method that inte-
    grates ID-based item embeddings with textual item features.
  * SFT mainly enhances instruction-following abilities, which
    do not aid in recommendation tasks (Zhou et al. 2024)

  * Ning et al. 2024; Zhai et al. 2024;

#### STAR: A Simple Training-free Approach for Recommendations using Large Language Models

![image-20241225184836438](./AI-Algorithms/image-20241225184836438.png)

* Intro
  * ![image-20241225204332446](./AI-Algorithms/image-20241225204332446.png)
* ç»“è®ºï¼š
  * å¬å›æ•ˆæœå¾ˆå¥½
    * â€œå¤´é‡è„šè½»â€çš„å¬å›æ’åºæ¶æ„
    * è¯­ä¹‰æ¯”CFé‡è¦
    * recency decay 0.7
    * length=3ï¼Œåªèƒ½å»ºæ¨¡çŸ­æ—¶åºåˆ—
  * LLMåšpairwiseæ’åºï¼Œèƒ½æå‡æ•ˆæœ
    * Table 5: ç›¸æ¯”ä»¥å¾€çš„Né€‰Mä»»åŠ¡ï¼Œwindow-basedæ’åºä»»åŠ¡é™ä½äº†éš¾åº¦ï¼Œæ•ˆæœæ›´å¥½
    * window size=4ï¼Œstride=2ï¼Œå‚ä¸æ’åºçš„recall len=20
    * Table 6: æ’åºpromptä¸­ï¼Œpopularity, co-occurrenceçš„ä½œç”¨ï¼Œçƒ­åº¦ä¿¡æ¯æ²¡ç”¨
      * previous research indicating that simple popularity bias
        is ineffective in addressing recommendation problems [60â€“62].
    * Table 7: LLMæ¨¡å‹èƒ½åŠ›å¯¹pairwiseæ’åºä»»åŠ¡çš„æ•ˆæœå½±å“æœ‰é™
  * collaborative informationåœ¨å¬å›å’Œæ’åºä¸­å¾ˆé‡è¦
  * æ¯”è¾ƒæœ‰è¶£çš„ç»“æœï¼šä¸è€ƒè™‘ratingï¼Œæ•ˆæœæ›´å¥½ï¼ŒåŸå› æ˜¯ç›®æ ‡æ˜¯ctrï¼Œä¸è€ƒè™‘ratingçš„åéªŒ

![image-20241225210031783](./AI-Algorithms/image-20241225210031783.png)

![image-20241226015142625](./AI-Algorithms/image-20241226015142625.png)

![image-20241226014245937](./AI-Algorithms/image-20241226014245937.png)

* æ¯”Avg Poolingå†å¬å›å¼ºå¾ˆå¤šï¼šåŸå› æ˜¯æ›´ç»†è…»çš„äº¤äº’ï¼Œä¸ä¸¢å¤±ä¿¡æ¯ï¼Œæœ¬è´¨ä¸Šå·²ç»æ˜¯ä¸€ä¸ªç­–ç•¥æ’åºæ¨¡å‹äº†ã€‚
  * å¯å‘ï¼šç”¨çŸ­åºåˆ—ä¸­çš„æ¯ä¸ªItemåšå¬å›ï¼Œæ¯ä¸ªHistory Itemå¯ä»¥ä½œä¸ºä¸€è·¯å¬å›ï¼Œä¸‰è·¯Merge

![image-20241226020026743](./AI-Algorithms/image-20241226020026743.png)

* å¬å›

  * using a combination of semantic similarity and collaborative
    commonality to the items in a userâ€™s history.
    * a temporal factor gives priority to userâ€™s recent interactions

  * Semantic relationshipï¼š
    * å…¥åº“ï¼šWe construct a prompt based on
      the item information and metadata, including the title, description,
      category, brand, sales ranking, and price.
  * Collaborative relationshipï¼š
    * ItemCF
  * èåˆï¼š
    * æ—¶é—´è¡°å‡ã€ratingï¼ˆè¡Œä¸ºç­‰çº§ï¼‰
    * ![image-20241226012231967](./AI-Algorithms/image-20241226012231967.png)

* æ’åºï¼šLLM as Ranker

  * pointwise
  * pairwise
    * ä»ä¸‹å¾€ä¸Š
  * listwise
    * a window size ğ‘¤ and a stride d
  * ç‰¹å¾ï¼š
    * Popularity
    * Co-occurence

![image-20241226012908127](./AI-Algorithms/image-20241226012908127.png)

* Prompt:

```
Analyze the userâ€™s purchase history to identify user preferences and purchase patterns. Then, rank the 4 items above based on their alignment
with the userâ€™s preferences and other contextual factors. All the items should be included and listed using identifiers, in descending order of the userâ€™s
preference. The most preferred recommendation item should be listed first. The output format should be [] > [], where each [] is an identifier, e.g., [1] >
[2]. Only respond with the ranking results, do not say any word or explain. Output in the following JSON format:
{
"rank": "[] > [] .. > []"
}
```



* Evaluation
  * å€’æ•°ç¬¬ä¸€ä¸ªï¼štestï¼› å€’æ•°ç¬¬äºŒä¸ªï¼švalidation

#### [LLMRec] Is ChatGPT a Good Recommender ? A Preliminary Study

> https://github.com/williamliujl/LLMRec

* Intro
  * taobaoçš„å°è¯•ï¼ŒPretrained Modelåšæ¨è
    * M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems.
    * Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5)
* å®éªŒç»“è®ºï¼š
  * ã€Œæœªç»finetuneçš„ChatGPTã€ performs well in rating prediction but poorly in sequential and direct recommendation tasks, achieving only similar performance levels to early
    baseline methods on certain metrics.
  * äººå·¥è¯„ä¼°ç»“æœï¼ŒExplanation Generationã€Review Sumarizationæ•ˆæœè¾ƒå¥½
* æ¶æ„ï¼š
  * different prompts are constructed based on the specific characteristics of the recommendation tasks (Section 3.1)
  * these prompts are used as inputs for ChatGPT, which generates the recommendation results according to the requirements specified in the prompts
  * the output from ChatGPT is checked and refined by the refinement module, and the refined results are returned to the user as the final recommendation results (Section 3.2).
    * æ£€æŸ¥gptçš„è¾“å‡ºæ˜¯å¦ç¬¦åˆæ ¼å¼
    * å¦‚æœè¾“å‡ºitemå’Œitem poolä¸åŒ¹é…ï¼Œåˆ™ç”¨BERTåšç›¸ä¼¼åº¦åŒ¹é…

![image-20241003193718138](./AI-Algorithms/llmrec.png)

* äº”ç§task
  * Rating Prediction
  * Sequential Recommendation
  * Direct Recommendation
  * Explanation Generation
  * Review Sumarization
* å®éªŒè®¾ç½®ï¼š
  * 10 itemsã€3 shotsã€gpt-3.5-turbo
  * direct recï¼š99è´Ÿä¾‹ã€1æ­£ä¾‹
  * æŒ‡æ ‡ï¼štop-k Hit Ratio (HR@k), top-k Normalized Discounted Cumulative Gain (NDCG@k)
* å…¶å®ƒï¼š
  * Figure2æä¾›äº†é’ˆå¯¹ä¸åŒrecommendation taskçš„ä¸€äº›prompt
* ç»“æœï¼š
  * rating predictæ•ˆæœè¿˜è¡Œ
  * sequential predictæ•ˆæœä¸å¥½ï¼š
    * focus more on semantic similarity rather than the transition relationships between items,
    * æ— æ³•æŠŠå€™é€‰éƒ½è¾“å…¥promptï¼Œè¾“å‡ºäº†å‡æ•°æ®
  * direct rec:
    * gptæœ‰biasï¼Œæ›´å®¹æ˜“æ¨èpromptä¸­æ’åœ¨å‰é¢å’Œåé¢çš„item

![image-20241003202813843](./AI-Algorithms/llmrec1.png)

#### GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation

> * Beam Searchç”Ÿæˆå¤šqueryï¼šä¼¼ä¹å¯è¢«LLMèƒ½åŠ›æ›¿ä»£

* Intro
  * we present GPT4Rec, a novel and flexible generative framework inspired by search engines.
    It first generates hypothetical "search queries" given item titles in a userâ€™s history, and then retrieves items for recommendation by searching these queries.
  * a multi-query generation technique with beam search.

![image-20241005210152630](./AI-Algorithms/gpt4rec.png)

* æ¶æ„
  * GPT4Rec formats the item titles with a prompt and uses a generative language model
    to learn both item and user embeddings in the language space.
    The model then generates multiple queries that represent userâ€™s
    interests, which will be fed to a search engine to retrieve items
    for recommendation.
  * prompt: "Previously, the customer has bought: <ITEM TITLE 1>. <ITEM TITLE 2>... In the future, the customer wants to buy"
  * beam search
  * BM25 matching score function [20], as it is one of the most widely used baseline search engines that accounts for the term frequency saturation and the document length with two corresponding parameters
  * **multi generationçš„ç®—æ³•**
    * ![image-20241005215520146](./AI-Algorithms/multi-generation.png)
* è®­ç»ƒç»†èŠ‚
  * åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ€æƒ³ï¼ŒT-1ä¸ªæ¨æµ‹ç¬¬Tä¸ª
  * å…ˆè®­ç»ƒå¥½backboneï¼Œå†è°ƒBM25çš„å‚æ•°
  * ![image-20241005220901898](./AI-Algorithms/image-20241005220901898.png)



* ç»“è®º
  * The comparison with baseline methods suggests that both item
    content information and modern language modeling are key ingredients for achieving superior performance. One the one hand, while BERT4Rec has the best performance among the baseline methods by leveraging modern language modeling techniques, it fails to fully utilize the item content information by treating items as IDs. On the other hand, ContentRecâ€™s use of item content information with bag- of-words embeddings and mean-pooling modeling is insufficient for achieving comparable performance.
  * In particular, generating K queries and retriev- ing one item per query yields the best performance of Recall@K. This finding suggests that each query contains enough detail to re- trieve a relevant item.
* å®šæ€§åˆ†æçš„è§’åº¦
  * diversityï¼šå…´è¶£æ¨¡ç³Šæ—¶ï¼Œæ¨é€æ–°ç±»åˆ«
  * coverageï¼šå…´è¶£å›ºå®šæ—¶ï¼Œæ¨é€å›ºå®šç±»åˆ«

### Evaluation

> æœ‰è¯„ä¼°ä»£ç çš„å¼€æºä»“åº“ï¼š
>
> https://github.com/bytedance/HLLM

#### Amazon Book Review

https://arxiv.org/pdf/2403.03952

https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023

#### PixelRec

https://github.com/westlake-repl/PixelRec

## LLM4Search

### äº§å“é€»è¾‘

#### Intro

* AI æœç´¢æ˜¯å¯¹ä¸€ä¸ªä¼ ç»Ÿäº’è”ç½‘æ€æ‰‹çº§åº”ç”¨æŠ€æœ¯çš„å¼ºå¤§é‡å¡‘ã€‚äº’è”ç½‘æœç´¢æ˜¯ä¸€ç§åŸºäºç½‘ç»œç´¢å¼•çš„å¯¼èˆªæŠ€æœ¯ã€‚AI æœç´¢åˆ™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æŠ€æœ¯ï¼Œèƒ½å¤Ÿé˜…è¯»å¹¶ä»è¯­ä¹‰å±‚é¢ç†è§£çŸ¥è¯†ã€‚å¯¹ç™½é¢†å·¥ä½œè€…æ¥è¯´ï¼Œè¿™å°†æ˜¯å·¨å¤§çš„ç¦éŸ³ã€‚AI æœç´¢å¯èƒ½ä¼šä½¿å½“å‰è¿™ä¸ªä¸€ç»Ÿå¤©ä¸‹çš„å¸‚åœºå‡ºç°åˆ†åŒ–ã€‚
* æˆ‘ä»¬å¯ä»¥æƒ³è±¡è¿™æ ·ä¸€ä¸ªä¸–ç•Œï¼šæ¯ä¸ªä¸“ä¸šéƒ½æœ‰è‡ªå·±ä¸“é—¨çš„ AI æœç´¢å¼•æ“â€”â€”
  * åˆ†æå¸ˆå’ŒæŠ•èµ„è€…é»˜è®¤ä½¿ç”¨ Perplexity
  * å¾‹å¸ˆä¼šä½¿ç”¨ Harvey è¿™æ ·çš„å¹³å°
  * åŒ»ç”Ÿåˆ™ä¼šä½¿ç”¨ OpenEvidence è¿™æ ·çš„è§£å†³æ–¹æ¡ˆã€‚
  * å¾ªç€è¿™ä¸ªæ€è·¯ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ Midjourney çœ‹ä½œæ˜¯å¯¹â€œåƒç´ å®‡å®™â€çš„æœç´¢
  * Github Copilot æ˜¯å¯¹â€œä»£ç å®‡å®™â€çš„æœç´¢
  * è€Œ Glean åˆ™æ˜¯å¯¹â€œæ–‡æ¡£å®‡å®™â€çš„æœç´¢
  * Agent Universe æ˜¯å¯¹â€œAgent æ™ºèƒ½ä½“â€çš„æœç´¢ã€‚
* ä¸ä¼ ç»Ÿæœç´¢ä¸åŒï¼ŒAI æœç´¢å¯ä»¥åœ¨è¯­ä¹‰å±‚é¢æ·±å…¥å¾—å¤šï¼Œå› æ­¤å…¶åŠŸèƒ½å¼ºå¤§ç¨‹åº¦è¦é«˜å‡ºä¸€ä¸ªæ•°é‡çº§ï¼Œå¸¦æ¥æ˜¾è‘—çš„å¢é‡ç”Ÿäº§åŠ›æå‡ã€‚æ–‡æœ¬å“åº”ä½œä¸ºä¸€ä¸ªäº§å“è¡¨ç°å½¢å¼ï¼Œå…¶æ·±åº¦è¶…å‡ºäº†è¡¨é¢æ‰€è§ã€‚å¹¶éæ‰€æœ‰çš„æ–‡æœ¬å“åº”éƒ½æ˜¯ä¸€æ ·çš„ã€‚æˆ‘ä»¬è®¤ä¸ºå¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªç»´åº¦å®ç°çœŸæ­£çš„äº§å“å·®å¼‚åŒ–ï¼Œåˆ›ä¸šè€…å°†å›´ç»•è¿™äº›èƒ½åŠ›æ‰“é€ é’ˆå¯¹ç‰¹å®šå®¢æˆ·ç¾¤ä½“çš„ç‹¬ç‰¹äº§å“ä½“éªŒï¼š
  * æ„å›¾æå–ï¼šé€šè¿‡é¢†åŸŸä¸“ä¸šåŒ–ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°å°†å“åº”ä¸ç”¨æˆ·æ„å›¾åŒ¹é…ã€‚ä¾‹å¦‚ï¼ŒåŒ»ç”Ÿå’Œæ‚£è€…é—®åŒä¸€ä¸ªé—®é¢˜æ—¶ä¼šéœ€è¦çœ‹åˆ°ä¸åŒç±»å‹çš„å“åº”ã€‚
    * å’Œä¼ ç»ŸDLæœç´¢æ¨èé¢†åŸŸçš„ä¸ªæ€§åŒ–ä¼¼ä¹æœ‰äº›å·®å¼‚
  * ä¸“æœ‰æ•°æ®ï¼šåœ¨ç™½é¢†é¢†åŸŸï¼Œç‹¬ç‰¹çš„æ•°æ®é›†å°†å¾ˆé‡è¦ï¼Œå¦‚å¾‹å¸ˆçš„åˆ¤ä¾‹æ³•ã€åˆ†æå¸ˆçš„è´¢åŠ¡æ•°æ®æˆ–ä¿é™©æ‰¿ä¿äººçš„å¤©æ°”æ•°æ®ã€‚
  * åœ¨å•†ä¸šç¯å¢ƒä¸‹ï¼Œå¾—åˆ°æ­£ç¡®ç­”æ¡ˆæ˜¯æœ€åŸºæœ¬çš„è¦æ±‚ã€‚
  * æ ¼å¼åŒ–ï¼šç»“æœå‘ˆç°ç»™ç”¨æˆ·çš„æ–¹å¼ï¼Œä¾‹å¦‚å“åº”çš„è¯¦ç•¥ç¨‹åº¦ã€è¦ç‚¹çš„ä½¿ç”¨ã€å¤šæ¨¡æ€å†…å®¹çš„ä½¿ç”¨ã€å¯¹æºçš„å¼•ç”¨ç­‰ã€‚
    * æ¯”å¦‚ï¼Œä¼šè®¡å¸ˆå’Œè®°è€…æ¶ˆåŒ–æ¥æ”¶ä¿¡æ¯çš„æ–¹å¼å°±ä¸åŒã€‚
    * é˜¶æ®µä¸€ï¼šæ„å›¾åˆ¤åˆ«+æ—¢å®šæ ·å¼
    * é˜¶æ®µäºŒï¼šæ ·å¼åˆ¤åˆ«
    * é˜¶æ®µä¸‰ï¼šæ ·å¼ç”Ÿæˆ
  * ç•Œé¢è®¾è®¡ï¼šä»£ç æœç´¢éœ€è¦å­˜åœ¨äº IDE ä¸­ï¼Œä¼šè®¡æ”¿ç­–æœç´¢éœ€è¦å­˜åœ¨äºä¼šè®¡ SaaS å¹³å°ä¸­ã€‚
* è¯­ä¹‰æœç´¢å—ç›Šäºç”¨æˆ·ç°æœ‰å·¥ä½œæµå’Œæ•°æ®çš„ä¸Šä¸‹æ–‡ã€‚ä¸åŒé¢†åŸŸéœ€è¦ä¸åŒçš„ç•Œé¢äº¤äº’ã€‚æ–°çš„ç‰¹å®šé¢†åŸŸ AI æœç´¢å¼•æ“å°†å°½å¯èƒ½åœ°æ˜ å°„å…¶ç›®æ ‡ç”¨æˆ·çš„â€œæ€ç»´æ¨¡å¼â€ã€‚åŒ»ç”Ÿã€å¾‹å¸ˆå’Œä¼šè®¡å¸ˆçš„æ€ç»´æ–¹å¼å¹¶ä¸ç›¸åŒã€‚å½“æˆ‘ä»¬æˆä¸ºæŸä¸ªé¢†åŸŸçš„ä¸“å®¶æ—¶ï¼Œæˆ‘ä»¬æå–çŸ¥è¯†å’Œåšå‡ºå†³ç­–çš„æ¨¡å¼å¼€å§‹å‡ºç°å·®å¼‚ã€‚åŒ»ç”Ÿé¢å¯¹åŒ»å­¦æ–‡çŒ®ï¼Œå¾‹å¸ˆé¢å¯¹æ³•æ¡ˆï¼ŒæŠ•èµ„è€…é¢å¯¹è´¢æŠ¥ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªé¢†åŸŸè§£æã€åˆ†æå’ŒåŸºäºè¿™äº›çŸ¥è¯†åšå‡ºå†³ç­–çš„æ–¹å¼éƒ½æ˜¯ä¸åŒçš„ã€‚



#### ç”µå•†

* [Product Search And Recommendation Trends In 2024 For Better Converting eCommerce Stores](https://blog.boostcommerce.net/posts/product-search-and-recommendation-trends)
* [Inside Product Recommendation Feature Of Boost AI Search & Discovery](https://blog.boostcommerce.net/posts/product-recommendation-feature-of-boost-ai-search-discovery)
* [Using Ecommerce Recommendation Engines to Keep Your Customers Coming Back](https://www.bigcommerce.com/articles/ecommerce/recommendation-engine/)
* [21 Ecommerce **Product Recommendation** Tips (That Increase Conversions)](https://thegood.com/insights/ecommerce-product-recommendation/)
* **Search relevance** is king!
  * The search bar is the go-to destination for [69%](https://www.nosto.com/blog/future-of-ecommerce-search-2023/) of shoppers
  * [82%](https://www.nosto.com/blog/future-of-ecommerce-search-2023/) of online businesses believe that the site search experience can be enhanced by providing more relevant results
  * and a compelling [79%](https://www.nosto.com/blog/future-of-ecommerce-search-2023/) of consumers surveyed expressed their likelihood to purchase a product that they had specifically searched for in the search results
* ä¸ªæ€§åŒ–ä¹Ÿé‡è¦
  * [Research](https://www.barilliance.com/personalized-product-recommendations-stats/) conducted by Barilliance in 2018 concluded that product recommendations accounted for up to 31 percent of ecommerce revenue. On average, customers saw 12 percent of their overall purchases coming from products that were recommended to them. 
  * [A Salesforce study](https://www.salesforce.com/blog/2017/11/personalized-product-recommendations-drive-just-7-visits-26-revenue.html) of product recommendations concluded that visits where the shopper clicked a recommendation comprise just 7 percent of total site traffic, but make up 24 percent of orders and 26 percent of revenue. 
  * The conversion rate for visitors clicking on product recommendations was found to be [5.5x higher](https://www.barilliance.com/personalized-product-recommendations-stats/) than for visitors who didnâ€™t click.
  * [An Accenture report](https://www.accenture.com/us-en/interactive-index) says personalization increases the likelihood of a prospect purchasing from you by 75 percent.
* No more â€œNo results foundâ€
  * **synonyms and autocorrect** to prevent unmatched search frustrating experiences
  * [Best Practices for Instant Search Box - What to Do With â€˜No Search Resultsâ€™](https://boostcommerce.net/blogs/all/instant-search-box-tips-with-no-search-results)
* The age of intelligent search continues
  * AL/ML-based models (can also use rules to set up)
    * content-based filtering
    * item-CFå’Œuser-CF
    * Frequently bought together (FBT)
      - FBTå’ŒComplementary productsçš„åŒºåˆ«æ˜¯ï¼Œå‰è€…åŸºäºåŠ å…¥è´­ç‰©è½¦çš„æ•°æ®ï¼Œåè€…åŸºäºå•†å“è¯­ä¹‰ä¿¡æ¯
    * Related items
      - Alternative products
      - Complementary products
      - Mix of the 2 sub-models
  * Statistic-based models
    - Newest arrivals
    - Trending products
    - Bestsellers
    - Most viewed
    - Recently viewed
  * Manual model
    - Hand-pick products ( fix the limelight for a particular set of products without changing or updating them.)
      - å®ç°æ—¶å¯ä»¥ç”¨tagæ ‡æ³¨
* Customers want personalized product recommendations
  * ä¸ªæ€§åŒ–å¯¹retaining customersï¼ˆç•™å­˜ï¼‰æœ‰å¸®åŠ©
* äº§å“èƒ½åŠ›ï¼š
  * **NLP-backed search engine** to better respond to long-tail queries
  * **Semantic search** to maximize the accuracy and relevance of search results
  * Enhanced **typo tolerance**
  * **Understanding search with high complexity**
  * AI-fueled **upselling and cross-selling**
    * such as a Bluetooth headset to go with their chosen laptop
  * secondary algorithm
    * ![img](https://cdn.prod.website-files.com/663e17fff238bd97b0a022cd/6645d914bd140fa3afeac447_Img_14_1_1344x.png)
  * **Analyzeèƒ½åŠ›**
    * **Analyze customer feedback and reviews**ï¼šåˆ†ææ•ˆæœ
    * **Identify the most popular products**ï¼šå†³å®špromote and stockå“ªäº›å•†å“
    * **Improve upselling and cross-selling**ï¼šcreate more cohesive marketing campaigns by bundling items in ways that appeal to customers
    * **Understand customer preferences and behavior**: Understanding which upselling and cross-selling offers customers respond to provides more insight into their purchase behavior and lets you make better-informed decisions about which products to restock. For example, if customers donâ€™t respond to upselling product suggestions, you might consider discontinuing the higher-end product.
    * **Show Bestsellers Across Different Categories**
  * *And many more*
* å¯¹åº”äº§å“åŠŸèƒ½ï¼š
  * Frequently Bought Together
    * "Viewed this, bought that."
    * Amazon does this by showing bundles of products frequently viewed in succession and enabling users to **add the entire bundle** to their shopping cart in one click.
      * **Provide Social Proof**  (Customers Who Bought This Item Also Bought)
  * Related Items (AI-powered)
    * â€œDeals based on your recent historyâ€
    * æ¨èç†ç”±
  * Recently Purchased
    * â€œBuy againâ€
  * Bestsellers
  * Hand-picked Products
  * Recently Viewed
    * â€œKeep shopping forâ€
  * Most Viewed
  * Newest Arrivals
  * Trending Products
    * based on new trends and seasons
  * Personalize Your Email Campaigns

* æ¨¡å‹è¾“å…¥ç‰¹å¾ï¼š

  * past purchases, viewed products
  * time spent on various pages
  * Location

  * å…ƒä¿¡æ¯
    * new trends and seasons (as prompt)
    * product titles and descriptions

* æŒ‡æ ‡metricsï¼š

  * average order value
  * upsell/cross-sell conversion rate
  * insight into user behavior.

* äº§å“é¡µé¢ï¼š[**How to Display Product Recommendations Throughout the Sales Cycle** ](https://thegood.com/insights/ecommerce-product-recommendation/#h-how-to-display-product-recommendations-throughout-the-sales-cycle-nbsp)

  * Homepage   ---> â€œMost Popularâ€ and â€œRecently Viewedâ€
    - **Trending products**
    - **Recently viewed**
    - Bestsellers
    - Most viewed
    - Newest arrivals
  * Collection page ---> most popular
    - **Trending products**
    - **Bestsellers**
    - **Most viewed**
    - Recently viewed
    - Newest arrivals
  * Product page
    - **Frequently bought together**
    - **Related items**
    - Newest arrivals
    - Trending products
    - Bestsellers
    - Most viewed
    - Recently viewed
    - Hand-pick products
    - **Note**: Frequently bought together & Related items can be displayed as Product Bundles.
  * Cart page
    - Frequently bought together
    - **Related items** -> cross-selling
    - Newest arrivals
    - Trending products
    - Bestsellers
    - Most viewed
    - Recently viewed

###  æœç´¢ç®—æ³•

#### Hybrid Search

* Hybrid search is a combination of full text and vector queries that execute against a search index that **contains both searchable plain text content and generated embeddings**. For query purposes, hybrid search is:
  * A single query request that includes both `search` and `vectors` query parameters
  * Executing in parallel
  * With merged results in the query response, scored using Reciprocal Rank Fusion (RRF)
* èƒŒæ™¯ï¼š
  * å®é™…ç”Ÿäº§ä¸­ï¼Œä¼ ç»Ÿçš„å…³é”®å­—æ£€ç´¢ï¼ˆç¨€ç–è¡¨ç¤ºï¼‰ä¸å‘é‡æ£€ç´¢ï¼ˆç¨ å¯†è¡¨ç¤ºï¼‰å„æœ‰ä¼˜åŠ£ã€‚
    * ä¸¾ä¸ªå…·ä½“ä¾‹å­ï¼Œæ¯”å¦‚æ–‡æ¡£ä¸­åŒ…å«å¾ˆé•¿çš„ä¸“æœ‰åè¯ï¼Œå…³é”®å­—æ£€ç´¢å¾€å¾€æ›´ç²¾å‡†è€Œå‘é‡æ£€ç´¢å®¹æ˜“å¼•å…¥æ¦‚å¿µæ··æ·†ã€‚
    * e.g. åœ¨åŒ»å­¦ä¸­â€œå°ç»†èƒè‚ºç™Œâ€å’Œâ€œéå°ç»†èƒè‚ºç™Œâ€æ˜¯ä¸¤ç§ä¸åŒçš„ç™Œç—‡

* [Relevance scoring in hybrid search using Reciprocal Rank Fusion (RRF)](https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking)
  * Kv search (BM25)
  * Vector search (HNSW)
  * RRF: $rrf(d)=\sum_{a\in A}\frac{1}{k+rank_a(d)}$

* [VantageDiscoveryçš„ç”µå•†æœç´¢å®è·µ](https://www.vantagediscovery.com/post/compound-ai-search-where-keywords-and-vectors-are-just-the-beginning)

  * é«˜çº§èƒ½åŠ›
    * **Intelligent Weighting**: Dynamically adjust the importance of different search factors based on business goals or seasonal priorities.
    * **Flexible Matching Criteria**: Find relevant results even with partial query matches, ensuring customers always find suitable products.
    * **Contextual Semantic Adjustment**: Control the degree of semantic interpretation based on product categories or query types, optimizing for both precision and recall.
    * **Category-Specific Models**: Utilize different AI models for various product types, ensuring specialized understanding across diverse catalogs.

  * Imagine a customer searching for a "cozy blue sweater for a winter wedding." A compound AI system handles this complex query by:
    * Analyzing intent: identifying style, color, item, and occasion.
    * Expanding context: considering related concepts like "formal knitwear" or "elegant cold-weather attire."
    * Performing semantic search using advanced embeddings.
    * Conducting traditional keyword search in parallel.
    * Blending results, prioritizing wedding-appropriate items.
    * Returning a curated selection of relevant products, including complementary accessories.
  * https://docs.vantagediscovery.com/docs/search-more-like-these-tm#example-soft-chair--item-27--two-pinterest-images
    * ![more-like-these-overview](./AI-Algorithms/more-like-these-overview.webp)





### LLM4ç”µå•†æœç´¢

#### Picnic: LLM å¢å¼ºç”µå•†æœç´¢

> https://blog.picnic.nl/enhancing-search-retrieval-with-large-language-models-llms-7c3748b26d72

* æ€è·¯ï¼šå¤§é‡çš„LLMç¦»çº¿é¢„å¤„ç†+åœ¨çº¿ç¼“å­˜
  * ç¦»çº¿ï¼š
    * LLM: å•†å“ -> Query + æè¿°
    * LLM: Query + List[æè¿°] -> æè¿°
  * åœ¨çº¿ï¼š
    * æè¿°å‹ Query -> ç›¸ä¼¼Queryæè¿° -> å¬å›éœ€æ±‚å•†å“
    * çœŸ Query -> ç¼“å­˜ -> å‘½ä¸­Queryæè¿° -> å¬å›ç›¸ä¼¼å•†å“
* prompt-based product description generation
  * transforming search terms into detailed, actionable queries

![img](./AI-Algorithms/0*8YkG715dCEE80t8s.png)



### Queryç†è§£å’Œåˆ†æ

> [ç”µå•†æœç´¢å…¨é“¾è·¯ï¼ˆPART IIï¼‰Queryç†è§£](https://mp.weixin.qq.com/s/GrMItUHW8Szghmveejn9XA)

![å›¾ç‰‡](./AI-Algorithms/640-20241011183258573)

![img](./AI-Algorithms/78aa0a537b0122edf97ec9a6d01a4fbf.png)

* Queryé¢„å¤„ç†
  * è¿è¥å®¡æ ¸å¹²é¢„
  * å½’ä¸€åŒ–ï¼šåŒ…æ‹¬å¤§å°å†™è½¬æ¢ã€ç¹ç®€ä½“è½¬æ¢ã€å…¨åŠè§’è½¬æ¢ã€ç¬¦å·è¡¨æƒ…ç§»é™¤ç­‰
  * é•¿åº¦æˆªæ–­ï¼šå¯¹è¶…é•¿çš„queryè¿›è¡Œæˆªæ–­
* Queryåˆ†è¯
  * ç›®å‰ä¸šç•Œä¸­å¤§éƒ¨åˆ†æœç´¢ç³»ç»Ÿä¸­çš„åˆ†è¯æ¨¡å—éƒ½ä¼šæœ‰ä¸“é—¨çš„åŸºç¡€ä¸­å°éƒ¨é—¨æ¥è¿­ä»£ä¼˜åŒ–ï¼Œäº¦æˆ–ç›´æ¥ä½¿ç”¨å¼€æºçš„åˆ†è¯å·¥å…·ï¼ˆè­¬å¦‚JieBaã€HanLPã€PyLTPã€LACç­‰ï¼‰
  * Review of Chinese Word Segmentation Studies: *https://manu44.magtech.com.cn/Jwk_infotech_wk3/CN/Y2020/V4/I2/3/1*
  * NLPåˆ†è¯ç®—æ³•æ·±åº¦ç»¼è¿°: *https://zhuanlan.zhihu.com/p/50444885*

```python
# æå–åè¯
values = [token.word for token in jieba.posseg.cut(query)
            if token.flag in {'n', 'nr', 'ns', 'nt', 'nz'}]
```



> Queryæ”¹å†™

- Queryçº é”™ï¼šæŠ€æœ¯æ–¹æ¡ˆä¸»è¦å¯ä»¥åˆ†ä¸ºpipelineå’Œend2endä¸¤ç§ç±»å‹

  - Pipelineé”™è¯¯æ£€æµ‹ï¼šè¯†åˆ«è¾“å…¥å¥å­ä¸­é”™è¯¯è¯çš„ä½ç½®ã€‚ä¸»è¦æ–¹æ³•æœ‰ä»¥ä¸‹å‡ ç§ï¼š

  - - åŸºäºè¯å…¸ï¼šå¯¹queryåˆ‡åˆ†åï¼Œæ£€æŸ¥å„ä¸ªè¯æ˜¯å¦åœ¨ç»´æŠ¤çš„è‡ªå®šä¹‰è¯è¡¨æˆ–æŒ–æ˜ç§¯ç´¯çš„å¸¸è§çº é”™pairä¸­ï¼›
    - åŸºäºè¯­è¨€æ¨¡å‹ï¼šç»Ÿè®¡å¤§è§„æ¨¡è¯­æ–™çš„n-gramä¿¡æ¯ï¼Œé¢‘ç‡å°äºä¸€å®šé˜ˆå€¼çš„å³è®¤ä¸ºæ˜¯é”™è¯¯è¯ï¼›
    - åŸºäºåºåˆ—æ ‡æ³¨ï¼šé€šè¿‡æ¨¡å‹ï¼ˆbi-LSTM-CRFã€BERT-CRFç­‰ï¼‰æ¥å­¦ä¹ é”™è¯¯è¯çš„å¼€å§‹å’Œç»“æŸä½ç½®ï¼Œ'0' è¡¨ç¤ºæ— é”™è¯¯ï¼Œ'1' è¡¨ç¤ºé”™è¯¯ï¼›

  - Pipelineé”™è¯¯çº æ­£ï¼šå®šä½åˆ°é”™è¯åï¼Œè¿›è¡Œé”™è¯çš„çº æ­£ã€‚é¦–å…ˆé‡‡ç”¨å¤šç§ç­–ç•¥ï¼ˆç¼–è¾‘è·ç¦»ã€HMMæ¨¡å‹ã€è®­ç»ƒæ·±åº¦æ¨¡å‹æŒ–æ˜ç­‰ï¼‰è¿›è¡Œçº é”™å€™é€‰å¬å›ï¼Œç„¶åå¯¹è¯¥å€™é€‰é›†åˆè¿›è¡Œæ’åºå¾—åˆ°æœ€ç»ˆçš„æ­£ç¡®queryã€‚

  - End2Endï¼š

    - å­—èŠ‚AI Labçš„Soft-Mask BERT
    - èš‚èšé‡‘æœSpellGCN
    - è…¾è®¯ PLOME

  - ä¸šç•Œæ¡ˆä¾‹ï¼šåœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œä¼šå­˜åœ¨å¾ˆå¤šè®ºæ–‡æœªæ¶‰åŠçš„é—®é¢˜

    - [ç™¾åº¦ï¼šä¸­æ–‡çº é”™æŠ€æœ¯](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247488610&idx=1&sn=c8793392f789ba5c39a9e8a4d7c6beac&scene=21#wechat_redirect)
    - [å“ˆå·¥å¤§è®¯é£æ–‡æœ¬çº é”™ç³»ç»Ÿ](http://cogskl.iflytek.com/archives/1306)
    - [å¹³å®‰å¯¿é™©AIï¼šæ–‡æœ¬çº é”™æŠ€æœ¯](https://zhuanlan.zhihu.com/p/159101860)
    - [é˜¿é‡Œï¼šè¯­éŸ³å¯¹è¯ä¸­çš„çº é”™ç³»ç»Ÿ](https://mp.weixin.qq.com/s?__biz=MzA3MTQ0NTUyMw==&mid=2247484572&idx=1&sn=de6d707458e05bec4d53c4e4427da0e2&scene=21#wechat_redirect)
    - [å°çˆ±ï¼šåŸºäºBERTçš„ASRçº é”™](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247503412&idx=1&sn=75ef312902713d3766a43a6c71e1024e&scene=21#wechat_redirect)
    - [æ»´æ»´ï¼šè¯­éŸ³äº¤äº’è‡ªç„¶è¯­è¨€ç†è§£æ¢ç´¢ä¸å®è·µ](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247529750&idx=2&sn=dbf897c5cb112fb87b6a1d9a37804548&scene=21#wechat_redirect)
    - [æµåˆ©è¯´ï¼šè‡ªåŠ¨è¯­æ³•çº é”™](https://mp.weixin.qq.com/s?__biz=MzI0NjIzNDkwOA==&mid=2247484827&idx=1&sn=137c9b927a9d77af73825eb24abb5c8f&scene=21#wechat_redirect)

![å›¾ç‰‡](./AI-Algorithms/640-20241011184242866)

- Queryå½’ä¸€ï¼šç›®æ ‡æ˜¯å°†é•¿å°¾å†·é—¨çš„query/è¯è¯­å½’ä¸€åˆ°çƒ­é—¨æ ‡å‡†query
  - æ¶‰åŠçš„ä¸»è¦æŠ€æœ¯æ˜¯åŒä¹‰è¯æŒ–æ˜åŠè¯­ä¹‰å®ä½“å¯¹é½ã€‚å…·ä½“å®ç°ä¸Šæœ‰å¾ˆå¤šæ–¹å¼ï¼Œè­¬å¦‚ï¼š
    - ä»çŸ¥è¯†åº“æˆ–è€…ç»“æ„åŒ–æ•°æ®æ„é€ è§„åˆ™æ¨¡æ¿æ¥æŒ–æ˜ï¼›
    - åˆ©ç”¨ä¸°å¯Œçš„è¡Œä¸ºæ•°æ®ï¼Œç»“åˆæ— ç›‘ç£è¯å‘é‡ï¼Œæ¥æŒ–æ˜è¯­ä¹‰ç›¸ä¼¼è¯ï¼›
    - é€šè¿‡æ·±åº¦åŒ¹é…æ¨¡å‹ã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹seq2seqç­‰å…ˆæŒ–æ˜å‡ºè¯­ä¹‰è¡¨è¾¾ç›¸è¿‘çš„query-queryã€item-itemæˆ–query-itemçŸ­è¯­å¯¹ï¼Œç„¶åå†å°†è¯­ä¹‰ç›¸è¿‘çš„query/itemçŸ­è¯­å¯¹è¿›è¡Œè¯­ä¹‰å¯¹é½ï¼›
- Queryæ‰©å±•ï¼šæ ¹æ®ç²’åº¦çš„ä¸åŒåˆ†ä¸ºTermç²’åº¦å’ŒQueryç²’åº¦ä¸¤ç§
  - ç¾å›¢æ–¹æ¡ˆï¼š
    - é¦–å…ˆç¦»çº¿é€šè¿‡ç”¨æˆ·æœç´¢æ—¥å¿—ã€ç¿»è¯‘ï¼ˆè¯å¯¹é½ç­‰ï¼‰ã€å›¾æ–¹æ³•ï¼ˆååŒè¿‡æ»¤ã€graph embeddingç­‰ï¼‰ã€è¯å‘é‡Embeddingç­‰æ–¹æ³•æŒ–æ˜å¾—åˆ°åƒä¸‡çº§åˆ«çš„å€™é€‰è¯­æ–™ï¼›
    - ä½†ä¸€èˆ¬ä¸Šè¿°æŒ–æ˜è¯­æ–™è´¨é‡ä¸å¤Ÿé«˜ï¼Œåˆè®¾è®¡äº†åŸºäºBERTçš„è¯­ä¹‰åˆ¤åˆ«æ¨¡å‹è¿›ä¸€æ­¥æé«˜æ”¹å†™pairå¯¹çš„å‡†ç¡®ç‡ï¼›
    - åœ¨çº¿çš„ç›®æ ‡æ˜¯è¿›ä¸€æ­¥æé«˜æ”¹å†™çš„æ•ˆæœï¼Œè®¾è®¡äº†é«˜ç²¾åº¦çš„è¯å…¸æ”¹å†™ã€è¾ƒé«˜ç²¾åº¦çš„æ¨¡å‹æ”¹å†™ï¼ˆåŸºäºSMTç»Ÿè®¡ç¿»è¯‘æ¨¡å‹å’ŒXGBoostæ’åºæ¨¡å‹ï¼‰ã€è¦†ç›–é•¿å°¾Queryçš„åŸºäºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¼˜åŒ–çš„NMTæ¨¡å‹ã€é’ˆå¯¹å•†æˆ·æœç´¢çš„å‘é‡åŒ–å¬å›å››ç§çº¿ä¸Šæ–¹æ¡ˆã€‚
  - å…¶å®ƒæ–¹æ¡ˆï¼š
    - [ä¸é¦™å›­ï¼šæœç´¢ä¸­çš„Queryæ‰©å±•æŠ€æœ¯](https://zhuanlan.zhihu.com/p/138551957)
    - [ä¸é¦™å›­ï¼šæœç´¢ä¸­çš„Queryæ‰©å±•æŠ€æœ¯(äºŒ)](https://zhuanlan.zhihu.com/p/296504323)
    - [Query ç†è§£å’Œè¯­ä¹‰å¬å›åœ¨çŸ¥ä¹æœç´¢ä¸­çš„åº”ç”¨](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247496409&idx=1&sn=7b2f5984d71454e1a2812321f6018cf8&scene=21#wechat_redirect)
    - [ç¾å›¢æœç´¢ä¸­æŸ¥è¯¢æ”¹å†™æŠ€æœ¯çš„æ¢ç´¢ä¸å®è·µ](https://tech.meituan.com/2022/02/17/exploration-and-practice-of-query-rewriting-in-meituan-search.htm)

### Query Rewrite

#### Literature Review

* Pseudo-Relevance Feed- back (PRF)
* Document Expansion
* æ•°æ®é›† Evaluationï¼šhttps://github.com/amazon-science/esci-data

#### A Survey of Query Optimization in Large Language Models

![image-20250113203747812](./AI-Algorithms/image-20250113203747812.png)

![image-20250113203846340](./AI-Algorithms/image-20250113203846340.png)

* **æŸ¥è¯¢æ‰©å±•ï¼ˆQuery Expansionï¼‰**ï¼š
  * åˆ†ä¸ºå†…éƒ¨æ‰©å±•å’Œå¤–éƒ¨æ‰©å±•ã€‚
  * å†…éƒ¨æ‰©å±•åˆ©ç”¨ LLM è‡ªèº«æˆ–åŸå§‹æŸ¥è¯¢ä¸­çš„ä¿¡æ¯ï¼Œå¦‚ GENREAD ä¾åˆå§‹æŸ¥è¯¢ç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æ¡£è¾…åŠ©å›ç­”ï¼›QUERY2DOC ç”¨ LLM ç”Ÿæˆä¼ªæ–‡æ¡£æ‰©å±•æŸ¥è¯¢æå‡æ£€ç´¢æ•ˆæœï¼›REFEED è¿­ä»£æ”¹è¿›è¾“å‡ºï¼›INTER æ„å»ºäº¤äº’æ¡†æ¶ååŒæ£€ç´¢ä¸ LLMï¼›HYDE ç»“åˆå‡è®¾æ–‡æ¡£ä¸å¯¹æ¯”ç¼–ç æ£€ç´¢ï¼›FLARE è¿­ä»£é¢„æµ‹æ£€ç´¢ï¼›MILL ç”Ÿæˆå­æŸ¥è¯¢ä¸æ–‡æ¡£ååŒï¼›GENQRENSEMBLE é›†æˆå…³é”®è¯å¢å¼ºæ£€ç´¢ï¼›ERRR æå–å‚æ•°çŸ¥è¯†ä¼˜åŒ–æŸ¥è¯¢ã€‚å¤–éƒ¨æ‰©å±•åˆ™ä»å¤–éƒ¨æ•°æ®æºï¼ˆå¦‚ç½‘ç»œã€çŸ¥è¯†åº“ï¼‰å¼•å…¥ä¿¡æ¯ï¼Œå¦‚ LameR ç”¨æ½œåœ¨ç­”æ¡ˆæ‰©å……æŸ¥è¯¢ï¼›GuideCQR ä¾æ£€ç´¢æ–‡æ¡£ä¼˜åŒ–æŸ¥è¯¢ï¼›CSQE æå–å…³é”®å¥æ‰©å±•ï¼›MUGI ç”Ÿæˆä¼ªå‚è€ƒå¢å¼ºæ£€ç´¢ã€‚
* **é—®é¢˜åˆ†è§£ï¼ˆQuestion Decompositionï¼‰**ï¼š
  * å¤æ‚æŸ¥è¯¢éœ€åˆ†è§£ä¸ºç®€å•å­æŸ¥è¯¢å†æ£€ç´¢ä¿¡æ¯æ•´åˆç­”æ¡ˆã€‚
  * å¦‚ DSP æ¡†æ¶åœ¨ LLM å’Œæ£€ç´¢æ¨¡å‹é—´å¤„ç†æ–‡æœ¬ï¼›LEAST - TO - MOST ç­‰æ–¹æ³•æŒ‰é¡ºåºåˆ†è§£è§£å†³é—®é¢˜ï¼›SELF - ASK æŒ‡å‡ºç»„åˆæ€§å·®è·ï¼›EAR ç­‰æ–¹æ³•æ‰©å±•æˆ–çº æ­£æŸ¥è¯¢ï¼›ICAT è½¬ç§»æ¨ç†èƒ½åŠ›ï¼›REACT ç»“åˆæ¨ç†ä¸è¡ŒåŠ¨ï¼›AUTOPRM ç­‰æ§åˆ¶åˆ†è§£ç²’åº¦ï¼›LPKG åŸºäºçŸ¥è¯†å›¾ç”ŸæˆæŸ¥è¯¢ï¼›ALTER ç­‰å¢å¼ºæ£€ç´¢æ¨ç†ï¼›REAPER è§„åˆ’æ£€ç´¢ï¼›HIRAG åˆ†è§£å¤šè·³æŸ¥è¯¢ï¼›MQA - KEAL åˆ©ç”¨å¤–éƒ¨è®°å¿†ï¼›RICHRAG å’Œ CONTREGEN æ”¹è¿›æ£€ç´¢è¿‡ç¨‹ï¼›PLANÃ—RAG æ„å»ºæ¨ç†å›¾ï¼›RAG - STAR é›†æˆä¿¡æ¯æ¨ç†ã€‚
* **æŸ¥è¯¢æ¶ˆæ­§ï¼ˆQuery Disambiguationï¼‰**ï¼š
  * é’ˆå¯¹æ¨¡ç³ŠæŸ¥è¯¢
  * æ–¹æ³•åŒ…æ‹¬ Ling ç­‰çš„æ¼”ç»æ¨ç†ã€ECHOPROMPT çš„é‡è¿°æŸ¥è¯¢ã€TOC çš„æ„å»ºæ­§ä¹‰æ ‘ã€INFOCQR çš„æ”¹å†™ç¼–è¾‘æ¡†æ¶ã€ADAQR çš„åå¥½ä¼˜åŒ–ã€MAFERW çš„å¤šæ–¹é¢åé¦ˆä¼˜åŒ–ã€CHIQ çš„åˆ©ç”¨ NLP èƒ½åŠ›ç­‰ï¼Œä»¥æ˜ç¡®ç”¨æˆ·æ„å›¾æé«˜æ£€ç´¢å‡†ç¡®æ€§ã€‚
* **æŸ¥è¯¢æŠ½è±¡ï¼ˆQuery Abstractionï¼‰**ï¼š
  * å¯¹äºå¤æ‚å¤šè·³æŸ¥è¯¢ï¼Œäººç±»å¸¸æŠ½è±¡æ±‚è§£ï¼Œç›¸å…³æ–¹æ³•å¦‚ STEP-BACK å¼•å¯¼ LLM æ¨ç†ï¼›Zhou ç­‰çš„æ¦‚å¿µæ¨ç†ï¼›COA çš„æŠ½è±¡æ¨ç†é“¾ï¼›AOT çš„æŠ½è±¡æ¡†æ¶ï¼›Baek ç­‰çš„å¢åŠ æŠ½è±¡ä¿¡æ¯ï¼›MA - RIR çš„å®šä¹‰æŸ¥è¯¢æ–¹é¢ï¼›META - REASONING çš„è¯­ä¹‰è§£æ„ï¼›RULERAG çš„è§„åˆ™å¼•å¯¼ï¼›SIMGRAG çš„å¤„ç†æŸ¥è¯¢ä¸çŸ¥è¯†å›¾å¯¹é½ã€‚
* æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘
  - **æŸ¥è¯¢ä¸­å¿ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹**ï¼šè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰è™½æœ‰æ½œåŠ›ï¼Œä½† CoT æ–¹æ³•ç”Ÿæˆè¿‡ç¨‹éš¾é¢„æµ‹ï¼Œæ„å»ºä»¥æŸ¥è¯¢å­é—®é¢˜ä¸ºä¸­å¿ƒçš„ PRMs å¯èƒ½æ˜¯ä¼˜åŒ–æ–¹å‘ã€‚
  - **æŸ¥è¯¢ä¼˜åŒ–åŸºå‡†**ï¼šç¼ºä¹ç»Ÿä¸€åŸºå‡†é˜»ç¢æŠ€æœ¯è¯„ä¼°æ¯”è¾ƒï¼Œå¼€å‘å…¨é¢è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†è‡³å…³é‡è¦ã€‚
  - **æé«˜æŸ¥è¯¢ä¼˜åŒ–æ•ˆç‡å’Œè´¨é‡**ï¼šç°æœ‰æ–¹æ³•å¤šæšä¸¾è€—æ—¶è€—èµ„æºï¼Œåº”è®¾è®¡é«˜æ•ˆç®—æ³•è¯†åˆ«æœ€ä¼˜è·¯å¾„ï¼Œå¦‚ä¼˜åŒ–æŸ¥è¯¢åˆ†è§£æ–¹å¼ã€‚
  - **Enhancing Query Optimization via**
    **Post-Performance**ï¼šåŸºäºæç¤ºçš„æ–¹æ³•ä¸­ LLM å¯¹æ£€ç´¢è´¨é‡æ„ŸçŸ¥ä¸è¶³ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶æ•´åˆæ’åç»“æœã€‚



#### Large Language Model based Long-tail Query Rewriting in Taobao Search

* BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries
  * multi-instruction supervised fine tuning (SFT)
    * based on rejection sampling and auxiliary tasks mixing to fine-tune LLM
  * offline feedback
  * objective alignment.
  * beam search to generate multiple candidate rewrites

* ç°æœ‰æŸ¥è¯¢æ”¹å†™æ–¹æ³•çš„å±€é™
  - åŸºäºåµŒå…¥çš„æ£€ç´¢èŒƒå¼ç»“æœéš¾è§£é‡Š
  - â€œæŸ¥è¯¢æ”¹å†™ & ç²¾ç¡®åŒ¹é…â€ èŒƒå¼ä¸­åˆ¤åˆ«å¼æ–¹æ³•éš¾ä»¥æ§åˆ¶è¯­ä¹‰èŒƒå›´å’Œç¡®ä¿ç›¸å…³æ€§
  - ç”Ÿæˆå¼æ–¹æ³•å—é™äºæ¨¡å‹è§„æ¨¡å¯¹é•¿å°¾æŸ¥è¯¢ç†è§£ä¸è¶³ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ”¹å†™æ–¹æ³•ç¼ºä¹å¾®è°ƒä¸ç›®æ ‡å¯¹é½

![image-20241117235808683](./AI-Algorithms/image-20241117235808683.png)

* å¤šæŒ‡ä»¤ SFT
  - æ”¶é›†æ”¹å†™ç›¸å…³ä»»åŠ¡æ•°æ®å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬:
  - æ„å»ºæŸ¥è¯¢æ”¹å†™æ•°æ®é›†ï¼ˆç»ä¸¤è½®æ‹’ç»é‡‡æ ·æå‡è´¨é‡å¹¶ç»“åˆè¾…åŠ©ä»»åŠ¡æ•°æ®ï¼‰
  - åˆ©ç”¨è¾…åŠ©ä»»åŠ¡æ•°æ®é›†ï¼ˆè´¨é‡åˆ†ç±»ã€äº§å“æ ‡é¢˜é¢„æµ‹ã€æ€ç»´é“¾ä»»åŠ¡ï¼‰å¢å¼ºæ¨¡å‹å¯¹é•¿å°¾æŸ¥è¯¢çš„ç†è§£

* Evaluation: åˆ©ç”¨taobao rele score functionï¼Œå®šä¹‰hit rate

#### Query Expansion by Prompting Large Language Models

* Intro
  * PRF-based approaches assume that the top retrieved documents are relevant to the query
  * we rely on the knowledge inherent in the LLM.
* ![image-20241114182225681](./AI-Algorithms/image-20241114182225681.png)

* ç»“è®ºï¼š
  * PRFå¯ä»¥å¢å¼ºæ’åº

#### Query2doc: Query Expansion with Large Language Models

* æ£€ç´¢sparseï¼šé‡å¤5éå†ç›¸è¿
* æ£€ç´¢denseï¼šç”¨[SEP]ç›¸è¿

### NL2Sql

#### Literature Review

* extracting the question-to-SQL patterns and generalizing them by training an
  encoder-decoder model with Text-to-SQL corpus

#### Evaluation

* https://bird-bench.github.io/
* https://yale-lily.github.io/spider



#### [DAIL-SQL] Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation

* Intro

  * prompt engineering methods, including question representation, example selection and example organization
  * DAIL- SQL encodes structure knowledge as SQL statements, selects examples based on their skeleton similarities and removes cross- domain knowledge from examples for token efficiency. Before

* Prompt Engineering

  * question representations in zero-shot scenario
  * example selection and organization strategies in few-shot scenario
    * the option of displaying full information, solely SQL queries or question-SQL pair.
  * verify the hypothesis that LLMs learn from the mappings between question and SQL skeleton

* Zero-shot -- Question Representation

  * database schema
  * prompt
    * basicï¼ˆBSï¼‰
    * Text representationï¼ˆTRï¼‰
    * **OpenAI Demostration Prompt (OD)**
      * â€œComplete sqlite SQL query only and with no explanationâ€
      * ç»¼åˆæ•ˆæœæœ€å¥½æœ€é€šç”¨
    * Code Representation Prompt
      * å®Œæ•´å»ºè¡¨è¯­å¥
    * AS
      * éœ€è¦SFTæ¨¡å‹æ‰è¡Œ

  ![image-20241109125459701](./AI-Algorithms/image-20241109125459701.png)

![image-20241109010143981](./AI-Algorithms/nl2sql-question-representation.png)

* å¢ç›Š
  * INS
  * **RI**
    * with no explanation æ•ˆæœå¥½
    * Let's think step by step æ•ˆæœä¸ç¨³å®š
  * FK

![image-20241109011512039](./AI-Algorithms/nl2sql-prompt-result.png)

![image-20241109012454931](./AI-Algorithms/nl2sql-prompts.png)

* Few-shot
  * èƒŒæ™¯settingï¼šcross-domain Text- to-SQL ï¼ˆä¾‹å­å¯èƒ½æ¥è‡ªäºåˆ«çš„æ•°æ®åº“ï¼‰
  * example selection
    * Random
    * Question Similarity Selection (QTS )
    * **Masked Question Similarity Selection ï¼ˆMQSï¼‰**
      * å…ˆmaskå®ä½“å†æ£€ç´¢ -> CBR-ApSQL
    * **Query Similarity Selection (QRS)**
      * å…ˆç”Ÿæˆï¼ˆæ‹Ÿåˆï¼‰queryå†æ£€ç´¢
    * æ€»ç»“ï¼štaking both question and SQL queries into con- sideration may benefit Text-to-SQL task
  * example organization
    * Full-Information Organization (FI)
    * SQL-Only Organization (SO).
    * æ€»ç»“ï¼šqualityå’Œquantityçš„æƒè¡¡
      * GPT 3.5 Turbo ä¸Šä¸‹æ–‡çŸ­ï¼ŒexampleåŠ å¤šäº†åè€Œä¸å¥½

![image-20241109021923944](./AI-Algorithms/dail-sql-prompt1.png)

* supervised fine-tuning (SFT)

  * **Alignment**çš„èŒƒå¼, which aligns LLMsâ€™ behavior to avoid generating offensive, biased responses and hallucinations
  * æ•°æ®å¯¹ -> (prompt, ground_truth)

  * ç»†èŠ‚ï¼š
    * Following the setting of supervised fine-tuning [34, 47], we block the gradients from prompt and only update weights with those from response (SQL queries).
    * 9000æ¡æ ·æœ¬

  * ç»“è®ºï¼š
    * SFTï¼š
      * Figure 6ï¼šLLAMA2-chat-7B ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼ŒEAè¾¾åˆ°70%
      * **Alpaca SFT Prompt**
      * å¾®è°ƒåï¼Œä¸åŒQuestion Representationçš„æ•ˆæœgapå˜å°
      * **fine-tuned LLMs fail to learn from examples.**
    * Zero-shot Scenario with Open-source LLM
      * code-llama-34B å‰å®³ï¼Œåªæœ‰ç”¨TRçš„æ—¶å€™æ•ˆæœå·®

![image-20241109043228932](./AI-Algorithms/nl2sql-sft.png)

* DAIL-SQL
  * èåˆäº†ä¸Šé¢çš„æŠ€æœ¯
  * Question Representation: CR-P
    * å¤–é”®->JOINè¯­å¥
    * pre-trained on extensive coding corpora, LLMs could better understand the prompt in CR ?? without too much additional effort.
  * Example Organization: DAIL Organization
  * Example Selection: MQS + masked QRS
    * æŒ‰MQSæ’åºï¼Œå†æŒ‰masked QRSä¼˜å…ˆçº§é‡æ’

* evaluation
  * exact-set-match accuracy (EM)
  * **execution accuracy (EX)**





#### [CBR-ApSQL] Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval

* Masked Question Similarity Selection (MQS)

### ç«å“

* [æ·±åº¦ï½œAI+ç”µå•†æœç´¢å¤§ç›˜ç‚¹ï¼Œåˆä¸€ä¸ªèµ„æœ¬é›†ä¸­ä¸‹æ³¨çš„ç»†åˆ†èµ›é“](https://mp.weixin.qq.com/s/zaczcDifgT-9Gt5q-R7azQ)
  * VantageDiscovery
  * DayDream
    * å¼ºè°ƒåŸºäºå¤šæ¨¡æ€ç†è§£çš„å•†å“æœç´¢èƒ½åŠ›ï¼Œä¾‹å¦‚å…¶å®˜ç½‘å±•ç¤ºçš„åœºæ™¯ä¸­ï¼Œç”¨æˆ·ä¸Šä¼ ä¸€å¼ å¸¦æœ‰æ¡çº¹çš„æ‰˜ç‰¹åŒ…ï¼Œå¹¶å¸Œæœ›æ‰¾åˆ°ä¸€æ¬¾ç±»ä¼¼çš„æ— æ¡çº¹æ¬¾ï¼ŒDayDream å¯ä»¥è½»æ¾åŸºäºè¿™äº›æç¤ºç»™å‡ºæœç´¢ç»“æœã€‚
  * Glaze
    * åœ¨è¯¥äº§å“ä¸­ï¼Œæ¯ä¸ªäººéƒ½ä¼šè·å¾—ä¸€ä¸ªåˆå§‹çš„è™šæ‹Ÿæ—¶å°šä¹°æ‰‹ Glazeã€‚ç”¨æˆ·å¯æ·»åŠ  Glaze çš„è”ç³»æ–¹å¼ï¼Œå¹¶æˆä¸ºä»–çš„å¥½å‹ã€‚éšååœ¨ç”¨æˆ·æµè§ˆ Insã€Pinterest ç­‰æ—¶å°šå†…å®¹ç½‘ç«™æ—¶ï¼Œå¯ä»¥é€šè¿‡åˆ†äº«æŒ‰é’®å°†ä½ å–œæ¬¢çš„å†…å®¹åˆ†äº«ç»™è¿™ä½åŠ©æ‰‹ã€‚**Glaze è´­ç‰©åŠ©æ‰‹ä¼šç§¯ç´¯è¿™äº›ç”¨æˆ·åˆ†äº«æ•°æ®ï¼Œå­¦ä¹ ç”¨æˆ·çš„ç©¿è¡£é£æ ¼ã€äº§å“åå¥½ï¼Œå¹¶éšæ—¶ä¸ºä½ æ¨èç›¸å…³å•†å“**ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥åœ¨ Glaze æ¨èçš„é“¾æ¥ä¸­è¿›è¡Œè´­ä¹°å’Œè½¬åŒ–ã€‚



#### WebKul

https://webkul.com/ai-semantic-search-services/





#### VantageDiscovery AI Search

> https://www.vantagediscovery.com/blog
>
> Demoï¼šhttps://demo.vantagediscovery.com/fashion/search

* Intro
  * **VantageDiscovery æœ€å¤§çš„ç«äº‰åŠ›æ¥è‡ªä»–ä»¬çš„è‡ªå®šä¹‰å‘é‡æ•°æ®åº“ã€‚**å°†ç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰ç†è§£å’Œå¯¹ç”¨æˆ·ä¸ªäººé£æ ¼çš„è¯­ä¹‰ç†è§£ç»“åˆèµ·æ¥ï¼Œåœ¨å‡ æ¯«ç§’å†…ä»æ•°ç™¾ä¸‡ä¸ªé¡¹ç›®ä¸­æ£€ç´¢å‡ºæœ€ä¸ªæ€§åŒ–ã€æœ€æœ‰é’ˆå¯¹æ€§çš„ç»“æœ
  * VantageDiscovery çš„å•†ä¸šç­–ç•¥æ˜¯ä¸ºé‚£äº›ç‹¬ç«‹ç«™å–å®¶ã€é›†åˆåº—ã€å¤§å‹è´­ç‰©ç½‘ç«™æä¾›é¢å‘å•†ä¸šçš„æœç´¢å¼•æ“ã€‚
* e.g.
  * â€œæ¯äº²èŠ‚ç»™å¦ˆå¦ˆä¹°ä»€ä¹ˆâ€æˆ–â€œä¸€ä¸ªæœ‰è¶£çš„å¤œæ™šå¤–å‡ºçš„è¡¬è¡«â€
  * recipes for a 6 year old's birthday party
  * graduation garden party -> floral sundressã€wide-brim sunhat
* æŠ€æœ¯æ–‡ç« ï¼š
  * æœç´¢æŠ€æœ¯å†å² https://www.vantagediscovery.com/post/ecommerce-search-transcended-for-the-ai-age
  * èµ‹èƒ½cooklistï¼Œsemantic search https://www.vantagediscovery.com/post/how-cooklist-brought-their-catalog-to-life-in-unexpected-ways
  * More-Like-This https://www.vantagediscovery.com/post/personalizing-discovery-in-e-commerce-with-more-like-this
  * CRS https://www.vantagediscovery.com/post/elevating-ecommerce-search-from-keywords-to-conversations
    * shift from precision-based to intent-based queries
    * "I'm looking for boots that won't give up on me in the rain but still let me look my best at a cafÃ©."
    * Cozy spot, pet-friendly romantic weekend getaway
  * å¯¹HNSWçš„æ”¹è¿› https://www.vantagediscovery.com/post/the-hush-hush-secret-of-accuracy-of-hnsw-and-vector-databases
  * PRæ–‡ç«  https://www.vantagediscovery.com/post/vantage-discovery-raises-16m-to-bring-ai-powered-product-discovery-to-commerce
  * Semantic searchçš„ç»éªŒ https://www.vantagediscovery.com/post/5-things-i-learned-building-85-semantic-search-indexes
    * A clear, concise, salient set of text (3-4 paragraphs is a good rule of thumb) that describes the style, use, and attributes in real human-understandable terms is the number one predictor of great results out of the box.
    * Pictures are worth way more than 1,000 words (or floats!).
    * You must process images with clever vision LLM prompts or an overlaid trained image+text embedding model and include that in the embedding to be searched. It's crucial the text and image are combined into a single embedding (or at least single model).
    * **Adjustments like** [**keyword boosting**](https://docs.vantagediscovery.com/docs/search-options#keyword-support), fine-tuned embedding models, and query augmentation allow reduction of these creative jumps. However, don't overdo this, as sometimes a little variety and some non-intuitive jumps can actually add to the diversity of your results. Variety in the results, even non-obvious ones, may benefit and delight your users. With keywords, you might have shown ZERO-ZILCH-NADA results before, but now you show some variety and the best if not creative results given your catalog!
  * èšç„¦æ•°æ®é¢„å¤„ç† https://www.vantagediscovery.com/post/is-ai-powered-data-engineering-the-key-to-unlocking-your-product-catalogs-potential
  * styleå‘é‡æ£€ç´¢ https://www.vantagediscovery.com/post/vector-math-never-looked-so-floral-how-vantage-is-revolutionizing-e-commerce-search
  * hybrid search https://www.vantagediscovery.com/post/compound-ai-search-where-keywords-and-vectors-are-just-the-beginning
  * semantic searchçš„ç§‘æ™® https://www.vantagediscovery.com/post/semantic-101
    * `text-embedding-3-large` model with 2048 dimensions
  * é«˜ç»´å‘é‡å¯è§†åŒ– https://www.vantagediscovery.com/post/from-high-dimensions-to-human-comprehension
  * AIå¯è§£é‡Šæ€§ https://www.vantagediscovery.com/post/the-future-of-e-commerce-is-ai-powered-and-interpretable
    * sparse autoencoders (SAEs) https://transformer-circuits.pub/2024/scaling-monosemanticity/
    * Hyper-Personalized Product Discovery
    * Optimized Merchandising and Assortment
    * Enhanced Explainable Search
  * æœç´¢ç”µå•†çš„å•†ä¸šé€»è¾‘å’Œå…³é”®æŠ€æœ¯ https://www.vantagediscovery.com/post/adapt-or-die-why-retailers-want-to-be-like-amazon
    * Implicit personalization at an n of 1
    * Blending keyword and semantic search 
    * Explicit style personalization
    * Personalized shopping assistants
  * Salesforce AppExchange https://www.vantagediscovery.com/post/introducing-vantage-discovery-for-salesforce-commerce-cloud-unlock-the-future-of-ai-powered-retail
  * å…³äºsemantic searchçš„ä¼˜åŒ– https://www.vantagediscovery.com/post/semantic-search-using-matryoshka-embedding-vectors
  * åˆ†æä¼ ç»Ÿsearchçš„ç¼ºç‚¹ https://www.vantagediscovery.com/post/ai-shopping-assistants-and-semantic-search
    * When searchers find what theyâ€™re looking for, 92% purchase that item and 78% buy at least one additional item with an average of 3 additional items purchased after a successful search. On the other hand, 53% of consumers abandon the website entirely when they have an unsuccessful search.
    * https://llcbuddy.com/data/e-commerce-search-statistics/
* Note:
  * search optionï¼Œæ¶‰åŠå…³é”®è¯æ£€ç´¢ç›¸å…³ https://docs.vantagediscovery.com/docs/search-options#field-value-weighting
  * å›¾ç‰‡ä¸Šå¢åŠ upvoteï¼Œç”¨äºé‡‡é›†æ•°æ®

![640](./AI-Algorithms/640.webp)



#### Google Vertex Search

> https://cloud.google.com/enterprise-search?hl=en

* æŠ€æœ¯ä»‹ç»ï¼ˆRAGï¼‰
  * simplified the end-to-end search and discovery process of managing ETL, OCR, chunking, embedding, indexing, storing, input cleaning, schema adjustments, information retrieval, and summarization to just a few clicks
  * èåˆAI for document understanding
  * **Your RAGs powered by Google Search technology**
    * https://cloud.google.com/blog/products/ai-machine-learning/rags-powered-by-google-search-technology-part-1
      * semantic search
      * *Neural matching learns the relationships between queries and documents*
      * A production-grade semantic search is not just a similarity search, but must provide smart recommendation to users.
      * å‘é‡æ£€ç´¢ä½¿ç”¨[ScaNN](https://blog.research.google/2020/07/announcing-scann-efficient-vector.html)
    * https://cloud.google.com/blog/products/ai-machine-learning/rags-powered-by-google-search-technology-part-2
  * [Groundingèƒ½åŠ›](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview)
  * åŸºç¡€çš„IRèƒ½åŠ›ï¼Œ[Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/quickstart)

![https://storage.googleapis.com/gweb-cloudblog-publish/images/3._Vertex_AI_Search.max-1300x1300.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/3._Vertex_AI_Search.max-1300x1300.png)

* äº§å“å½¢æ€ https://cloud.google.com/use-cases/recommendations?hl=zh-cn
  * [Vertex AI Search for retail](https://cloud.google.com/solutions/retail-product-discovery) offers retailers the ability to improve the search, product recommendations, and browsing experience on their channels.
    * Retail companies are harnessing AI with Google Cloud today to recommend **tailored products and promotions to shoppers** and reap business results, such as **increased sales,** **average order value****, and** **customer lifetime value**.
    * LLM based Recommendationæ–¹ä¾¿å•†å®¶äººå·¥å¹²é¢„æ¨èç»“æœï¼Œè¿›è¡Œè¿è¥ä¼åˆ’æ´»åŠ¨ã€‚å®ç°saaså®šåˆ¶åŒ–éœ€æ±‚çš„æˆæœ¬ä½ã€‚
  * [Vertex AI Search for media](https://cloud.google.com/generative-ai-app-builder/docs/about-media) offers media and entertainment companies the ability to provide more personalized content recommendations powered by generative AI, increasing consumer time spent on their platforms, which can lead to higher engagement, revenue, and retention. 
  * [Generic Recommendation Data Store](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-recommendations)
  * [Vertex AI Search for healthcare and life sciences](https://cloud.google.com/generative-ai-app-builder/docs/create-app-hc) is a medically tuned search that improves patient and provider experience. [æ”¯æŒåŒ»ç–—æœç´¢](https://www.googlecloudpresscorner.com/2023-10-09-Google-Cloud-Adds-New-Features-to-Vertex-AI-Search-for-Healthcare-and-Life-Science-Companies)

![image-20240920165612409](./AI-Algorithms/vertex-search.png)

* LLMå’ŒSemantic Searchäº’ç›¸å¢å¼ºï¼š
  * Promptï¼šGiven that it's the beginning of winter, a customer is browsing for clothing on an e-commerce site. Winters are cold in their city. They entered "warm clothing for winter" as a search term on the site. What other search terms might they use to find related and cross-sell items?
  * Responses from an LLM may include the following queries:
    - Type-specific: Warm winter jackets, Cozy knitwear, Thermal leggings, Waterproof snow boots
    - Activity-specific: Ski clothing, Winter running gear, Work-appropriate winter outfits, Cozy homewear
    - Style-specific: Cashmere sweaters, Puffer vests, Statement scarves, Athleisure-inspired winter looks
  * ä¼˜åŠ¿ï¼šå¤šæ ·æ€§å¼º
  * å±€é™æ€§ï¼šå†·å¯åŠ¨The models may not be familiar with newly added product names or trained to memorize millions of product model numbers in its embedding space. ç”¨hybrid searchè§£å†³
* demo
  * stackoverflowçš„æ£€ç´¢ï¼šhttps://ai-demos.dev/demos/matching-engine
    * https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings?hl=en

* ç®—æ³•è¿›é˜¶
  * å¬å›+rerank
  * Filtering and boosting
    * https://cloud.google.com/generative-ai-app-builder/docs/filter-search-metadata
  * Extraction and generation
    * ä¸‰å±‚ï¼š*Snippet, Extractive answer, and Extractive Segment*
  * Summarization and multi-turn search
    * generating [search result summaries](https://cloud.google.com/generative-ai-app-builder/docs/get-search-summaries) and also supports [follow-up questions](https://cloud.google.com/generative-ai-app-builder/docs/multi-turn-search) with multi-turn search to provide a context-aware search.
  * document processing
    * **Document understanding and text chunking**
    * **Document and query annotation with Knowledge Graph**
    * ![https://storage.googleapis.com/gweb-cloudblog-publish/images/15._document_processing.max-1100x1100.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/15._document_processing.max-1100x1100.png)

* æ”¯æŒcustom embeddingèƒ½åŠ›
  * https://cloud.google.com/generative-ai-app-builder/docs/bring-embeddings
  * åœºæ™¯ï¼š
    * Your embeddings have been trained on custom words, such as internal terms whose semantic similarity wouldn't be captured by training on public dataâ€”for example, organization-specific terms that appear only in private documents.
    * You've created embeddings for user profiles and want to use these to create a personalized, semantically-relevant document ranking. You can use your embeddings to get personalization-based ranking, which can augment Google's document embeddings for relevance-based ranking.
  * `0.5 * relevance_score + 0.3 * dotProduct(example_embedding_field)`
* Collect scattered enterprise data
  * **Blended Search and web crawling**
    * All you have to do is specify[ the URL or URL pattern](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es#website) and Vertex AI Search will instantly create a data store with all the relevant website pages. You can then use [Google Search Central](https://developers.google.com/search), to manage site crawling on your website.
  * **Connectors**ï¼š [Connectors](https://cloud.google.com/generative-ai-app-builder/docs/prepare-data) 

![https://storage.googleapis.com/gweb-cloudblog-publish/images/17._blended_search.max-1300x1300.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/17._blended_search.max-1300x1300.png)

* æ›´å¤šèµ„æ–™ï¼š
  * LangChain-based samples and documents: [RAG sample notebooks using Vertex AI Search, PaLM, and LangChain](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/search/retrieval-augmented-generation) and [Vertex AI Search support in LangChain](https://python.langchain.com/docs/integrations/retrievers/google_vertex_ai_search)
  * [Grounding in Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/grounding/ground-language-models): provides a quick and easy way for grounding
  * [Check Grounding API](https://cloud.google.com/generative-ai-app-builder/docs/check-grounding?hl=en) provides a grounding score for an answer candidate
  * Vertex AI Conversation-based grounding: [Vertex AI Search and Conversation: search with follow-ups](https://cloud.google.com/generative-ai-app-builder/docs/multi-turn-search)
  * [How to use custom embedding with Vertex AI Search](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/custom-embeddings/custom_embeddings.ipynb)
  * [Vertex AI Search and Conversation product page](https://cloud.google.com/vertex-ai-search-and-conversation?hl=en)
  * [Get started with Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search)
  * [Vertex AI Search sample notebooks](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/search) on GitHub Gen AI repo
  * Video: [Harnessing the power of generative AI to deliver next-gen search experiences](https://youtu.be/HD_xreaLKb4?feature=shared)





#### Azure (AI-Enhanced Search)

* Build-in vectorization
  * Data chunking during indexing
  * Text-to-vector conversion during indexing
  * Text-to-vector conversion during queries
* å¤šè¯­è¨€ï¼ŒTranslation and language detection for multi-lingual search
* å®ä½“æŠ½å–ï¼ŒEntity recognition to extract people names, places, and other entities from large chunks of text
* å®ä½“æ ‡æ³¨æŠ½å–ï¼ŒKey phrase extraction to identify and output important terms
* OCRï¼ŒOptical Character Recognition (OCR) to recognize printed and handwritten text in binary files
* å›¾æ–‡å¤šæ¨¡ï¼ŒImage analysis to describe image content, and output the descriptions as searchable text file

#### Algolia (ç”µå•†æœç´¢æ¨è)

* https://www.algolia.com/
* [Get started with click and conversion events](https://www.algolia.com/doc/guides/sending-events/getting-started/?utm_medium=page_link&utm_source=dashboard)
* å¯¹æ¥äº†åŒ…æ‹¬shopifyç­‰æ¥è¿‘10ä¸ªç”µå•†å¹³å°ç³»ç»Ÿï¼Œæ— ç¼è¡”æ¥æ•°æ®ï¼Œæä¸ºç®€å•æ–¹å¼ï¼ˆ4æ­¥ï¼‰æä¾›ä»¥ä¸‹æ¨è
  - [Frequently Bought Together](https://www.algolia.com/doc/guides/building-search-ui/ui-and-ux-patterns/recommend/js/?utm_medium=page_link&utm_source=dashboard#frequently-bought-together) ï¼ˆé¢‘ç¹è´­ä¹°æ”¾ä¸€èµ·ï¼‰
  - [Related Products and Related Content](https://www.algolia.com/doc/guides/building-search-ui/ui-and-ux-patterns/recommend/js/?utm_medium=page_link&utm_source=dashboard#related-products-and-related-content) ï¼ˆç›¸å…³äº§å“ï¼Œç›¸å…³å†…å®¹ï¼‰
  - [Trending Products](https://www.algolia.com/doc/guides/building-search-ui/ui-and-ux-patterns/recommend/js/?utm_medium=page_link&utm_source=dashboard#trending-items) ï¼ˆæµè¡Œäº§å“ï¼‰
  - [Looking Similar](https://www.algolia.com/doc/guides/building-search-ui/ui-and-ux-patterns/recommend/js/?utm_medium=page_link&utm_source=dashboard#looking-similar) ï¼ˆlook-alikeï¼‰

* ç®—æ³•ï¼š
  * ç®—æ³•ç»†èŠ‚ï¼šhttps://www.algolia.com/doc/guides/algolia-recommend/overview/
  * [å½’å› ](https://www.algolia.com/doc/guides/sending-events/getting-started/)
    * Client-side versus server-side events

#### ChatGPT Shopping

https://www.bbc.com/news/articles/c87p2rppx4po



#### ACCIOï¼ˆç”µå•†å¯¼è´­æœç´¢ï¼‰

> https://www.accio.com/

#### Gleanï¼ˆä¼ä¸šå†…éƒ¨æœç´¢ï¼‰

> https://mp.weixin.qq.com/s/a3DhXOykVslxXbpobzIUCg



#### Cohereï¼ˆä¼ä¸šå†…éƒ¨æœç´¢ï¼‰

> https://zhuanlan.zhihu.com/p/11930776501

#### é“¾ä¼AIï¼ˆæ·±åº¦æœç´¢ï¼‰

> https://www.lianqiai.cn/

#### 360 AIæœç´¢

> [åŒ10äº¿ï¼šAIé‡å¡‘æœç´¢ | ä¸€æ–‡çœ‹æ‡‚AIæœç´¢ç°çŠ¶å’Œæœªæ¥](https://mp.weixin.qq.com/s/DvEnhyk6ytQ8NcSGCvgSUw)

![å›¾ç‰‡](./AI-Algorithms/640)

* ä¸€æ¬¡AIæœç´¢ï¼Œè°ƒç”¨1æ¬¡å¤§æ¨¡å‹ï¼Ÿ
  * é”™ï¼ç­”æ¡ˆæ˜¯**ä¼šè°ƒç”¨9æ¬¡å¤§æ¨¡å‹ï¼Œå¹¶ä¸”ä¸åŒä»»åŠ¡é‡Œä¼šæœ‰ä¸åŒå¤§æ¨¡å‹å‚ä¸**
  * ç”¨æˆ·è¾“å…¥Queryå¹¶ç‚¹å‡»æœç´¢åï¼Œ360ä¼šå…ˆè°ƒç”¨ä¸€ä¸ª2B-7Bä¹‹é—´å“åº”éå¸¸å¿«çš„æ„å›¾è¯†åˆ«æ¨¡å‹ï¼Œå¿«é€Ÿç†è§£Queryï¼Œå¹¶åšå…³é”®è¯è¯†åˆ«ã€å‚æ•°æŠ½å–ã€æ„å›¾è¯†åˆ«ã€æœç´¢è¯æ”¹å†™ç­‰å·¥ä½œã€‚
  * æ¥ä¸‹æ¥ä¼šè¿›è¡Œ5æ¬¡æœç´¢ï¼Œæœç´¢å®Œåå¯¹å†…å®¹è¿›è¡ŒReRankï¼Œè¿™é‡Œé¢æ’åºã€é€‰æ‹©å“ªäº›å†…å®¹ä½œä¸ºä¸»ç­”æ¡ˆï¼Œä¹Ÿéœ€è¦LLMå‚ä¸ã€‚
  * åŸºäºç”Ÿæˆçš„ç­”æ¡ˆï¼Œè¦ç”Ÿæˆå„ç§è¿½é—®ã€å»¶ä¼¸é˜…è¯»ã€ç›¸å…³äº‹ä»¶ã€æ€ç»´å¯¼å›¾ï¼Œè¿™é‡Œé¢360æ€ç»´å¯¼å›¾çš„æ•ˆæœä¹‹æ‰€ä»¥å¥½ï¼Œå°±æ˜¯360ä¸“é—¨å»è®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰èƒ½å¤ŸæŠŠç»“æ„åŒ–çš„ä¿¡æ¯æ¯”è¾ƒå‡†ç¡®å’Œæœ‰ç»“æ„çš„æè¿°å‡ºæ¥ã€‚

* ç§˜å¡”AIæœç´¢çš„é—®é¢˜
  * å°å‚æ•°é‡æ¨¡å‹*ä¸Šä¸‹æ–‡çª—å£å¾ˆçŸ­ï¼Œæ‰€ä»¥åªèƒ½è®¨å·§ï¼š**è¿­ä»£è°ƒç”¨ï¼Œåˆ†æ‰¹ç”Ÿæˆ**
    * ç¬¬ä¸€æ­¥å…ˆç”Ÿæˆä¸€ä¸ªå¤§çº²ï¼Œç„¶åå†åŸºäºå¤§çº²å»åšé€æ­¥çš„æ‰©å†™ï¼Œæ•´ä½“æ‹¼å‡‘ä¸‹æ¥å°±èƒ½å¤Ÿå¾—åˆ°å¾ˆé•¿çš„æ–‡ç« ã€‚
    * å¯èƒ½å¯¼è‡´å†…å®¹é‡å¤ã€å†…å®¹è´¨é‡ä½
  * 360çš„ç­–ç•¥ï¼š**ç”¨ä¸­æ–‡æœä¸€æ¬¡ã€å†ç”¨è‹±æ–‡æœä¸€æ¬¡**
* å…³äºprompt
  * å¯¹äºæ€»ç»“ï¼Œä»–ä»¬ç¡®å®å¯ä»¥ç”¨ä¸€ä¸ªå¾ˆçŸ­çš„Promptï¼ˆè¿­ä»£äº†éå¸¸å¤šçš„ç‰ˆæœ¬ï¼‰è·å¾—å¾ˆå¥½çš„æ€»ç»“ç»“æœï¼Œ**ä½†æ˜¯AIæœç´¢ç”¨æˆ·çš„æ„å›¾æœ‰å¾ˆå¤šï¼Œå¹¶ä¸å•çº¯æ˜¯å†…å®¹æ€»ç»“ã€‚**å¯¹äºæ¨¡å‹æ¥è¯´ï¼Œå¯¹ä¸åŒçš„æœç´¢æ„å›¾ï¼Œæƒ³è¦ç”Ÿæˆå¥½çš„ç»“æœï¼Œæ˜¯éœ€è¦ä¼ é€’ç»™å¤§æ¨¡å‹ä¸åŒçš„ä»·å€¼å–å‘çš„ã€‚
  * queryçš„æ„å›¾è¯†åˆ«åˆ†ç±»åšåˆ°äº†4000å¤šç§ï¼Œæ¯ä¸€ç§éœ€æ±‚é…å¯¹åº”çš„Prompt
* æœŸæœ›AIæœç´¢å¤„ç†å¤æ‚é—®é¢˜
  * å‡è®¾ä½ åœ¨æœç´¢â€œæ‰¾åˆ°æ³¢å£«é¡¿æœ€å—æ¬¢è¿çš„ç‘œä¼½æˆ–æ™®æ‹‰æå·¥ä½œå®¤ï¼Œå¹¶æ˜¾ç¤ºå…¶å…¥é—¨ä¼˜æƒ å’Œä»Beacon Hillæ­¥è¡Œçš„æ—¶é—´â€ã€‚å¤šæ­¥æ¨ç†çš„AIä¼šï¼š
    * è¯†åˆ«å‡ºä½ è¦æ‰¾çš„æ˜¯ç‘œä¼½æˆ–æ™®æ‹‰æå·¥ä½œå®¤ã€‚
    * æ‰¾åˆ°æ³¢å£«é¡¿åœ°åŒºçš„ç›¸å…³å·¥ä½œå®¤ã€‚
    * ç­›é€‰å‡ºé‚£äº›åœ¨å½“åœ°å—æ¬¢è¿çš„å·¥ä½œå®¤ã€‚
    * æ£€æŸ¥è¿™äº›å·¥ä½œå®¤æ˜¯å¦æä¾›æ–°ä¼šå‘˜çš„å…¥é—¨ä¼˜æƒ ã€‚
    * è®¡ç®—æ¯ä¸ªå·¥ä½œå®¤ä»Beacon Hillæ­¥è¡Œçš„æ—¶é—´ã€‚
    * ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºä¸€ä¸ªè¯¦ç»†çš„ç»“æœåˆ—è¡¨ã€‚
* ç´¢å¼•åº“çš„æˆæœ¬ï¼šçˆ¬5000ä¸‡çš„ç½‘é¡µï¼Œå¤§æ¦‚éœ€è¦ä¸€ä¸¤ç™¾ä¸‡RMBã€‚ï¼ˆFrom 360AIï¼‰
* AI SEOï¼šæ¯å¤©ç”Ÿæˆæ•°ç™¾ä¸‡ä¸ªç­”æ¡ˆç½‘é¡µï¼Œè¦†ç›–è‡ªå·±æœç´¢å†…çš„æµé‡
* æœ¬åœ°å¤§æ¨¡å‹

![å›¾ç‰‡](./AI-Algorithms/640-20241019015912504)

#### Perplexity

* [Perplexity CEOæ­ç§˜ğŸ¤–æœç´¢å†…æ ¸ï¼šä¸è®­ç»ƒæ¨¡å‹ã€å…³é”®åœ¨äºè·¯ç”±ç¼–æ’ã€æ¯”æœç´¢å¼•æ“æ›´æ•¢è¯´](https://mp.weixin.qq.com/s/aBAd6-mDEgNCo8s2hOsE3w)
  * AIæœç´¢ä¼˜åŠ¿ï¼š
    * å¯¹äºåŸºäºLLMçš„ç­”æ¡ˆå¼•æ“ï¼Œä¼ ç»Ÿçš„æ£€ç´¢æ–¹å¼åªéœ€è¦ä¼˜åŒ–å¬å›ç‡å³å¯
  * å…³äºä½¿ç”¨ç½‘é¡µ
    * **ä½¿ç”¨æ¥è‡ªå¤šä¸ªæœç´¢æä¾›å•†çš„å¤§é‡æ’åä¿¡å·ã€‚æˆ‘ä»¬å®é™…ä¸Šæ„å»ºäº†è‡ªå·±çš„ç´¢å¼•ï¼Œä½†ä¹Ÿä¾èµ–äºå¤§é‡æ•°æ®æä¾›å•†çš„æ’åä¿¡å·**ã€‚å¯¹äºæŸäº›æˆ‘ä»¬ä¸è‡ªè¡ŒæŠ“å–æˆ–çˆ¬å–çš„ç½‘ç»œåŸŸåï¼Œæˆ‘ä»¬è¿˜ä¾èµ–äºç¬¬ä¸‰æ–¹æ•°æ®æä¾›å•†ï¼Œè¿™äº›æä¾›å•†åªæä¾›é«˜å±‚çº§çš„æ‘˜è¦ç‰‡æ®µå’Œä¸URLç›¸å…³çš„å…ƒæ•°æ®ï¼Œè€Œä¸æ˜¯å®é™…å†…å®¹ã€‚
    * äººä»¬å¯¹è¿™äº›åŸŸåçš„ä¿¡ä»»ç¨‹åº¦
  * queryåŒ¹é… - ngramé‡å  - è¯­ä¹‰æ£€ç´¢
    * åŸºäºæŸ¥è¯¢è¯åŒ¹é…ï¼Œè¿™ç±»ä¼¼äºä¼ ç»Ÿçš„æ£€ç´¢ï¼Œä¾‹å¦‚TF-IDFé£æ ¼çš„æ£€ç´¢ã€‚
  * æœé›†å¼€æ”¾å¼ä¿¡æ¯
    * æˆ‘åº”è¯¥æŠ•èµ„è‹±ä¼Ÿè¾¾å—ï¼Ÿæˆ‘ä¸å¤ªæ˜ç™½ã€‚æ‰€æœ‰ä¿¡æ¯éƒ½å·²è¢«è®¡å…¥ä»·æ ¼äº†å—ï¼Ÿé»‘è‰²ä¸–ç•ŒèŠ¯ç‰‡å»¶è¯¯ä¼šæ€æ ·ï¼Ÿå¯¹è®­ç»ƒGPUçš„éœ€æ±‚å¦‚ä½•ï¼Ÿè‹±ä¼Ÿè¾¾ç°åœ¨çš„ç«äº‰å¯¹æ‰‹æ˜¯è°ï¼Ÿå®ƒä»ç„¶æ²¡æœ‰ç«äº‰å¯¹æ‰‹å—ï¼Ÿäº”å¹´åçš„å¸‚åœºä¼šæ€æ ·ï¼Ÿè¿™å°†å¦‚ä½•å½±å“äºšé©¬é€Šç½‘ç»œæœåŠ¡ï¼ˆAWSï¼‰çš„æ”¶å…¥ï¼Ÿè‹±ä¼Ÿè¾¾çš„åˆ©æ¶¦ç‡æ˜¯å¦‚ä½•è¢«æŒ¤å‹çš„ï¼Ÿè°å¯èƒ½ä¼šè¿™ä¹ˆåšï¼Ÿ

#### å…¶å®ƒ

* è§†é¢‘/æ’­å®¢ï¼š
  * https://dexa.ai/
  * ç»¼è¿°ï¼šhttps://mp.weixin.qq.com/s/t09ffrqc9C5xMj48zna-0A

* [å‚ç›´](https://www.bigcommerce.com/articles/ecommerce/recommendation-engine/#h2_best_ecommerce_recommendation_engines)ï¼šalgoliaã€boomreachã€clerkã€emrsysã€nostoã€[Boost Commerce](https://boostcommerce.net/)
* æ—¥æœ¬ï¼š silvereggï¼ˆæ¨èï¼‰ï¼ŒES(æœç´¢ï¼‰ã€zeta search/algolia (æœæ¨ï¼‰



## CRS å¯¹è¯å¼æœæ¨



### Literature Review

* CRSï¼šå‚è€ƒã€ŒRecommender AI Agentã€çš„æ–‡çŒ®ç»¼è¿°

  * attribute-based question-answering CRS
    * aims to recom- mend suitable items to users within as few rounds as possible. The interaction between the system and users primarily revolves around question-answering concerning desired item attributes, iteratively refining user interests
    * Key research challenges in this area include developing strategies for selecting queried attributes(Mirzadeh, Ricci, and Bansal 2005; Zhang et al. 2018)
    * addressing the exploration- exploitation trade-off(Christakopoulou, Radlinski, and Hof- mann 2016; Xie et al. 2021).


  * open-ended conversation CRS
    * leveraging pre- trained language models for conversation understanding and response generation
      * incorporated external knowledge

### å·¥ç¨‹

* semantic cacheä¼˜åŒ–ï¼šhttps://www.couchbase.com/blog/faster-llm-apps-semantic-cache-langchain-couchbase/

### ç®—æ³•å·¥ç¨‹

#### [CRS] [Google] [RecLLM] Leveraging Large Language Models in Conversational Recommender Systems

* Intro

  * éš¾ç‚¹ï¼ša large, evolving item corpus and a lack of conversational data for training.
    * making it challenging for an LLM to memorize the corpus within its parameters.
    * Evaluation of CRSs is difficult in part due to the generative and open-ended nature of the mixed-initiative dialogue [39]
  * we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture powered by LLMs. 
    * For improved personalization, we describe how an LLM can consume interpretable natural language user profiles and use them to modulate session-level context.
    * To overcome conversa- tional data limitations in the absence of an existing production CRS, we propose techniques for building a controllable LLM-based user simulator to generate synthetic conversations.
  * å®éªŒè®¾ç½®ï¼šIn terms of the item corpus, RecLLM recommends from the cor-
    pus of all public YouTube videos
    * there are no logs of users interacting with this system to jumpstart training of the model(s)
  * å¯¹è¯èƒ½åŠ›ï¼š
    * retaining context
    * handling topic shifts
    * referencing slate items.

* Dialogue Management

  * extra challenges:
    * control
      * preference elicitationâ€”in which the system must figure out when and how to best query the user in order to extract maximal information about their preferencesâ€”is an entire subfield of CRS dialogue management [11, 74, 83, 112].
    * ambiguity
      * Short-term satisfaction and long-term coverage: Understanding how users tolerate algorithmic exploration.
    * Grounding
  * **Unified LLM Impl**: one of the distinguishing features of this architecture is that there no longer exists a hardcoded policy graph with fixed dialogue states
    * on a given system turn the LLM generates a sequence of natural language outputs that encapsulate all context tracking, intermediate reasoning, natural language generation, and API calls to the rest of the system
    * System calls Hardcode: "Response: <message>";   "Request: <query>"
    * Other outputs of the LLM can function as chain-of-reasoning steps, instructions to itself to follow, or dialogue state tracking inferences
    * åˆ©ç”¨in-context few-shot learning or tuningè®©LLMæŒæ¡æ–°çŠ¶æ€çš„å¤„ç†ï¼Œè€Œä¸æ˜¯å¼€å‘æ–°ç»„ä»¶å¢åŠ çŠ¶æ€
      * In Section 4.2 we discuss ideas for overcoming this limita- tion by tuning our dialogue manager and recommendation modules with larger amounts of synthetically generated data.
  * ![image-20241005123415030](./AI-Algorithms/dialog-management.png)

* Recommendations and Refinement - Retrieval

  * Two-stage: with the added twist that the **ranker also jointly generates natural language explanations** for why each item is being selected
  * ![image-20241005130712872](./AI-Algorithms/llm-retrieval.png)

  * Generalized Dual Encoder Model: 
    * using an LLM as a context encoder
      * embeddingï¼šgenerated by extracting and then projecting a suitable activation layer from the model.
    * ç¼ºç‚¹ï¼šrequire large amounts of training data to constrain the context tower embeddings to occupy the same subspace as the item tower embedding
      * ä¸ºäº†ç”¨ä¸Šuser featureå’Œcontext feature
  * Direct LLM Search
    * ç¼ºç‚¹ï¼šè®°ä¸ä½å…¨é‡corpus
  * Concept Based Search
    * In this method the LLM outputs a list of concepts, which are then embedded and aggregated by the recom- mendation engine into a single context embedding
      * Concept Activation Vectors [43]
    * ä¼˜åŠ¿ï¼š
      * è®©LLMæå–conceptå¾ˆç®€å•
      * æ— éœ€tuning item embsï¼ˆå¯ä»¥ç›´æ¥ç”¨pretrained embï¼‰
    * ç¼ºç‚¹ï¼šone limitation is that lists of concepts are often a coarse representation of a conversation and similar to continuous bag-of-words methods [60] are lossy with respect to word order and other nuances of language, which can negatively affect retrieval quality.
      * æ€è€ƒï¼šæŒ‰ä¿¡æ¯ä»·å€¼æ’åº
  * Search API Lookup
    * ä¼˜åŠ¿åŒconcept based search
    * ä¾èµ–search apiçš„èƒ½åŠ›

* Rerank
  * within RecLLM we use the simple approach of bucketing the range of possible scores and having the LLM output a semantically meaningful phrase (e.g. "excellent fit") corresponding to a bucket id
  * scores the item using chain-of-thought reasoning[95]

![image-20241005140444126](./AI-Algorithms/rerank.png)

* User Profile
  * ç”¨è‡ªç„¶è¯­è¨€è¡¨ç¤º
    * ã€ŠOn Natural Language User Profiles for Transparent and Scrutable Recommendationã€‹
  * In RecLLM we build user profiles **based on a userâ€™s repeated interaction** with the system over multiple sessions, although it would be possible to incorporate other data sources as well.
    * Memory Extraction: ç”¨LLM
    * Triggeringï¼šç”¨RAGæ–¹æ³•ï¼Œåˆ¤æ–­ä¸Šä¸€å¥å’Œuser profileçš„ä½™å¼¦ç›¸ä¼¼åº¦
    * system integrationï¼š
      * For instance, the sys- tem may know that the user is allergic to seafood, but if the user explicitly says they want to see some videos about fish recipes to pass along to a friend itâ€™s important that the system overrides this preference from the user profile and gives the user what they are asking for
      * äº¤ç»™LLMï¼

![image-20241005140932414](./AI-Algorithms/user-profile.png)

* SIMULATION AND LARGE-SCALE TUNING

  * user simulation:
    * inputæ˜¯ä¹‹å‰æ‰€æœ‰å¯¹è¯
  * è¯„ä¼°realismçš„æ–¹æ³•ï¼šä¼—åŒ…ã€æ¨¡å‹ã€ensembleåˆ†ç±»çš„åˆ†å¸ƒ
    * diversityï¼šdefining a notion of entropy of Q with respect to the classifier ensemble

  * Controlled Simulationï¼šwe condition the user simulator on additional latent (to the CRS) variables that allow us to guide its behavior in a certain direction
    * Session-level controlï¼šuser profile
    * Turn-level controlï¼šuser intent
  * Generating Synthetic Training Data.
    * ![image-20241005145620374](./AI-Algorithms/generate-synthetic.png)

  * Tuning System Modules

    * Retrieval - tune a Generalized Dual Encoder Model
      * Regardless of whether we choose to tune only the adapter layers of the two tower model or the LLM params as well, the loss is fully differentiable and normal supervised learning with gradient descent suffices
    * Retrieval - tune Search API
      * we can reframe the setup as a contextual bandit problem [5], where the LLM is a policy, the labels are rewards signals, and the black box search algorithm is treated as the environment (see Figure 10b)
      * ![image-20241005150637875](./AI-Algorithms/tune-recllm.png)

    * Dialog system
      * æ–¹æ¡ˆä¸€ï¼šç»™1000ä¸ªä¾‹å­
      * æ–¹æ¡ˆäºŒï¼šRLHF
        * Generate a set of simulated sessions Q using a user simulator as outlined in Section 4.1
        * Have crowdsource workers evaluate our unified LLM by **rating per turn responses** within Q in terms of fluency, interestingness, groundedness etc, as well as giving session level ratings based on overall how effective the system was at helping the user explore the recommendations corpus
        * Train reward models on this rating data (likely also using LLMs with chain-of-thought reasoning).
        * Further tune the unified LLM on simulated sessions through reinforcement learning to optimize for proxy rewards generated by these reward models

* Related Work
  * In [33, 63, 100] a pretrained language model is tuned to process
    documents as part of a dual encoder retrieval model, and in [32] this is extended to full conversations as in the Generalized Dual Encoder proposal from Section 4.2. When the ground truth labels do not enable a fully differentiable loss function (such as in Search API Lookup), [65, 82] show it is still effective to tune LLMs for language generation tasks using techniques derived from reinforce- ment learning. Other works [14, 81] also use reinforcement learning to tune LLMs for open ended or task based dialogue using reward signals inferred from the conversations (e.g. through sentiment analysis or a notion of task completion).

#### [InteRecAgent] [CRS] Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations

> https://aka.ms/recagent
>
> figure 5: plan-first å’Œ reflection æœ€æœ‰ç”¨
>
> é—®é¢˜ï¼š
>
> * candidate busæ€ä¹ˆåšçš„ï¼Ÿ

* Intro
  * LLMs lack the knowledge of domain-specific item catalogs and be- havioral patterns, particularly in areas that diverge from gen- eral world knowledge, such as online e-commerce
    * fail to capture fine-grained, domain-specific behavior patterns, especially in domains with massive training data
  * InteRecAgentçš„ä»‹ç»
    * employs LLMs as the brain and recommender models as tools
    * a minimal set of essential tools required to transform LLMs into InteRecAgent
    * an efficient workflow within InteRecAgent for task execution, in- corporating key components such as memory components, dynamic demonstration-augmented task planning, and reflec- tion
  * InteRecAgentçš„è®¾è®¡æ€è·¯ï¼šInteractive Recommender Agent
    * â€œshared candidate busâ€
    * â€œlong-term and short-term user profileâ€
    * â€œplan-first executionâ€(plan-then-tool) strategy
      * InteRecAgent generates all the steps of tool- calling at once and strictly follows the execution plan to ac- complish the task.
      * a reflection strategy
    * åŸºäºGPT-4ç”Ÿæˆdatasetï¼Œå†finetune LLAMA2

![image-20241007231933770](./AI-Algorithms/inte-rec-agent.png)

* Methodology

  * hard conditions and soft conditions.
    * Hard conditions refer to explicit demands on items, such as â€œI want some popular sports gamesâ€ or â€œRecommend me some RPG games under $100â€.
    * Soft conditions pertain to demands that cannot be explicitly expressed with discrete attributes and require the use of semantic matching models, like â€œI want some games similar to Call of Duty and Fortniteâ€.
  * æ½œåœ¨çš„Hard conditionsï¼šSQL Query Tool â†’ SQL Retrieval Tool â†’ Ranker Tool
    * æƒ³è¦æ¯”xxxè´µçš„
  * è§£å†³ReActçš„ç¼ºé™·
    * To tackle these chal- lenges, we enhance the three critical components of a typical LLM-based agent, namely memory (Section 3.2), task planning (Section 3.3 and 3.4), and tool learning abilities (Section 3.5).

* Frameworkç»†èŠ‚

  * The Candidate Bus, accessible by all tools, comprises two parts: a data bus for storing can- didate items, and a tracker for recording each toolâ€™s output.
  * Which ofthese movies do you think is most suitable for me: [Movie List]?â€ In this case, the LLM will call a special toolâ€”**the memory initialization tool**â€”to set the user-specified items as the initial candidate items.
  * User Profile
    * åŸºäºå¯¹è¯å†å²åˆ†æUser Profileï¼Œæœ‰â€œlikeâ€ã€â€œdislikeâ€ã€â€œexpectâ€ä¸‰ç§
      - ä¸ºäº†é¿å…å‚¨å­˜å¤ªé•¿çš„å¯¹è¯å†å²ï¼Œè®¾å®šäº†â€œlong-term"ã€â€œshort-termâ€ï¼Œå½“è¶…è¿‡å¯¹è¯æ¡†ï¼Œå°±ç”¨short-term interestæ›´æ–°long-term interest

* Plan-first Execution with Dynamic Demonstrations

  * ç›¸æ¯”step-by-stepçš„ä¼˜ç‚¹
    * step-by-stepä¸æ–¹ä¾¿å¯¹å„ç§dynamic toolè°ƒç”¨åšin-context learning
    * è€Œè¿™ä¸ªæ–¹æ³•å¯ä»¥å†™å¾ˆå¤š âŸ¨query, planâŸ© pairs
  * ![image-20241020001429229](./AI-Algorithms/image-20241020001429229.png)

  * To address the challenge, we introduce a dynamic demonstration strategy, where only a few demonstrations that are most simi- lar to current user intent are incorporated into the prompt.
    * **example sampler**
  * LLMç”Ÿæˆexamplesï¼š
    * plan -> intent -> plan
    * The inconsistency indicates that the quality of the generated intent is not high enough, and we only retain those consistent demonstrations. 

* Reflection

  * actor-critic reflection mechanism

* å¾®è°ƒ7Bå°æ¨¡å‹

  * [instructions, tool execution plans] pairs

* Evaluation

  * å¯¹è¯å¼ï¼šHit@k and AT@k, representing the success of recommending the target item within k turns and the average turns (AT) re- quired for a successful recommendation
  * æ¯”Chat-Recæ•ˆæœå¥½ï¼Œå¯èƒ½çš„ä¼˜åŠ¿ç‚¹æœ‰å¾ˆå¤š
    * SASRecåšrerank
    * æ›´åˆç†çš„plan
    * reflection

* Casesï¼šFigure 6

* ç»“è®ºï¼š

  * figure 5: plan-first å’Œ reflection æœ€æœ‰ç”¨

* Prompts

  * User simulator

    * ```
      You are a user chatting with a recommender for {item} rec- ommendation in turn. Your history is {history}. Your tar- get items: {target}. Here is the information about target you could use: {target item info}. You must follow the rules below during chat. If the recommender recommends {target}, you should ac- cept. If the recommender recommends other items, you should refuse them and provide the information about {target}. If the recommender asks for your preference, you should provide the information about {target}. You could provide your history. Your output is only allowed to be the words from the user you act. If you think the con- versation comes to an ending, output a âŸ¨ENDâŸ©. You should never directly tell the target item. Only use the provided in- formation about the target. Never give many details about the target items at one time. Less than 3 conditions is better. Now lets start, you first, act as a user. Here are the previous conversation you have completed: {chat history}.
      ```

  * Task Descriptionsï¼š Figure C1

  * Tool Descriptionsï¼šFigure C2-C5

  * Reflectionï¼šC6

  * Demonstration Generationï¼š

    * generating planï¼šC7
    * ä¸¤ç§ç”Ÿæˆintentï¼šC8ã€C11

  * å¤§æ¨¡å‹åšæ¨èï¼šC9ã€C10
