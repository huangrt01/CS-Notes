[toc]

plethora of ML frameworksï¼šNCCL, Horovod, BytePS, Mesh-TensorFlow, Gpipe, Ray, HugeCTR, DALI

### èµ„æ–™æœé›†

* RecSys2024 https://recsys.acm.org/recsys24/accepted-contributions/
* ç¾å›¢ï¼šhttps://space.bilibili.com/18937923

### Intro

* 100TB model = 50ä¸‡äº¿å‚æ•°
  * 1ä¸‡äº¿=1000B=1Tï¼Œå‚æ•°å­˜å‚¨ç”¨ fp16
* MLSyså†å²
  * å‰å¤§æ¨¡å‹æ—¶ä»£MLSysçš„èŠ‚å¥ï¼š  
    * Ring AllReuce, Hovord (2017) -> PyTorch (2020) 
    * Activation Checkpoint (2014) -> PyTorch (2019) 
    * Automatic Mixed Precision (2017) -> PyTorch (2019) 
    * Int8 Quant (2015) -> TensorRT (2018)
  
  * å¤§æ¨¡å‹æ—¶ä»£ï¼š
    * Memory Efficient Attention with Online Softmax (2021) -> FlashAttention in Megatron-LM (2022) 
    * Continuous Batching (2022), Paged Attention (2023) -> vLLM, TensorRT-LLM (2023) 
    * Speculative Sampling (2023) -> Everywhere in LLM Serving (2023)
    * Sequence Parallel (2023) ->  Megatron-LLM (2023) 
  
* MLSysèµ°å‘ä½•æ–¹
  * æ— è®ºæ˜¯NVå†…éƒ¨NVLinkå…šå’ŒMellanoxå…šçš„äº‰è®º,è¿˜æ˜¯å…¶ä»–å¾ˆå¤šä¼ä¸šéƒ½é¢ä¸´ç›¸åŒçš„é—®é¢˜, è®¡ç®—/ç½‘ç»œ/å­˜å‚¨/æœåŠ¡å™¨/èŠ¯ç‰‡ç­‰å¤šä¸ªå›¢é˜Ÿå¦‚ä½•ç´§è€¦åˆ, æ›´è¿›ä¸€æ­¥çš„æ‰©å±•åˆ°ä¸Šå±‚çš„ç®—å­/å¹¶è¡Œç­–ç•¥/ç®—æ³•ç­‰å¤šä¸ªå›¢é˜Ÿçš„ååŒ. â€”â€” zartbot
  * ç°åœ¨è¿™äº›æ¨¡å‹çš„Tensorä¼ è¾“é‡æ¥çœ‹, å°½é‡çš„åšå¥½Overlapå’Œæå‡å¸¦å®½å°±å¤Ÿäº†. æ˜¯å¦è¿˜è¦Load/Store. å¦‚æœç¨€ç–æ¨¡å‹æ˜¯ä¸€æ¡è·¯,é‚£ä¹ˆå°±ä¸€å®šè¦. 
    * ä¾‹å¦‚ä¸€ä¸ªé›†ç¾¤é€šè¿‡ä¸€äº›ç½‘ç»œæ‹“æ‰‘æŠŠAllreduceçš„é—®é¢˜è§£å†³å¹²å‡€äº†, MoEç­‰å…¶å®ƒç»“æ„ä¸€æ¥,AlltoAllåˆä¸è¡Œäº†.

  * è¿™ä¸€æ¬¡äººå·¥æ™ºèƒ½é©å‘½çš„æ•°å­¦åŸºç¡€æ˜¯ï¼šèŒƒç•´è®º/ä»£æ•°æ‹“æ‰‘/ä»£æ•°å‡ ä½•è¿™äº›äºŒåä¸–çºªçš„æ•°å­¦ç¬¬ä¸€æ¬¡ç™»ä¸Šå•†ç”¨è®¡ç®—çš„èˆå°ã€‚

* Parameter Server

  * Spark MLlib: åŒæ­¥é˜»æ–­å¼

  * Parameter Server: å¼‚æ­¥éé˜»æ–­å¼
  * ä¸¤è€…åŒºåˆ«åœ¨äºæ¨¡å‹å‚æ•°çš„åˆ†å‘æ˜¯å¦åŒæ­¥


* RecSysä¸­å·¥ç¨‹ä¸ç†è®ºä¹‹é—´çš„å‡è¡¡
  * end2endï¼šå¼ºè°ƒæ¨¡å‹ä¸€è‡´æ€§çš„æ”¶ç›Š
  * two stagesï¼šå¼ºè°ƒæ¨¡å‹å®æ—¶æ€§çš„æ”¶ç›Š

#### æˆæœ¬å’Œæ€§èƒ½è¯„ä¼°

å‚è€ƒ ã€ŒLLM-MLSysã€


#### [Google Research: Themes from 2021 and Beyond](https://ai.googleblog.com/2022/01/google-research-themes-from-2021-and.html)

  * Trend 1: More Capable, General-Purpose ML Models
    * CoTrain models: PolyViT https://arxiv.org/abs/2111.12993
    * Pathways: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/
      * æœ¬è´¨æ˜¯å¢å¼ºç¥ç»ç½‘ç»œç»“æ„çš„ç¨€ç–æ€§ï¼Œä¸ä»…ä»…æ˜¯å‚æ•°çš„ç¨€ç–æ€§
      * Discussion: https://www.zhihu.com/question/495386434/answer/2199374013
  * Trend 2: Continued Efficiency Improvements for ML
    * Continued Improvements in ML Accelerator Performance
      * TPUv4
      * Device-ML: https://ai.googleblog.com/2021/11/improved-on-device-ml-on-pixel-6-with.html
    * Continued Improvements in ML Compilation and Optimization of ML Workloads
      * XLA: https://www.tensorflow.org/xla
      * https://mangpo.net/papers/xla-autotuning-pact2021.pdf
      * GSPMD: https://ai.googleblog.com/2021/12/general-and-scalable-parallelization.html
    * Human-Creativityâ€“Driven Discovery of More Efficient Model Architectures
      * Transformetã€ViT
    * Machine-Driven Discovery of More Efficient Model Architectures
      * NAS -> Primerã€EfficientNetV2
      * RL: https://ai.googleblog.com/2020/07/automl-zero-evolving-code-that-learns.html
    * Use of Sparsity
      * Switch Transformer
  * Trend 3: ML Is Becoming More Personally and Communally Beneficial
    * ä» ML+äº§å“ï¼ˆPixelæ‰‹æœºï¼‰ åˆ° è”é‚¦å­¦ä¹  
    * phone cameras
    * live translate/caption
    * [federated analytics](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html) and federated learning
      * å¤ç”¨ FL çš„ evaluating èƒ½åŠ› (without the learning part)
      * Now Playing: on-device database
      * https://arxiv.org/pdf/1902.01046.pdf
      * secure aggregation protocol
  * Trend 4: Growing Impact of ML in Science, Health and Sustainability
      * Large-Scale Application of Computer Vision for New Insights
          * [large-scale study of synaptic connectivity in the human cortex](https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html)
          * [deep-learningâ€“based approach to weather forecasting](https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html)
    * Automated Design Space Exploration
      * a Transformer-based variational autoencoder learns to [create aesthetically-pleasing and useful document layouts](https://ai.googleblog.com/2021/06/using-variational-transformer-networks.html)
      * [automates the exploration of the huge design space of tweaks for computer game rules](https://ai.googleblog.com/2021/03/leveraging-machine-learning-for-game.html)
    * Application to Health
      * Our [ML-based phenotyping](https://www.sciencedirect.com/science/article/pii/S0002929721001889) method improves the scalability of converting large imaging and text datasets into phenotypes usable for genetic association studies, and our [DeepNull](https://www.biorxiv.org/content/10.1101/2021.05.26.445783v2) method better leverages large phenotypic data for genetic discovery. We are happy to release both as [open-source methods](https://github.com/Google-Health/genomics-research) for the scientific community.
    * ML Applications for the Climate Crisis
      * [eco-friendly routing in Google Maps](https://blog.google/products/maps/3-new-ways-navigate-more-sustainably-maps/)
      * making our [Maps products smarter about electric vehicles](https://ai.googleblog.com/2021/01/addressing-range-anxiety-with-smart.html) can help alleviate range anxiety, encouraging people to switch to emissions-free vehicles
        * the fear that the car will run out of power before reaching a charging station
        * the charging time can be a significant fraction of the total travel time and can vary widely by station, vehicle model, and battery level. In addition, the charging time is non-linear â€” e.g., it takes longer to charge a battery from 90% to 100% than from 20% to 30%.
        * this high density implies that a trip between two stations that are relatively far apart will undoubtedly pass through multiple other stations. In this case, maintaining information about the long edge is redundant, making it possible to simply add the smaller edges (*[spanners](https://en.wikipedia.org/wiki/Geometric_spanner)*) in the graph, resulting in sparser, more computationally feasible, graphs.
      * On a longer time scale, **fusion** holds promise as a game-changing renewable energy source. In a long-standing collaboration with TAE Technologies, we have [used ML to help maintain stable plasmas](https://ai.googleblog.com/2021/11/another-step-towards-breakeven-fusion.html) in their fusion reactor by suggesting settings of the more than 1000 relevant control parameters. With our collaboration, TAE achieved their major goals for their [Norman](https://en.wikipedia.org/wiki/TAE_Technologies#C-2W/Norman) reactor, which brings us a step closer to the goal of [breakeven fusion](https://en.wikipedia.org/wiki/Fusion_energy_gain_factor#Breakeven). The machine maintains a stable plasma at 30 million Kelvin (donâ€™t touch!) for 30 milliseconds, which is the extent of available power to its systems. They have completed a design for an even more powerful machine, which they hope will demonstrate the conditions necessary for breakeven fusion before the end of the decade.
      * [wildfire boundary map](https://blog.google/products/search/mapping-wildfires-with-satellite-data/)
        * https://arxiv.org/abs/2111.02780
      * carbon neutral
        * https://arxiv.org/abs/2104.10350

* Trend 5: Deeper and Broader Understanding of ML
  * [Marian Croakâ€™s vision for responsible AI at Google](https://blog.google/technology/ai/marian-croak-responsible-ai/)
  * RecSys
    * [Recent work has helped to better understand these relationships of the individual components](https://research.google/pubs/pub49284/)
    * [learn in an unbiased manner](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/54a3b73ea1e85e94e5d5bb5a9df821a1f32aa783.pdf)
  * Political Correctness
    *  [reducing gender bias in our translation systems](https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html)
    *   [designing for digital wellbeing](https://design.google/library/designing-for-digital-wellbeing/#normalizing-compassion) and [addressing racial equity issues in ML systems](https://design.google/library/racial-equity-everyday-products/), including [improving our understanding of the experience of Black Americans using ASR systems](https://www.frontiersin.org/articles/10.3389/frai.2021.725911/full?&utm_source=Email_to_authors_&utm_medium=Email&utm_content=T1_11.5e1_author&utm_campaign=Email_publication&field=&journalName=Frontiers_in_Artificial_Intelligence&id=725911)
  * distributional shift
    * [Deep Bootstrap framework](https://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html)
  * [data cascades in ML](https://ai.googleblog.com/2021/06/data-cascades-in-machine-learning.html)
	  * [PAIR Guidebook](https://pair.withgoogle.com/guidebook/)
		* ![img](./MLSys/data-cascades.png)
	* [Know Your Data](https://knowyourdata.withgoogle.com/)
	  *  [finding and investigating anomalous data](https://ai.googleblog.com/2021/09/discovering-anomalous-data-with-self.html)
	  * [methods to better understand the influence that particular training examples](https://ai.googleblog.com/2021/02/tracin-simple-method-to-estimate.html) can have on an ML model
	  * a [case study of how to use the Know Your Data tool](https://ai.googleblog.com/2021/08/a-dataset-exploration-case-study-with.html) to explore issues like gender bias and age bias in a dataset.
	* more inclusive and less biased public datasets
	  * [Dataset Search](https://datasetsearch.research.google.com/)
	  * [Perspective API](https://www.perspectiveapi.com/case-studies/) tool
	* we introduced a comprehensive taxonomy to reason about [the changing landscape of online hate and harassment](https://research.google/pubs/pub49786/). We also investigated [how to detect covert forms of toxicity](https://aclanthology.org/2021.hcinlp-1.3), such as microaggressions, that are often ignored in online abuse interventions, studied how conventional approaches to deal with disagreements in data annotations of such subjective concepts might [marginalize minority perspectives](https://arxiv.org/abs/2110.05699), and proposed a new [disaggregated modeling approach that uses a multi-task framework](https://arxiv.org/abs/2110.05719) to tackle this issue
	* å¯è§£é‡Šæ€§
	  * [understanding the acquisition of human chess concepts](https://arxiv.org/abs/2111.09259)
	  * [Language Interpretability Tool](https://pair-code.github.io/lit/)
	* Sociotechnical aspects
	  * [supporting family caregiving.](https://research.google/pubs/pub49916/)
	* ML and privacy
	  * highlighting that[ training data can sometimes be extracted from large models](https://www.usenix.org/system/files/sec21-carlini-extracting.pdf) and pointing to how privacy can be achieved in large models, e.g., as in[ differentially private BERT](https://arxiv.org/abs/2108.01624).
	  * federated learning and analytics
	  * other techniques: [ private clustering](https://ai.googleblog.com/2021/10/practical-differentially-private.html),[ private personalization](https://proceedings.neurips.cc/paper/2021/hash/f8580959e35cb0934479bb007fb241c2-Abstract.html),[ private matrix completion](http://proceedings.mlr.press/v139/chien21a/chien21a.pdf),[ private weighted sampling](http://proceedings.mlr.press/v130/cohen21b.html),[ private quantiles](http://proceedings.mlr.press/v139/gillenwater21a.html),[ private robust learning of halfspaces,](http://proceedings.mlr.press/v130/ghazi21a.html) and in general,[ sample-efficient private PAC learning](https://dl.acm.org/doi/10.1145/3406325.3451028)
	  * privacy notions: [label privacy](https://proceedings.neurips.cc/paper/2021/file/e3a54649aeec04cf1c13907bc6c5c8aa-Paper.pdf) and[ user](https://proceedings.neurips.cc/paper/2021/file/67e235e7f2fa8800d8375409b566e6b6-Paper.pdf) versus[ item level privacy](https://proceedings.neurips.cc/paper/2021/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf).
	* Other Work
	  * Quantun AI: https://blog.google/technology/research/2021-year-review-google-quantum-ai/ã€https://quantumai.google/learn/map
	    * the Noisy, Intermediate Scale Quantum (NISQ) computing era
	    * Qubit å’Œ Bit çš„åŒºåˆ«ï¼Œåœ¨äºå‰è€…çš„ä¸ç¡®å®šæ€§ï¼Œç»™å­˜å‚¨ã€è®¡ç®—ã€ç®—æ³•çš„å½¢å¼éƒ½å¸¦æ¥å‰§å˜
	    * 100 qubits to build a prototype of a error-corrected **logical qubit**
	      * 1000 qubits make a logical qubit long-live
	      * 10000 qubits => complex softwares requires significant work on fabrication technology, control software, and more
	      * 100000 qubits ~ 100 logical qubits: a quantum computer
	    * TFQ: https://www.tensorflow.org/quantum

### æˆæœ¬å’Œæ€§èƒ½è¯„ä¼°

* [MFUä¸FLOPsè®¡ç®—](https://zhuanlan.zhihu.com/p/690804699?utm_psn=1830997251394240513)
  * æ¨¡å‹ç®—åŠ›åˆ©ç”¨ç‡ï¼ˆModel FLOPs Utilizationï¼Œ MFUï¼‰å’Œç¡¬ä»¶ç®—åŠ›åˆ©ç”¨ç‡ï¼ˆHardware FLOPs Utilizationï¼Œ HFUï¼‰
  * æ¨¡å‹ç®—åŠ›åˆ©ç”¨ç‡æ˜¯æŒ‡ æ¨¡å‹ä¸€æ¬¡å‰åå‘è®¡ç®—æ¶ˆè€—çš„çŸ©é˜µç®—åŠ› ä¸æœºå™¨ç®—åŠ›çš„æ¯”å€¼
  * ç¡¬ä»¶ç®—åŠ›åˆ©ç”¨ç‡æ˜¯æŒ‡ è€ƒè™‘é‡è®¡ç®—åï¼Œæ¨¡å‹ä¸€æ¬¡å‰åå‘è®¡ç®—æ¶ˆè€—çš„çŸ©é˜µç®—åŠ› ä¸æœºå™¨ç®—åŠ›çš„æ¯”å€¼
  * llmçš„flopsï¼šhttps://xffxff.github.io/posts/flops

### ç®—æ³•å·¥ç¨‹ååŒ

* embeddingå’Œç‰¹å¾å¸¦æ¥çš„åœ¨çº¿å†…å­˜æˆæœ¬ï¼š
  * ftrl -> ç¨€ç–æ€§ã€å…¨0 Embedding
  * lasso
  * sparseNAS



### åˆ†å¸ƒå¼è°ƒåº¦æ¡†æ¶

* æ¨¡å‹+æ•°æ®+èµ„æº+è°ƒåº¦
  * èµ„æºï¼šæ± åŒ–/æ§½ä½ï¼Œæ··éƒ¨/æ½®æ±/å¼‚æ„/å¤šæœºæˆ¿/æ–°ç½‘ç»œæ¶æ„
    * æ··éƒ¨ï¼šç¦»çº¿/è¿‘çº¿ï¼Œå¼¹æ€§è®¡ç®—
  * æ•°æ®ï¼šbatch/streamï¼Œstreamæ•°æ®è¯­ä¹‰æ›´è´´è¿‘åœ¨çº¿è¯­ä¹‰
    * ç‰©ç†æ•°æ®ï¼ˆè¡Œé—´æ•°æ®å¤„ç†ï¼‰ã€é€»è¾‘æ•°æ®ï¼ˆè¡Œå†…æ•°æ®å¤„ç†ï¼‰
* å¤šè§’è‰²ç”Ÿå‘½å‘¨æœŸç®¡ç†
  * éœ€æ±‚ï¼šè§’è‰²ä¾èµ–æ•°æ®ï¼ˆpromise->actorï¼‰ã€è§’è‰²ä¾èµ–è§’è‰²ï¼ˆactor->actorï¼‰ã€å¤šè§’è‰²ç”Ÿå‘½å‘¨æœŸæ–¹æ³•æ‰§è¡Œçš„åŒæ­¥
    * checkpoint/failover
  * æ€è·¯ï¼š
    * å…¨å±€çŠ¶æ€æœº ï¼ˆå°†æœ€é‡çš„æ•°æ®ç›¸å…³æœ‰çŠ¶æ€è§’è‰²æŠ½è±¡ä¸ºå…¨å±€çŠ¶æ€æœºï¼‰/ å…¨å±€å¤šä¸ªç‹¬ç«‹å®šæ—¶ä»»åŠ¡
    *  æ— çŠ¶æ€op
      * å…¨å±€çŠ¶æ€æœºï¼ˆroot AMï¼‰+å±€éƒ¨çŠ¶æ€æœºï¼ˆrole AMï¼‰è¿‡äºå¤æ‚
    * Op æ¶‰åŠçš„æ•°æ®å±‚é¢ï¼š1ï¼‰metadataï¼›<-- 2ï¼‰resource or dataï¼›<-- 3ï¼‰business logic

å¼€æºæ¡†æ¶

* TFX on kubeflow, MLOpsçš„å®è·µ
	
	* å„ç»„ä»¶ç‹¬ç«‹æ€§æ›´å¼ºï¼Œæ•°æ®è½ç›˜ç¯èŠ‚å¤š
	* kubeflow pipeline: DSLæè¿°ç»„ä»¶æ‹“æ‰‘ã€æ¯ä¸ªç»„ä»¶æ‰“åŒ…æˆdockerç‹¬ç«‹è¾“å…¥è¾“å‡ºï¼Œæä¾›äº†åŸºäºDAGä¸k8sçš„å·¥ä½œæµè°ƒåº¦
	  * https://cloud.google.com/blog/products/ai-machine-learning/getting-started-kubeflow-pipelines
	* å·¥ä½œæµæ‹“æ‰‘å’Œå·¥ä½œè´Ÿè½½é«˜åº¦å®¢åˆ¶åŒ–
	  * å·¥ä½œæµè°ƒåº¦ <- Argo
	  * å·¥ä½œè´Ÿè½½å®¹å™¨åŒ–
	* kubeflow pipelineç»„ä»¶
	  * è½»é‡çº§ï¼š [`kfp.components.func_to_container_op`](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)
	  * [å¯é‡å¤ä½¿ç”¨çš„component](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/component-development/)
	  * Google Cloud TFX......
	
	![Google Cloud ä¸ŠåŸºäº TFX çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ­¥éª¤](./MLSys/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build-3-tfx-google-cloud.svg)
	
	
	* [TFX.periodic_training](https://github.com/tensorflow/community/blob/master/rfcs/20210115-tfx-periodic-training.md)
	
	
	    * ExampleGen processes and outputs one Span
	    
	        * spanæœ‰versionçš„æ¦‚å¿µ
	    
	    * Frontfill/Backfill:
	    
	        * If an older Span needs to be updated, a new version of the Span is generated by **a backfill pipeline run** with specified Span id.
	        * ä¸å»ºè®®çƒ­æ›´ï¼Œå»ºè®®æ–°version+backfillæœºåˆ¶
	    
	    * ä¸å»ºè®®concurrentæ‰§è¡Œï¼Œå› ä¸ºThe execution of TFX Components depends on the ML Metadata status before executing
	    
	        * killandrun,allowtocomplete
	
	* Preprocessing: æ•°æ®æ ¡éªŒã€schemaç”Ÿæˆ
	
	
	    * range config: 1ï¼‰æ¨¡å‹spanä¸æ•°æ®spanå¯¹åº”ï¼›2ï¼‰warmup startï¼›3ï¼‰è¾“å‡ºæ¨¡å‹çš„metadata
	
	* Metadata: å®ç°äº†æ•°æ®ã€æ¨¡å‹çš„ä¸Šä¸‹æ¸¸è¡€ç¼˜è¿½è¸ªï¼Œæ¯”å¦‚æ¨¡å‹æ€§èƒ½å˜åŒ–çš„å› æœæ¨ç†ã€æ›´åŠ ç²¾å¯†çš„åƒåœ¾å›æ”¶
	
	* é—®é¢˜ï¼š
	
	
	  * user costå’Œsystem costçš„æŠ˜ä¸­ï¼Œæ¯”å¦‚Workflowå±‚é¢çš„è°ƒåº¦å¦‚ä½•ä¸åº•å±‚è°ƒåº¦ç»Ÿä¸€
	  * å…¼é¡¾æ ¸å¿ƒåœºæ™¯ï¼ˆæœå¹¿æ¨ï¼‰å’Œé•¿å°¾åœºæ™¯ï¼ˆæ— ç›‘ç£ã€MABï¼‰
	
	  
	
	  
	
	  
	


  * Flink ML
    * éš¾ä»¥æè¿°å¤æ‚çš„å¤šè§’è‰²æ‹“æ‰‘å…³ç³»

  * Ray: å‚è€ƒã€Distributed-Systemsç¬”è®°ã€‘

```java
// Flink ML
val trainingData: DataSet[LabeledVector] = ...
val testingData: DataSet[Vector] = ...

val scaler = StandardScaler()
val polyFeatures = PolynomialFeatures().setDegree(3)
val mlr = MultipleLinearRegression()

// Construct pipeline of standard scaler, polynomial features and multiple linear regression
val pipeline = scaler.chainTransformer(polyFeatures).chainPredictor(mlr)

// Train pipeline
pipeline.fit(trainingData)

// Calculate predictions
val predictions: DataSet[LabeledVector] = pipeline.predict(testingData)
```

### æ•°æ®æµã€æ•°æ®å­˜å‚¨

> å¯åˆ©ç”¨å¤§æ¨¡å‹

* æ•°æ®æ¸…æ´—
  * æ— æ ‡ç­¾ï¼šä½åˆ†è¾¨ç‡ã€é«˜å™ªå£°ã€é«˜æ›å…‰ï¼›å¥‡æ€ªç¬¦å·æ ‡ç‚¹
* æ•°æ®ç¦»çº¿é¢„å¤„ç†
  * æ•°å€¼èŒƒå›´æ ‡å‡†åŒ–ï¼šmin-max scaling
  * æ•°æ®ç¼–ç ï¼šone-hot
  * æ•°æ®åˆ†å¸ƒä¸å‡è¡¡
    * å‡é™é‡‡æ ·
    * æ•°æ®å¢å¼ºï¼š
      * å›¾ç‰‡ï¼šæ”¹å˜äº®åº¦ã€å¯¹æ¯”åº¦ã€æ—‹è½¬ã€ç¿»è½¬ã€éšæœºcrop
      * æ–‡æœ¬ï¼šåŒä¹‰è¯æ›¿æ¢ã€å¥å­é‡æ’
  * ç‰¹å¾æå–ï¼š
    * e.g. stable diffusionï¼Œå…ˆVAEæå–64*64ç‰¹å¾

#### æ•°æ®æ ¼å¼

![20250416-011642](./MLSys/20250416-011642.jpeg)

#### æ•°æ®æµ

* åˆ†å¸ƒå¼å­˜å‚¨ï¼š
  * HDFS
  * NFSï¼š Network File System
    * ç§æœ‰åŒ–
    * æ‰‹åŠ¨å¤‡ä»½æ•°æ®
  * äº‘å­˜å‚¨ï¼šå¯¹è±¡å­˜å‚¨æœåŠ¡
    * å…¬æœ‰äº‘
    * æ‰©å±•æ€§ã€æŒä¹…æ€§é«˜ï¼Œè‡ªåŠ¨å‰¯æœ¬ã€å¤šåŒºåŸŸå­˜å‚¨

* æ•°æ®æµï¼Œå››ç§æ¶æ„ï¼šæ‰¹å¤„ç†ã€æµè®¡ç®—ã€Lambdaã€Kappa
  * æ‰¹å¤„ç†ï¼šåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ˆHDFSï¼‰+Map Reduce
  * æµè®¡ç®—ï¼š
    * æ»‘åŠ¨çª—å£åˆ†é’Ÿçº§ï¼ŒFlink
    * ç¼ºç‚¹ï¼š
      * æ•°æ®åˆæ³•æ€§æ£€æŸ¥ã€æ•°æ®å›æ”¾ã€å…¨é‡æ•°æ®åˆ†æ
      * æ—¶é—´çª—å£è¾ƒçŸ­æ—¶ï¼Œæ—¥å¿—ä¹±åºã€joinæ“ä½œä¼šå¯¼è‡´æ•°æ®è¯¯å·®ç´¯è®¡

  * Lambda: å®æ—¶æµ+ç¦»çº¿å¤„ç†
    * æµè®¡ç®—ä»¥å¢é‡è®¡ç®—ä¸ºä¸»ï¼Œæ‰¹å¤„ç†è¿›è¡Œå…¨é‡è®¡ç®—ï¼›
    * å­˜å…¥æ•°æ®åº“ä¹‹å‰ï¼Œå¯¹å®æ—¶æµå’Œç¦»çº¿å±‚æ•°æ®åˆå¹¶ï¼Œåˆ©ç”¨ç¦»çº¿å±‚æ•°æ®å¯¹å®æ—¶æµæ•°æ®è¿›è¡Œæ ¡éªŒå’Œæ£€é”™
    * ç¼ºç‚¹ï¼šæµªè´¹èµ„æº

  * Kappa
    * Everything is streaming
    * æ€è·¯ï¼šæ•°æ®å­˜å‚¨+æ•°æ®é‡æ’­
    * ç¼ºç‚¹ï¼šå›æ”¾æ•ˆç‡ï¼Œæ‰¹å¤„ç†å’Œæµå¤„ç†æ“ä½œä¸ä¸€å®šèƒ½å®Œå…¨å…±äº«
    * ![image-20250106004639825](./MLSys/image-20250106004639825.png)


* æ¨èç³»ç»Ÿçš„æ•°æ®æµ
  * è®­ç»ƒæ•°æ®å¤„ç† + ç‰¹å¾çš„é¢„è®¡ç®—

### ç‰¹å¾å·¥ç¨‹

* å‚è€ƒã€tensorflowç¬”è®°ã€‘çš„python/dataéƒ¨åˆ†
* ç‰¹å¾è½¬æ¢
  * æ— è½¬æ¢ï¼Œé€‚ç”¨äº**intã€float**æˆ–è€…å¯¹åº”ç±»å‹çš„**å®šé•¿åˆ—è¡¨**çš„ç‰¹å¾ï¼Œå¯ä»¥ç›´æ¥è¾“å…¥ç¥ç»ç½‘ç»œã€‚ä¸ºäº†ä¿æŒç»Ÿä¸€ï¼Œæˆ‘ä»¬å°†è¿™ä¸€ç±»å‘½åä¸ºPretrainEmbeddingã€‚
  * è½¬æ¢ä¸ºont-hotæˆ–è€…multi-hotç±»å‹ï¼Œé€‚ç”¨äº**intã€string**ï¼Œæˆ–å¯¹åº”å®šç±»å‹çš„**å®šé•¿ã€å˜é•¿åˆ—è¡¨**çš„ç‰¹å¾ã€‚è¿™ç§è½¬æ¢æ–¹å¼é€‚åˆå¤„ç†å°è§„æ¨¡ç±»åˆ«å‹ç‰¹å¾ï¼Œå…¶ç‰¹å¾è½¬æ¢åçš„å¯ç†è§£æ€§ï¼Œä»¥åŠä¸åŒç‰¹å¾å€¼ä¹‹é—´çš„å·®å¼‚éƒ½æ˜¯å¯è¡¡é‡çš„ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸å¤Ÿå¤§æ—¶ï¼Œæ˜¯å¯ä»¥ä¼˜å…ˆè€ƒè™‘çš„ç±»å‹ã€‚è¿™ç§è½¬æ¢æ–¹å¼ï¼Œæˆ‘ä»¬å‘½åä¸ºEncodingã€‚
  * è½¬æ¢ä¸ºembeddingç±»å‹ï¼Œé€‚ç”¨äº**intã€string**ï¼Œæˆ–å¯¹åº”å®šç±»å‹çš„**å®šé•¿ã€å˜é•¿åˆ—è¡¨**çš„ç‰¹å¾ã€‚è¿™ç§æ–¹å¼é€‚åˆå¤§è§„æ¨¡çš„idç±»ç‰¹å¾ï¼Œéœ€è¦é€šè¿‡å¤§è§„æ¨¡çš„æ ·æœ¬è®­ç»ƒæ¥æ‰¾å‡ºå½“å‰ç‰¹å¾å€¼çš„æœ€ä¼˜ç©ºé—´ä½ç½®ã€‚è¿™ç§è½¬æ¢æ–¹å¼ï¼Œæˆ‘ä»¬å‘½åä¸ºEmbeddingã€‚
    * è¿˜æœ‰ä¸€äº›è¯¸å¦‚embeddingåŠ æƒã€å¤šå€¼embeddingèšåˆï¼Œå…±äº«embedding tableç­‰ç­‰æ›´ç»†èŠ‚çš„ä¼˜åŒ–æ–¹å¼
* å¤šæ¬¡å“ˆå¸Œï¼šä¸è®ºæ˜¯Encodingè¿˜æ˜¯Embeddingï¼Œéƒ½å¯èƒ½éœ€è¦å¯¹ç‰¹å¾å€¼è¿›è¡Œå“ˆå¸Œï¼Œè€Œè¿™å°±é¢ä¸´å“ˆå¸Œå†²çªçš„é—®é¢˜ï¼Œå¸¸è§æœ‰ä¸¤ç§å¤„ç†æ‰‹æ®µ
  * å¢åŠ å“ˆå¸Œç©ºé—´å¤§å°ã€‚å“ˆå¸Œç©ºé—´å¤§å°çš„é…ç½®é¦–å…ˆå¿…ç„¶å—ç‰¹å¾å€¼ç©ºé—´çš„å½±å“ï¼Œå¦‚æœå“ˆå¸Œç©ºé—´å°äºç‰¹å¾å€¼ç©ºé—´ï¼Œåˆ™å“ˆå¸Œå†²çªæ¦‚ç‡å¾ˆå¤§ï¼›å¦‚æœè¿œè¿œå¤§äºç‰¹å¾å€¼ç©ºé—´ï¼Œåˆ™ä¼šäº§ç”Ÿå†…å­˜æµªè´¹ã€‚å› æ­¤ï¼Œåœ¨åˆç†èŒƒå›´å†…ï¼Œé€šè¿‡å¢åŠ å“ˆå¸Œç©ºé—´æ¥å‡å°‘å“ˆå¸Œå†²çªæ¦‚ç‡æ˜¯éå¸¸ç›´è§‰çš„åšæ³•ã€‚
  * å¤šæ¬¡å“ˆå¸Œã€‚å³ä¸€ä¸ªç‰¹å¾å€¼ç”±å¤šä¸ªä¸åŒå“ˆå¸Œç§å­çš„ç»“æœæ¥è¡¨è¾¾ï¼Œè¿™æ ·åªéœ€è¦ä»»æ„ä¸€ç§å“ˆå¸Œçš„ç»“æœä¸å†²çªï¼Œåˆ™æœ€ç»ˆå“ˆå¸Œç»“æœä¸ä¼šå†²çªã€‚ä½†æ˜¯å¤šæ¬¡å“ˆå¸Œä¼šæ˜¾è‘—æå‡è®¡ç®—é‡ï¼Œå› æ­¤ä¹Ÿä¹Ÿéœ€è¦åœ¨åˆç†èŒƒå›´å†…é€‰æ‹©å“ˆå¸Œæ¬¡æ•°ã€‚

### å¬å›

* ç´¢å¼•æ–¹å¼
  * BF (BruteForce): ç§’çº§åˆ°åˆ†é’Ÿçº§æ„å»ºï¼Œåä¸‡åˆ°ç™¾ä¸‡é‡çº§
  * IVF (Inverted File System): åˆ†é’Ÿçº§åˆ°å°æ—¶çº§æ„å»ºï¼Œç™¾ä¸‡åˆ°äº¿çº§
    * GPU å¯¹èšç±»è¿›è¡ŒåŠ é€Ÿ
  * HNSW: åˆ†é’Ÿçº§åˆ°å¤©çº§æ„å»ºï¼Œåƒä¸‡åˆ°ç™¾äº¿çº§å®æ—¶æ€§
    * å¯èƒ½ä¼š sharding
* é‡åŒ–æ–¹å¼
  * Int8 
  * PQ

### æ£€ç´¢åŠ é€Ÿ

* åŸºäºæ ‘
  * KD Tree
  * Annoy: https://github.com/spotify/annoy
* Hash
  * Local Sensitive Hashing: https://falconn-lib.org/
    * åˆ†æ¡¶çš„â€œä¸â€ã€â€œæˆ–â€ç­–ç•¥
    * â€œä¸â€ç­–ç•¥å¯èƒ½æ¼æ‰è¾¹ç•Œç‚¹
  
* PQ
  * https://github.com/facebookresearch/faiss
* Learning to hash

#### Semantic search

* [OpenAI Embedding Model](https://openai.com/blog/new-and-improved-embedding-model/)

  * text search, code search, sentence similarity, text classification

  * Unification of capabilities

  * **Longer context.** The context length of the new model is increased by a factor of four, from 2048 to 8192, making it more convenient to work with long documents.

  * **Smaller embedding size.** The new embeddings have only 1536 dimensions, one-eighth the size of `davinci-001` embeddings, making the new embeddings more cost effective in working with vector databases.

  * **Reduced price.** We have reduced the price of new embedding models by 90% compared to old models of the same size. The new model achieves better or similar performance as the old Davinci models at a 99.8% lower price.

* [VantageDiscoveryçš„ç§‘æ™®](https://www.vantagediscovery.com/post/semantic-101)

  * `text-embedding-3-large` model with 2048 dimensions

* [Semantic Search using Matryoshka Embedding Vectors](https://www.vantagediscovery.com/post/semantic-search-using-matryoshka-embedding-vectors)

  * Unlike traditional embeddings, LLMs produce embeddings that consider the entire context in which a word or phrase appears, leading to more precise search results.
  * å¸¸è§„ä¼˜åŒ–æ€è·¯ï¼šreduced vector precision, and ANN (approximate nearest neighbors).
  * æ–°ä¼˜åŒ–æ€è·¯ï¼šreduced RAM footprint by storing only parts of the embedding vectors in RAM
  * **Matryoshka Representation Learning (MRL)** constructs embedding vectors by embedding information at multiple granularity levels within such vectors.
    * https://arxiv.org/abs/2205.13147
    * inspired by the nesting concept of Russian Matryoshka dolls.
    * æ€è·¯æ˜¯ä»ä½ç»´å¼€å§‹å¾€é«˜ç»´è®­
  * ä½¿ç”¨ï¼štext-embedding-3-largeçš„dimensionæ¥å£
  * OpenAIï¼šhttps://openai.com/index/new-embedding-models-and-api-updates/
  * e.g. text-embedding-3-small
    * 1536=512 + 1024
    * retain the top 5000 results from the 1st tier, and process only these 5000 results for the 2nd tier, finally retaining the top 120 results based on their full score computation
    * æœ€ç»ˆæ•ˆæœï¼š
      * æŸ¥è¯¢æ—¶é—´å‡åŠï¼Œæ•ˆæœå·®ä¸å¤š
    * With a Tiering split of (512, 1024), without locking Tier 2 in RAM, and preloading Tier 2 Pagesï¼Œè¿™æ ·è€—æ—¶å¾ˆç¨³
  * Noteï¼š
    * è¦éµå®ˆå®˜æ–¹split

#### HNSW

* [The Hush-Hush Secret of Accuracy of HNSW and Vector Databases](https://www.vantagediscovery.com/post/the-hush-hush-secret-of-accuracy-of-hnsw-and-vector-databases)
  * The key advantage of HNSW is its ability to perform approximate nearest neighbor (ANN) searches quickly in high-dimensional spaces.
  * ç¼ºç‚¹ï¼š
    * The 'Loss-eee-ness' Phenomenonï¼š approximateçš„å®ç°ï¼Œå¯èƒ½å¿½ç•¥æœ€å‡†ç¡®çš„ç»“æœï¼Œå°¤å…¶æ˜¯skewedæ•°æ®ä¸‹
    * Lack of Real-time Tunabilityï¼š æ— æ³•åœ¨çº¿tuneï¼Œåªèƒ½reindex
    * Precision-Recall Trade-off
    * Sensitivity to Data Distribution
    * Complexity in High-dimensional Spaces

  * Dynamic Precision-Recall Curve
    * We leverage the precision-recall curve in what we consider a novel way. Users can visualize and manipulate this curve, allowing them to prioritize either precision or recall based on their immediate needs. We believe this dynamic approach ensures that the search can be optimized for various contexts without sacrificing overall performance.
  * Addressing 'Loss-eee-ness'
    * By allowing fine-tuning of search parameters, our algorithm directly tackles the 'loss-eee-ness' issue. Users can enhance recall without a proportional increase in latency, maintaining speed while mitigating accuracy loss.
  * Improved ANN Searches
    * We employ advanced techniques for approximate nearest neighbor searches in high-dimensional spaces. By utilizing the inner product to measure similarity, we aim to ensure that results align closely with the user's intent, even in complex semantic contexts.
  * Adaptive to Data Distribution
    * Our algorithm is designed to be more robust to varying data distributions. We believe this adaptability ensures more consistent performance across different types of datasets and query patterns.
  * Balancing Act Between Speed and Accuracy
    * While HNSW often requires choosing between speed and accuracy, our approach aims to provide a more nuanced balance. Users can change this balance based on their specific needs, without drastic trade-offs.

### ç²—æ’

#### COLD : Towards the Next Generation of Pre-Ranking System

é˜¿é‡Œå®šå‘å¹¿å‘Šæœ€æ–°çªç ´ï¼šé¢å‘ä¸‹ä¸€ä»£çš„ç²—æ’æ’åºç³»ç»ŸCOLD - è§ç‘Ÿçš„æ–‡ç«  - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/186320100

Computing power cost-aware Online and Lightweight Deep pre-ranking system

å°ç²¾æ’æ”¯æŒå¤æ‚ç®—æ³•æ¢ç´¢

* SENet: æ¨¡å‹è®­ç»ƒæ—¶è·å–ç‰¹å¾é‡è¦æ€§æ•°æ®
* å¹¶è¡ŒåŒ–ï¼šåœ¨å–PSä¹‹ååšæ¨¡å‹å¹¶è¡Œé¢„ä¼°ï¼Œèƒ½æ¯”è®ºæ–‡ä¸­çš„å®ç°æ›´é«˜æ•ˆ
* åˆ—å­˜ï¼šå…¨é“¾è·¯åˆ—å­˜
* fp16
  * mix precision: fp32 BN + fp16 fully-connected layers
  * parameter-free normalization

#### Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach, SIGIR 2021

Feature Selection method based on feature Complexity and variational Dropout (FSCD)

2.1 FSCD for Pre-Ranking Modelï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ variational dropout

* ç‰¹å¾é€‰å–æ¦‚ç‡ä¸ç‰¹å¾å¤æ‚åº¦è´Ÿç›¸å…³
* ç‰¹å¾å¤æ‚åº¦çš„å› ç´ ï¼šç‰¹å¾ç±»å‹ã€embedding sizeã€key size (èƒ½ç±»æ¯”äºå€™é€‰æ•°é‡)
* æ•°å­¦æ‰‹æ®µï¼š
  * å…¬å¼(3)ï¼ŒæŸå¤±å‡½æ•°ï¼Œå‚æ•°zçš„æ­£åˆ™åŒ–
    * ç‰¹å¾å¤æ‚åº¦è¶Šå¤§ï¼Œæ­£åˆ™é¡¹ç³»æ•°è¶Šå¤§
    * æŸå¤±å‡½æ•°åŠæ­£åˆ™åŒ–ç³»æ•°çš„æ¨å¯¼ï¼ˆè§é™„å½•ï¼‰
  * å…¬å¼(5)ï¼ŒBernoulli åˆ†å¸ƒçš„å¹³æ»‘åŒ–

2.2 Fine-Tuning the Pre-Ranking Model

* ç”¨ç²¾æ’æ¨¡å‹å‚æ•°æ¥åˆå§‹åŒ–å‚æ•°ï¼Œfine-tune åŠ é€Ÿè®­ç»ƒ
* $\gamma_3=10^{-7}$ æè¿°å€™é€‰æ•°é‡ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªè¡¡é‡ç‰¹å¾å¤æ‚åº¦çš„å‚æ•°

### æ€§èƒ½ä¼˜åŒ– Performance Tuning

https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html

https://huggingface.co/docs/transformers/v4.15.0/performance

https://docs.nvidia.com/deeplearning/performance/index.html

### é‡åŒ–ã€æ··åˆç²¾åº¦è®­ç»ƒæ¨ç†

#### Intro - é‡åŒ–ç›®æ ‡ã€ç²¾åº¦ä»‹ç»

* é‡åŒ–çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ â€”â€” å¤šç›®æ ‡ä¼˜åŒ–

  - ä¼˜åŒ–è®¡ç®—
    - tensorcoreï¼ŒInt8/fp8/fp16/bf16 matmul
  - ä¼˜åŒ–æ˜¾å­˜
    - æ˜¾å­˜åŒ…æ‹¬ paramsã€gradsã€activationsï¼ˆå­˜å‚¨ç”¨äºbackwardï¼‰ç­‰
      - The activation memory of a transformer-based model is proportional to the number of transformer layers Ã— hidden dimensions Ã— sequence length Ã— batch size. For a GPT-2 like architecture the total activations is about 12 Ã— hidden dim Ã— batch Ã— seq length Ã— transformer layers.
  - ä¼˜åŒ–å†…å­˜å¸¦å®½
    - small-batch inference is bandwidth-bound by model weights; 
    - long-sequence decoding is bandwidth-bound by KV cache
      - e.g. Increasing the speed at which **the user receives generated results** is challenging, as compute is **dominated by matrix-vector products**. Unlike matrix-matrix products, these are primarily limited by memory bandwidth.
  - å‡å°‘ç²¾åº¦æŸå¤±ï¼šå…ˆå†³æ¡ä»¶

* é‡åŒ–ç²¾åº¦

* ![image-20250404210744334](./MLSys/image-20250404210744334.png)

  * [denormalized numbers](https://cs.stackexchange.com/questions/101632/understanding-denormalized-numbers-in-floating-point-representation)
    * **æŒ‡æ•°å›ºå®šä¸º - 126**ï¼ˆè§„æ ¼åŒ–æ•°çš„æŒ‡æ•°é€šè¿‡ `exponent - 127` è®¡ç®—ï¼‰ã€‚
    * **å°¾æ•°æ— éšå«çš„ 1**ï¼ˆç›´æ¥ä½¿ç”¨å°¾æ•°ä½çš„äºŒè¿›åˆ¶å°æ•°éƒ¨åˆ†ï¼‰
    * FP32 can represent precision up to 2^(-23)*2^(-126)=2^(-149)
    * FP16 can represent precision up to 2^(10)*2^(-14)=2^(-24)

  * FP64: 8ä¸ªå­—èŠ‚, 1ä½ç¬¦å·, 11ä½æŒ‡æ•°, 52ä½å°æ•°ï¼Œ**æœ‰æ•ˆä½æ•°ä¸º16ä½**. å¸¸ç”¨äºç§‘å­¦è®¡ç®—, ä¾‹å¦‚: è®¡ç®—åŒ–å­¦, åˆ†å­å»ºæ¨¡, æµä½“åŠ¨åŠ›å­¦

  * FP32: 4ä¸ªå­—èŠ‚, 1ä½ç¬¦å·, 8ä½æŒ‡æ•°, 23ä½å°æ•°ï¼Œ**æœ‰æ•ˆä½æ•°ä¸º7ä½**. å¸¸ç”¨äºå¤šåª’ä½“å’Œå›¾å½¢å¤„ç†è®¡ç®—ã€æ·±åº¦å­¦ä¹ ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸ
    * FP32 = (-1)^(sign) Ã— 2^(decimal exponent -127 ) Ã— (implicit leading 1 + decimal mantissa), [where](https://en.wikipedia.org/wiki/Exponent_bias) 127 is the biased exponent value.

    * the value range for FP32 is approximately [-2Â¹Â²â·, 2Â¹Â²â·] ~[-1.7*1e38, 1.7*1e38]

    * excluding the largest value 0xFF as it represents NAN

  * FP16: 2ä¸ªå­—èŠ‚ 1ä½ç¬¦å·, 5ä½æŒ‡æ•°, 10ä½å°æ•°ï¼Œ**æœ‰æ•ˆä½æ•°ä¸º3ä½**. å¸¸ç”¨äºç²¾åº¦æ›´ä½çš„æœºå™¨å­¦ä¹ ç­‰
    *  For FP16, the formula becomes (-1)^(sign) Ã— 2^(decimal exponent â€“ 15) Ã— (implicit leading 1 + decimal mantissa), where 15 is the corresponding biased exponent value
    * the value range for FP16 is approximately [-2Â¹âµ, 2Â¹âµ]=[-32768, 32768]

* ç²¾åº¦èŒƒå›´ï¼š
  * ![image-20250331122231657](./MLSys/image-20250331122231657.png)

* fp4

  * | ç¬¦å·ï¼ˆSï¼‰ | æŒ‡æ•°ï¼ˆEï¼‰ | å°¾æ•°ï¼ˆMï¼‰ | æ•°å€¼ï¼ˆåè¿›åˆ¶ï¼‰               | ç›¸é‚»æ•°é—´éš”ï¼ˆä¸å‰ä¸€ä¸ªæ­£æ•°çš„å·®ï¼‰ |
    | --------- | --------- | --------- | ---------------------------- | ------------------------------ |
    | 0         | -1        | 0         | $$1.0 \times 2^{-1} = 0.5$$  | -                              |
    | 0         | -1        | 1         | $$1.1 \times 2^{-1} = 0.75$$ | 0.25                           |
    | 0         | 0         | 0         | $$1.0 \times 2^{0} = 1$$     | 0.25                           |
    | 0         | 0         | 1         | $$1.1 \times 2^{0} = 1.5$$   | 0.5                            |
    | 0         | 1         | 0         | $$1.0 \times 2^{1} = 2$$     | 0.5                            |
    | 0         | 1         | 1         | $$1.1 \times 2^{1} = 3$$     | 1                              |

* ç¡¬ä»¶æ”¯æŒï¼šå‚è€ƒã€ŒGPU.md â€”â€” ç¡¬ä»¶ç²¾åº¦æ”¯æŒã€

* eXmY https://arxiv.org/abs/2405.13938

#### é‡åŒ–æŠ€æœ¯åˆ†ç±»

> * PTQ v.s. QAT
>
> * åœ¨çº¿é‡åŒ– v.s. ç¦»çº¿é‡åŒ–
>   * åœ¨çº¿é‡åŒ–:å®æ—¶ç»Ÿè®¡ inputs çš„ maxï¼Œç®—å‡ºscaleå…·æœ‰å®æ—¶æ€§é«˜ï¼Œç²¾åº¦å¥½ç­‰ä¼˜ç‚¹ï¼Œå®æ—¶ç»Ÿè®¡inputæ•°æ®ï¼Œä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½å¼€é”€
>   * ç¦»çº¿é‡åŒ–: è®¡ç®—å¥½inputs çš„maxï¼Œåœ¨é‡åŒ–æ—¶ç›´æ¥ä½¿ç”¨ï¼Œæ€§èƒ½å¥½ï¼Œä½†éœ€è¦é¢„å…ˆè®¡ç®—çš„maxå’Œå®é™…inputä¸ä¸€è‡´ï¼Œå¯¼è‡´é‡åŒ–è¯¯å·®

* Mixed Precision Training
  * weight/activationé‡åŒ–
  * weight updateç”¨fp32/bf16
  * quantization lattice is non-uniform
* Post Training Quantization (PTQ)
  * weight/activationé‡åŒ–ï¼Œæ¨ç†ç²¾åº¦å’Œæ€§èƒ½çš„æœ€ä¼˜è§£ï¼Œä½†å¯èƒ½ç²¾åº¦ä¸‹é™å¤§
  * å†³ç­–ç‚¹ï¼š
    * clipping error and the rounding errorçš„å¹³è¡¡
    * æœ€å°è¯¯å·®çš„æ±‚è§£æ–¹æ³•
    * ...
  * å¸¸è§æ€è·¯ï¼š
    - W8A8: smoothQuantã€DeepSeek Fp8
    - W4A16: DecoupleQã€AWQã€GPTQ
* Quantization Aware Training (QAT)
  * è®­ç»ƒç”¨å…¨ç²¾åº¦ï¼Œå¹¶æ¨¡æ‹Ÿé‡åŒ–ï¼Œä»¥ä¼˜åŒ–æ¨ç†é‡åŒ–ç²¾åº¦ï¼ŒPTQçš„æ•ˆæœå‡çº§ç‰ˆï¼Œä½†æˆæœ¬é«˜
* Quantized Training (QT)
  * å…¨éƒ¨æ­¥éª¤é‡åŒ–ï¼ŒåŒ…æ‹¬weight/activation/weight update

  * only seen success up to 8-bits, whereas QAT is effective even at lower bit-widths.
    * https://cloud.google.com/blog/products/compute/accurate-quantized-training-aqt-for-tpu-v5e


* Q-Lora
  * Quantize base weight to NF4, while LoRA weights are in high precision (FP32 or BF16)
  *  Quantized base weights are not trained.

#### Literature Review

* PTQï¼ˆè®­ç»ƒåé‡åŒ–ï¼‰ ã€GPTQã€‘
  * GPTQ è™½ç”¨äºŒé˜¶ä¿¡æ¯è¡¥å¿è¯¯å·®ï¼Œä½†é‡å»ºè¿‡ç¨‹æ˜“è¿‡æ‹Ÿåˆæ ¡å‡†é›†ï¼Œå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
  * AdaRound method (Nagel et al., 2020) computes a data-dependent rounding by annealing a penalty term, which encourages weights to move towards grid points corresponding to quantization levels
  * BRECQ (Li et al., 2021) introduces Fisher information into the objective, and optimizes layers within a single residual block jointly.
  * Optimal Brain Quantization (OBQ) (Frantar et al., 2022) generalizes the classic
    Optimal Brain Surgeon (OBS) second-order weight pruning framework to apply to quantization.
    * OBQ quantizes weights one-by-one, in order of quantization error, always adjusting the remaining weights.
    * While these approaches can
      produce good results for models up to â‰ˆ 100 million parameters in a few GPU hours, scaling them to networks orders of magnitude larger is challenging.
  * ç ”ç©¶Outlier
    * Gobo: Quantizing attention-
      based nlp models for low latency and energy eï¬ƒcient inference
      * introduce centroid-based quantization method, where outlier numbers use FP32 format and the rest numbers are quantized using non-uniform quantization.
      * As such, it is hard to get the real inference latency benefit on general compute accelerators, e.g., CPU and GPU, because the parallel processing units in these hardware do not support efficient computation of mixed data types.
  * expensive hidden-states knowledge distillation [2, 36] is used
    for ultra-low precision quantization to close the accuracy gapã€ZeroQuantã€‘
    * (1) KD needs to hold a teacher and a student model together during the training, which dramatically increases the memory and compute cost;
    * (2) KD usually requires full training of the student model. Therefore, several copies (gradient, first/second order momentum) of the weight parameters need to be stored in memory to update the model;
    * (3) KD generally requires original training data, which sometimes are not accessible due to privacy/confidential issues.
    * --> ZeroQuant LKD
  * å…¶å®ƒï¼š
    * ã€ŠPost-training quantization for vision transformerã€‹ï¼ˆNIPS 2021ï¼‰
    * ã€ŠUp or down? adaptive rounding for post-training quantizationã€‹
  
* QAT
  * Q-Bert and ã€ŠQ8BERT: Quantized 8bit bertã€‹ are the first few works to quantize BERT models using integer numbers for both weight and activations.
    * Q-Bert utilizes Hessian information to push the weight bit-precision to even INT2/INT4, and it also proposes group-wise quantization to quantize the weight matrix in a more fine-grained granularity compared to single matrix quantization.

  * ã€ŠTraining with quantization noise for extreme ï¬xed-point compressionã€‹ introduces quantization noise to alleviate the variations of QAT.

* Quantized Training
  * ã€ŠPareto-Optimal Quantized ResNet Is Mostly 4-bitã€‹
    * INT8 ResNet outperforms BF16 ResNet (at the same params count)
    * INT4 ResNet is the best (for a given model size in MB/GB)

  * Binarized Neural Machine Translation
    * Inspired BitNet

  * Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization
    * Tile-wise quantization, with quantized matmul outputs
    * INT8 LayerNorm and INT8 GELU

* Zero-shot Quantization
  * ã€ŠZeroq: A novel zero shot quantization framework.ã€‹
  * ã€ŠData-free quantization through weight equalization and bias correctionã€‹

* Weight Only Quant
  * developing a quantized-matrix full-precision-vector product kernel which performs a matrix vector product by dynamically dequantizing weights when needed. Most notably, this does not require any activation quantization. While dequantization consumes extra compute, the kernel has to access a lot less memory, leading to significant speedups, as shown in Table 6 ã€GPTQã€‘
* Large-model Quantization
  * While all existing worksâ€”ZeroQuant (Yao et al., 2022), LLM.int8() (Dettmers et al., 2022), and nuQmm (Park et al., 2022)â€” carefully select quantization granularity, e.g., vector-wise, they ultimately just round weights to the nearest (RTN) quantization level, in order to maintain acceptable runtimes for very large models.
  * **ZeroQuant further proposes layer-wise knowledge distillation**, similar to AdaQuant, but the largest model it can apply this approach to has only 1.3 billion parameters. At this scale, **ZeroQuant already takes â‰ˆ 3 hours of compute; GPTQ quantizes models 100Ã— larger in â‰ˆ 4 hours**.
  * LLM.int8() observes that **activation outliers in a few feature dimensions break the quantization of larger models**, and proposes to fix this problem by keeping those dimensions in higher precision. Lastly, nuQmm develops efficient GPU kernels for a specific **binary-coding based quantization scheme**.
* å­¦æœ¯å‘åˆ†ç±»
  * both PTQ and QAT were susceptible to outliers. In addition to simple clamping and regularization during fine-tuning, we can explore techniques that allow the network to learn how to control these outliers (e.g. [learned quantization ranges](https://arxiv.org/pdf/1902.08153), [clipped softmax](https://arxiv.org/pdf/2306.12929), and [gated attention](https://arxiv.org/pdf/2306.12929)), or possibly even borrow outlier suppression techniques from post-training settings (e.g. [SpinQuant](https://arxiv.org/pdf/2405.16406), [SmoothQuant](https://arxiv.org/pdf/2211.10438)) and apply them sparingly throughout the fine-tuning process.

* åœ¨çº¿é‡åŒ– vs delayedé‡åŒ–
  * Delayed quantization is employed in tensor-wise quantization frame-
    works (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute values across prior iterations to infer the current value ã€DeepSeek-v3ã€‘

* æ¨¡å‹ç»“æ„åˆ†ç±»
  * KV Cache
    * [LLM-QAT](https://arxiv.org/pdf/2305.17888) explored quantizing the KV cache alongside activations and weights.

  * Embedding Layer
    * [Prior work](https://arxiv.org/pdf/2109.12948) has also had success with quantizing the embedding layer down to 2-bits in other transformer-based models.

#### Mixed Precision Training (ICLR 2018)

> - æŠ€æœ¯ï¼š
>   - æ¢¯åº¦æ›´æ–°ç”¨fp32, an FP32 master copy of weights is used for updates -> **é’ˆå¯¹æ¢¯åº¦ç´¯åŠ æ—¶è¢«è§†ä½œ0**
>   - Loss Scaling -> **é’ˆå¯¹****æ¢¯åº¦****çš„è¡¨ç¤ºä¸‹æº¢**
>   - FP16 arithmetic used Tensor Core operations with accumulation into FP32 for convolutions
> - å±€é™æ€§ï¼š
>   - ä¸æ˜¯æ‰€æœ‰å±‚éƒ½é‡åŒ–
>   - åªèƒ½ä¼˜åŒ–computeï¼Œæ˜¾å­˜çš„ä¼˜åŒ–æœ‰é™ï¼ˆå–å†³äºactivationä¼˜åŒ–æƒ…å†µï¼Œä¸”é¢å¤–å­˜fp16 copyï¼‰
>   - åªä¼˜åŒ–åˆ°fp16
>   - fp16çš„ç²¾åº¦å±€é™æ€§ï¼Œå°½ç®¡æœ‰loss scalingè§£å†³underflowçš„é—®é¢˜ï¼Œbf16-pretrained modelsä¼šoverflow

* Baseline (FP32) : Single-precision storage is used for activations, weights and gradients. All arithmetic is also in FP32. 
* Mixed Precision (MP): 
  * ![image-20250301234600987](./MLSys/image-20250301234600987.png)
  * bf16è®­ç»ƒæ—¶ï¼Œä¸€èˆ¬éœ€è¦ä¿ç•™ä¸€ä»½fp32çš„Wï¼Œç”¨æ¥åšç´¯åŠ 
* Loss Scaling

  * if gradient statistics are available, directly by choosing a factor so that its product with the maximum absolute gradient value is below 65,504 (the maximum value representable in FP16).
  * åªè¦æ²¡æœ‰overflowï¼Œå°±æ²¡æœ‰å‰¯ä½œç”¨
* è§£å†³çš„é—®é¢˜ï¼š

  * fp16è®­ç»ƒï¼Œæ¢¯åº¦è¢«å¿½ç•¥ï¼š
    1. updates (weight gradients multiplied by the learning rate) become too small to be represented in FP16 - any value whose magnitude is smaller than 2âˆ’24 becomes zero in FP16.
    2. when the magnitude of a normalized weight value is at least 2048 times larger that of the weight update. Since FP16 has 10 bits of mantissa, the implicit bit must be right-shifted by 11 or more positions to potentially create a zero

  * åŒä¸Šï¼šä¾§é‡åˆ©ç”¨fp16çš„exponentialä½æ•°
  * ç´¯åŠ ã€reduceç­‰æ“ä½œçš„æ¢¯åº¦å™ªå£°å¤§
* Arithmetic precisionçš„åˆ†æ
  * three categories: vector dot-products, reductions,
    and point-wise operations
  * To maintain model accuracy, we found that some networks require that FP16 vector dot-product accumulates the partial products into an FP32 value, which is converted to FP16 before writing to memory.
  * Large reductions (sums across elements of a vector) should be carried out in FP32. Such reductions mostly come up in batch-normalization layers when accumulating statistics and softmax layers.
    Both of the layer types in our implementations still read and write FP16 tensors from memory, performing the arithmetic in FP32. **This did not slow down the training process since these layers are memory-bandwidth limited and not sensitive to arithmetic speed**
  * Point-wise operations, such as non-linearities and element-wise matrix products, are memory-bandwidth limited. Since arithmetic precision does not impact the speed of these operations, either
    FP16 or FP32 math can be used
* å®ç°ï¼šscaleç³»æ•°8ï¼Œå®ç°æ—¶åœ¨forward scaleï¼Œåœ¨backwardä¹‹åã€gradient clippingä¹‹å‰rescaleã€‚ ç¡®ä¿weight decayä¸å—å½±å“
* ç»“è®ºï¼š
  * æŠ€æœ¯1ä¿ä½CV CNN backboneç²¾åº¦
  * Loss Scaling
    * Scaling factor of 8 ä¿ä½Faster R-CNNã€Multibox SSDç²¾åº¦
    * Scaling factor of 128 ä¿ä½ LSTM ç²¾åº¦
    * è¯­éŸ³æ¨¡å‹åœºæ™¯ the half-precision storage format may act as a regularizer during training
    * ganä¸éœ€è¦scaling

#### Bf16-Mixed-Precision-Training

https://arxiv.org/pdf/1905.12322

#### Fp8-Mixed-Precision-Training

![image-20250331122321267](./MLSys/image-20250331122321267.png)

##### Literature Review

* per tensor scale
  * tensor scaling techniques are proposed (Sun et al., 2019;
    Micikevicius et al., 2022)ã€FP8-LMã€‘

##### FP8-LM by å¾®è½¯

> FP8-LM: Training FP8 Large Language Models

* **three levels** gradually incorporate
  * FP8 communication
  * FP8 optimizer
  * FP8 distributed training.

* ç²¾åº¦æŠ€æœ¯
  * **precision decoupling**
    * decoupling the influence of data precision
      on parameters such as weights, gradients, optimizer states, and assigning reduced precision to components that are not precision sensitive

* FP8 communication
  * **automatic scaling**
    * to preserve gradient values within the representation range
      of FP8 data formats through the dynamic adjustment of tensor scaling factors, thereby alleviating underflow and overflow occurrences during all-reduce communication.
    * pre-scaling and post-scaling
      * pre-scaling underflowï¼Œpost-scaling overflow
      * <img src="./MLSys/image-20250507132354443.png" alt="image-20250507132354443" style="zoom:50%;" />
    * auto-scaling
      * è¿™ä¸ªç®€åŒ–æœ‰ç‚¹ç¦»è°±ï¼Œç›´æ¥å–global scalingæœ€å°å€¼ï¼Œå¯èƒ½é€ æˆç²¾åº¦æŸå¤±
      * ![image-20250507133406617](./MLSys/image-20250507133406617.png)
* fp8 optimizer
  * the gradient statistics can use lower precision, while the master weights necessitate high precision
    * ä»…é™first orderï¼Œå› ä¸ºsecond-orderä»£è¡¨æ–¹å‘æ›´æ•æ„Ÿ
    * å­˜fp32 master weightsç­‰ä»·ä¸ºå­˜fp16+scaling
    * ![image-20250507192731407](./MLSys/image-20250507192731407.png)
* ç»“è®ºï¼š
  * æ˜¾å­˜ï¼š29% reduction for GPT-7B while 39% for GPT-175B
  * weight-related communication: -63%~65%
  * E2e: during the training of GPT-175B model, our FP8 mix-precision
    framework reduces training time by 37% compared to TE (Nvidia, 2022b), while consuming 42% less memory on H100 GPU platform
  * RLHFï¼šyield a 32% reduction in model weights and a 62% reduction in optimizer statesâ€™ memory consumption



##### DeepSeek-V3

* Inspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022;
  Peng et al., 2023b), While low-precision training holds great promise, it
  is often limited by the presence of outliers in activations, weights, and gradients (Fishman
  et al., 2024; He et al.; Sun et al., 2024).
* To address this challenge and effectively extend the dynamic
  range of the FP8 format, we introduce **a fine-grained quantization strategy: tile-wise grouping**
  **with 1 Ã— ğ‘ğ‘ elements or block-wise grouping with ğ‘ğ‘ Ã— ğ‘ğ‘ elements**

* Moreover, to further reduce memory and communication overhead in MoE training, we **cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16**

![image-20250501183219211](./MLSys/image-20250501183219211.png)

* æ”¶ç›Šï¼š
  * compute
  * activationæ˜¾å­˜
* ç¨³å®šæ€§ï¼š
  * we maintain the original precision (e.g., BF16 or FP32) for the following components:
    * the embedding module, the output head, MoE gating modules, normalization operators, and attention operators
* ![image-20250501185219385](./MLSys/image-20250501185219385.png)
* é‡åŒ–æ–¹å¼ï¼š
  * (1) for activations, we group and scale elements on a 1x128 tile basis (i.e., **per token per 128 channels**); 
    * the introduction of **per-group scaling factors along the inner dimension of GEMM operations**
    * æ€è·¯å¯¹é½ ã€ŠMicroscaling data formats for deep learning.ã€‹ã€https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/
  * (2) for weights, we group and scale elements on a 128x128 block basis (i.e., **per 128 input channels per 128 output channels**)
  * Bf16 optimizer
  * the master weights (stored by the optimizer) and gradients (used for batch size accumulation) are still retained in FP32 to ensure numerical stability throughout training.
* Increasing Accumulation Precision
  * It is worth noting that **this modification reduces the WGMMA (Warpgroup-level Matrix-Multiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800**
    **architecture, it is typical for two WGMMA to persist concurrently: while one warpgroup**
    **performs the promotion operation, the other is able to execute the MMA operation.** This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting ğ‘ğ¶ = 128 elements, equivalent to 4 WGMMAs, represents the
    minimal accumulation interval that can significantly improve precision without introducing substantial overhead.
  * WGMMA çš„å¼‚æ­¥èƒ½åŠ› https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/
* å…¶å®ƒç‰¹ç‚¹
  * Mantissa over Exponents.
    * In contrast to the hybrid FP8 format adopted by prior work
      (NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and
      3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad,
      **we adopt the E4M3 format on all tensors for higher precision.**
  
  * Online Quantization
  * Activationçš„é‡åŒ–ç»†èŠ‚
    * (1) **Inputs of the Linear after the attention operator**. These activations are also
      used in the backward pass of the attention operator, which makes it sensitive to
      precision. We adopt a customized **E5M6** data format exclusively for these activations.
      Additionally, these activations will be converted from an 1x128 quantization tile to
      an 128x1 tile in the backward pass. To avoid introducing extra quantization error,
      all the scaling factors are round scaled, i.e., integral power of 2.
    * (2) **Inputs of the SwiGLU operator in MoE**. To further reduce the memory cost, we
      cache the inputs of the SwiGLU operator and recompute its output in the backward
      pass. These activations are also stored in **FP8** with our fine-grained quantization
      method, striking a balance between memory efficiency and computational accuracy.
  
  * é€šä¿¡çš„é‡åŒ–ç»†èŠ‚ï¼š
    * **Fp8**: activation before MoE up-projectionsã€activation gradient before MoE down-projections
    * **Bf16:** both the forward and backward combine components
  

* ç¡¬ä»¶æå‡æ–¹å‘ï¼š
  * To address this inefficiency, we recommend that future chips **integrate FP8 cast and TMA (Tensor Memory Accelerator) access into a single fused operation**, so quantization can be completed during the transfer of activations from global memory to shared memory, avoiding frequent memory reads and writes.
  * We also recommend supporting a **warp-level cast instruction** for speedup, which further facilitates the better fusion of layer normalization and FP8 cast. Alternatively, a near-memory computing approach can be adopted, where compute logic is placed near the HBM. In this case, BF16 elements can be cast to FP8 directly as they are read from HBM into the GPU, reducing off-chip memory access by roughly 50%.
  * Support for Transposed GEMM Operations.



#### [TorchAO - Advanced Quantization](https://www.youtube.com/watch?v=1u9xUK3G4VM)

> GPU/Quantization Cuda vs Triton.pdf
>
> GPU Mode Lecture 7

![image-20250409025047175](./MLSys/image-20250409025047175.png)

- è¿­ä»£è·¯çº¿ï¼š

  - dynamic(per tensor/per token/per channel)->weight only->QATã€GPTQ

- TorchAO - https://github.com/pytorch-labs/ao

  - Int8 Dynamic Quantizationï¼šW8A8
    - i8i8->i32 vs i8i8->bf16

  - Int 8 Weight Only Quantizationï¼šW8A16
    - bf16i8->bf16
  - Int 4 Weight Only Quantizationï¼šW4A16
    - bf16i4->bf16

- Techniquesï¼šæ˜ç¡®é‡åŒ–çš„ç›®çš„

  - dynamic: é’ˆå¯¹compute bound
  - weight only: é’ˆå¯¹memory h2d boundã€æ˜¾å­˜ç“¶é¢ˆçš„åœºæ™¯ï¼Œå»é™¤äº†å¯¹activationåšquantizeçš„ç³»åˆ—æ“ä½œï¼Œé€‚åˆllamaç­‰
  - --> dynamic quantizeåœ¨llamaæˆä¸ºç“¶é¢ˆï¼Œå†…å­˜æ“ä½œå¤š

![image-20250302195633990](./MLSys/image-20250302195633990.png)

* Dynamic Quantization Flow
  * ![image-20250303021206950](./MLSys/image-20250303021206950.png)
  * é—®é¢˜1ï¼šæ˜¾å­˜å¢é•¿ï¼ŒåŸå› æ˜¯int8ä¹˜æ³•çš„accumulationçŸ©é˜µéœ€è¦æ˜¯int32
    * è§£æ³•ï¼šfusionï¼Œç›´æ¥ç”¨bf16ä½œä¸ºaccumulateçŸ©é˜µ
    * ä»£ç ï¼šconfig.force_fuse_int_mm_with_mul
  * é—®é¢˜2: é‡åŒ–ç²¾åº¦æŸå¤±
    * è§£æ³•ï¼šper-tokenã€per-channel
* INT8 Weight Only Quantization
  * çŸ©é˜µè®¡ç®—ï¼Œfp32å’Œbf16çš†å¯ï¼Œfp16å®¹æ˜“overflow; accumulationä¹Ÿå¯è€ƒè™‘bf16
  * ![image-20250306022930749](./MLSys/image-20250306022930749.png)
  * é—®é¢˜ï¼šä¸€å¼€å§‹æ¯”cublasæ…¢ä¸€åŠ
    * åˆ†æ1: é¢å¤–åšäº†castã€rescale
    * åˆ†æ2: The Blocksize is limited to be >= 16, meaning The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
      * è§£æ³•ï¼šconfig.coordinate_descent_tuningä¼˜åŒ–
      * ![image-20250306031254122](./MLSys/image-20250306031254122.png)
    * ![image-20250306031735121](./MLSys/image-20250306031735121.png)
  * Side Note:
    * Weight Only Quantizationä¸­ï¼ŒDequantizeæ˜¯å¯é€‰çš„ï¼Œå¯ä»¥int4ç›´æ¥å’Œfloatçš„fractional partç›¸ä¹˜

* **Int4 Weight Only Quantization**

  * ![image-20250306215423050](./MLSys/image-20250306215423050.png)
  * é—®é¢˜ï¼šno torch dtype
    * è§£æ³•ï¼šç”¨uint8=int4*2å­˜ä¸¤ä¸ªint4
  * é—®é¢˜ï¼šå¦‚ä½•unpack/pack
    * è§£æ³•ï¼šå³ä¸‹è§’çš„æ’å¸ƒï¼Œunpackä¸€åˆ—åªéœ€è¦loadä¸€åˆ—
    * ![image-20250306215741041](./MLSys/image-20250306215741041.png)

  * é—®é¢˜ï¼šæ€§èƒ½å·®
    * è§£æ³•ï¼šint4 groupwise quant
    * ![image-20250307025857799](./MLSys/image-20250307025857799.png)

* GPT-Q
  * ![image-20250307033049737](./MLSys/image-20250307033049737.png)

* Triton Limitations
  * It runs into trouble when trying to work with complicated operations and nonstandard dtypes
    * Int4
    * batchsize>1 int8/int4 weight only
      * L2 Cache Optimization
  * Config consistency
    * Issues with Heuristics, in some of the tests
    * the best configurations arenâ€™t available or are heuristically discarded.

#### W8A8 - Dynamic Weight and Act

##### SmoothQuant

> æœ¬è´¨ä¸Šæ˜¯ä¸ºäº†è§£å†³GPUè®¡ç®—per-channel activation quantizationä¸é«˜æ•ˆï¼Œæ‰€ä»¥å°†outlierå¹³ç§»åˆ°weightã€‚ 

$$\hat{X} = X \cdot \text{diag}(s)^{-1}, \quad \hat{W} = \text{diag}(s) \cdot W$$

ä¿æŒæ•°å­¦ç­‰ä»·æ€§ï¼š$$Y = \hat{X} \cdot \hat{W}$$

![image-20250416153941610](./MLSys/image-20250416153941610.png)

![image-20250416155502889](./MLSys/image-20250416155502889.png)

* Intro
  * SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation.
  * SmoothQuant relies on a key observation: even if activations are much harder to quantize than weights due to the presence of outliers (Dettmers et al., 2022), **different tokens exhibit similar variations across their channels.**
* ä¸€äº›åˆ†æï¼š
  * Activations are harder to quantize than weights.
  * Outliers make activation quantization difficult
  * Outliers persist in fixed channels
* ç»“è®º
  * åœ¨175Bæ¨¡å‹ï¼Œç²¾åº¦ä¼˜äºzeroquant ï¼ˆsection5.2ï¼‰
  * ![image-20250430154044610](./MLSys/image-20250430154044610.png)
  * ![image-20250430154148177](./MLSys/image-20250430154148177.png)
  
* å…³äºper channel activation quantizationçš„åˆ†æ

  * per-channel activation quantizationï¼ˆéœ€è¦å¯¹æ¿€æ´»å€¼çš„æ¯ä¸ªé€šé“è¿›è¡Œç‹¬ç«‹çš„ç¼©æ”¾ï¼‰ä¸ç¡¬ä»¶åŠ é€Ÿçš„ GEMM kernelsï¼ˆé€šå¸¸åˆ©ç”¨ Tensor Core æ‰§è¡Œ MMAï¼‰ä¸å¤ªå…¼å®¹ã€‚
  * åŸå› å°±åœ¨äºï¼š
    * Tensor Core MMA è®¾è®¡ç”¨äºé«˜é€Ÿå¤„ç† æ•´å—çŸ©é˜µ çš„ä¹˜åŠ è¿ç®—ï¼Œå½¢æˆé«˜æ•ˆçš„æŒ‡ä»¤æµæ°´çº¿ã€‚
    * Per-channel scaling æ˜¯ä¸€ä¸ª ç²’åº¦æ›´ç»† çš„æ“ä½œï¼Œéœ€è¦åœ¨çŸ©é˜µä¹˜æ³•è¿‡ç¨‹ä¸­æˆ–ä¹‹åï¼Œå¯¹ç»“æœçŸ©é˜µçš„ç‰¹å®šç»´åº¦ï¼ˆé€šé“ç»´åº¦ï¼‰è¿›è¡Œ é€å…ƒç´ æˆ–é€å‘é‡ çš„ä¹˜æ³•ã€‚
    * å¦‚æœè¯•å›¾åœ¨ Tensor Core çš„ MMA æŒ‡ä»¤åºåˆ— ä¸­é—´ æ’å…¥è¿™ç§ä½ååé‡çš„ã€é€šå¸¸ç”± CUDA Core æ‰§è¡Œçš„ FMA æˆ–å…¶ä»–æ ‡é‡/å‘é‡æŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼ŒåŠ è½½ç¼©æ”¾å› å­å¹¶è¿›è¡Œä¹˜æ³•ï¼‰ï¼Œå°±ä¼šæ‰“æ–­ Tensor Core çš„é«˜æ•ˆæµæ°´çº¿ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
    * å› æ­¤ï¼Œç¡¬ä»¶åŠ é€Ÿåº“é€šå¸¸é€‰æ‹©åœ¨æ•´ä¸ª Tensor Core MMA è®¡ç®— å®Œæˆä¹‹å ï¼Œå†å¯¹è¾“å‡ºçŸ©é˜µçš„â€œå¤–éƒ¨ç»´åº¦â€ï¼ˆå¦‚ token ç»´åº¦æˆ–è¾“å‡ºé€šé“ç»´åº¦ï¼‰è¿›è¡Œç»Ÿä¸€çš„ç¼©æ”¾ï¼Œè¿™æ ·å¯ä»¥ä¿æŒ Tensor Core çš„é«˜æ•ˆç‡ã€‚è€Œ Triton è¿™æ ·çš„å·¥å…·åˆ™æä¾›äº†æ›´çµæ´»çš„ kernel fusion èƒ½åŠ›ï¼Œå…è®¸åœ¨ MMA è®¡ç®— tile ä¹‹åã€å†™å›å†…å­˜ä¹‹å‰ï¼Œèåˆè¿›è¿™äº› per-channel æ“ä½œï¼Œä»è€Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚


  * æ–°ç¡¬ä»¶æ”¯æŒï¼š
    * ã€ŠFused FP8 4-Way Dot Product With Scaling and FP32 Accumulationã€‹ [https://www.ac.uma.es/arith2024/papers/Fused%20FP8%204-Way%20Dot%20Product%20with%20Scaling%20and%20FP32%20Accumulation.pdf](https://www.ac.uma.es/arith2024/papers/Fused FP8 4-Way Dot Product with Scaling and FP32 Accumulation.pdf)

* æ–¹æ¡ˆï¼š

  * ![image-20250430153746632](./MLSys/image-20250430153746632.png)
    * activation outliers are more significantçš„åœºæ™¯ï¼Œè°ƒå¤§alpha


â€‹    

##### DeepSpeed â€”â€” ZeroQuant

> ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers

* è¦ç‚¹ï¼š
  * W8A8ï¼ˆLKDä¸‹ï¼ŒFFCæ”¯æŒW4A8ï¼‰
    * 5.19x/4.16x speedup
    * 3x memory footprint reduction
  * a fine-grained hardware-friendly quantization scheme for both weight and activations;
    * **group-wise quantization for weight and token-wise quantization for activations.** 
  * a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data;
  * a highly-optimized quantization system backend support to remove the quantization/dequantization overhead
* ç»“è®ºï¼š
  * ç›´æ¥PTQ + GPT-3/Bertï¼šW8A16æ•ˆæœæ— æŸï¼Œæ¥ä¸‹æ¥æœ‰æŸ ï¼ˆtable1ï¼‰
    * ![image-20250329011956361](./MLSys/image-20250329011956361.png)
    * æ¯ä¸€å±‚å†…ï¼Œä¸åŒtokençš„rangeåˆ†å¸ƒå·®è·å¤§ --> token-wise
    * output attn matrixï¼Œä¸åŒè¡Œçš„åˆ†å¸ƒå·®å¼‚å¤§
  * generation taskæ¯”eval taskæ›´æ•æ„Ÿ
  * Table2ã€Table4:
    * W8A8ã€W4/8A16æ•ˆæœå¥½
    * W4/8A16åœ¨zeroquant+lkdåå¯ç”¨
    * for W4/8, we quantize the MHSAâ€™s weight to INT8 and FFCâ€™s weight to INT4; for A8/16, we use FP16 activation for self-attention calculation (i.e., the GeMM related to Wq/k/v) and use INT8 for the rest calculation
  * table 6: w8a8çš„åŠ é€Ÿæ¯”ï¼Œå°batch2-3ï¼Œ64batch 4-5
  * 5.6 No Access to The Original Training Dataï¼Œå½±å“ä¸å¤§

* 4.1 Fine-grained Hardware-friendly Quantization Scheme
  * group-wise quantization for weight
    * å€Ÿé‰´Q-Bert
  * Token-wise Quantization for Activations
* å·¥ç¨‹ä¼˜åŒ–ï¼ˆ4.3 Quantization-Optimized Transformer Kernelsï¼‰
  * ![image-20250330153101483](./MLSys/image-20250330153101483.png)
  * quantï¼škernel fusion technique to fuse quantization operator with its previous operator, like layer normalization, to alleviate the data movement cost from token-wise quantization
    * æ¯ä¸ªSMå¯ä»¥quantize one row/token
  * dequantï¼šthe dequantization cost of the different GeMMsâ€™ output is alleviated by scaling the INT32 accumulation using both the weight and activation quantization scales, before writing the final FP16 result back to the main memory for the next FP16 operator (like GeLU)
    * å¼‚æ­¥è¯»å–é‡åŒ–scale
  * CUTLASS INT8 GeMMï¼šç›¸æ¯”cudnnçš„ä¼˜åŠ¿æ˜¯å®¹æ˜“å’Œdequantåšfuse
    * https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/
    * use the CUTLASS profiler tool that explores the tiling dimensions on the thread-blocks, WARPs, and WMMA (Tensor cores), as the three compute hierarchies available within the Ampere GPU architecture
  * å¼€å¯cuda graphï¼Œä¼˜åŒ–å°æ¨¡å‹æ€§èƒ½
* LKD
  * ![image-20250330153038937](./MLSys/image-20250330153038937.png)



##### PTQåŸºç¡€

![image-20250329010304813](./MLSys/image-20250329010304813.png)

* Sçš„é€‰å–
  * weight matrixï¼šmax(abs(X))
  * activationï¼š
    * dynamic
    * staticï¼šcalibrated using training data (e.g., momentum based
      averaging) and ï¬xed during inference [23]
      * ![image-20250329010432713](./MLSys/image-20250329010432713.png)

#### W4A16 - Weight Only

##### AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION

> æ¯”è¾ƒå®ç”¨ï¼Œç­–ç•¥+è§‚å¯Ÿï¼Œæš´åŠ›è®¡ç®—æœç´¢

* Intro
  * æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ï¼ˆAWQï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¿ç•™ 1% æ˜¾è‘—æƒé‡å’Œchannel-wiseç¼©æ”¾é™ä½é‡åŒ–è¯¯å·®ï¼Œä¸”ä¸ä¾èµ–åå‘ä¼ æ’­æˆ–é‡å»ºï¼Œæ³›åŒ–æ€§å¼ºã€‚
  * åŒæ—¶è®¾è®¡ TinyChat æ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨å³æ—¶åé‡åŒ–ã€SIMD æ„ŸçŸ¥æƒé‡æ‰“åŒ…å’Œå†…æ ¸èåˆç­‰æŠ€æœ¯ï¼Œåœ¨æ¡Œé¢å’Œç§»åŠ¨ GPU ä¸Šç›¸æ¯” Huggingface FP16 å®ç°æœ‰ 3 å€ä»¥ä¸ŠåŠ é€Ÿï¼ŒåŠ©åŠ› LLMs åœ¨è¾¹ç¼˜è®¾å¤‡çš„éƒ¨ç½²ã€‚å®éªŒè¡¨æ˜ï¼ŒAWQ åœ¨å¤šç§ä»»åŠ¡ã€æ¨¡å‹ä¸Šæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´å’Œå¤šæ¨¡æ€æ¨¡å‹é‡åŒ–ä¸­è¡¨ç°å‡ºè‰²
  * **ä½æ¯”ç‰¹æƒé‡é‡åŒ–çš„å›°å¢ƒ**ï¼šä½æ¯”ç‰¹æƒé‡é‡åŒ–å¯å‡å°‘å†…å­˜å ç”¨ï¼Œä½†QATæˆæœ¬é«˜ï¼ŒPTQåœ¨ä½æ¯”ç‰¹è®¾ç½®ä¸‹ç²¾åº¦ä¸‹é™å¤§ã€‚GPTQ è™½ç”¨äºŒé˜¶ä¿¡æ¯è¡¥å¿è¯¯å·®ï¼Œä½†é‡å»ºè¿‡ç¨‹æ˜“è¿‡æ‹Ÿåˆæ ¡å‡†é›†ï¼Œå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
* æ ¸å¿ƒæ€è·¯ï¼š
  * **ä¿ç•™ 1% æ˜¾è‘—æƒé‡æå‡é‡åŒ–æ€§èƒ½**ï¼šå‘ç° LLMs ä¸­éƒ¨åˆ†ï¼ˆ0.1%-1%ï¼‰æ˜¾è‘—æƒé‡å¯¹æ¨¡å‹æ€§èƒ½å½±å“å¤§ï¼Œè·³è¿‡è¿™äº›æƒé‡çš„é‡åŒ–å¯å‡å°‘é‡åŒ–æŸå¤±ã€‚**åŸºäºæ¿€æ´»å¹…åº¦è€Œéæƒé‡å¹…åº¦é€‰æ‹©æ˜¾è‘—æƒé‡**ï¼Œèƒ½æ˜¾è‘—æå‡é‡åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä½†æ··åˆç²¾åº¦æ•°æ®ç±»å‹ä¼šå¢åŠ ç³»ç»Ÿå®ç°éš¾åº¦ã€‚
  * **æ¿€æ´»æ„ŸçŸ¥ç¼©æ”¾ä¿æŠ¤æ˜¾è‘—æƒé‡**ï¼šæå‡ºæŒ‰é€šé“ç¼©æ”¾æ–¹æ³•é™ä½æ˜¾è‘—æƒé‡çš„é‡åŒ–è¯¯å·®ã€‚é€šè¿‡åˆ†æé‡åŒ–è¯¯å·®ï¼Œå¾—å‡º**ç¼©æ”¾æ˜¾è‘—é€šé“**å¯å‡å°ç›¸å¯¹è¯¯å·®çš„ç»“è®ºã€‚ä¸ºå¹³è¡¡æ˜¾è‘—å’Œéæ˜¾è‘—æƒé‡ï¼Œè‡ªåŠ¨æœç´¢æœ€ä¼˜ç¼©æ”¾å› å­ï¼Œé‡‡ç”¨ç®€å•æœç´¢ç©ºé—´å’Œå¿«é€Ÿç½‘æ ¼æœç´¢ç¡®å®šæœ€ä½³è¶…å‚æ•° Î±ï¼Œå¹¶åº”ç”¨æƒé‡è£å‰ªæœ€å°åŒ–é‡åŒ–å‡æ–¹è¯¯å·®ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–å›å½’æˆ–åå‘ä¼ æ’­ï¼Œå¯¹æ ¡å‡†é›†ä¾èµ–å°ï¼Œæ³›åŒ–æ€§å¼º
    * æ ¹æ®activationå†³ç­–æ˜¾è‘— -> scaling upæ˜¾è‘—çš„weight -> ç›¸å…³inputå˜å° -> é‡åŒ–è¯¯å·®å˜å°
    * ![image-20250410131426026](./MLSys/image-20250410131426026.png)
    * åŸºäºä¸€ä¸ªå‡è®¾ï¼šæ”¾å¤§æ˜¾è‘—çš„channelåï¼Œé‡åŒ–scaleå˜åŒ–ä¸å¤§
    * ![image-20250410130650937](./MLSys/image-20250410130650937.png)
* ç›¸æ¯”GPTQï¼š
  * å¯¹æ ¡å‡†é›†ä¸æ•æ„Ÿ
  * æ•ˆæœå¥½

##### GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers

> PTQï¼Œä¸»è¦åº”ç”¨äºæ¨ç†åœºæ™¯
>
> ç”¨äºŒé˜¶ä¿¡æ¯è¡¥å¿è¯¯å·®ï¼Œä½†é‡å»ºè¿‡ç¨‹æ˜“è¿‡æ‹Ÿåˆæ ¡å‡†é›†ï¼Œå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
>
> W4A16

* GPTQ, a new one-shot weight quantization method based on approximate second-order information

  * reducing the bitwidth down to 3 or 4 bits per weight
  * It therefore remains open whether one-shot **post-training quantization** to higher compression rates is generally-feasible.

  * gptq = gpt + ptq
  * LLMé‡åŒ–ï¼šå‚æ•°è¶Šå¤šï¼Œé‡åŒ–è¶Šæœ‰æ•ˆ

* ç»“è®ºï¼š
  * Further, we show that our model can also provide robust results in the extreme quantization regime, in which models are quantized to 2 bits per component, or even ternary values
  * reducing the bitwidth down to 3 or 4 bits per weight
  * **æœ¬è´¨ä¸Šæ˜¯LLMå‚æ•°é‡éå¸¸å¤§ï¼Œå­˜åœ¨å‚æ•°å†—ä½™**
  * Practical Speedups. Finally, we study practical applications. As an interesting use-case, we focus on the OPT-175B model: **quantized to 3 bits, this model takes approximately 63GB of memory**, including the embeddings and the output layer, which are kept in full FP16 precision. Additionally, storing the **complete history of keys and values for all layers,** a common optimization for generation tasks, **consumes another â‰ˆ 9GB for the maximum of 2048 tokens**. Hence, we can actually fit the entire quantized model into a single 80GB A100 GPU, which can be executed by dynamically dequantizing layers as they are required during inference (the model would not fully fit using 4 bits). For reference, standard FP16 execution requires 5x80GB GPUs, and the state-of-the-art 8bit LLM.int8() quantizer (Dettmers et al., 2022) requires 3 such GPUs
* æ–¹æ³•ï¼šlayer-wise
  * ![image-20250325020148022](./MLSys/image-20250325020148022.png)
  * å‡è®¾ï¼šthe quantization grid for W is ï¬xed before the process
* Optimal Brain Quantization
  * é—®é¢˜åˆ†è§£ï¼šå°†è®­ç»ƒåé‡åŒ–çš„ç›®æ ‡è½¬åŒ–ä¸ºæœ€å°åŒ–æŸå¤±è¯¯å·®ï¼Œé€šè¿‡æ³°å‹’çº§æ•°è¿‘ä¼¼ï¼Œå°†æŸå¤±è¯¯å·®è¡¨ç¤ºä¸ºä¸æµ·æ£®çŸ©é˜µç›¸å…³çš„å½¢å¼ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºé€å±‚ç‹¬ç«‹çš„å‡¸é—®é¢˜ï¼Œè¿›ä¸€æ­¥æŒ‰è¡Œåˆ†è§£ä¸ºç‹¬ç«‹é—®é¢˜ã€‚
  * è¿­ä»£é‡åŒ–ï¼šåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œé€‰æ‹©å•è¡Œä¸­çš„å•ä¸ªæƒé‡è¿›è¡Œé‡åŒ–ã€‚è®¡ç®—è¯¥è¡Œä¸­æ¯ä¸ªæƒé‡é‡åŒ–åçš„æŸå¤±è¯¯å·®ï¼Œå…¬å¼ä¸º$$\delta L_{q}=\frac{1}{2}\frac{w_{q}^{2}}{(H^{-1})_{qq}}$$ï¼Œå…¶ä¸­$$w_{q}$$æ˜¯æƒé‡ï¼Œ$$(H^{-1})_{qq}$$æ˜¯é€†æµ·æ£®çŸ©é˜µçš„ç¬¬qä¸ªå¯¹è§’å…ƒç´ ã€‚è´ªå¿ƒåœ°é€‰æ‹©å…·æœ‰æœ€å°æŸå¤±è¯¯å·®çš„æƒé‡è¿›è¡Œé‡åŒ–ã€‚ 
  * æƒé‡æ›´æ–°ï¼šå°†é€‰ä¸­çš„æƒé‡å››èˆäº”å…¥åˆ°é‡åŒ–ç½‘æ ¼ä¸Šçš„æœ€è¿‘å€¼ï¼Œç„¶åæ›´æ–°åŒä¸€è¡Œä¸­å°šæœªé‡åŒ–çš„å‰©ä½™æƒé‡ä»¥è¡¥å¿å¼•å…¥çš„è¯¯å·®ã€‚æ›´æ–°å…¬å¼é€šè¿‡æ±‚è§£æ‹‰æ ¼æœ—æ—¥å‡½æ•°å¾—åˆ°ï¼Œé‡åŒ–åçš„æœ€ä¼˜æƒé‡æ‰°åŠ¨ä¸º$$\delta W^{\top}=-\frac{w_{q}}{(H^{-1})_{qq}}e_{q}^{\top}H^{-1}$$ï¼Œå…¶ä¸­$$e_{q}$$æ˜¯ç¬¬qä¸ªæ ‡å‡†åŸºå‘é‡ã€‚ 
  * çŸ©é˜µæ›´æ–°ï¼šæ›´æ–°å‰©ä½™æƒé‡çš„é€†æµ·æ£®çŸ©é˜µï¼Œé€šè¿‡ç§»é™¤å·²é‡åŒ–æƒé‡å¯¹åº”çš„è¡Œå’Œåˆ—æ¥å®ç°ã€‚ 
  * é‡å¤è¿­ä»£ï¼šç»§ç»­ä¸Šè¿°è¿­ä»£è¿‡ç¨‹ï¼Œç›´åˆ°æ‰€æœ‰æƒé‡éƒ½è¢«é‡åŒ–ã€‚ 
* åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡å’Œé˜²æ­¢æ•°å€¼ä¸å‡†ç¡®æ€§ç´¯ç§¯ï¼ŒGPTQç®—æ³•å¯¹OBQè¿›è¡Œäº†æ”¹è¿›ï¼Œå¦‚é‡‡ç”¨**å›ºå®šçš„éè´ªå¿ƒé¡ºåºå¯¹æ‰€æœ‰è¡Œè¿›è¡Œé‡åŒ–**(å‡å°‘è®¡ç®—é‡)ã€**ä¸€æ¬¡ä¿æŒæƒé‡æ›´æ–°åœ¨åˆ—çš„å—å†…**(ä¸€æ¬¡128ä¸ªcolumnï¼Œbatchæ“ä½œ)ã€**å¯¹æµ·æ£®çŸ©é˜µçš„å¯¹è§’é¡¹åº”ç”¨è½»å¾®é˜»å°¼**(å¢åŠ æ•°å€¼ç¨³å®šæ€§)ä»¥åŠ**åˆ©ç”¨é€†æµ·æ£®çŸ©é˜µçš„Choleskyåˆ†è§£**(é€†æµ·æ£®çŸ©é˜µå®¹æ˜“inf)ç­‰ã€‚
  * ![image-20250325141614872](./MLSys/image-20250325141614872.png)

* ç®—æ³•
  * ![image-20250325143626232](./MLSys/image-20250325143626232.png)



#### 

#### TorchAO - Quantized Training

> [GPU Mode Lecture 30](https://www.youtube.com/watch?v=Br07GsnnvWc&t=798s)
>
> [Quantization-Aware Training for Large Language Models with PyTorch](https://pytorch.org/blog/quantization-aware-training/)

##### Intro

* overview
  * forward
    * weight only
    * dynamic act-weight
    * static act-weight
      * llmé€šå¸¸ä¸ä¼šå¯¹activationsåšstatic quant
  * backwardã€optimizer
    * fuse gradients + optimizer step
    * activation ckpt
  * distributedé€šä¿¡ï¼Œä¹Ÿå ç”¨æ˜¾å­˜

![image-20250315203511421](./MLSys/image-20250315203511421.png)

##### Low-bit optimizer

* é—®é¢˜ï¼špytorch optimizerä¸æ”¯æŒfp32 param + bf16 optimizerï¼Œå¼ºåˆ¶è¦æ±‚param-gradients-optimizerä¸‰è€…çš„dtypeä¸€è‡´
* ã€ŠMemory Efficient Optimizers with 4-bit Statesã€‹
* ã€Š8-bit Optimizers via Block-wise Quantizationã€‹
  * 8bit: bitandbytes
  * ![image-20250315205836858](./MLSys/image-20250315205836858.png)
* ![image-20250315210506126](./MLSys/image-20250315210506126.png)
  * æ€è·¯ï¼šfuse kernelï¼Œä¸å°†ä¸­é—´çŠ¶æ€å­˜å…¥gpuçš„global memory
    * **block-wise**è€Œä¸æ˜¯tensor-wiseï¼Œæ‰èƒ½**ç¡®ä¿è®¡ç®—scaleæ—¶åœ¨shared memoryè¿›è¡Œ**
    * å¯èƒ½è€ƒè™‘ TMA ï¼ˆtensor memory acceleratorï¼‰ï¼Ÿ

##### Low-bit weight-only training

* æ ¸å¿ƒé—®é¢˜ï¼šCan we train quantized weights without high precision copy?
* è§£æ³•ï¼šstochastic rounding in optimizerï¼ŒSRç”¨äºæ¢¯åº¦æ›´æ–°
* ç»“è®ºï¼š
  * 1Bä»¥ä¸Šçš„LLMæ¨¡å‹ï¼Œç›¸æ¯”bf16è®­ç»ƒçš„losså·®è·å°
  * finetuneæŸä¸ªæ¨¡å‹ï¼ŒLr 1e-5æ—¶ï¼Œbf16è®­ç»ƒç²¾åº¦ä½äºint8 SR
    * https://github.com/pytorch/ao/pull/644
    * ![image-20250331011617617](./MLSys/image-20250331011617617.png)

* Bf16 w/ SR

  * https://arxiv.org/abs/2010.06192

  * https://github.com/karpathy/llm.c/blob/7ecd8906afe6ed7a2b2cdb731c042f26d525b820/llmc/adamw.cuh#L19-L46

    https://github.com/gau-nernst/quantized-training/blob/c42a7842ff6a9fe97bea54d00489e597600ae683/other_optim/bf16_sr.py#L108-L122

##### Low-bit mixed-precision training

* é—®é¢˜ï¼šå¦‚ä½•int8ã€fp8è®­ç»ƒ
* è§£æ³•ï¼š
  * scaled matmul
    * tensorwise
    * å·¦ä¹˜matrixç”¨row-wiseã€å³ä¹˜matrixç”¨column-wise
  * å·¥ç¨‹å®ç°ï¼štriton kernel
    * torch compileçš„é—®é¢˜ï¼š1ï¼‰æ— æ³•fuseä¸¤ä¸ªscalingï¼›2ï¼‰auto configä¸å¥½
  * ![image-20250331123031392](./MLSys/image-20250331123031392.png)

* ç»“è®ºï¼š
  * int8çŸ©é˜µè®¡ç®—åŠ é€Ÿæ¯”ï¼Œa100åœ¨2å·¦å³
  * Comparing BF16 w/ padding and INT8 mixed-precision w/ padding, there is ~20% speedup

##### è®¨è®º

* INT8 weight-only + INT8 matmul?
  * Row-wise scaling in forward pass become column-wise scaling in backward pass
  * Tensor-wise scaling wonâ€™t have this issue.
  * Also possible to dequant and re-quant in the other axis, but will incur extra overhead.
  * QLoRA + FP8/INT8 matmul: need to dequant weight before matmul anyway.
* Ideas to explore
  * low-bit allreduce using Stochastic Rounding
  * INT4 Tensor Cores ğŸ‘€ (requires cutlass)
  * Output low-bit activations from matmul -> low-bit RMSNorm / GELU / SiLU

##### BitNet æ”¯æŒ

* BitNet 1.58-bit https://arxiv.org/abs/2402.17764
  * Weight: tensor-wise abs-mean scaling to ternary (-1, 0, 1)
  * Activation: per-token (row-wise) abs-max scaling to INT8
  * Originally trained with Quantization-Aware Training (QAT)
  * We can use INT8 Tensor Cores! (and 2-bit all-gather for FSDP)
  * https://github.com/pytorch/ao/pull/930
  * ![image-20250331153107702](./MLSys/image-20250331153107702.png)

##### W4A8 + QAT

>  [Quantization-Aware Training for Large Language Models with PyTorch](https://pytorch.org/blog/quantization-aware-training/)

* ![image-20250328174952301](./MLSys/image-20250328174952301.png)

  * **W4A8: int8 per token dynamic activations + int4 grouped per channel weights**

* ç»“è®º

  * **8da4w**ï¼šTable 1
    * QAT achieved 16.8% lower perplexity and unchanged model sizes and on-device inference and generation speeds on the Llama3-8B model lowered to XNNPACK.
  * Lower Bit Weight Only Quantization
    * group size 32
    * **applying QAT while skipping quantization for the first 3 and last 2 layers** æ•ˆæœå¯æ¥å—

* QAT: **8da4w**, only applied to linear layers

  * > ä¼¼ä¹å’Œ ZeroQuant çš„ç»“è®ºå¯¹é½

  * int8 per token dynamic activations

  * int4 grouped per channel weights

    - use a group size of 256 for weights

  * æŠ€å·§ï¼šdisable fake quantization for the first 1000 steps

* PTQ

  - embeddings are additionally quantized to int4 using a group size of 32

* QAT overhead
  * ~34% slower
  * æ˜¾å­˜å¢é•¿ï¼Œå¼€activation checkpointingç¼“è§£

#### DeepSpeed â€”â€” Mixture-of-Quantization (MoQ)

> https://www.deepspeed.ai/2021/05/04/MoQ.html
>
> https://www.deepspeed.ai/tutorials/MoQ-tutorial/

**MoQ**

* å’ŒQ-Bertç»“åˆ

  * Q-Bert

    * use **the second-order gradient (eigenvalue) of the parameters** to adjust the quantization schedule during training.

    * **use grouped quantization with a large grouping size (128)** when quantizing a parameter matrix to gain higher accuracy, but they are still inferior to the baseline.

  * To combine this with MoQ, we **cluster the eigenvalues into several regions based on their absolute values and tune the quantization period for each region accordingly**, the higher the magnitude of eigenvalue, the larger the factor and the slower the precision decreases.

* Stochastic Rounding

* å·¥ç¨‹æŠ€å·§ï¼š

  * weight onlyæ–¹æ³•ï¼Œkernelå®ç°dequant
  * **support both symmetric and asymmetric quantization** as the two mostly used schemes. We applied both techniques for QAT and see very similar results, however since symmetric approach is simpler to implement, we implement our inference kernels based on that. Regarding the rounding, we support **stochastic rounding** as another option besides the normal rounding. We have seen that for reducing the precision to as low as 4-bit or lower, stochastic rounding is more helpful as it has an unbiased random behavior during training.

* ç»“è®ºï¼š

  * ç²¾åº¦æ¯”ç›´æ¥ç”¨ W8A16 é«˜
  * ä¸åŒlayerå¯¹ç²¾åº¦çš„æ•æ„Ÿåº¦æœ‰å·®å¼‚ï¼Œä¸”ä¸åŒä»»åŠ¡ä¸ä¸€æ ·
    - Bert + GLUE Task: 0-4å±‚æœ€æ•æ„Ÿ
    - Bert-Large for SQuAD finetuningï¼š åé¢å±‚æ•æ„Ÿ
  * Enabling eigenvalue doesnâ€™t guarantee better accuracy result, usually it needs tuning with other settings, such as `start_bits`, `quantize_period` and `quantize_groups`.

#### [å­¦æœ¯]æ¨¡å‹é‡åŒ–ä»‹ç»

* ç¥ç»ç½‘ç»œï¼šå¤šå‡½æ•°çš„åµŒå¥—è¡¨ç¤º
  * è¶Šæ¥è¶Šä¸è§„åˆ™
* è®­ç»ƒé‡åŒ–å’Œæ¨ç†é‡åŒ–çš„å¼‚åŒ
  - è®­ç»ƒé‡åŒ–ï¼šç”¨äºè®¡ç®—çš„æ¨¡å‹é‡åŒ–
    - æƒé‡å’Œè¾“å…¥éƒ½æœ‰deltaï¼ˆé¢„ä¼°æ—¶è®¤ä¸ºæƒé‡deltaä¸ºé›¶ï¼‰
    - åå¾®åˆ†å…¬å¼ -> æ¯å±‚çš„è¾“å‡ºåˆ°ä¸‹ä¸€å±‚çš„è¾“å…¥å¾ˆé‡è¦
      - åŒæ ·çš„é‡åŒ–æ–¹å¼ï¼Œç›¸åŒé‡åŒ–ç²¾åº¦ç»™ä¸åŒå±‚çš„è¾“å…¥å¸¦æ¥ä¸åŒçš„è¯¯å·®
      - å­˜å‚¨é‡åŒ– v.s è®¡ç®—é‡åŒ–ï¼Œåè€…æ›´å¼ºè°ƒåœ¨å­˜å‚¨çº¦æŸä¸‹æ±‚è§£æœ€ä¼˜ç²¾åº¦
    - æ ¸å¿ƒï¼šæ§åˆ¶æ¢¯åº¦å™ªéŸ³çš„èŒƒæ•°
    - ä¸€ç§å¯æ±‚é—­å¼è§£ï¼ˆåˆ†å±‚é‡åŒ–æ¨¡å‹ï¼‰ï¼šé‡åŒ–æ ‡å‡†æ’åºã€æ¢¯åº¦æ’åºï¼Œä¸€ä¸€å¯¹åº”ï¼Œæ’åºä¸ç­‰å¼è¯æ˜
      * e.g. HAWQ-v2
  - æ¨ç†é‡åŒ–ï¼šç”¨äºå­˜å‚¨çš„æ¨¡å‹é‡åŒ–
    - ä¼ ç»Ÿé—®é¢˜å±€é™æ€§ï¼šæ±‚è§£é‡åŒ–è¯¯å·®æœ€å°ï¼Œä¸é¢å‘losså‡½æ•°ï¼Œé¢å‘ç­–ç•¥ï¼Œä¸å¯è§£
  - é‡åŒ–è®­ç»ƒå’Œé¢„æµ‹æ˜¯ä¸¤ä¸ªç›®æ ‡ï¼Œè®­ç»ƒç»“æœåº”è¯¥æ¢å¤æˆå…¨ç²¾åº¦å†ç”¨é¢„æµ‹å‹ç¼©çš„è¿‡ç¨‹å‹ç¼©ä¸€é


* Training é‡åŒ–

  * é‡åŒ–æ„ŸçŸ¥è®­ç»ƒçš„åŸç†ï¼šææ²çš„psæ–‡ç« ã€Šcommunication efficient distributed machine learning with the parameter serverã€‹https://www.cs.cmu.edu/~muli/file/parameter_server_nips14.pdf

  * ç»“è®ºï¼šæ§åˆ¶æ¢¯åº¦å™ªéŸ³çš„èŒƒæ•°
    * å°ç»“è®ºï¼šé‡åŒ–è®­ç»ƒå®Œåè¦æ¢å¤å…¨ç²¾åº¦è¿›è¡Œè®¡ç®—ï¼Œå†ç”¨è®­ç»ƒåé‡åŒ–æ‰‹æ®µè¿›è¡Œé‡åŒ–
    * å®ç°ä¸Šï¼šé‡åŒ–çš„æ­£ä¼ ï¼Œé‡åŒ–/å…¨ç²¾åº¦çš„åä¼ ï¼Œé‡åŒ–çš„æ›´æ–°
      * å…¨ç²¾åº¦åä¼ ï¼Œä¸è‡ªåŠ¨æ±‚å¯¼æ¨¡å—çš„å®ç°æœ‰å…³ï¼Œå¯èƒ½å­˜åœ¨


* æ€»ç»“ï¼š

  * é‡åŒ–é—®é¢˜æœ¬è´¨æ˜¯NP-hardé—®é¢˜ï¼Œéƒ¨åˆ†æƒ…å†µä¸‹å¯è½¬æ¢æˆæŒ‡æ•°è§„åˆ’é—®é¢˜

  * é‡åŒ–è®­ç»ƒå’Œé¢„æµ‹æ˜¯ä¸¤ä¸ªç›®æ ‡ï¼Œè®­ç»ƒç»“æœåº”è¯¥æ¢å¤æˆå…¨ç²¾åº¦å†ç”¨é¢„æµ‹å‹ç¼©çš„è¿‡ç¨‹å‹ç¼©ä¸€é

##### [(Stochastic Rounding): Deep Learning with Limited Numerical Precision](https://arxiv.org/abs/1502.02551)

* å½“å‰å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ ç³»ç»Ÿ**æœªå……åˆ†åˆ©ç”¨ç¥ç»ç½‘ç»œçš„å®¹é”™æ€§**ã€‚
* æœ¬æ–‡ **å‘ç°é‡‡ç”¨éšæœºèˆå…¥æ—¶ï¼Œ16 ä½å®šç‚¹æ•°è¡¨ç¤ºè®­ç»ƒæ·±åº¦ç½‘ç»œåˆ†ç±»ç²¾åº¦å‡ ä¹æ— ä¸‹é™**ã€‚
  * é€šè¿‡ MNIST å’Œ CIFAR10 æ•°æ®é›†å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•
  * è®¾è®¡äº†åŸºäº FPGA çš„ç¡¬ä»¶åŠ é€Ÿå™¨
    * åˆ©ç”¨å¤§é‡å®šç‚¹è¿ç®—å•å…ƒã€æ•°æ®æµæ¶æ„å’Œéšæœºèˆå…¥æ¨¡å—ï¼Œå®ç°é«˜ååé‡å’Œä½åŠŸè€—ï¼Œä¸ºè½¯ç¡¬ä»¶ååŒè®¾è®¡çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿå‘å±•æä¾›äº†æ€è·¯ã€‚
* ç»“è®ºï¼š
  * 8ä½fractional lengthï¼ŒMNIST + DNNç²¾åº¦ä¸‹é™å°
  * CIFAR10ï¼ŒFL14ç²¾åº¦éƒ½ä¸å¤Ÿ......ä½†éšæœºèˆå…¥å¾ˆæœ‰ç”¨
  * FPGAï¼š**è„‰åŠ¨é˜µåˆ—æ¶æ„**ï¼šæ¯ä¸ªèŠ‚ç‚¹å« DSP å•å…ƒï¼Œå®ç°ä¹˜ç§¯ç´¯åŠ æ“ä½œã€‚ç»“æœç»éšæœºèˆå…¥å’Œæˆªæ–­å¤„ç†åå­˜å‚¨ã€‚éšæœºèˆå…¥ç¡¬ä»¶å¼€é”€å°äº 4%ã€‚
* ç†è®º Setting
  * å®šç‚¹æ•°è¡¨ç¤ºï¼šæ ‡å‡†æ·±åº¦å­¦ä¹ è®­ç»ƒå¸¸ç”¨ 32 ä½æµ®ç‚¹æ•°ï¼Œæœ¬æ–‡é‡‡ç”¨å¹¿ä¹‰å®šç‚¹æ•°è¡¨ç¤º [QI.QF]ï¼Œç”¨âŸ¨IL, FLâŸ©è¡¨ç¤ºï¼Œå…¶ç²¾åº¦ä¸º FL ä½ï¼ŒèŒƒå›´æ˜¯$$[-2^{IL - 1}, 2^{IL - 1} - 2^{-FL}]$$ ï¼Œæœ€å°æ­£æ•°$$\epsilon = 2^{-FL}$$ã€‚
  * èˆå…¥æ¨¡å¼
    - **å°±è¿‘èˆå…¥**ï¼šæ ¹æ®æ•°ä¸ç›¸é‚»æ•´æ•°å€$$\epsilon$$çš„è·ç¦»å†³å®šèˆå…¥å€¼ã€‚
    - **éšæœºèˆå…¥**ï¼šæ•°èˆå…¥åˆ°$$\lfloor x\rfloor$$çš„æ¦‚ç‡ä¸å®ƒå’Œ$$\lfloor x\rfloor$$çš„æ¥è¿‘ç¨‹åº¦æˆæ­£æ¯”ï¼Œæ˜¯æ— åèˆå…¥ï¼Œé¢„æœŸèˆå…¥è¯¯å·®ä¸º 0ã€‚
    - **é¥±å’Œå¤„ç†**ï¼šè‹¥æ•°è¶…å‡ºâŸ¨IL, FLâŸ©èŒƒå›´ï¼Œå°†ç»“æœé¥±å’Œåˆ°ä¸Šä¸‹é™ã€‚
  * **ä¹˜ç§¯ç´¯åŠ ï¼ˆMACCï¼‰æ“ä½œ**ï¼šåˆ†ä¸¤æ­¥ï¼Œå…ˆè®¡ç®—å‘é‡å†…ç§¯å’Œ$$z=\sum_{i = 1}^{d}a_{i}b_{i}$$ ï¼Œå†å°†zè½¬æ¢ä¸ºç›®æ ‡å®šç‚¹æ ¼å¼$$c_{0}=Convert(z,<\tilde{IL}, \tilde{IF}>)$$ã€‚è¯¥æ–¹æ³•æ¨¡æ‹Ÿç¡¬ä»¶è¡Œä¸ºï¼Œå‡å°‘éšæœºèˆå…¥ç¡¬ä»¶å¼€é”€ï¼Œä¾¿äºä½¿ç”¨ CPU/GPU å’Œ BLAS åº“æ¨¡æ‹Ÿå®šç‚¹è®¡ç®—ã€‚

* å¯¹æ¯”å°±è¿‘èˆå…¥ï¼š

  * æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯ä¿ç•™

    * ä¼ ç»Ÿèˆå…¥å°† $$\Delta W \in (-\epsilon/2, \epsilon/2)$$ å¼ºåˆ¶å½’é›¶ï¼Œå®Œå…¨ä¸¢å¤±æ¢¯åº¦ä¿¡æ¯ã€‚
    * éšæœºèˆå…¥é€šè¿‡æ¦‚ç‡æœºåˆ¶ï¼ˆå¦‚ $$p = \frac{\Delta W}{\epsilon}$$ï¼‰ä¿ç•™éé›¶æ›´æ–°çš„å¯èƒ½æ€§ï¼Œç¡®ä¿æ¢¯åº¦æ–¹å‘çš„ç»Ÿè®¡æ­£ç¡®æ€§ã€‚

  * å™ªå£°æ­£åˆ™åŒ–æ•ˆåº”

    - éšæœºèˆå…¥å¼•å…¥çš„å™ªå£°ç­‰ä»·äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨å…¥éšæœºæ‰°åŠ¨ï¼Œç±»ä¼¼äº Dropout æˆ–æ•°æ®å¢å¼ºï¼Œå¯æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
    - æ•°å­¦ä¸Šï¼Œå™ªå£°ä½¿ä¼˜åŒ–è¿‡ç¨‹æ›´æ˜“è·³å‡ºå±€éƒ¨æå°å€¼ï¼Œå¢å¼ºé²æ£’æ€§ï¼ˆå‚è€ƒ Bishop, 1995ï¼‰

  * ä¼ ç»Ÿèˆå…¥çš„è¯¯å·®æ–¹å‘å›ºå®šï¼Œå¯èƒ½å¯¼è‡´è¯¯å·®ç´¯ç§¯ï¼ˆå¦‚æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼‰ã€‚

  * æµ®ç‚¹æ ¼å¼çš„å±€é™æ€§

    - æµ®ç‚¹æ•°ï¼ˆå¦‚ 32 ä½ï¼‰çš„ç²¾åº¦ç”±å°¾æ•°å†³å®šï¼Œä½ç²¾åº¦å®šç‚¹æ•°çš„èˆå…¥è¯¯å·®å¯èƒ½æ›´æ˜¾è‘—ã€‚

    - éšæœºèˆå…¥é€šè¿‡æ¦‚ç‡æœºåˆ¶å°†è¯¯å·®å‡åŒ€åˆ†å¸ƒåœ¨é‡åŒ–æ­¥é•¿ $$\epsilon$$ å†…ï¼Œå‡å°‘å¯¹æ¨¡å‹çš„ç³»ç»Ÿæ€§å¹²æ‰°ã€‚

* åº”ç”¨

  * **å‰å‘ä¼ æ’­ä¸­çš„ Activation è®¡ç®—**
    * â€œæ··åˆâ€ç²¾åº¦è®­ç»ƒï¼š
      * åœ¨ CIFAR10 æ•°æ®é›†è®­ç»ƒæ—¶ï¼Œä½ç²¾åº¦å®šç‚¹è¿ç®—ï¼ˆå¦‚ 16 ä½å®šç‚¹æ•°ï¼‰ç»“åˆéšæœºèˆå…¥ï¼Œå‰æœŸè®­ç»ƒèƒ½ä¿æŒä¸€å®šç¨³å®šæ€§ï¼Œä½†éšç€ç²¾åº¦é™ä½ï¼ˆå¦‚ 12 ä½ï¼‰ï¼Œæ”¶æ•›é€Ÿåº¦ä¼šå˜æ…¢ï¼Œå­¦ä¹ æ•ˆæœå˜å·®ã€‚æ­¤æ—¶åˆ‡æ¢åˆ°æ›´é«˜ç²¾åº¦ï¼ˆå¦‚ 20 ä½ï¼‰ï¼Œç½‘ç»œæ€§èƒ½å¯å¿«é€Ÿæå‡ã€‚è¿™æ˜¯å› ä¸ºå‰æœŸä½ç²¾åº¦è®­ç»ƒèƒ½åˆ©ç”¨å…¶è®¡ç®—ä¼˜åŠ¿ï¼ŒåæœŸé«˜ç²¾åº¦è®­ç»ƒå¯å¼¥è¡¥ä½ç²¾åº¦å¸¦æ¥çš„æ¢¯åº¦ä¿¡æ¯æŸå¤±ï¼Œæé«˜æœ€ç»ˆæ€§èƒ½ã€‚
  * ç”¨äºæ¢¯åº¦æ›´æ–°

##### QAT

> ã€ŠQuantization and Training of Neural Networks for Efï¬cient
> Integer-Arithmetic-Only Inferenceã€‹

* Intro:

  * achieved by simulating quantization numerics during training while keeping the weights and/or activations in the original data type, typically float, effectively â€œfake quantizingâ€ the values instead of actually casting them to lower bit-widths

  * ![image-20250328190021045](./MLSys/image-20250328190021045.png)

  * ```Python
    # PTQ: x_q is quantized and cast to int8
    # scale and zero point (zp) refer to parameters used to quantize x_float
    # qmin and qmax refer to the range of quantized values
    x_q = (x_float / scale + zp).round().clamp(qmin, qmax).cast(int8)
    
    # QAT: x_fq is still in float
    # Fake quantize simulates the numerics of quantize + dequantize
    x_fq = (x_float / scale + zp).round().clamp(qmin, qmax)
    x_fq = (x_fq - zp) * scale
    ```

##### Scaling Laws for Precision [ICLR 2025 Oral]

> GPU Mode Lecture 52: https://www.youtube.com/watch?v=YCfzf0TunOM
>
> https://openreview.net/forum?id=wg1PCg3CUP

* é—®é¢˜ï¼š
  * Scientific question; 1b with INT4 weights vs 500m in BF16, which wins?
  * noise with variance O(2^{-P})

* è¦ç‚¹:
  * ![image-20250411001753645](./MLSys/image-20250411001753645.png)
  * lower precision reduces the modelâ€™s effective parameter count
  * **training larger models in lower precision may be compute optimal**
  * overtrained modelsï¼Œå—é‡åŒ–å½±å“æ›´å¤§ï¼ˆå‚æ•°å°‘ã€æ•°æ®å¤šï¼‰
    * overtrainçš„å®šä¹‰ï¼šå‚è€ƒchinchilla paperï¼Œtokens seen
    * å’ŒQLoRaçš„è§‚å¯Ÿç›¸ç¬¦ï¼Œè¿™ä¹Ÿæ˜¯QLoRaä¸ºä»€ä¹ˆæå‡ºNF4
* ç†è®ºåˆ†æï¼š
  * ![image-20250411001515823](./MLSys/image-20250411001515823.png)
  * $$\delta_{\mathrm{PTQ}}(N, D, P_{\mathrm{post}}) = C_T \left( \frac{D^{\gamma_D}}{N^{\gamma_N}} \right) e^{-P_{\mathrm{post}} / \gamma_{\mathrm{post}}}$$
    * ç”±äºæŒ‡æ•°è¶…å‚è¿‘ä¼¼ï¼Œç±»ä¼¼äº D/N çš„ power law
  * Quantized training: $$L(N, D) = A[N(1 - e^{-P_{\mathrm{w}} / \gamma_{\mathrm{w}}})]^{-\alpha} + B D^{-\beta} + E$$
  * ![image-20250411115840121](./MLSys/image-20250411115840121.png)

* SCALING LAWS FOR PTQ
  * ![image-20250411013134252](./MLSys/image-20250411013134252.png)
  * ä¸€ç§è§£é‡Šï¼šDè¶Šå¤§ï¼ŒNä¼šå­¦åˆ°è¶Šå¤šä¿¡æ¯ï¼Œå› æ­¤é‡åŒ–çš„æŸå¤±å¢åŠ 
  * å¯¹GPTQã€AWQã€æ™®é€šé‡åŒ–ï¼Œå‡æœ‰æ•ˆ

* Scaling law for quantized training
  * ![image-20250411023740824](./MLSys/image-20250411023740824.png)
  * æ¨¡å‹è¶Šå¤§ï¼Œé‡åŒ–å¯ä»¥è¶Šæ¿€è¿›
  * ![image-20250411025219080](./MLSys/image-20250411025219080.png)
    * æ•æ„Ÿç¨‹åº¦ï¼ša > w > kv cache
* Guidance
  * 4.3.1 IF YOU MUST TRAIN IN LOW PRECISION, INCREASE PARAMETERS BEFORE DATA
    * ![image-20250411114729068](./MLSys/image-20250411114729068.png)
  * 4.3.2 COMPUTE-OPTIMAL PRETRAINING PRECISION IS IN GENERAL INDEPENDENT OF
    COMPUTE
  * 4.3.3 BUT COMPUTE-OPTIMAL PRETRAINING PRECISION CAN INCREASE IN COMPUTE IF
    MODEL SIZE N IS CONSTRAINED
    * è®¡ç®—å‚è€ƒ E.2 COMPUTE-OPTIMALITY CALCULATIONS
* ç»†èŠ‚ã€ä¾‹å­ï¼š
  * å¯¹Q + KV cacheé‡åŒ–å’Œä»…å¯¹KV cacheé‡åŒ–åŒºåˆ«ä¸å¤§
  * llama3é‡åŒ–æ¯”llama2æ›´éš¾ï¼ŒåŸå› æ˜¯llama3 overtrained
  * per channelä¼šæ”¹å˜ç³»æ•°ï¼Œä½†ä¸ä¼šæ”¹å˜scaling law
* Literature Review
  * On the theoretical front, work on scaling laws (Bahri et al., 2024; Bordelon et al., 2024; Lin et al., 2024b) finds that noise to various parts of model or data affects loss in a predictable way. While previous works have explored the scaling behavior of post-training quantization in terms of total model bits (Dettmers & Zettle-moyer, 2023) and knowledge capacity (Allen-Zhu & Li, 2024), we focus instead on data scaling.
  * We note that in general the exact fitted values of all coefficients and exponents can vary drastically
    based on **small implementation differences**: Besiroglu et al. (2024) find different constants when
    attempting to replicate (Hoffmann et al., 2022), Sardana & Frankle (2023) fit coefficients A,B of
    different orders of magnitude.
  * **Overtraining.** In practice, accounting for inference costs means training smaller models for sub-
    stantially longer than Chinchilla-optimal (Sardana & Frankle, 2023; Gadre et al., 2024). For in-
    stance, Llama-3-8B is trained to **D/N â‰ˆ 2000** (Dubey et al., 2024) and the Gemma-2 series up
    to **D/N > 1000** (Team et al., 2024). We refer to such models as â€œovertrainedâ€ in this paper, with
    the token/parameter ratio D/N being a key quantity throughout. Work on inference-time compute
    (Snell et al., 2024; Brown et al., 2024) and on synthetic and multimodal data (Yang et al., 2024; Fan
    et al., 2024; Bauer et al., 2024) suggests future models may be even more overtrained.
* å®éªŒsetting
  * datasetï¼šdolma https://huggingface.co/datasets/allenai/dolma



#### Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT

* **Q-BERT**é€šè¿‡**Hessian åˆ†æ**æå‡ºäº†ä¸€ç§é’ˆå¯¹ BERT æ¨¡å‹çš„**è¶…ä½ç²¾åº¦é‡åŒ–æ–¹æ³•**ï¼Œç»“åˆ**æ··åˆç²¾åº¦ç­–ç•¥**å’Œ**åˆ†ç»„é‡åŒ–æ–¹æ¡ˆ**ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å®ç°äº†**13 å€å‚æ•°å‹ç¼©**å’Œ**4 å€æ¿€æ´» / åµŒå…¥å‹ç¼©**ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ SST-2ã€MNLIã€CoNLL-03 å’Œ SQuAD ä»»åŠ¡ä¸­ï¼ŒQ-BERT çš„æ€§èƒ½æŸå¤±æœ€å¤§ä»…ä¸º**2.3%**ï¼Œæ˜¾è‘—ä¼˜äºç›´æ¥é‡åŒ–æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼ŒSQuAD ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™ä¸æ¨¡å‹æœªæ”¶æ•›è‡³å±€éƒ¨æå°ç‚¹æœ‰å…³ï¼Œè€Œåˆ†ç»„é‡åŒ–é€šè¿‡ç²¾ç»†åŒ–è°ƒæ•´é‡åŒ–èŒƒå›´æœ‰æ•ˆç¼“è§£äº†ç²¾åº¦æŸå¤±ã€‚
  * é¦–æ¬¡å®ç°äº† BERT æ¨¡å‹åœ¨**2-bit ç²¾åº¦ä¸‹çš„æœ‰æ•ˆå‹ç¼©**ï¼Œä¸ºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚
* **Hessian çŸ©é˜µè®¡ç®—ä¸åˆ†æ**
  - è®¡ç®—æ–¹æ³•
    - ä½¿ç”¨**çŸ©é˜µè‡ªç”±å¹‚è¿­ä»£æ³•**ï¼ˆMatrix-Free Power Iterationï¼‰[39] ä¼°è®¡ Hessian çš„æœ€å¤§ç‰¹å¾å€¼ï¼Œæ— éœ€æ˜¾å¼æ„é€  Hessian çŸ©é˜µã€‚
      - https://arxiv.org/pdf/1802.08241 åˆ†æäº†Hessianç‰¹å¾å€¼å’Œç¥ç»ç½‘ç»œæ‰°åŠ¨çš„å…³ç³»
    - å…·ä½“æ­¥éª¤ï¼š
      1. éšæœºåˆå§‹åŒ–å‘é‡vï¼Œä¸å½“å‰å±‚å‚æ•°ç»´åº¦ä¸€è‡´ã€‚
      2. è®¡ç®—æ¢¯åº¦$$g_i = \nabla L$$ã€‚
      3. è¿­ä»£æ›´æ–°$$v = \frac{Hv}{\|Hv\|_2}$$ï¼Œå…¶ä¸­Hvé€šè¿‡åå‘ä¼ æ’­æ¢¯åº¦$$g_i^T v$$å¾—åˆ°ï¼ˆå¼ 3.1ï¼‰ã€‚
      4. é‡å¤å¤šæ¬¡è¿­ä»£åï¼Œvè¿‘ä¼¼ä¸ºæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚
  - å…³é”®å‘ç°
    - BERT å„å±‚ Hessian ç‰¹å¾å€¼åˆ†å¸ƒå·®å¼‚æ˜¾è‘—ï¼ˆå›¾ 2ï¼‰ï¼Œä¸­é—´å±‚ï¼ˆ4-8 å±‚ï¼‰å‡å€¼å’Œæ–¹å·®æœ€å¤§ï¼Œæœ«å±‚æœ€ç¨³å®šã€‚
    - SQuAD ä»»åŠ¡çš„ Hessian å­˜åœ¨è´Ÿç‰¹å¾å€¼ï¼ˆå›¾ 3ï¼‰ï¼Œè¡¨æ˜æ¨¡å‹æœªæ”¶æ•›è‡³å±€éƒ¨æå°ç‚¹ã€‚

![image-20250328155538247](./MLSys/image-20250328155538247.png)

* æ··åˆç²¾åº¦é‡åŒ–ç­–ç•¥

  - æ•æ„Ÿæ€§æŒ‡æ ‡
    - $$\Omega_i = |\text{mean}(\lambda_i)| + \text{std}(\lambda_i)$$ï¼Œç»¼åˆç‰¹å¾å€¼çš„å‡å€¼å’Œæ–¹å·®ã€‚
    - **ç¤ºä¾‹**ï¼šSQuAD ç¬¬ 7 å±‚$$\lambda_i$$å‡å€¼ä¸º 1.0ï¼Œä½†æ–¹å·®é«˜è¾¾ 61.6ï¼Œéœ€åˆ†é…æ›´é«˜ç²¾åº¦ã€‚

  - ä½åˆ†é…è§„åˆ™
    - æŒ‰$$\Omega_i$$é™åºæ’åˆ—å„å±‚ï¼Œå‰ 50% åˆ†é… 3-bitï¼Œå 50% åˆ†é… 2-bitï¼ˆ2/3-bit æ··åˆï¼‰ã€‚
    - é’ˆå¯¹ä¸åŒä»»åŠ¡è°ƒæ•´åˆ†é…æ¯”ä¾‹ï¼ˆå¦‚ SQuAD æ›´ä¿å®ˆï¼‰ã€‚

  - æ¶ˆèå®éªŒ
    - **åå‘åˆ†é…ï¼ˆQ-BERTMP-revï¼‰**ï¼šå°†æ•æ„Ÿå±‚åˆ†é…ä½ä½ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆMNLI å‡†ç¡®ç‡ä¸‹é™ 2.8%ï¼Œè¡¨ 4ï¼‰ï¼ŒéªŒè¯ Hessian æŒ‡å¯¼çš„å¿…è¦æ€§ã€‚

* **åˆ†ç»„é‡åŒ–æŠ€æœ¯**

  - åˆ†ç»„ç­–ç•¥

    - **å±‚æ¬¡åˆ†ç»„**ï¼šå°†å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚çš„çŸ©é˜µæŒ‰å¤´åˆ†ç»„ï¼ˆ12 ç»„ï¼‰ï¼Œæ¯ç»„**å†æŒ‰è¾“å‡ºç¥ç»å…ƒåˆ’åˆ†å­ç»„**ï¼ˆæ¯ç»„ 6 ä¸ªç¥ç»å…ƒï¼Œå…± 128 ç»„ï¼‰ã€‚
    - **åŠ¨æ€èŒƒå›´è°ƒæ•´**ï¼šæ¯ç»„ç‹¬ç«‹è®¡ç®—é‡åŒ–èŒƒå›´ï¼Œå‡å°‘è·¨ç»„æ•°æ®åˆ†å¸ƒå·®å¼‚çš„å½±å“ï¼ˆå›¾ 4ï¼‰ã€‚
    - group-wiseé‡åŒ–åœ¨ 4-bit ä¸‹æ¯”layer-wiseé‡åŒ–åœ¨ SST-2 ä»»åŠ¡ä¸­æå‡ 7% å‡†ç¡®ç‡ï¼ˆè¡¨ 2ï¼‰ã€‚

  - ![image-20250328152923360](./MLSys/image-20250328152923360.png)

    - d/Nhæ˜¯æ¯ä¸ªå¤´çš„è¾“å‡ºç»´åº¦

  - | **ä»»åŠ¡** | **å‹ç¼©æ¯”ï¼ˆå‚æ•°ï¼‰** | **æ€§èƒ½æŸå¤±ï¼ˆF1 / å‡†ç¡®ç‡ï¼‰** | **å…³é”®è§‚å¯Ÿ**                 |
    | -------- | ------------------ | --------------------------- | ---------------------------- |
    | SST-2    | 13Ã—                | â‰¤1.1%                       | æ··åˆç²¾åº¦ï¼ˆ2/3-bitï¼‰æ•ˆæœæœ€ä½³  |
    | MNLI     | 13Ã—                | â‰¤2.3%                       | ä¸­é—´å±‚å¯¹é‡åŒ–æ›´æ•æ„Ÿ           |
    | CoNLL-03 | 13Ã—                | â‰¤1.1%                       | ä½ç½®åµŒå…¥é‡åŒ–æ•æ„Ÿæ€§é«˜äºè¯åµŒå…¥ |
    | SQuAD    | 13Ã—                | â‰¤2.3%                       | æ¨¡å‹æœªæ”¶æ•›å¯¼è‡´æ€§èƒ½ä¸‹é™æ›´æ˜¾è‘— |

* ç®—æ³•ç»†èŠ‚ï¼š

  * backwardä½¿ç”¨Straight-Through Estimators

* å·¥ç¨‹å®ç°ç»†èŠ‚

  - Group-wiseçš„LUTå®ç°ï¼šå¢åŠ **æŸ¥æ‰¾è¡¨ï¼ˆLUTï¼‰**æ•°é‡ï¼Œä½†é€šè¿‡å®éªŒå‘ç° 128 ç»„æ—¶æ€§èƒ½å¢ç›Šé¥±å’Œï¼ˆè¡¨ 2ï¼‰ï¼Œå¹³è¡¡ç²¾åº¦ä¸å¤æ‚åº¦ã€‚
  - dequantï¼šå…ˆçŸ©é˜µè®¡ç®—ï¼Œå†rescale

* ç»“è®ºï¼š

  - **å‚æ•°å‹ç¼©**ï¼šæƒé‡ä» 32-bit é™è‡³ 2/3-bitï¼Œå®ç° 13Ã— å‹ç¼©ï¼ˆBERT-Base ä» 410MBâ†’30.5MBï¼‰ã€‚
  - **æ¿€æ´»å‹ç¼©**ï¼šæ¿€æ´»å€¼é‡åŒ–è‡³ 8-bitï¼Œå‡å°‘ 4Ã— å†…å­˜å ç”¨ã€‚
  - **åµŒå…¥å‹ç¼©**ï¼šè¯åµŒå…¥ 4-bit + ä½ç½®åµŒå…¥ 8-bit æ··åˆï¼ŒåµŒå…¥å±‚ä» 91MBâ†’11.6MBï¼ˆ8Ã— å‹ç¼©ï¼‰ã€‚

#### Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications

1. Introduction

* general requirements for new DL hardware designs:
  * High memory bandwidth and capacity for embeddings 
  * Support for powerful matrix and vector engines 
  * Large on-chip memory for inference with small batches 
  * Support for half-precision floating-point computation

2. Characterization of DL Inference

* Ranking and Recommendation
  * embedding lookup ç¡¬ä»¶å±‚é¢åˆ†æ
    * ç‰¹ç‚¹æ˜¯ low spatial localityï¼Œcaching éš¾åº¦é«˜
    * High-bandwidth memory (HBM): æ€§èƒ½é«˜ï¼Œå®¹é‡ä¸å¤Ÿ
    * Non-volatile memory (NVM): bandwidthä½ ä¸å¯è¡Œã€æˆæœ¬ä½
* CV: å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€è§†é¢‘ç†è§£
  * number of operations per weight é«˜
  * number of operations per activation ä¸é«˜
* NLP: NMT(Neural machine translation) uses seq2seq
  * parallelism: é’ˆå¯¹ RNN-based approaches çš„å¹¶è¡ŒåŒ–æ–¹æ¡ˆï¼Œæ¯”å¦‚ stacked conv, transformer

* computation kernels åˆ†æ
  * æ•°æ®ä¸­å¿ƒæˆæœ¬ï¼šfc > embedding lookup > tensor manipulation > conv
  * fc layer åˆ†æï¼šå›¾å†…ç¬¬ä¸€å±‚è¿ç®—æŠ½è±¡æˆçŸ©é˜µä¹˜ï¼ˆå½“batch size M è¾ƒå°æ—¶ï¼ŒBLAS3 è¶‹è¿‘äº BLAS2ï¼Œmatrix multiplication engine æ•ˆæœå‰Šå¼±ï¼‰
    * When an MÃ—K matrix activation matrix is multiplied with a KÃ—N weight matrix, we compute 2MKN operations while reading KN weights, leading to 2M operations per weight.
    * Similarly, the number of operations per activation is 2N.

3. Performance Optimizations

* bf16 sum pooling æ˜¯ä¼˜åŒ–æ–¹å‘
* intel int8 multiplication with 16-bit accumulation æå‡ä¸€å€åå

* FBGEMM, an algebra engine
  * outlier-aware quantization: $W = W_{main}+W_{outlier}$
    * outlier uses 32-bit accumulation. We find that Woutlier becomes a sparse matrix, often with density less than 0.1%, especially when combined with symmetric quantization [39].

* accuracy challenges
  * Fine-grain Quantization
  * Quantization-aware Training
  * Selective Quantization
  * Outlier-aware Quantization: æ›´ç²¾ç»†ã€æ›´çª„åœ°é€‰å– quantize range
  * Net-aware Quantization: if an operator is only followed by ReLU, we can narrow down the range by excluding negative values

* HPC challenges
  * HPC ä¹ æƒ¯ â€œpackâ€ a block of input matrices into a format friendly for vectorization and cache locality, ä½†å¯¹äºDLé¢†åŸŸ tall-skinny matricesï¼Œpack ä¼šå¸¦æ¥ overhead
  * DLä¸å®Œå…¨æ˜¯çŸ©é˜µä¹˜ï¼šæ¯”å¦‚conv opï¼Œè½¬åŒ–ä¸ºçŸ©é˜µä¹˜éœ€è¦æå‰åš `im2col` æ“ä½œï¼Œæœ‰ overheadï¼Œå› æ­¤éœ€è¦ä¸“é—¨åš kernel fusion æä¾› conv interface
    * This will also enable algorithmic optimizations such as Winograd or FFT-based convolution as in cuDNN with automatic choice of the best algorithm for given tensor shapes.
    * reduced-precision è®¡ç®—ä¹Ÿéœ€è¦ä¸“é—¨çš„ fusionï¼Œä¸€äº›åº“æœªèƒ½æ»¡è¶³

```c++
template<typename T_PACK_A, typename T_PACK_B, typename T_C, typename OUT_FUNCTOR>
void gemmPacked(
  // packed inputs
  T_PACK_A& packA, T_PACK_B& packedB,
  // output
  T_C* C, uint32_t ldc,
  // post-processing functor, e.g. Relu
  OUT_FUNCTOR& outProcess);
```

* The packing of matrix A can be specialized and fused with memory bandwidth bound operations such as `im2col`, row-wise sum for asymmetric quantization, or depth-wise convolution.

* whole graph optimization
  * æ‰‹åŠ¨ fusion ä»æœ‰å¿…è¦

4. Application Driven HW Co-design Directions

* Recommendation models not only require a huge memory capacity but also high bandwidth.
* ä¼˜åŒ–çš„å‰¯ä½œç”¨ï¼šæ¯”å¦‚ avx512 é™é¢‘ï¼Œè§ ã€ŒComputer-Architecture.mdã€
* å¢åŠ  tiers çš„ trade-offsï¼šä¼ è¾“ã€å‹ç¼©è§£å‹å¼€é”€ï¼Œa hypothetical accelerator with 100 TOP/s compute throughput would require a few GB/s PCIe and/or network bandwidth

5. Related Work

* matrix-vector engineã€FPGAã€TPU

* ML benchmark

#### é‡åŒ–å’Œå¹¶è¡Œè®­ç»ƒçš„å…³ç³»

> FP8-LM paper

![image-20250507193147805](./MLSys/image-20250507193147805.png)

* ZeROï¼Œæ”¹å˜äº†distributionæ–¹å¼

![image-20250507193223244](./MLSys/image-20250507193223244.png)



#### Q-Lora

![image-20250330233937812](./MLSys/image-20250330233937812.png)

#### 4bitä»¥ä¸‹

##### BitNet b1.58

* ç»“è®ºï¼š
  * BitNet b1.58 can match full precision (i.e., FP16) baselines in terms of both perplexity and end-task performance, **starting from a 3B size**, when using the same configuration
* æ–¹æ¡ˆï¼š
  * 1.58-bit weights and INT8 activations.
  * activationé‡‡ç”¨å¯¹ç§°é‡åŒ–

![image-20250331150031977](./MLSys/image-20250331150031977.png)



### ç®—åŠ›ä¼˜åŒ–

* è¾¹ç¼˜è®¡ç®—

  * [EdgeRecï¼šæ­ç§˜è¾¹ç¼˜è®¡ç®—åœ¨æ·˜å®æ¨èç³»ç»Ÿçš„é‡è¦å®è·µ](https://developer.aliyun.com/article/742144)
* å¼¹æ€§è¿‘çº¿è®¡ç®—

  * [ç™¾åº¦ä¿¡æ¯æµå’Œæœç´¢ä¸šåŠ¡ä¸­çš„å¼¹æ€§è¿‘çº¿è®¡ç®—æ¢ç´¢ä¸åº”ç”¨](https://mp.weixin.qq.com/s/53KLAPphK9t4G3q-78S9mg)
  * å¼¹æ€§è¿‘çº¿è®¡ç®—ç³»ç»Ÿä¸»è¦åŒ…æ‹¬å‡ ä¸ªå­ç³»ç»Ÿï¼š

    * è§¦å‘æ§åˆ¶ç³»ç»Ÿï¼šä¸»è¦è´Ÿè´£æ ¹æ®ä¸šåŠ¡å‚æ•°ï¼Œæ§åˆ¶è¿‘çº¿è®¡ç®—çš„è§¦å‘ï¼Œè¾¾åˆ°å‰Šå³°å¡«è°·çš„ç›®çš„ã€‚

    * åŠ¨æ€ç®—åŠ›å’ŒåŠ¨æ€è°ƒå‚ç³»ç»Ÿï¼šå®ƒç›¸å½“äºå¼¹æ€§è¿‘çº¿è®¡ç®—ç³»ç»Ÿçš„å¤§è„‘ï¼Œæ ¹æ®é›†ç¾¤çš„èµ„æºæƒ…å†µï¼Œåˆ†é…è¿‘çº¿è®¡ç®—ç®—åŠ›ï¼›å†æ ¹æ®ç®—åŠ›æƒ…å†µï¼Œè®¡ç®—æ§åˆ¶å‚æ•°ï¼Œä»è€Œæ§åˆ¶è·Ÿç®—åŠ›åŒ¹é…çš„è´Ÿè½½ã€‚

    * å†å²æ•°æ®ä¸­å¿ƒï¼šä¿å­˜è¿‘çº¿è®¡ç®—å†å²çš„è®¡ç®—è®°å½•ã€‚å¯ä»¥æ ¹æ®èµ„æºçš„æƒ…å†µï¼Œå¤ç”¨å†å²è®¡ç®—ç»“æœï¼Œæ¥è°ƒèŠ‚å¯¹ç®—åŠ›çš„ä½¿ç”¨ã€‚

    * ä¸šåŠ¡è¿‘çº¿è®¡ç®— & æ§åˆ¶ç³»ç»Ÿï¼šè¿™ä¸ªä¸»è¦æ˜¯å’Œä¸šåŠ¡æ¥å…¥è¿‘çº¿è®¡ç®—ç›¸å…³çš„ä¸€äº›æ¶æ„æœºåˆ¶è®¾è®¡ï¼Œæ¯”å¦‚è¯´è¾“å…¥è¾“å‡ºç¼“å­˜çš„è¯»å†™ï¼Œè®¡ç®—çš„æ‹†åŒ… / å¹¶åŒ…ç­‰ç­‰ï¼Œä¸šåŠ¡è®¡ç®—ä¸å¤±è´¥ä¿¡å·çš„åé¦ˆç­‰ç­‰ã€‚
      * å®è·µä¸‹æ¥ï¼Œé€šè¿‡é”™å³°è°ƒåº¦ï¼Œé¢„ä¼°èµ„æºéœ€æ±‚å¹¶æå‰åˆ†é…è®¡ç®—èµ„æºæ˜¯æ¯”è¾ƒæœ‰æ•ˆçš„æå‡ç®—åŠ›çš„åŠæ³•ï¼Œå¯ä»¥ç†è§£æ˜¯å¦‚æœåœ¨èµ„æºå·²ç»ç´§å¼ çš„æ—¶å€™ï¼Œå†è¿›è¡Œè¿‘çº¿è®¡ç®—æ¨¡å—çš„è°ƒåº¦ï¼Œæ–°çš„è¿‘çº¿è®¡ç®—æ¨¡å—çš„ç®—åŠ›æ¶ˆè€—æœ‰æ¦‚ç‡æœ¬èº«å°±é€ æˆå±€éƒ¨çƒ­ç‚¹ï¼Œå¯¼è‡´æ‰©å±•èµ„æºè¢«å›æ”¶ï¼Œé€ æˆè°ƒåº¦å¤±è´¥

    * ä¸šåŠ¡åœ¨çº¿æ¥å…¥ï¼šéƒ¨åˆ†ä¸»è¦æ˜¯ä¸šåŠ¡æ¥å…¥è¿‘çº¿è®¡ç®—ç³»ç»Ÿä¸Šçš„ä¸€äº›è®¾è®¡ã€‚è¿™å—ä¸»è¦è€ƒè™‘çš„æ˜¯å¦‚ä½•é«˜æ•ˆçš„æ¥å…¥ä¸šåŠ¡ï¼Œé¿å…ä¸šåŠ¡æ¥å…¥è¿‡é«˜çš„æ¶æ„äººåŠ›æˆæœ¬ã€‚
  * åº”ç”¨åœºæ™¯ï¼š

    * Feed åœ¨çº¿ & è¿‘çº¿æ··åˆè®¡ç®—æ¶æ„ï¼šå°†å¤šä¸ªæ¨èç®—æ³•æœåŠ¡çš„å¬å›ç»“æœè¿›è¡Œç¼“å­˜ï¼Œå†æ”¾åˆ°è¿‘çº¿ç³»ç»Ÿæ¥è¿›è¡Œç»¼åˆæ‰“åˆ†ä¹‹åç”Ÿæˆä¸€ä¸ªç”¨æˆ·å¯¹åº”çš„è¿‘çº¿è®¡ç®—å€™é€‰é›†ï¼Œä½œä¸ºä¸€è·¯æ–°çš„å¬å›ã€‚ä¹‹ååœ¨çº¿è¯·æ±‚æ¥äº†ä¹‹åï¼Œä½¿ç”¨è¿™è·¯æ–°çš„å¬å›ç»“æœï¼Œè¿›è¡Œè½»é‡çš„åœ¨çº¿è®¡ç®—ä¹‹åå°±å¯ä»¥è¿”å›ç»™ç”¨æˆ·ï¼Œè¿™æ ·å°±æŠŠå¬å›å±‚åˆæ­¥æ’åºæ‰“åˆ†çš„è®¡ç®—è§„æ¨¡æå‡äº† 1 ä¸ªæ•°é‡çº§ã€‚

### Caching

* perceptual hashing for images to cache similar input images.

### å¹¶è¡Œè®­ç»ƒ

#### Literature Review

* DDP (PyTorch DDP Paper)

  * Jayarajan et al. [22] proposed to prioritize gradient synchronizations and **parameter updates based on the forward order** instead of the backward order
    * å¯èƒ½è®©ä¸‹ä¸€æ¬¡è®¡ç®—æå‰

  * ByteScheduler https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf
  * the recursive halving and doubling algorithm [30, 37] and the
    bucket algorithm (also known as the ring algorithm) [2]. ã€Accurate Large MiniBatchã€‘
    * While the halving/doubling algorithm con-
      sists of 2log2(p) communication steps, the ring algorithm
      consists of 2(p âˆ’ 1) steps. This generally makes the halv-
      ing/doubling algorithm faster in latency-limited scenarios
      (i.e., for small buffer sizes and/or large server counts).
    * To support non-power-of-two number of servers, we
      used the binary blocks algorithm [30].

* sparse & denseåˆ†ç¦»

  * Parallax [24] explored a hybrid structure that combines parameter-server [27] and collective communications.
    * Models are partitioned based on sparsity, where
      dense parameters are communicated using AllReduce and
      sparse tensors are placed to parameter servers. This design
      avoids densifying sparse tensors and communicating empty
      values, which is especially helpful for NLP models.
  
* æ¨¡å‹å¹¶è¡Œ
  * TPã€PPæ˜¯MPçš„ç‰¹æ®Šå½¢å¼
  * ZeRO [32] also combines data parallelism with model parallelism, but with minimum model replication to support fast training on su-
    per large models. The authors observed that main memory
    consumption contributors are input data, model parame-
    ters, gradients, optimizer states, and activations. Splitting
    input data is trivial. However, model parameters and ac-
    tivations are compulsory ingredients for backward passes.
    ZeRO addressed this problem by partitioning parameters,
    gradients, and optimizer states on each DDP instance.
    * Parameters are broadcast from the owner DDP instance to all
      others when necessary. Activations are recomputed during
      the backward pass. Compared to PyTorch DDP, ZeRO can
      scale to much larger models as each process only needs to
      maintain a small partition of the model. The high scalabil-
      ity is achieved by sacriï¬cing the training speed, as the ad-
      ditional re-computation, broadcast, and gather overhead
    * ZeRO obtains the same or better memory efficiency than PP without incurring functionality, performance and convergence related restrictions of PP. ã€ZeROè®ºæ–‡ã€‘
  

#### é€šä¿¡æˆæœ¬å¯¹æ¯”

| **é˜¶æ®µ**                     | **é€šä¿¡é‡**ï¼ˆå•å¡ communication volumeï¼‰ | **ä¸ DP å¯¹æ¯”** | **ä¸ MP å¯¹æ¯”**                                 |
| ---------------------------- | --------------------------------------- | -------------- | ---------------------------------------------- |
| åŸºçº¿ DP                      | 2Î¨ï¼ˆAll-Reduceï¼‰                        | 1x             | -                                              |
| ZeRO-DP (P<sub>os+g</sub>)   | 2Î¨ï¼ˆReduce-Scatter+All-Gatherï¼‰         | 1x             | è¿œä½äº MP è·¨èŠ‚ç‚¹é€šä¿¡ï¼ˆå¦‚ 12.5GB/s vs 300GB/sï¼‰ |
| ZeRO-DP (P<sub>os+g+p</sub>) | 3Î¨ï¼ˆå¹¿æ’­ + All-Gatherï¼‰                 | 1.5x           | ä»ä¼˜äº MPï¼ˆå¦‚ 3Î¨ vs 12Î¨/ å±‚ï¼‰                  |

- æ¿€æ´»åˆ†ç‰‡é€šä¿¡
  - **MP åŸºçº¿**ï¼šæ¯ Transformer å±‚ 12Ã—seqÃ—hidden é€šä¿¡ï¼ˆAll-Reduceï¼‰ã€‚
  - **ZeRO-R**ï¼šæ¯æ¿€æ´»æ£€æŸ¥ç‚¹ 1Ã—seqÃ—hidden é€šä¿¡ï¼ˆAll-Gatherï¼‰ï¼Œä»…ä¸º MP çš„ 1/12ã€‚

#### DP

> PyTorch DP
>
> https://zhuanlan.zhihu.com/p/343951042

![image-20250308203837702](./MLSys/image-20250308203837702.png)

- åŸç†ï¼š
  - å’ŒPSéå¸¸æ¥è¿‘
- è¿‡ç¨‹
  - è¿‡ç¨‹ä¸€ï¼ˆå›¾ä¸­çº¢è‰²éƒ¨åˆ†ï¼‰ï¼šå„å¡åˆ†åˆ«è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
  - è¿‡ç¨‹äºŒï¼ˆå›¾ä¸­è“è‰²éƒ¨åˆ†ï¼‰ï¼šæ‰€æœ‰æ¢¯åº¦æ•´åˆåˆ° device[0]
  - è¿‡ç¨‹ä¸‰ï¼ˆå›¾ä¸­ç»¿è‰²éƒ¨åˆ†ï¼‰ï¼šdevice[0] è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå…¶ä»–å¡æ‹‰å– device[0] çš„å‚æ•°è¿›è¡Œæ›´æ–°

![image-20250308204304450](./MLSys/image-20250308204304450.png)

![image-20250309012139694](./MLSys/image-20250309012139694.png)

#### Distributed Training and Communication Protocols é€šä¿¡åŸè¯­

> MLSys CSE 599W Lecture 11

##### Intro

* äº’ä¸ºåå‘ï¼š
  * Broadcast <-> Reduce Sumäº’ä¸ºåå‘
  * scatter <-> gather
  * All gather <-> all reduce sum 
    * e.g. DDP
* äº’ç›¸è½¬æ¢ï¼š
  * All reduce = reduce + broadcast
  * All gather = scatter + gather



![image-20250312023205177](./MLSys/image-20250312023205177.png)

![image-20250312023215715](./MLSys/image-20250312023215715.png)

![image-20250312025718940](./MLSys/image-20250312025718940.png)

##### AllReduce

* How to do Synchronization over Network
  * Distributed Gradient Aggregation, Local Update
* The AllReduce operation expects each participating process to provide an equally-sized tensor, collectively applies a given arithmetic operation (**e.g., sum, prod, min, max**) to
  input tensors from all processes, and **returns the same result tensor to each participant**
  * AllReduce = AllGather + LocalReduce
  * with TF: TFOptimizer çš„ ApplyGradient æ–¹æ³•æ›´æ–°æ¢¯åº¦ï¼Œæ˜“äºç›´æ¥ä½¿ç”¨TFåŸç”Ÿä¸layerwiseçš„Optimizer

![all-reduce](./MLSys/all-reduce.png)

```python
grad = gradient(net, w)
for epoch, data in enumerate(dataset):
  g = net.run(grad, in=data)
  gsum = comm.allreduce(g, op=sum)
  w -= lr * gsum / num_workers 
```

![network-topology](./MLSys/network-topology.png)

* How to implement AllReduce
  * Tree-Shape
    * Logically form a reduction tree between nodes
    * Aggregate to root then broadcast
    * https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/
  * Ring
    * Form a logical ring between nodes
    * Streaming aggregation
    * åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šScatter Reduceå’ŒAll Gather
      * Scatter Reduce
        * Each node have correctly reduced result of one segment!
      * All Reduceçš„é€šä¿¡æˆæœ¬ä¸ºï¼š$$T=2(N-1)\frac{K}{N}$$
      * https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/
      * https://zhuanlan.zhihu.com/p/72939003
    * ![image-20250309030219921](./MLSys/image-20250309030219921.png)

* AllReduce Libraries
  * MPI offers efficient CPU allreduce
  * dmlc/rabit: fault tolerant variant
  * facebookincubator/gloo
  * Parameter Hub: from UW
  * NCCL: Nvidiaâ€™ efficient multiGPU collective
* GPUDirect and RMDA
  * å‰è€…ä¸ç»è¿‡ç½‘å¡
* NCCL: Nvidiaâ€™s Efficient Multi-GPU Collective
  * Uses unified GPU direct memory accessing
  * Each GPU launch a working kernel, cooperate with each other to do ring based reduction
  * A single C++ kernel implements intra GPU synchronization and Reduction
* Schedule Allreduce Asynchronously
  * `B = comm.allreduce(A)`
  * `engine.push( lambda: B.data=allreduce(A.data), read=[A.var], mutate=[B.var, comm.var])`

* Discussion: Whatâ€™s Special about Communication Requirements for Model Parallel Training?
  * Track dependency correctly
  * Resolve resource contention and allocation
  * Some special requirement on channel
    * Allreduce: ordered call

```python
for i in range(num_layers):
  for t in range(num_time_stamp):
    out, state = layer[i].forward(data[i][t], state)
    data[i+1][t] = out.copyto(device[i])
```

#### DDP

##### Intro

* An Introduction to Distributed Deep Learning https://sebarnold.net/dist_blog/

##### PyTorch Distributed: Experiences on Accelerating Data Parallel Training

* Intro

  * bucketing gradients, overlapping compu-
    tation with communication, and skipping gradient synchro-
    nization

  * è¦ç‚¹ï¼šMathematical equivalenceã€Non-intrusive and interceptive APIã€High performance

* parameter averaging æŠ€æœ¯çš„é—®é¢˜ï¼š

  * ä¸ç­‰ä»·ï¼Œå°¤å…¶when the optimizer relies on past local gradients val-
    ues (e.g., momentum)
  * causing conï¬‚icting gradient descent directions
  * orchestrates computation (i.e., backward pass) and communication (i.e.,
    computing average) into **non-overlapping phases**,

* ![image-20250310160918593](./MLSys/image-20250310160918593.png)

* **bucketing gradients**

  * motivated by the observation that collective communications are more efficient on large tensors
  * ![image-20250310155726634](./MLSys/image-20250310155726634.png)
  * å®éªŒinsightï¼šbucket numæ¯”è¾ƒå¤§çš„æ—¶å€™ï¼Œä»16 gpu scalingåˆ°32gpuï¼Œé€šä¿¡é€Ÿåº¦è¡°å‡å°‘

* **overlapping computation with communication**

  * DDP registers one autograd hook for each gradient accumulator. The hook fires after its corresponding accumulator updating the gradients, and will inspect the bucket it pertains. If hooks of all gradients in the same buckets have fired, the last hook will trigger an asynchronous AllReduce on that bucket
  * ![image-20250310160223744](./MLSys/image-20250310160223744.png)
  * using the reverse order of model.parameters() as the bucketing order
  * è§£å†³æ‰§è¡Œsubgraphéƒ¨åˆ†æ¢¯åº¦ä¸å­˜åœ¨çš„é—®é¢˜ï¼š
    * DDP traverses the autograd graph from the output
      tensors of the forward pass to ï¬nd all participating param-
      eters.
    * proactively marking them ready at the end of the forward pass

* **skipping gradient synchronization**

  * é—®é¢˜ï¼šä¸Šé¢çš„algorithm would mark unused pa-
    rameters as ready at the end of every forward pass, while
    those unused parameters in one iteration still could partici-
    pate in subsequent iterations.
  * no_sync

* ç»“è®ºï¼š

  * near-linear scalability using 256 GPUs.
  * communication is the dominant
    training latency contributor, and its impact increases with
    model sizes; 
    * ![image-20250310220751232](./MLSys/image-20250310220751232.png)
  * bucket sizes considerably aï¬€ect communica-
    tion eï¬ƒciency, which could lead to more than 2X speedup if
    conï¬gured properly; 
  * skipping synchronizations appropri-
    ately would signiï¬cantly reduce amortized communication
    overhead without noticeably degrading convergence speed.
  *  round robin process groupsï¼Œå¡å¤š+æ¨¡å‹å¤æ‚ï¼Œ3-5 groupsæ¯”è¾ƒå¥½![image-20250311015454643](./MLSys/image-20250311015454643.png)

* è®¨è®ºï¼š
  * keep the DDP group within the same machineï¼Œå•æœºNå¡ï¼Œæ•ˆç‡æœ€é«˜
  * æå‡æ–¹å‘ï¼š
    * Gradient Order Prediction
    * Layer Dropping
    * Gradient Compression

#### å¤§ Batch è®­ç»ƒ

åˆ†å¸ƒå¼SGDåœ¨ç®—æ³•æ–¹é¢çš„æŒ‘æˆ˜

* throughput ~ GPU num
  * æ·±åº¦å­¦ä¹ çš„å¤§è§„æ¨¡è®­ç»ƒé€šå¸¸ä»¥çº¿æ€§å¢åŠ çš„ç†æƒ³æƒ…å†µä¸ºåŸºå‡†ï¼ŒHorovodå’ŒNCCLåº“åœ¨ä¿æŒé«˜ååé‡æ–¹é¢åšå¾—å¾ˆå¥½ï¼Œä½†æ˜¯ä»–ä»¬çš„æ€§èƒ½ä¸æ‰€ä½¿ç”¨çš„ç¡¬ä»¶æœ‰ç€åƒä¸ä¸‡ç¼•çš„è”ç³»ã€‚é«˜å¸¦å®½å’Œä½å»¶è¿Ÿçš„è¦æ±‚å¯¼è‡´äº†NVLinkäº’è¿çš„å¼€å‘ï¼Œå®ƒæ˜¯æœ¬è¯¾ç¨‹æ‰€ä½¿ç”¨çš„æœåŠ¡å™¨ç”¨æ¥äº’è¿ä¸€ä¸ªèŠ‚ç‚¹ä¸Šçš„å¤šä¸ªGPUçš„æ–¹æ³•ã€‚ NVIDIA DGX-2é€šè¿‡NVSwitchå°†è¿™ç§äº’è¿åˆæ¨è¿›ä¸€æ­¥ï¼Œè¯¥äº’è¿ç»“æ„å¯ä»¥300GB/sçš„å³°å€¼åŒå‘å¸¦å®½è¿æ¥å¤šè¾¾16ä¸ªGPUã€‚

* critical batch size ~ gradient noise scale (openai)
* å¯¹ç²¾åº¦çš„å½±å“ï¼šæœ´ç´ çš„æ–¹æ³•ï¼ˆæ¯”å¦‚ä¸åŠ data augmentationï¼‰ä¼šé™ä½ç²¾åº¦
  * ImageNet training in minutes. CoRR
  * [Train longer, generalize better: closing the generalization gap in large batch training of neural networks](https://arxiv.org/abs/1705.08741)
  * [On large-batch training for deep learning: Generalization gap and sharp minima](https://arxiv.org/abs/1609.04836)
  * [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)

* åº”å¯¹ç­–ç•¥

  * æé«˜å­¦ä¹ ç‡ï¼šOne weird trick for parallelizing convolutional neural networks
  * æ—©æœŸå­¦ä¹ ç‡çƒ­èº«ï¼š Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.
* Batch Normalization
  * BNé€šè¿‡æœ€å°åŒ–æ¯ä¸ªå±‚çš„è¾“å…¥åˆ†å¸ƒä¸­çš„æ¼‚ç§»æ¥æ”¹å–„å­¦ä¹ è¿‡ç¨‹
    * ç¼“è§£äº†æ·±å±‚ç½‘ç»œä¸­â€œæ¢¯åº¦å¼¥æ•£â€çš„é—®é¢˜ï¼ˆInternal Covariate Shiftï¼‰
  * æé«˜å­¦ä¹ é€Ÿåº¦å¹¶å‡å°‘ä½¿ç”¨ Dropout çš„éœ€æ±‚
  * æƒ³æ³•æ˜¯é’ˆå¯¹æ¯æ‰¹æ•°æ®å¯¹**æ‰€æœ‰å±‚**çš„è¾“å…¥ è¿›è¡Œè§„ä¸€åŒ–ï¼ˆè¿™æ¯”ç®€å•åœ°åªå¯¹è¾“å…¥æ•°æ®é›†è¿›è¡Œè§„ä¸€åŒ–æ›´ä¸ºå¤æ‚ï¼‰
    * ä¸ºäº†ä¿æŒæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¼•å…¥å¯å­¦ä¹ çš„å‚æ•°ï¼Œç¼©æ”¾å› å­å’Œå¹³ç§»å› å­
* Ghost BN
  * è®¡ç®—æ›´å°æ‰¹é‡çš„ç»Ÿè®¡æ•°æ®ï¼ˆâ€œghost æ‰¹é‡â€ï¼‰
    * å¼•å…¥å…¶ä»–å™ªå£°
  * æŒ‰ GPU é€ä¸ªå•ç‹¬æ‰§è¡Œæ‰¹é‡å½’ä¸€åŒ–ï¼Œè§£å†³åŒæ­¥ BN é€šä¿¡å¼€é”€é—®é¢˜
* å°†å™ªå£°æ·»åŠ è‡³æ¢¯åº¦
  * ç¡®ä¿æƒé‡æ›´æ–°çš„åæ–¹å·®éšç€æ‰¹é‡å¤§å°çš„å˜åŠ¨ä¿æŒä¸å˜ 
  * ä¸ä¼šæ”¹å˜æƒé‡æ›´æ–°çš„å¹³å‡å€¼ 
  * $$\hat{g}=\frac{1}{M}\sum^{N}_{n\in B}g_n z_n$$
* æ›´é•¿çš„é«˜å­¦ä¹ ç‡è®­ç»ƒæ—¶é—´
* å¢åŠ æ‰¹é‡å¤§å°ä»£æ›¿å­¦ä¹ ç‡è¡°å‡
* LARS â€“ æŒ‰å±‚è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
  *  [LARSè®ºæ–‡](https://arxiv.org/abs/1904.00962): å¤§LR -> LR warm-up -> LARSï¼Œåªæ˜¯èƒ½ä¿è¯å¤§batchè®­ç»ƒèƒ½è®­ï¼Œå…³äºæ•ˆæœé—®é¢˜ï¼Œä½œè€…è®¤ä¸ºâ€œincreasing the batch does not give much additional gradient information comparing to smaller batches.â€
  *  [LARC](https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py): å¸¦æ¢¯åº¦è£å‰ªçš„åˆ†å±‚è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œä»¥å…·æœ‰åŠ¨åŠ›çš„SGDä½œä¸ºåŸºç¡€ä¼˜åŒ–å™¨
  *  [LAMB](https://arxiv.org/abs/1904.00962): åˆ†å±‚è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œä»¥ Adam ä½œä¸ºåŸºç¡€ä¼˜åŒ–å™¨ï¼Œåœ¨BERTç­‰è¯­è¨€æ¨¡å‹ä¸Šæ¯”LARCæ›´æˆåŠŸ
  *  [NovoGrad](https://arxiv.org/abs/1905.11286): æŒ‰å±‚è®¡ç®—çš„ç§»åŠ¨å¹³å‡å€¼ï¼Œåœ¨å‡ ä¸ªä¸åŒçš„é¢†åŸŸä¹Ÿæœ‰ä¸é”™çš„è¡¨ç°

![training_result](./MLSys/training_result.png)

#### ZeRO-DPã€ZeRO-R

> * trivial: Reducescatteræ¢¯åº¦ + allgatheræ¢¯åº¦ + applyå…¨éƒ¨æ¢¯åº¦
>
> - Zero1: Reducescatteræ¢¯åº¦ + applyéƒ¨åˆ†æ¢¯åº¦ + AllGatherå‚æ•°
> - Zero2: åå‘ä¼ æ’­ä¸­ReduceScatteræ¢¯åº¦ + applyéƒ¨åˆ†æ¢¯åº¦ + AllGatherå‚æ•°
>   - zero2ç›¸æ¯”zero1ï¼Œæ˜¾å­˜å’Œé€Ÿåº¦éƒ½æœ‰ä¼˜åŠ¿ï¼šé€šä¿¡è®¡ç®—å¹¶è¡Œï¼Œå¹¶ä¸€å®šç¨‹åº¦å‡å°å­˜å‚¨gradæ‰€éœ€çš„æ˜¾å­˜
> - Zero3: å‰å‘/åå‘ä¸­AllGatherå‚æ•° + åå‘ä¼ æ’­ä¸­ReduceScatteræ¢¯åº¦ + applyéƒ¨åˆ†æ¢¯åº¦

* Intro
  * **ZeRO**é€šè¿‡**ä¸‰é˜¶æ®µå†…å­˜ä¼˜åŒ–**ï¼ˆä¼˜åŒ–å™¨çŠ¶æ€åˆ†åŒºã€æ¢¯åº¦åˆ†åŒºã€å‚æ•°åˆ†åŒºï¼‰ï¼Œæ˜¾è‘—æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„å†…å­˜æ•ˆç‡ï¼Œæ”¯æŒåœ¨ç°æœ‰ç¡¬ä»¶ä¸Šè®­ç»ƒè¶…å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ä¸‡äº¿å‚æ•°çº§åˆ«ï¼‰ã€‚å…¶**ZeRO-DP**æ¶ˆé™¤äº†æ•°æ®å¹¶è¡Œä¸­çš„å†—ä½™å†…å­˜ï¼Œç»“åˆ**ZeRO-R**ä¼˜åŒ–æ¿€æ´»å†…å­˜å’Œç¢ç‰‡ç®¡ç†ï¼Œå®ç°äº†**8 å€æ¨¡å‹å°ºå¯¸å¢é•¿**å’Œ**10 å€è®­ç»ƒé€Ÿåº¦æå‡**ï¼Œå¹¶æˆåŠŸè®­ç»ƒå‡ºä¸–ç•Œæœ€å¤§çš„ 17B å‚æ•°è¯­è¨€æ¨¡å‹ Turing-NLGï¼ŒåŒæ—¶ä¿æŒæ˜“ç”¨æ€§ã€‚

  * Zero3æŠŠå‚æ•°ä¹Ÿåˆ‡äº†ï¼Œä¸ºå•¥è¿˜è¯´ä»–æ˜¯ä¸€ä¸ªDPï¼ˆæ•°æ®å¹¶è¡Œï¼‰æ¡†æ¶ï¼Œè€Œä¸æ˜¯æ¨¡å‹å¹¶è¡Œçš„æ¡†æ¶ã€‚æ˜¯å› ä¸ºï¼ŒåŒºåˆ†æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œçš„æœ¬è´¨ï¼Œä¸æ˜¯æ¨¡å‹å‚æ•°æœ‰æ²¡æœ‰åˆ‡å¼€å­˜å‚¨ã€‚è€Œæ˜¯ï¼š
    - è¾“å…¥æ•°æ®æœ‰æ²¡æœ‰åˆ‡åˆ†å¼€å‘é€åˆ°ä¸åŒçš„è®¡ç®—èŠ‚ç‚¹ï¼Œ**å¦‚æœæ˜¯æ•°æ®å¹¶è¡Œï¼Œè¾“å…¥å°±è¦åˆ‡å¼€ï¼Œå¦‚æœæ˜¯æ¨¡å‹å¹¶è¡Œï¼Œè¾“å…¥å°±ä¸éœ€è¦åˆ‡å¼€ã€‚**


* **ç ”ç©¶èƒŒæ™¯ä¸æŒ‘æˆ˜**

  - **æ¨¡å‹è§„æ¨¡å¢é•¿**ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸæ¨¡å‹ä» BERT-largeï¼ˆ0.3Bï¼‰å‘å±•åˆ° T5ï¼ˆ11Bï¼‰ï¼Œä½†ä¸‡äº¿å‚æ•°æ¨¡å‹è®­ç»ƒé¢ä¸´**å†…å­˜ç“¶é¢ˆ**ã€‚

  - ç°æœ‰æ–¹æ³•å±€é™æ€§
    - **æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰**ï¼šå†…å­˜å†—ä½™ï¼Œæ— æ³•æ‰©å±•ã€‚
    - **æ¨¡å‹å¹¶è¡Œï¼ˆMPï¼‰**ï¼šé€šä¿¡å¼€é”€å¤§ï¼Œè·¨èŠ‚ç‚¹æ•ˆç‡ä½ï¼ˆå¦‚ 40B æ¨¡å‹è·¨èŠ‚ç‚¹æ•ˆç‡ä»… 5% ç¡¬ä»¶å³°å€¼ï¼‰ã€‚

* ![image-20250328142209236](./MLSys/image-20250328142209236.png)
  * K=12ï¼ŒåŸå› æ˜¯mixed precision trainingè¦ä¿ç•™fp32 copy

* Zero-DP

  * osã€gåˆ†åŒº
  * å‚æ•°påˆ†åŒºï¼š
    * å‰å‘ä¼ æ’­æ—¶ï¼ŒæŒ‰å±‚é¡ºåºå¹¿æ’­å‚æ•°åˆ†ç‰‡ã€‚
    * åå‘ä¼ æ’­åï¼ŒAll-Gather å‚æ•°åˆ†ç‰‡ã€‚

* **ZeRO-Rï¼ˆæ¿€æ´»ä¸ç¢ç‰‡ä¼˜åŒ–ï¼‰**

  - **æ¿€æ´»åˆ†åŒºï¼ˆPaï¼‰**ï¼šç»“åˆæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œé€šè¿‡ All-Gather æŒ‰éœ€é‡æ„ï¼Œå†…å­˜èŠ‚çœä¸ MP åº¦æ•°æˆæ­£æ¯”ã€‚
    - **æ¡ˆä¾‹**ï¼š100B æ¨¡å‹ + 16-way MPï¼Œæ¿€æ´»å†…å­˜ä» 33GB/GPU é™è‡³ 2GB/GPUã€‚

  - **CPU å¸è½½ï¼ˆPa+cpuï¼‰**ï¼šæç«¯æƒ…å†µä¸‹å°†æ¿€æ´»è½¬ç§»è‡³ CPUï¼Œå†…å­˜æ¥è¿‘é›¶ã€‚
    - é€‚ç”¨äºå¼€å¯Paåä»ç„¶æ˜¯é€šä¿¡ç“¶é¢ˆçš„åœºæ™¯

  - **å†…å­˜ç¢ç‰‡ç®¡ç†ï¼ˆMDï¼‰**ï¼šé¢„åˆ†é…è¿ç»­å†…å­˜å—ï¼Œå‡å°‘åˆ†é…å¤±è´¥ã€‚
    - interleaving of short term and long term memory causes memory fragmentation

* **å®éªŒæ•°æ®ä¸é…ç½®**

  - ç¡¬ä»¶é…ç½®
    - 400 NVIDIA V100ï¼ˆ32GBï¼‰ï¼Œ25 DGX-2 èŠ‚ç‚¹ï¼Œ800 Gbps Infinibandã€‚

  - æ¨¡å‹é…ç½®ï¼ˆç¤ºä¾‹ï¼‰

    | **æ¨¡å‹å¤§å°** | **å±‚æ•°** | **éšè—ç»´åº¦** | **æ³¨æ„åŠ›å¤´** | **æ‰¹å¤§å°** | **æ€»æ‰¹å¤§å°** |
    | ------------ | -------- | ------------ | ------------ | ---------- | ------------ |
    | 170B         | 212      | 8192         | 64           | 12         | 300          |
    | 60B          | 75       | 8192         | 32           | 64         | 1600         |

  - æ€§èƒ½ç»“æœ
    - **ååé‡**ï¼š15 PetaFlopsï¼ˆ30% ç¡¬ä»¶å³°å€¼ï¼‰ã€‚
    - **æ‰©å±•æ€§**ï¼š400 GPU vs 64 GPUï¼Œé€Ÿåº¦æå‡ 2.3 å€ï¼ˆè¶…çº¿æ€§ï¼‰

- å½“å‰é™åˆ¶
  - **å‚æ•°åˆ†åŒºé˜¶æ®µ**ï¼šé€šä¿¡é‡å¢åŠ  50%ï¼Œå¯èƒ½æˆä¸ºè¶…å¤§è§„æ¨¡é›†ç¾¤ç“¶é¢ˆã€‚
  - **CPU å¸è½½å¼€é”€**ï¼šæ¿€æ´»ä¼ è¾“å»¶è¿Ÿå¯èƒ½æŠµæ¶ˆæ‰¹å¤§å°å¢åŠ çš„æ”¶ç›Šã€‚
- æœªæ¥è®¡åˆ’
  - **ä¸‡äº¿å‚æ•°æ”¯æŒ**ï¼šå®ç° ZeRO-DP å…¨ä¸‰é˜¶æ®µï¼ˆP<sub>os+g+p</sub>ï¼‰ï¼Œç»“åˆ MPï¼ˆå¦‚ 16-wayï¼‰å’Œ DPï¼ˆ64-wayï¼‰ã€‚
  - **åŠ¨æ€ä¼˜åŒ–ç­–ç•¥**ï¼šæ ¹æ®ç¡¬ä»¶æ¡ä»¶è‡ªåŠ¨é€‰æ‹© Pa/cpu æ¨¡å¼ã€‚
  - **å¼‚æ„æ”¯æŒ**ï¼šæ‰©å±•è‡³ CPU/TPU é›†ç¾¤ã€‚

#### FSDP

> https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html

#### DP + TP + PPï¼Œå¹¶è¡Œè®­ç»ƒç»„åˆ

* DP + TP + PP
  * æŠŠæœºå™¨åˆ†æˆNç»„ï¼Œç»„ä¹‹é—´ç”¨DP
  * ä¸€ç»„æœºå™¨æœ‰Må°æœºå™¨ï¼Œä¸åŒå°ä¹‹é—´ç”¨PP
  * ä¸€å°æœºå™¨æœ‰8å¼ å¡ï¼Œä¸åŒå¡ä¹‹é—´ç”¨TP
* https://huggingface.co/docs/transformers/v4.15.0/en/parallelism

#### PP

* **PP splits a model horizontally across layers** running each partition on a different device and **use micro-batching to hide the pipeline bubble** [10, 11]. Model functionalities such as tied-weights and batch-normalization are difficult to implement due to horizontal splitting and micro-batching, respectively. 
* G-pipe
  * æ€è·¯ï¼šå¤§batchæ‹†æˆè‹¥å¹²ä¸ªå°batchï¼Œè¿™æ ·æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥pipelineæ‰§è¡Œå¤šä¸ªå°batch
  * partitions both model parameters and total activations but **requires a batch size proportional to number of pipeline partitions to hide the pipeline bubble**. **The large batch size can affect the convergence rate, while also requiring significant memory to store activations.** 
* PipeDream
  * æ€è·¯ï¼šæ¯ä¸ªèŠ‚ç‚¹äº¤æ›¿è¿›è¡Œforwardã€backwardï¼Œå°½æ—©å¯åŠ¨backwardçš„æµæ°´çº¿
  * keeps multiple copies of stale parameters to hide the pipeline bubble without increasing the batch size significantly, making it less memory efficient. 
  * the implementation is not equivalent to the standard DL training and has implications on training convergence.
  * ![image-20250415030814219](./MLSys/image-20250415030814219.png)

* DualPipe
  * compared with ZB1P (Qi et al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles
    while only increasing the peak activation memory by 1
    ğ‘ƒğ‘ƒ times. Although DualPipe requires
    keeping two copies of the model parameters, this does not significantly increase the memory
    consumption since we use a large EP size during training. Compared with Chimera (Li and
    Hoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by
    2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe,
    neither the bubbles nor activation memory will increase as the number of micro-batches grows.
  * ![image-20250501025515656](./MLSys/image-20250501025515656.png)
  * ![image-20250501025502918](./MLSys/image-20250501025502918.png)

#### SP (sequence parallelism)

[2021] Colossal AI æå‡ºäº†Sequence Parallelsimï¼Œè®ºæ–‡ https://arxiv.org/pdf/2105.13120

[2022] Megatron-LM åœ¨åºåˆ—ç»´åº¦æ‹†åˆ† Dropout å’Œ LayerNormï¼Œè®ºæ–‡ https://arxiv.org/abs/2205.05198

[2023] DeepSpeed Ulyssesï¼Œè®ºæ–‡ https://arxiv.org/abs/2309.14509

[2023] UCB Ring Attentionï¼Œè®ºæ–‡ https://arxiv.org/abs/2310.01889

[2024] Megatron-LM æå‡ºäº† [Context Parallelism](https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html)ã€‚

1/3/4/5æ˜¯ä»è¾“å…¥å±‚å¼€å§‹åœ¨æ‹†åˆ†åºåˆ—ï¼Œ2 åˆ™æ˜¯åœ¨ä¸­é—´å±‚



#### TP

* åŠ¨æœºï¼šPPä¸‹ï¼Œå•å¡æ— æ³•å®¹çº³æ¨¡å‹ä¸­æ˜¾å­˜å ç”¨æœ€å¤§çš„å±‚
* æœ¬è´¨ï¼šæ‹†åˆ†W
  * æŒ‰åˆ—æ‹†åˆ†Wï¼š
    * æœ€åall gather
  * æŒ‰è¡Œæ‹†åˆ†Wï¼š
    * Xåªéœ€è¦éƒ¨åˆ†
    * æœ€åall reduce
* å±€é™æ€§ï¼šä¼ è¾“å¤§é‡æ•°æ®ï¼Œé€šå¸¸åªåœ¨å•æœºå¤šå¡å†…çš„nvlinkä½¿ç”¨

![20250427-033638](./MLSys/20250427-033638.jpeg)





### Parameter Server

#### Intro

* PS Interface for Data Parallel Training
  * Synchronous: bulk synchronous parallel (BSP)
  * Asynchronous
    * gradient staleness
    * å¯è®¾ç½®â€œæœ€å¤§å»¶è¿Ÿâ€ï¼ŒNè½®è¿­ä»£å†…ï¼Œæ¨¡å‹å‚æ•°å¿…é¡»æ›´æ–°ä¸€æ¬¡
  * Integrate Schedule with Networking using Events
    * Use the callback to notify engine that data receive is finished

```python
grad = gradient(net, w)
for epoch, data in enumerate(dataset):
  g = net.run(grad, in=data)
  ps.push(weight_index, g)
  w = ps.pull(weight_index)
```

* The Cost of PS Model: All to All Pattern
  * Each worker talks to all servers
  * Shard the parameters over different servers

#### Scaling distributed machine learning with the parameter server, OSDI 2014

PSæ¶æ„çš„ä¼˜åŠ¿ä¸»è¦è¿˜æ˜¯é«˜å¯ç”¨(system efficiency)

* Intro
  * distributed subgradient descent


* 3.6 User-defined Filters

  * signifi-cantly modified filter

  * KKT(è§5.1)ï¼šç‰¹å¾é‡è¦æ€§ç­›é€‰


* 4.2 Messages

  * key-caching and value-compression can be used jointly.

  * key-cacheè®©senderåªéœ€è¦ä¼ key listsçš„hash

  * ç”¨snappyå‹ç¼© zero value


* 4.3 Consistent Hashing
  * ä¸€è‡´æ€§hashå’Œ key-range çš„æ¦‚å¿µç´§å¯†ç›¸è¿
  * hashç©ºé—´ç­‰åˆ† nm ä¸ªèŒƒå›´
  * è®ºæ–‡ Chord: A scalable peer-to-peer lookup protocol for Internet applications

* 4.5 Server Management

  * è®¡ç®—èŠ‚ç‚¹åˆ†ä¸ºserver nodeå’Œworker node

  * serverå…±åŒç»´æŒå…¨å±€å…±äº«çš„æ¨¡å‹å‚æ•°

  * workersä¿ç•™ä¸€éƒ¨åˆ†çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”æ‰§è¡Œè®¡ç®—

  * workeråªå’Œserveræœ‰é€šä¿¡ï¼Œäº’ç›¸ä¹‹é—´æ²¡æœ‰é€šä¿¡


* examples
  * CountMin Sketch Algo æœ‰ç‚¹åƒ bloom filter


* PSè¿ç»´ï¼š

  * expectation - current_situation = operations

  * æœåŠ¡å‘ç°ã€æ•°æ®å‘ç°


* æ€§èƒ½ä¼˜åŒ–ï¼š

  * åŒbuffer + RCUï¼Œè¯»ä¸è¢«é”é˜»ç¢

  * ç®€åŒ–ç‰ˆè¯»å†™é”ï¼Œä¼˜åŒ–ç³»ç»Ÿæ€å¼€é”€

#### HugeCTR SoK

>  https://medium.com/@kuwarkapur/guide-to-embeddings-in-recommender-systems-with-hugectr-fc4f413bd624

* åŸºäº Model Parallel
* slotçš„æ¦‚å¿µ
  * To reduce the overhead when looking up multiple embedding tables with identical embedding vector sizes, the embedding combines them as one huge embedding table. Each sub-embedding table is called a slot, which is also known as a feature field. 

* Sparse Emb Layer
  * ç±»ä¼¼ **tf.nn.embedding_lookup_sparse** ï¼Œå¢åŠ äº†MPçš„è®¾è®¡
  * The distributed sparse embedding scatters keys across GPUs by computing `gpu_id = key % number_of_gpus`

### æ˜¾å­˜ä¼˜åŒ–

|              | å•å¡              | å¤šå¡      |
| ------------ | ----------------- | --------- |
| é™ä½é™æ€æ˜¾å­˜ | offload           | ZeRO/FSDP |
| é™ä½åŠ¨æ€æ˜¾å­˜ | act recomputation | æ¨¡å‹å¹¶è¡Œ  |

* ZeRO
* ä¼˜åŒ–activation recomputationï¼šhttps://arxiv.org/pdf/2205.05198
  * é—´éš”ç€å­˜ï¼Œæ¯”å¦‚å­˜2ã€5ã€8å±‚ï¼Œé€‰æ‹©åˆé€‚çš„å±‚ï¼ˆæ¿€æ´»å€¼å¤§ã€è®¡ç®—ç®€å•ï¼‰åšé‡è®¡ç®—
  * deepseek-v3ï¼šRecomputation of RMSNorm and MLA Up-Projection.
* cpu offload
  * deepseek-v3ï¼šExponential Moving Average in CPU.
* æ¨¡å‹å¹¶è¡Œ
  * PP æ—¶çš„æ˜¾å­˜ä¼˜åŒ–ï¼š
    * deepseek-v3ï¼š**Shared Embedding and Output Head for Multi-Token Prediction**. With the DualPipe strategy, we deploy the shallowest layers (including the embedding layer) and deepest layers (including the output head) of the model on the same PP rank. This arrangement enables the physical sharing of parameters and gradients, of the shared embedding and output head, between the MTP module and the main model. This physical sharing mechanism further enhances our memory efficiency.

### é€šä¿¡ä¼˜åŒ–

* allreduceï¼Œå‚è€ƒã€Œå¹¶è¡Œè®­ç»ƒã€
* grad acc
* leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. ã€deepseek-v3ã€‘
* customize **efficient cross-node all-to-all communication kernels (including dispatching and combining)** to conserve the number of SMs dedicated to communication.ã€deepseek-v3ã€‘
  * In detail, we employ the **warp specialization technique** (Bauer et al., 2014) and partition
    20 SMs into 10 communication channels. 
  * During the dispatching process, (1) IB sending, (2)
    IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The
    number of warps allocated to each communication task is dynamically adjusted according to the
    actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending,
    (2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also
    handled by dynamically adjusted warps.
  * In addition, both dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and the interference to other SMs.



### è®­ç»ƒæ¡†æ¶ Intro

* æµç¨‹ï¼š
  * æ•°æ®åŠ è½½
    * prefetch
  * æ•°æ®é¢„å¤„ç†
    * ç¦»çº¿
  * forward
  * backward
  * ï¼ˆèŠ‚ç‚¹é€šä¿¡ï¼‰
  * optimize

* spark MLlib
  * å‚è€ƒã€Œæ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿã€6.2.1
  * åˆ†å¸ƒå¼ + DAG
  * å†…éƒ¨å¹¶è¡Œã€è¾¹ç•Œæ¶ˆè€—èµ„æºshuffle/reduce
  * ç¼ºç‚¹ï¼š
    * å‚æ•°å…¨å±€å¹¿æ’­
    * åŒæ­¥é˜»æ–­å¼çš„æ¢¯åº¦ä¸‹é™
    * ä¸æ”¯æŒå¤æ‚DLç½‘ç»œ

* tf
* pytorch



### PyTorch: An Imperative Style, High-Performance Deep Learning Library

> å‚è€ƒã€Œpytorch.mdã€ã€ã€Œsnippetsã€

* PyTorch builds on these trends by providing an **array-based programming model accelerated by GPUs**
  **and differentiable via automatic differentiation integrated in the Python ecosystem.**
* PyTorch foregoes the potential beneï¬ts of a graph-metaprogramming based approach to preserve the imperative programming model of Python

```Python
class LinearLayer(Module):
  def __init__(self, in_sz, out_sz):
    super().__init__()
    t1 = torch.randn(in_sz, out_sz)
    self.w = nn.Parameter(t1)
    t2 = torch.randn(out_sz)
    self.b = nn.Parameter(t2)
  def forward(self, activations):
    t = torch.mm(activations, self.w)
    return t + self.b
  
class FullBasicModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv = nn.Conv2d(1, 128, 3)
    self.fc = LinearLayer(128, 10)
  def forward(self, x):
    t1 = self.conv(x)
    t2 = nn.functional.relu(t1)
    t3 = self.fc(t1)
    return nn.functional.softmax(t3)
```

* autograd
  * PyTorch uses the operator overloading approach, which builds up a representation of the computed function every
    time it is executed.
  * In its current implementation [30], PyTorch performs **reverse-mode automatic differentiation**, which computes the gradient of a scalar output with respect to a multivariate input.
    Differentiating functions with more outputs than inputs is more efï¬ciently executed using forward-mode automatic differentiation, but this use case is less common for machine learning applications.
    PyTorch can be easily extended to perform forward-mode differentiation using array-level dual
    numbers [31, 32].

* æ€§èƒ½ä¼˜åŒ–ï¼š

  * An efï¬cient C++ core
    * Python bindings are generated using YAML meta-data ï¬les.
    * æ¯”å¦‚å¯ä»¥ç”¨torchscriptå•ç‹¬è·‘ https://pytorch.org/docs/stable/jit.html
  * Separate control and data ï¬‚ow
    * PyTorch is designed to execute operators asynchronously on GPU by leveraging the CUDA stream mechanism [38] to queue CUDA kernel invocations to the GPUs hardware FIFO
  * Custom caching tensor allocator
    * cache cuda memory
      * rounds up allocations to multiples of 512 bytes to avoid fragmentation issues.
    * One-pool-per-stream Designï¼š
      * Moreover, it maintains a distinct pool of memory for every CUDA stream (work queue).
      * åªè¦æ–°çš„å†…å­˜åˆ†é…æ“ä½œä¸ä¹‹å‰é‡Šæ”¾çš„å†…å­˜åŒºåŸŸä½¿ç”¨åœ¨åŒä¸€ä¸ªæµä¸­ï¼Œå†…å­˜åˆ†é…å™¨å°±å¯ä»¥ç«‹å³é‡æ–°åˆ†é…è¿™å—å·²ç»åœ¨ CPU ç«¯é‡Šæ”¾çš„å†…å­˜
        * åˆ©ç”¨CPUé‡Šæ”¾æ›´å¿«çš„ç‰¹ç‚¹ã€æµçš„åºåˆ—åŒ–æ‰§è¡Œç‰¹æ€§
      * limitï¼šthe allocations end up fragmented per stream
        * å¾ˆå°‘ç”¨å¤šæµï¼ŒData loading and distributed computing utilities are exceptionsï¼Œç²¾å¿ƒå®ç°
  * multiprocessingï¼š
    * PyTorch extends the Python multiprocessing module into torch.multiprocessing, which is a drop-in replacement for the built in package and automatically moves the data of tensors sent to other processes to shared memory instead of sending it over the communication channel.
    * Another unique feature of this system is that it transparently handles sharing of CUDA tensors, making it easy to implement techniques like Hogwild [42].
    
  * ref count
    * PyTorch tracks both references internal to the libtorch library and external references made by
    users in their Python code by integrating with Pythonâ€™s own reference counting mechanism

### Go+Torch

https://github.com/wangkuiyi/gotorch

* Q: TensorFlowä¸ºä»€ä¹ˆéœ€è¦å¼•å…¥å›¾è¿™ä¸ªæ¦‚å¿µï¼Ÿ

  * A1: backwardè‡ªåŠ¨æ±‚å¯¼ï¼Œéœ€è¦å®šä¹‰å‰å‘çš„æ•°æ®ç»“æ„
  * A2: pythonæ‰§è¡Œé€Ÿåº¦æ…¢ï¼Œå†³å®šæ‰§è¡Œæ•ˆç‡çš„æ˜¯å›¾çš„è§£é‡Šå™¨ã€‚å›¾æ˜¯pythonä»£ç çš„å¦ä¸€ç§è¡¨ç¤ºå½¢å¼ï¼Œå¼€å§‹åŒ…æ‹¬å‰å‘è®¡ç®—è¿‡ç¨‹ï¼Œé€šè¿‡è°ƒç”¨TensorFlow APIï¼ŒåŠ å…¥å…¶å®ƒopåŒ…æ‹¬åå‘è®¡ç®—è¿‡ç¨‹å’Œæ¨¡å‹æ›´æ–°è¿‡ç¨‹ã€‚æ„é€ å›¾æœ¬è´¨ä¸Šæ˜¯åœ¨ç¼–è¯‘ã€‚

  * [TFRT](https://github.com/tensorflow/runtime)


* è°ƒç”¨libtorchå†…éƒ¨çš„native functionç±»æ¯”tfçš„opï¼Œä½†native functionæ˜¯å‡½æ•°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªclassï¼Œæ¯ä¸€ä¸ªfunctionå¯ä»¥ç”¨HLOï¼ˆä¸€ç§å¤è€çš„é€‚ç”¨äºæ•°å€¼è®¡ç®—çš„è¯­è¨€ï¼‰å†™ä¸€éã€‚gotorchè°ƒlibtorchè°ƒpytorch XLAé‡Œçš„HLOç¨‹åºï¼Œç¿»è¯‘æˆç‰¹å®šè®¾å¤‡ä¼˜åŒ–çš„ä»£ç 

  * native functionæœ‰YAMLæè¿°ï¼Œå¯è‡ªåŠ¨ç”ŸæˆGo Wrapper

  * torchscriptsï¼šç”¨åˆ°çš„pythonè¯­æ³•çš„å­é›† => pythoné«˜å±‚apiå¯ç¿»è¯‘æˆtorchscriptså†ç¿»è¯‘


* å¦‚æœ Go+Torch åœ¨æœªæ¥ä¸€å¹´é‡Œå­•è‚²æˆç†Ÿï¼Œæœ‰æœ›ä¼˜åŒ–ä»¥ä¸‹æ ¸å¿ƒåº”ç”¨åœºæ™¯:
  * ç»Ÿä¸€è®­ç»ƒå’Œé¢„æµ‹ç³»ç»Ÿ(ç›®å‰è®­ç»ƒç”¨ Python å†™ï¼Œé¢„æµ‹ç”¨ C++)
  * ç»Ÿä¸€äº‘å’Œç«¯ç³»ç»Ÿ(ç›®å‰äº‘ä¸Šç”¨ TensorFlowï¼Œç«¯ä¸Šæ¯”å¦‚ xNN è°ƒç”¨ TensorFlow Lite)
  * ç»Ÿä¸€è®­ç»ƒå’Œé¢„æµ‹æ—¶çš„æ•°æ®å¤„ç†æµç¨‹(ç›®å‰éœ€è¦ç”¨ Pythonå’ŒC++åˆ†åˆ«åšä¸¤å¥—ï¼Œå¼€é”€å¤§ï¼Œè€Œä¸”å®¹æ˜“å‡ºé”™)
  * ç»Ÿä¸€æœç´¢ã€æ¨èã€å¹¿å‘Šã€é‡‘èæ ¸å¿ƒã€ç§»åŠ¨æ™ºèƒ½å’Œç«¯æ™ºèƒ½ã€æ— äººé©¾é©¶ç­‰å¤šä¸ªé¢†åŸŸçš„åŸºç¡€æ¶æ„
  * èƒ½æ”¯æŒæ–°çš„æœºå™¨å­¦ä¹ æ¨¡å¼â€”â€”online learningã€GANã€reinforcement learningã€imitation learningç­‰ã€‚

### OneFlow: å¤§è§„æ¨¡åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ æ¡†æ¶

æ•°æ®å¹¶è¡Œï¼šallreduce + PS

æ¨¡å‹å¹¶è¡Œï¼šå‚æ•°å¦‚ä½•åˆ’åˆ†ï¼Ÿå¤æ‚çš„é€šä¿¡æ¨¡å¼

![platforms](./MLSys/platforms.jpg)

æ¨ªå‘æ‹“å±•ï¼šç‰‡é—´é«˜é€Ÿäº’è”ï¼Œe.g. TPU

çºµå‘æ‹“å±•ï¼šå•ä¸ªèŠ¯ç‰‡ä»é€šç”¨åˆ°ä¸“ç”¨



é™æ€è°ƒåº¦ä¸æµå¼æ‰§è¡Œç³»ç»Ÿ![layers](./MLSys/layers.jpg)



OneFlowæ¶æ„

* actoråŠæµæ°´çº¿
  * å†…å­˜æ§½ï¼Œç”¨ç±»ä¼¼rustçš„ownershipè§£å†³å†…å­˜å†²çªé—®é¢˜ï¼ŒownershipéšçŠ¶æ€è½¬ç§»

![memory-pipeline](./MLSys/memory-pipeline.jpg)

* node placement: consistent view
  * SBP, åœ¨opå±‚é¢å®ç°æ•°æ®å’Œæ¨¡å‹å¹¶è¡Œ 
  ![SBP](./MLSys/SBP.jpg)

### æ¨¡å‹æ¨ç†

* èŒƒå¼ï¼š
  * é¢„è®­ç»ƒEmbedding+è½»é‡åŒ–çº¿ä¸Šæ¨¡å‹

### å›¾ä¼˜åŒ–

* é™æ€å›¾çš„ä¼˜åŠ¿ï¼š
  * ç§»é™¤æ— ç”¨opã€è·¨opä¼˜åŒ–ã€op fusion

#### XLA

* å¸¸é‡æŠ˜å ã€å…¬å…±è¡¨è¾¾å¼æ¶ˆé™¤ã€æ­»ä»£ç æ¶ˆé™¤ç­‰ç»å…¸ç¼–è¯‘ä¼˜åŒ–
* XLA è¿˜æ”¯æŒKernel Fusionï¼Œå‡å°‘ Kernel Launch æˆæœ¬å’Œæ˜¾å­˜ IO å¼€é”€

#### PyTorch å›¾ä¼˜åŒ–

* PyTorch JIT â€“ fusing pointwise operations into one kernel has been key
  * e.g. to get LSTMs close to CuDNN perf.
* 2nd gen PyTorch JIT fusers added contractions etc. 
  * NVFuser going beyond PyTorch in https://github.com/NVIDIA/Fuser and learning new things every week
  * NVFuser https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/
* Todayâ€™s inductor / Triton based optimizations are also partly with that, but supports more complex ops

##### Inductor

https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747

##### Lightning Thunder

https://www.nvidia.com/en-us/on-demand/session/gtc24-s62544/

https://lightning.ai/docs/thunder/latest/





### MLOps

* ç£ç›˜Uå½¢æ•…éšœç‡ ~ GPUæ•…éšœç‡å»ºæ¨¡



### AWS - SageMaker

#### Sagemaker Immersion Labs

https://sagemaker-immersionday.workshop.aws/

[Github Link](https://github.com/aws-samples/amazon-sagemaker-immersion-day)

* Lab 1. Feature Engineering
* Lab 2. Train, Tune and Deploy XGBoost
  * Hyperparameter tuner
* Lab 3. Bring your own model
  * Bring your own container
  * Bring your own script
* Lab 4. Autopilot, Debugger and Model Monitor
  * Autopilot: generates notebooks for you
  * debug hooks that listen to events
  * class activation maps with SageMaker Debugger
* Lab 5. Bias and Explainability
* Lab 6. SageMaker Pipelines

æ€»ä½“åˆ†æ

* python SDK, IDE å¼çš„å¼€å‘ä½“éªŒ
* instance per notebook
  * brings elastic, dedicated compute for each person, project, dataset, step in your ML lifecycle
* Train a model
  * Build-in algorithms
  * Script mode
  * Docker container
  * AWS ML marketplace
  * Notebook instance
* use_spot_instances=True

#### SageMaker Debugger

ã€ŠAMAZON SAGEMAKER DEBUGGER: A SYSTEM FOR REAL-TIME INSIGHTS INTO MACHINE LEARNING MODEL TRAININGã€‹

https://github.com/awslabs/sagemaker-debugger#run-debugger-locally

* ç—›ç‚¹ï¼šè®­ç»ƒè¿‡ç¨‹é•¿ã€ä¸é€æ˜ï¼ˆè®­ç»ƒè¿›ç¨‹ã€åº•å±‚èµ„æºæƒ…å†µï¼‰
  * e.g. é‡åˆ°è¿‡æ‹Ÿåˆï¼Œç»ˆæ­¢è®­ç»ƒä»»åŠ¡çš„æœºåˆ¶
* å…³é”®ç‰¹æ€§
  * æ•°æ®é‡‡é›†ï¼šé›¶ä»£ç ä¿®æ”¹ï¼›æŒä¹…åŒ–å­˜å‚¨
  * è‡ªåŠ¨æ•°æ®æ£€æµ‹ï¼šæ£€æµ‹è®­ç»ƒè¿‡ç¨‹ã€ç³»ç»Ÿç“¶é¢ˆï¼›æå‰ç»ˆæ­¢ï¼›è‡ªå®šä¹‰è§„åˆ™ï¼›ä¸ Cloudwatch äº‹ä»¶é›†æˆ
  * å®æ—¶ç›‘æ§ï¼šæŒ‡æ ‡è°ƒè¯•ï¼›é€šè¿‡è®­ç»ƒçš„ step æˆ–æ—¶é—´é—´éš”è¿›è¡Œèµ„æºåˆ©ç”¨ç‡åˆ†æ
    * ç®—æ³•å±‚é¢ï¼šç‰¹å¾é‡è¦æ€§ã€layer weight/gradient ä¿¡æ¯å±•ç°ã€å¸®åŠ©ç†è§£ serving/training ä¸€è‡´æ€§ (data drift)
  * èŠ‚çœæ—¶é—´å’Œæˆæœ¬ï¼šåŸå‹éªŒè¯ï¼›èµ„æº
  * é›†æˆåœ¨ Studio ç¯å¢ƒä¸­
* å®ç°
  * è®­ç»ƒå®¹å™¨ ---> å­˜å‚¨ ---> Debugger å®¹å™¨ ---> actions
    * Actions: [cloudwatch](https://aws.amazon.com/cn/cloudwatch/) + [lambda function](https://aws.amazon.com/cn/lambda/)
  * [smdebug](https://pypi.org/project/smdebug/#description)
  * Profiling
    * åŸç”Ÿæ¡†æ¶åˆ†æï¼šå¯èƒ½å¢åŠ  GPU å†…å­˜æ¶ˆè€—
    * æ•°æ®åŠ è½½åˆ†æï¼šè°ƒè¯•å™¨æ”¶é›† DataLoader äº‹ä»¶ä¿¡æ¯ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ€§èƒ½
    * pythonï¼šcProfile (python operator), Pyinstrument (éš”æ®µæ—¶é—´è®°å½•å †æ ˆæƒ…å†µ)
  * Debugger Hook: ç±»ä¼¼ Tensorflow çš„ hookï¼Œä¼ å…¥ callback å¯¹è±¡ï¼Œé‡‡é›†æŒ‡æ ‡åˆ°å­˜å‚¨
  * Rule é›†æˆåœ¨ Hook ä¸­: ç³»ç»Ÿå±‚ã€æ¨¡å‹å±‚ï¼ˆè¿‡æ‹Ÿåˆã€æ¢¯åº¦æ¶ˆå¤±ï¼‰

```python
# record tensors
import smdebug.tensorflow as smd
hook = smd.KerasHook("/opt/ml/tensors")
model.fit(x, y, epochs=10, callbacks=[hook])

custom_collection=CollectionConfig(
	name="relu_ouput",
	parameters={
		"include_regex": ".*relu_output",
		"save_interval": "500"
  }
)

# access tensors
from smdebug.trials import create_trial
trial = create_trial("/opt/ml/tensors")
trial.tensor_names(regex=".*")
trial.tensor("conv0").value(step)

# monitor tensors
while not trial.loaded_all_steps:
  steps = trial.steps(mode=modes.EVAL)
	t = trial.tensor("conv1").value(steps[-1])
	plt.hist(t.flatten(), bins=100)
  
# analyze tensors
labels = "CrossEntropyLoss_input_0"
predictions = "CrossEntropyLoss_input_1"
inputs = "ResNet_input_0"
for step in trial.steps():
  l = trial.tensor(labels).value(step)
	p = trial.tensor(predictions).value(step)
	i = trial.tensor(inputs).value(step)
for prediction, label, img in zip(p,l,i):
	if prediction != label:
		plt.imshow(img)
```

* Challenges
  * Scale rule analysis by offloading into separate containers
  * Reduce overhead when recording and fetching tensors
    * optimize data retrieval with the help of index files that store metadata such as name, shape, and step along with the location of tensor objects
  * Separate compute and storage and minimize impact on training
* Rules
  * datasets
  * activation functions: sigmoid's vanishing gradients, dead ReLU
  * poor initialization: éšæœº weights æ˜¯ä¿è¯ independently learn

![debug-rule](./MLSys/debug-rule.png)

* Deployment Results and Insights
  * latent space + [t-SNE]()
  * Using Debugger for iterative model pruning
    * Many types of pruning techniques are known, for example, structured versus unstructured prun-ing, randomly removing weights versus removing by size or rank, and iterative pruning versus one-shot pruning (Blalock et al., 2018). In case of CNNs, iterative filter pruning is known to achieve state of the art results




### æœç´¢ç”µå•†æ¶æ„

#### ç»¼è¿°

> ç”µå•†æœç´¢å…¨é“¾è·¯ï¼ˆPART Iï¼‰Overviewhttps://mp.weixin.qq.com/s/8-JtKugK-zf9In2ZdI3lrg

![å›¾ç‰‡](./MLSys/640)

* ç”µå•†æœç´¢å’Œç½‘é¡µæœç´¢çš„åŒºåˆ«
  * äº¿çº§ vs ä¸‡äº¿çº§
  * æ•°æ®ç»“æ„åŒ– vs éç»“æ„åŒ–
  * ç›¸å…³æ€§æ—¶æ•ˆæ€§+CTR/GMV
* é˜¿é‡ŒKDD'21çš„è®ºæ–‡ï¼šã€ŠEmbedding-based Product Retrieval in Taobao Searchã€‹ï¼Œç»å…¸æ¶æ„

![å›¾ç‰‡](./MLSys/640-20241010191312101)



* æ€»ç»“ï¼š
  - æ¼”è¿›è·¯çº¿ï¼šä¼ ç»Ÿlexical matching -> æ·±åº¦æ¨¡å‹ -> NLP-basedæ£€ç´¢ -> ä¸ªæ€§åŒ–æ¨¡å‹
  - **NLPç›¸å…³æ€§æ¨¡å‹/ç­–ç•¥çš„èƒ½åŠ›**æ˜¯å‘å±•é‡ç‚¹
  - å¸¸è§çš„ç²¾æ’æ¨¡å‹ç»“æ„ä»æ˜¯åŒå¡”æ¨¡å‹ï¼ˆç®—æ³•æ•ˆæœä¾èµ– query*doc cross featureï¼‰
  - LLMå…´èµ·åï¼Œå¯èƒ½ **ä½æˆæœ¬æ‰“å¹³ç”šè‡³è¶…è¿‡** ä»¥å¾€åŸºäºNLPæŠ€æœ¯çš„ç®—æ³•è¿­ä»£



#### ç¾å›¢æ¶æ„

> https://www.bilibili.com/video/BV1gM4m1r7DQ
>
> https://tech.meituan.com/2024/07/05/the-practice-of-search-advertising-recall-technology-in-meituan.html
>
> ä»æ¶æ„æ¼”è¿›çš„è§’åº¦è®²è§£ï¼š
>
> é‡ç‚¹ï¼šå…³é”®è¯æŒ–æ˜æŠ€æœ¯ã€ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯å’Œè¯­ä¹‰ä¸ªæ€§åŒ–ä¿¡æ¯åˆ†åˆ«å­¦ä¹ ã€æœç´¢æ¨èåŒ–è§£å†³æ³›æ„å›¾å¼±ä¾›ç»™

* ä¸šåŠ¡ç‰¹ç‚¹
  * æœå•†å“ï¼ˆ80%+ï¼‰ + æœå•†å®¶ + çŒœä½ å–œæ¬¢
  * ç™¾ä¸‡çº§å•†å®¶ã€åäº¿çº§åˆ«å•†å“
  * ä¸­å°å•†å®¶å¤šï¼Œå†…å®¹è´¨é‡ä¸é«˜
  * LBSå±æ€§ï¼Œä¾›ç»™ä¸å……åˆ†ï¼Œå¯¹å¬å›ç‡è¦æ±‚æ›´é«˜

![image-20241004205944993](./MLSys/meituan0.png)

![img](https://p1.meituan.net/travelcube/d1e0aed8bb38220792a3337d9ac211e8728900.png)

![img](https://p0.meituan.net/travelcube/68f8473fef2b195795238fda49311e4d767762.png)

![img](https://p1.meituan.net/travelcube/30482573c6a09cb8e3384db6dc660a0e829404.png)

* é˜¶æ®µä¸€ï¼šå¤šç­–ç•¥å…³é”®è¯æŒ–æ˜
  * SPUé€šè¿‡ç¦»çº¿æ–¹å¼ï¼ŒæŒ–æ˜æ ¸å¿ƒå…³é”®è¯ï¼Œåœ¨çº¿ä¸Queryç²¾ç¡®åŒ¹é…
  * **ç‰¹ç‚¹**ï¼š
    * åªèšç„¦äºé€šè¿‡ç¦»çº¿æ–¹å¼è¦†ç›–é«˜é¢‘æµé‡ï¼›
    * ç¼ºä¹çº¿ä¸Šçš„è¡Œä¸ºæ•°æ®ï¼Œä»¥NLPçš„æŒ–è¯æŠ€æœ¯ä¸ºä¸»ï¼›
    * ä¸ºäº†è¿½æ±‚æ›´å¤šçš„è¦†ç›–ï¼Œé‡‡ç”¨äº†å¤šç­–ç•¥å¹¶è¡Œçš„æ–¹å¼ï¼Œä¸æ–­å åŠ æ–°çš„å¬å›ç­–ç•¥ï¼Œä»¥è¾¾åˆ°æ›´é«˜çš„æµé‡è¦†ç›–
  * ä¸€ç”±äºQueryå¾ˆçŸ­ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“é€šè¿‡ä¿¡æ¯æŠ½å–ï¼ŒæŠŠè¯æˆ–å®ä½“æ ¸å¿ƒä¿¡æ¯æŠ½å–å‡ºæ¥ï¼›äºŒæ˜¯å› ä¸ºå¤´éƒ¨æ•ˆåº”æ¯”è¾ƒæ˜æ˜¾ï¼ŒTop2ä¸‡çš„Queryè¦†ç›–äº†å¾ˆå¤šæµé‡ï¼Œé‡‡ç”¨è¿™ç§ç¦»çº¿æ–¹å¼èƒ½å¿«é€Ÿæ‹¿åˆ°å¤§éƒ¨åˆ†æ”¶ç›Šï¼›ä¸‰æ˜¯ç”±äºå•†å®¶æ²¡æœ‰ä¹°è¯èƒ½åŠ›ï¼Œå¦‚æœç”¨Queryç›´æ¥åŒ¹é…å•†å“ï¼Œä¼šæ¶‰åŠåˆ°ä¼ å¯¼æ–‡æœ¬åŒ¹é…é—®é¢˜ï¼ŒåŒ¹é…éš¾åº¦ä¼šæ›´é«˜ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ€åé‡‡ç”¨æ¨¡å‹ä»å•†å®¶å•†å“é‡ŒæŒ–æ˜æ ¸å¿ƒè¯ï¼Œåœ¨çº¿åšçŸ­ä¸²åŒ¹é…çš„æ–¹å¼ã€‚
  * ç¬¬ä¸€ç‰ˆï¼šæ›´å¤šé‡‡ç”¨åŸºäºè§„åˆ™çš„æŒ–æ˜å¼ç­–ç•¥ï¼ŒæŠŠæµé‡åˆ†æˆäº†å•†å®¶è¯ã€å•†å“è¯å’Œå“ç±»è¯ã€‚å•†å“è¯é€šè¿‡åˆ†è¯å’Œè¯é¢‘è´¡çŒ®çš„ç®—æ³•ï¼ŒæŒ–æ˜æ ¸å¿ƒå…³é”®è¯ï¼Œç”±äºå“ç±»å­—é¢æ²¡æœ‰å®Œå…¨åŒ¹é…çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡äº’ä¿¡æ¯ï¼Œæ„å»ºè¯ä¹‹é—´çš„æƒé‡å»æŒ–æ˜ã€‚ä½†é—®é¢˜ä¸€æ˜¯è§„åˆ™èƒ½åŠ›è¾ƒå¼±ï¼›ç¬¬äºŒæ˜¯åªèƒ½æŒ–æ˜å‡ºè¿ç»­çš„çŸ­è¯ï¼Œæ¯”å¦‚â€œç‚’è¥¿çº¢æŸ¿é¸¡è›‹â€ï¼Œå®ƒåªèƒ½æŒ–æ˜å‡ºâ€œç‚’è¥¿çº¢æŸ¿â€ï¼ŒæŒ–æ˜ä¸å‡ºâ€œç‚’é¸¡è›‹â€ã€‚
  * ç¬¬äºŒç‰ˆï¼šæŠ½å–å¼æ¨¡å‹
    * åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼šåªèƒ½æŒ–æ˜å‡ºè¿ç»­çŸ­ä¸²ï¼Œå¥½å¤„æ˜¯æŒ–æ˜æ•ˆç‡æ¯”åŸºäºè§„åˆ™çš„æŒ–æ˜æ¨¡å¼é«˜ï¼Œä½†ä¼šå¯¼è‡´å¾ˆå¤šå…³é”®è¯å—é™äºè¿ç»­çŸ­ä¸²çš„æ–¹å¼è€ŒæŒ–æ˜ä¸å‡ºæ¥
    * æ ‡æ³¨ç»„åˆæ¨¡å‹å’ŒæŒ‡é’ˆç»„åˆæ¨¡å‹ï¼šæ ‡æ³¨ç»„åˆæ¨¡å‹èƒ½å¤Ÿè·¨è¶Šè¿ç»­çŸ­ä¸²æŒ–æ˜ï¼Œä½†å®ƒæœ‰ä¸€ä¸ªé¡ºåºæ¦‚å¿µåœ¨é‡Œé¢ï¼›æŒ‡é’ˆç»„åˆæ¨¡å‹å¯ä»¥åœ¨åŸæœ‰çŸ­ä¸²é‡Œéšæœºç»„åˆè¯ï¼Œçªç ´é¡ºåºå’Œè¿ç»­çš„å±€é™ã€‚ä½†æŠ½å–å¼æ¨¡å‹çš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œæ¢ç´¢ç©ºé—´ä¸è¶³
  * ç¬¬ä¸‰ç‰ˆï¼šç”Ÿæˆå¼æ¨¡å‹
    * æ·±åº¦åˆ†ç±»æ¨¡å‹ï¼šå°†SPUå•†å“æ–‡æœ¬ç›´æ¥åˆ†ç±»åˆ°è¿™2ä¸‡ä¸ªQueryæ ‡ç­¾é‡Œï¼Œåšè¯å’ŒQueryé—´çš„åŒ¹é…ï¼Œä½†è¿™ç§å¤šåˆ†ç±»æ¨¡å‹è¾ƒéš¾ä¼˜åŒ–ï¼Œä¹Ÿä¸èƒ½æ³›åŒ–å‡ºæ›´å¤šçš„Queryï¼Œæ—¶æ•ˆæ€§å’Œæ›´æ–°é¢‘ç‡ä¹Ÿæœ‰é™
    * æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼šå—é™äºæ¨¡å‹è§„æ¨¡å’Œæ ·æœ¬ä¸°å¯Œåº¦ï¼Œå‡†ç¡®æ€§ä¸å¤ªå¥½ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨åé¢åŠ äº†æ ‡æ³¨å’Œç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å…·å¤‡ç”Ÿæˆæ³›åŒ–æ€§çš„åŒæ—¶ï¼Œå°½é‡æ§åˆ¶Queryè´¨é‡

![img](https://p1.meituan.net/travelcube/ce161f376ffa89b2baed47bc8e7c4765967044.png)

* é˜¶æ®µäºŒï¼šåˆ†å±‚å¬å›ä½“ç³»
  * ç‰¹ç‚¹ï¼š
    * åœ¨ä¸€ä¸ªä¸šåŠ¡èŒƒç•´å†…ï¼Œé€šè¿‡æŠŠæŠ€æœ¯åšæ·±èƒ½å¤Ÿå–å¾—ä¸šåŠ¡æ•ˆæœçš„æå¤§æå‡ï¼›
    * éšç€åŸºå»ºèƒ½åŠ›çš„æå‡ï¼Œæ›´å¤šçš„æ˜¯æŠŠå¬å›ç”±ç¦»çº¿åˆ‡æ¢æˆåœ¨çº¿ï¼Œä»¥æ­¤è¦†ç›–æ›´å¤šçš„æµé‡ï¼›
    * åœ¨å•é€šè·¯çš„å¬å›èƒ½åŠ›ä¸Šï¼Œæˆ‘ä»¬çªç ´äº†ä¼ ç»Ÿå•ä¸€NLPæŠ€æœ¯ç“¶é¢ˆï¼Œå¼€å§‹å¤§è§„æ¨¡ä½¿ç”¨ä¸ªæ€§åŒ–/å›¾/å¤šæ¨¡æ€ç­‰æ–°çš„å¬å›æŠ€æœ¯ã€‚åœ¨2022å¹´åº•ï¼Œæ•´ä¸ªåˆ†å±‚å¬å›ä½“ç³»å–å¾—äº†ä¸å°‘æˆæ•ˆã€‚
  * ç¬¬ä¸€æ˜¯å¼ºæ„å›¾æœ‰ä¾›ç»™ï¼Œé€šè¿‡å…³é”®è¯å°±èƒ½è¾ƒå¥½æ»¡è¶³ï¼Œå› æ­¤åœ¨è¿™ä¸ªè±¡é™é‡Œï¼Œæˆ‘ä»¬æ›´å¤šæ˜¯åœ¨è¿­ä»£å…³é”®è¯å¬å›æŠ€æœ¯ã€‚
    * ä¸€æ˜¯é€šè¿‡ç¦»çº¿ç»Ÿä¸€åˆ°ç”Ÿæˆå¼çš„æ–¹å¼ã€‚å‰é¢ä»‹ç»ç¦»çº¿å…³é”®è¯æŒ–æ˜ç­–ç•¥å¯èƒ½ä¼šæœ‰åå‡ ä¸ªé€šé“ï¼Œä¸ç®¡è¿­ä»£å“ªä¸ªé€šé“ï¼Œç­–ç•¥å¬å›çš„è¦†ç›–é¢éƒ½æ˜¯æœ‰é™çš„ï¼Œè€Œä¸”å›¢é˜Ÿä¹Ÿæ²¡é‚£ä¹ˆå¤šäººè¿­ä»£ï¼Œä½†è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æŠŠæ•´ä¸ªç¦»çº¿å…³é”®è¯åå¤šè·¯çš„æŒ–æ˜ç­–ç•¥é€šè¿‡è§„æ¨¡è¾ƒå¤§çš„ç”Ÿæˆå¼æ¨¡å‹åšäº†ç»Ÿä¸€ï¼Œå¼•å…¥äº†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œåšåˆ°äº†æ•°æ®æ›´å¤šã€æ¨¡å‹æ›´å¤šä»¥åŠå¬å›ç›®æ ‡æ›´å¤šçš„æƒ…å†µï¼ŒåæœŸåªéœ€è¦é€šè¿‡ä¼˜åŒ–æ¨¡å‹èƒ½åŠ›ï¼Œå°±èƒ½å–å¾—çº¿ä¸Šå…¨æµé‡è¦†ç›–çš„æ•ˆæœï¼›
    * äºŒæ˜¯é€šè¿‡ç¦»çº¿å…³é”®è¯çš„æ–¹å¼åšåˆ°äº†åœ¨çº¿ã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰é‡‡ç”¨ä¸šç•Œä¼ ç»Ÿçš„å¸ƒå°”æ£€ç´¢ï¼Œè¿™ç§æ–¹å¼æœ‰ä¸¤ä¸ªå±€é™ï¼Œä¸€æ˜¯Queryæ”¹å†™ä»¥åŠå•†å“åˆ†è¯åŸºäºè¾ƒæµ…å±‚çš„æ¨¡å‹ï¼Œæ•´ä½“æ•ˆæœä¼šå—é™äºæ¨¡å‹æ•ˆæœã€‚äºŒæ˜¯å®ƒæ²¡æœ‰åšåˆ°æ£€ç´¢å’Œæœ€ç»ˆç›®æ ‡çš„åŒ¹é…ã€‚
      * åœ¨çº¿ç¨€ç–åŒ–æ£€ç´¢æ–¹å¼ç±»ä¼¼äºåŒå¡”å‘é‡æ£€ç´¢ï¼Œä½†æ¯ä¸ªæ¨¡å‹å‡ºæ¥ä¸æ˜¯ä¸€ä¸ªç¨ å¯†çš„å‘é‡ï¼Œè€Œæ˜¯ä¸€ä¸ªå‡ ä¸‡ç»´ç¨€ç–çš„termç²’åº¦ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å»ºæ¨¡æ–¹å¼ï¼ŒæŠŠQueryå’Œå•†å“æ˜ å°„åˆ°ä¸€ä¸ªç¨€ç–çš„å‡ ä¸‡ç»´æ§½ä½çš„å‡ ä¸ªæ§½ä½é‡Œï¼Œç¦»çº¿è®­ç»ƒæ—¶é€šè¿‡æ§½ä½ç«¯åˆ°ç«¯çš„å»ºæ¨¡ï¼Œå®ç°ç›®æ ‡æ£€ç´¢å’Œç›®æ ‡ä¸€è‡´æ€§ï¼Œåœ¨çº¿æ£€ç´¢æ—¶ï¼ŒåŸºäºæ§½ä½æ„å»ºå€’æ’æ£€ç´¢ï¼Œå…·å¤‡ä¸€å®šçš„å¯è§£é‡Šæ€§ã€‚
  * ç¬¬äºŒä¸ªæ˜¯æ³›æ„å›¾æœ‰ä¾›ç»™ï¼Œä½“ç°äº†ç”¨æˆ·çš„ä¸ªæ€§åŒ–åå¥½ï¼Œé€šè¿‡è¿­ä»£å‘é‡å¬å›æ¨¡å‹è¦†ç›–è¿™ä¸ªåœºæ™¯ã€‚å‘é‡å¬å›ç»è¿‡äº†ä¸‰ç‰ˆè¿­ä»£ã€‚
    * ç¬¬ä¸€ç‰ˆæ˜¯åŸºäºä¼ ç»Ÿè¯­ä¹‰ç›¸å…³æ€§çº¦æŸçš„åŒå¡”æ¨¡å‹ï¼Œå’Œä¸šç•Œçš„åšæ³•ç±»ä¼¼ï¼›
    * ç¬¬äºŒç‰ˆå°†ç”¨æˆ·ä¸ªæ€§åŒ–æä¸Šäº†æ—¥ç¨‹ï¼Œä½†å¦‚æœåªæŠŠç”¨æˆ·ä¸ªæ€§åŒ–ç‰¹å¾å’Œä¼ ç»Ÿè¯­ä¹‰ç‰¹å¾èåˆåœ¨ä¸€èµ·ï¼Œ**é»‘ç›’å¼å­¦ä¹ å¾ˆå®¹æ˜“è¢«ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯å¸¦å**ï¼Œæœ€åæˆ‘ä»¬è®©**ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯å’Œè¯­ä¹‰ä¸ªæ€§åŒ–ä¿¡æ¯åˆ†åˆ«å­¦ä¹ **ï¼Œé€šè¿‡æ˜¾å¼å åŠ çš„æ–¹å¼åšç«¯åˆ°ç«¯çš„å»ºæ¨¡ã€‚è¿™ç§æ£€ç´¢æ–¹å¼èƒ½å¤Ÿå…¼é¡¾ä¸ªæ€§åŒ–å’Œè¯­ä¹‰ç›¸å…³æ€§ä¿¡æ¯ï¼›
    * ç¬¬ä¸‰ç‰ˆæ˜¯åŸºäºå¹³å°çš„å¤šæ ·åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦å¯¹é½åé“¾è·¯çš„ç²¾æ’ç›®æ ‡ï¼Œåœ¨å¬å›é˜¶æ®µè€ƒè™‘æ•´ä½“å•†ä¸šä»·å€¼ã€‚
  * ç¬¬ä¸‰ä¸ªæ˜¯æ³›éœ€æ±‚å¼±ä¾›ç»™ï¼Œæ¯”å¦‚æœç´¢â€œæ±‰å ¡ç‹â€ï¼Œä½†ç»™TAä¸€ä¸ªâ€œè‚¯å¾·åŸºâ€ï¼ŒTAä¹Ÿä¼šä¸‹å•ï¼Œé€šè¿‡**æœç´¢æ¨èåŒ–**çš„æ–¹å¼è¦†ç›–å’Œè§£å†³ã€‚
    * è¿™ä¸ªåœºæ™¯æ¯”è¾ƒå¤æ‚ï¼Œä»ä¸šåŠ¡æ¥çœ‹ï¼Œå®ƒéœ€è¦åšå¼•å¯¼å’Œæ¨èï¼Œåœ¨ç»“æœé¡µé‡Œä¹Ÿåšåæ³›ç»“æœçš„æ¨èï¼Œæ¶‰åŠåˆ°æœç´¢å‰å’Œæœç´¢ä¸­ï¼Œæœç´¢ä¸­æ—¢æœ‰å•†å®¶ä¹Ÿæœ‰èœå“ï¼Œæ—¢æ¶‰åŠè¦æ¨èä»€ä¹ˆæ ·çš„èœå“ï¼Œä¹Ÿæ¶‰åŠæ¨èä»€ä¹ˆæ ·çš„å•†å®¶ï¼›
    * å¦å¤–æ¨èæœ¬èº«æ˜¯ä¸€ä¸ªå…³ç³»å»ºæ¨¡ã€‚æˆ‘ä»¬æœ€åé€‰æ‹©åŸºäºå›¾æ¨¡å‹çš„è¿­ä»£ï¼Œå› ä¸ºå›¾æ¨¡å‹é¦–å…ˆæ˜¯ä¸€ä¸ªåŸºäºå…³ç³»çš„å»ºæ¨¡ï¼Œè€Œä¸”å›¾æ¨¡å‹å…·å¤‡å¤šåœºæ™¯æµ·é‡ä¿¡æ¯çš„å®¹çº³èƒ½åŠ›ï¼Œåœ¨å›¾å»ºæ¨¡é‡Œï¼Œä¸€æ˜¯æ„å»ºäº†å¼‚æ„çš„å¤šèŠ‚ç‚¹ç™¾äº¿è§„æ¨¡å›¾ï¼Œé€šè¿‡å›¾é¢„è®­ç»ƒåŠ å¾®è°ƒçš„æ–¹å¼è¯†åˆ«å¤šä¸ªåœºæ™¯ï¼Œæˆ‘ä»¬æœ€è¿‘ä¹Ÿåœ¨å°è¯•åšå›¾å’Œå¤§æ¨¡å‹è®­ç»ƒç›¸ç»“åˆçš„æ–¹å¼ï¼›
    * äºŒæ˜¯æˆ‘ä»¬æŠŠæ•´ä¸ªå›¾æ£€ç´¢æ¬åˆ°åœ¨çº¿ï¼Œå› ä¸ºåœ¨æœç´¢åœºæ™¯ä¸­ï¼Œç”¨æˆ·éœ€æ±‚æ˜¯å³æ—¶éœ€æ±‚ï¼Œå±æ€§è¾ƒå¼ºï¼Œåªæœ‰æŠŠæ£€ç´¢æ¬åˆ°åœ¨çº¿ï¼Œé€šè¿‡å›¾åœ¨çº¿çš„å®æ—¶æ£€ç´¢èšåˆåˆ°ç”¨æˆ·å½“å‰æœ€æœ‰å¯èƒ½çš„æ½œåœ¨å…´è¶£æƒ…å†µä¸‹ï¼Œæ‰èƒ½å®ç°æ”¶ç›Šæœ€å¤§åŒ–ã€‚
  * ç¬¬å››ä¸ªæ˜¯æ²¡æœ‰ä¾›ç»™çš„åœºæ™¯ï¼Œé€šè¿‡æµé‡ç»“åˆä¾›ç»™è¿è¥åŒ–çš„æ–¹å¼è§£å†³ã€‚

![img](https://p0.meituan.net/travelcube/cb8c69f866c07b7bbe28f99acbc845f7640525.png)

* é˜¶æ®µä¸‰ï¼šç”Ÿæˆå¼å¬å›
  * æ ¸å¿ƒæ€è·¯æ˜¯æŒ‰ç…§æµé‡å’Œä¾›ç»™ç‰¹ç‚¹åˆ†ç±»ï¼Œå¼ºæ„å›¾æ˜¯ç›´æ¥æœç´¢ä¸€ä¸ªå•†å“ï¼›æ³›æ„å›¾æ¯”å¦‚æœç´¢â€œçƒ§çƒ¤â€è¿™ä¸ªå“ç±»ï¼Œæ³›æ„å›¾ç”¨æˆ·è™½ç„¶è¡¨è¾¾äº†éœ€æ±‚ï¼Œä½†æ»¡è¶³éœ€æ±‚çš„å€™é€‰å¯ä»¥å¾ˆå¹¿ï¼Œç”šè‡³å¯ä»¥æ›¿ä»£ï¼›ä¾›ç»™å±‚é¢åˆ†ä¸ºæœ‰ä¾›ç»™ã€å¼±ä¾›ç»™å’Œæ²¡æœ‰ä¾›ç»™ä¸‰ä¸ªè±¡é™
  * æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆå¤§æ¨¡å‹æˆ–ç”Ÿæˆå¼æŠ€æœ¯æ€æƒ³ï¼Œæé«˜å¬å›ç®—æ³•çš„å†³ç­–ç©ºé—´ï¼Œæå‡æ¨¡å‹çš„åŒ¹é…èƒ½åŠ›ã€‚ç»è¿‡ä¸€æ®µæ—¶é—´è¿­ä»£ï¼Œæˆ‘ä»¬æŠ½è±¡å‡ºå¹¿å‘Šå­æ¨¡å—ç»“åˆLLMè½åœ°çš„ä¸‰ç±»æ€æƒ³åŠæ–¹å¼ï¼Œåˆ†åˆ«æ˜¯ç”¨æ€æƒ³ã€å­¦èƒ½åŠ›ã€ç”¨LLMã€‚å…·ä½“å’Œå­æ¨¡å—ç»“åˆçš„ä¸€äº›æ¢ç´¢å¦‚ä¸‹ï¼š
    * ä¸€æ˜¯ç¦»çº¿å…³é”®è¯å¬å›æ–¹å‘ã€‚å¦‚åˆšæ‰ä»‹ç»ï¼Œæˆ‘ä»¬å·²ç»æŠŠæ•´ä¸ªç¦»çº¿å…³é”®è¯å¬å›æŠ€æœ¯æ–¹å¼ç»Ÿä¸€åˆ°äº†è§„æ¨¡ä¸é”™çš„ç”Ÿæˆå¼æ¨¡å‹æ–¹å¼ä¸Šã€‚å¤§æ¨¡å‹å‡ºæ¥åï¼Œç›´æ¥ç”¨å¤§æ¨¡å‹å…¶å®è¿˜å­˜åœ¨ç€ç®—åŠ›åŠæ•ˆæœçš„2ä¸ªæŒ‘æˆ˜ã€‚ä½†æˆ‘ä»¬è®¤ä¸ºå¤§æ¨¡å‹çš„ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯æ€æƒ³ï¼š**Cotï¼ˆChain-of-thoughtï¼Œèƒ½ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„è¯­è¨€è¯·æ±‚ï¼‰æ¨ç†å’ŒRLHFï¼ˆReinforcement Learning from Human Feedbackï¼Œä¸€ç§åŸºäºäººç±»åå¥½çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼‰å¯¹é½äººç±»åé¦ˆæ€æƒ³**ï¼Œå¯¹æˆ‘ä»¬ç°æœ‰æ¨¡å‹çš„ä¼˜åŒ–ä¹Ÿæ˜¯æœ‰å¸®åŠ©çš„ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨å¤§æ¨¡å‹çš„è¿™äº›æŠ€æœ¯æ€æƒ³æ¥æ”¹é€ ç¦»çº¿ç”Ÿæˆå¼å¬å›æ¨¡å‹ã€‚
    * äºŒæ˜¯åœ¨å‘é‡å¬å›æ–¹å‘ã€‚æˆ‘ä»¬å·²ç»å°†å‘é‡è¡¨å¾å‡çº§ä¸ºå¤šæ¨¡æ€æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æˆ‘ä»¬æ€è€ƒï¼Œ**LLMè¯­è¨€å¤§æ¨¡å‹å¯¹äºç¦»æ•£Tokençš„ä¿¡æ¯å½’çº³åŠè¡¨å¾æ˜¯æœ‰æ¯”è¾ƒå¤§çš„æå‡çš„**ï¼Œä½†æ˜¯åœ¨ç¨ å¯†è¡¨å¾é¢†åŸŸï¼Œä¸€ä¸ªå€¼å¾—å€Ÿé‰´çš„æ–¹æ³•æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå› ä¸ºæ‰©æ•£æ¨¡å‹ä¹Ÿæ˜¯é€šè¿‡å¤šæ­¥å»å™ªçš„æ–¹å¼æ¥ç”Ÿæˆç›®æ ‡ï¼Œé€šè¿‡æ‰©æ•£å¤šæ­¥è¿‡ç¨‹ï¼Œåœ¨å…¶ä¸­å¼•å…¥å¤šå…ƒä¿¡æ¯å¤šæ­¥èåˆçš„æ€è·¯ï¼Œæå‡æ•´ä¸ªå‘é‡å¬å›çš„å‘é‡è¡¨å¾èƒ½åŠ›ã€‚
    * ä¸‰æ˜¯éšç€æˆ‘ä»¬æ¢ç´¢çš„æ·±å…¥åŠå¯¹åº”ç®—æ³•èƒ½åŠ›çš„æå‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¾å›¢é¢†åŸŸå¹¿å‘Šå¤§æ¨¡å‹ï¼Œå°è¯•ç›´æ¥æŠŠå¤§æ¨¡å‹ç”¨åˆ°ç¾å›¢å®é™…åœºæ™¯é‡Œåšå…³é”®è¯å¬å›ï¼Œå°†ç¦»çº¿ä¸­ç­‰è§„æ¨¡çš„ç”Ÿæˆå¼æ¨¡å‹ç›´æ¥æ›¿æ¢æˆå¤§æ¨¡å‹ï¼Œå¹¶æ¢ç´¢å¤§æ¨¡å‹åœ¨çº¿åŒ–ã€‚
    * ç¬¬å››ä¸ªæ˜¯è’¸é¦å¤§æ¨¡å‹èƒ½åŠ›ï¼Œä¸»è¦åœ¨ç›¸å…³æ€§åœºæ™¯è½åœ°ï¼Œç›®å‰è’¸é¦äº†ä¸¤å—èƒ½åŠ›ï¼ŒCotæ¨ç†èƒ½åŠ›å’Œæ¨¡å‹éšå±‚çŸ¥è¯†èƒ½åŠ›è’¸é¦
  * ç”Ÿæˆå¼å…³é”®è¯å¬å›
    * ç”Ÿæˆå¼å¬å›ä¸»è¦å€Ÿé‰´å¤§æ¨¡å‹æ€æƒ³ï¼Œæˆ‘ä»¬å·²ç»å‡çº§ä¸ºç»Ÿä¸€çš„ç”Ÿæˆå¼æ¨¡å‹ï¼Œå®ƒçš„å·¥ä½œæ–¹å¼æ˜¯åŸºäºbeamsearchçš„æ–¹å¼ï¼Œä¸€æ¬¡ç”Ÿæˆå¤šä¸ªç»“æœï¼Œä½†ç»“æœä¹‹é—´æ˜¯äº’ç›¸çœ‹ä¸åˆ°çš„ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§æ–¹å¼ä¼šå­˜åœ¨é—®é¢˜ï¼Œå¦å¤–ï¼Œä»çº¿ä¸Šå’Œå®é™…ç”Ÿæˆç»“æœæ¥çœ‹ï¼Œè¯ä¹‹é—´æ˜¯æœ‰å…³ç³»çš„ï¼ŒæŒ‰ç…§æ¦‚ç‡æ–¹å¼æ¥çœ‹ï¼Œå¦‚æœä¸€ä¸ªå…³é”®è¯èƒ½å¤Ÿæ¨ç†å‡ºå¦ä¸€ä¸ªå…³é”®è¯ï¼Œå¤§æ¦‚ç‡å‰é¢è¿™ä¸ªå…³é”®è¯è¦æ¯”ä¸‹ä¸€ä¸ªå…³é”®è¯çš„ä¿¡æ¯å«é‡å¤šï¼Œé‚£èƒ½å¦å€Ÿé‰´å¤§æ¨¡å‹æ¨ç†æ€æƒ³ï¼Œ**æŒ‰ç…§åºåˆ—ç”Ÿæˆæ–¹å¼é€æ­¥æ¨ç†å‡ºå¤šä¸ªå…³é”®è¯**ã€‚
    * æˆ‘ä»¬é€šè¿‡æ„å»ºæ¦‚ç‡è´¡çŒ®å›¾çš„æ–¹å¼ï¼Œé‡‡æ ·å¾—åˆ°å…³é”®è¯ä¹‹é—´çš„å¯¼å‡ºå…³ç³»ï¼Œåœ¨ä¸€æ¬¡ç”Ÿæˆæ—¶ï¼Œç›´æ¥ç”Ÿæˆå¤šä¸ªå…³é”®è¯ï¼Œè¿™å¤šä¸ªå…³é”®è¯ä¹‹é—´æœ‰æ¨ç†å…³ç³»ï¼Œæ¯”å¦‚è¦ç»™â€œèŠ±ä»™å¥³é²œèŠ±åº—â€å•†å®¶ç”Ÿæˆå…³é”®è¯ï¼Œç¬¬ä¸€ä¸ªå…³é”®è¯å°±æ˜¯ç›¸å¯¹å…·è±¡çš„â€œé²œèŠ±åº—â€ï¼Œå®ƒçš„å«ä¹‰å’Œå•†å®¶çš„å•†å“æè¿°æ˜¯ç¡®å®šçš„ï¼Œåœ¨ç”Ÿæˆâ€œé²œèŠ±åº—â€æ—¶ï¼Œå¯ä»¥æ¨ç†æˆâ€œèŠ±åº—â€ï¼Œè¿›ä¸€æ­¥å¯èƒ½ä¼šç”Ÿæˆæ–°å…³é”®è¯ï¼Œé€šè¿‡è¿™ç§åºåˆ—æ¨ç†æ–¹å¼ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°åˆ©ç”¨å…³é”®è¯ä¹‹é—´çš„å…³ç³»ã€‚
    * åœ¨åºåˆ—æ¨ç†ç”Ÿæˆå…³é”®è¯æ—¶ï¼Œæ¯”å¦‚ç”Ÿæˆäº†5ä¸ªå…³é”®è¯ï¼Œæœ‰ä¸€ä¸ªå…³é”®è¯ä¸ç›¸å…³ï¼Œå‰©ä¸‹çš„4ä¸ªå…³é”®è¯æ˜¯ç›¸å…³çš„ï¼Œé‚£å¦‚ä½•é€šè¿‡æ¨¡å‹è¯†åˆ«å‡ºè¿™ç§ä¸ä¸€è‡´ç°è±¡ï¼Œèƒ½å¦å€ŸåŠ©äººç±»åé¦ˆæ–¹å¼ï¼Œå®ç°æ¨¡å‹åºåˆ—å¥½åç«¯åˆ°ç«¯çš„åˆ¤æ–­ã€‚æ¨¡å‹ç”Ÿæˆçš„å…³é”®è¯åºåˆ—ä¸äººå·¥æ ‡æ³¨æ˜¯å¦ä¸€è‡´ï¼Œé€šè¿‡è¿™ç§åé¦ˆå¯¹é½çš„æ–¹å¼å–‚ç»™æ¨¡å‹ï¼Œæå‡æ•´ä¸ªåºåˆ—ç”Ÿæˆç»“æœçš„ä¸€è‡´
    * ![img](https://p0.meituan.net/travelcube/0a64745f26ec8939c7f4e17424273d161277430.png)
* å¯¹äºç¦»çº¿å…³é”®è¯ï¼Œå‰é¢æ˜¯ä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æœ€è¿‘æŠŠæ•´ä¸ªç¦»çº¿å…³é”®è¯æ›¿æ¢æˆå¤§æ¨¡å‹ï¼Œä¹‹å‰æ²¡æœ‰æ›¿æ¢æ˜¯å› ä¸ºå¼€æºé€šç”¨å¤§æ¨¡å‹èƒ½åŠ›åœ¨é¢†åŸŸåœºæ™¯é‡Œï¼ŒæŒ–æ˜è¯çš„å‡†ç¡®æ€§å’Œé€šç”¨æ€§æœ‰é™ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨æ„å»ºç¾å›¢å¹¿å‘Šé¢†åŸŸçš„å¤§æ¨¡å‹ï¼Œé€šè¿‡æ¿€å‘å¤§æ¨¡å‹çŸ¥è¯†ï¼Œç”Ÿæˆæ›´å…¨é¢å‡†ç¡®çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åšäº†3ä¸ªé˜¶æ®µçš„ä¼˜åŒ–ã€‚
  * ç¬¬ä¸€æ˜¯èåˆé¢†åŸŸçŸ¥è¯†ï¼Œæ¯”å¦‚å¥èº«å’Œè½»é£Ÿç›¸å…³ï¼Œè¿™æ˜¯é¢†åŸŸçŸ¥è¯†ï¼Œé€šè¿‡é¢†åŸŸå…¨å‚æ•°è®­ç»ƒå¾—åˆ°ä¸€ä¸ªåŸºç¡€çš„å¹¿å‘Šé¢†åŸŸæ¨¡å‹ã€‚
  * ç¬¬äºŒæ˜¯èå…¥åœºæ™¯çŸ¥è¯†ï¼Œç¾å›¢æœ‰å¾ˆå¤šåº—é“ºå’Œå•†å“åï¼Œæ¯”å¦‚å·èœå’Œçœ‰å·ä¸œå¡åœ¨åº—é“ºé‡Œæœ‰å¾ˆå¤šç›¸å…³æ•°æ®ã€‚é€šè¿‡è¿™ç§æŒ‡ä»¤å¾®è°ƒçš„æ–¹å¼å­¦ä¹ åº—é“ºçŸ¥è¯†ï¼Œåœ¨å®é™…åº”ç”¨æ—¶ï¼Œå†å­¦ä¹ åå®é™…çš„çŸ¥è¯†ï¼Œæ¯”å¦‚æœç´¢â€œçŒªæ‰‹â€æ—¶ï¼Œå‘ç°ä»–ä¹‹å‰æ£€ç´¢è¿‡å¾ˆå¤šâ€œçŒªè‚˜åˆ‡ç‰‡â€ï¼Œé€šè¿‡è¿™ç§æ£€ç´¢æ–¹å¼å¢å¼ºå¤§æ¨¡å‹å½“å‰æ¨ç†çŸ¥è¯†èƒ½åŠ›ã€‚
  * æœ€åé€šè¿‡æ„å»ºé¢†åŸŸå¤§æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºèŒƒå¼ï¼Œåœ¨ä¸€äº›åœºæ™¯é‡Œæ›¿æ¢ä¼ ç»Ÿå¤§æ¨¡å‹ï¼Œè¿™æ ·ï¼Œæˆ‘ä»¬å‘ç°å¬å›æ•ˆç‡æ˜æ˜¾æå‡ã€‚
  * ![img](https://p0.meituan.net/travelcube/b9d1d0d7bcc6265f296c2ff425f5954a774735.png)

* **å¤šæ¨¡æ€ç”Ÿæˆå¼å‘é‡å¬å›â€”â€”ç»“åˆæ‰©æ•£æ¨¡å‹ï¼Œå¤šé˜¶æ®µç”Ÿæˆå‘é‡è¡¨å¾**
  * æˆ‘ä»¬æ”¹é€ æˆ–ä¼˜åŒ–å¤šæ¨¡æ€å‘é‡å¬å›ï¼Œåœ¨è¡¨å¾é‡Œç»“åˆæ‰©æ•£æ¨¡å‹åšäº†ä¼˜åŒ–ï¼Œå¦‚ä¸‹å›¾å·¦è¾¹æ‰€ç¤ºï¼Œä¼ ç»Ÿçš„å¤šæ¨¡æ€å‘é‡å¬å›æ›´å¤šæ˜¯åœ¨itemä¾§è¡¨å¾é‡Œï¼Œå°†å•†å“å›¾ç‰‡å’Œæ–‡æœ¬æ¨¡æ€ä¿¡æ¯èåˆåœ¨ä¸€èµ·ï¼Œå¾—åˆ°ä¸€ä¸ªè¡¨å¾ï¼Œé‚£èƒ½å¦é€šè¿‡ä¸€äº›æ–¹å¼åœ¨Queryä¾§ä¹Ÿå®ç°å¤šæ¨¡æ€è¡¨å¾ã€‚ä¸€ä¸ªç”¨æˆ·åœ¨ç¾å›¢åœºæ™¯é‡Œæœç´¢ä¸€ä¸ªQueryæ—¶ï¼Œå¤§æ¦‚ç‡ä»–çš„è„‘æµ·é‡Œå·²ç»æœ‰å…³äºè¿™ä¸ªQueryæ‰€å¯¹åº”èœå“å›¾ç‰‡çš„å¤§è‡´å°è±¡ã€‚é‚£æˆ‘ä»¬å¦‚ä½•é€šè¿‡æ¨¡å‹å»ºæ¨¡çš„æ–¹å¼è¿˜åŸå›¾ç‰‡çš„å°è±¡ï¼Œæ ¸å¿ƒåœ¨äºè¿˜åŸç”¨æˆ·çš„æ½œåœ¨æ„è¯†ã€‚
    * æˆ‘ä»¬çš„åšæ³•æ˜¯ï¼Œä¸€æ˜¯æŠŠQueryå†å²ç‚¹å‡»çš„å›¾ç‰‡ä¿¡æ¯æ±‡é›†åœ¨ä¸€èµ·ï¼Œè¡¨å¾Queryæ‰€ä»£è¡¨çš„é€šç”¨è§†è§‰ä¿¡æ¯ï¼›äºŒæ˜¯å°†ç”¨æˆ·å†å²ç‚¹å‡»å›¾ç‰‡ä»£è¡¨ç”¨æˆ·ä¸ªæ€§åŒ–è§†è§‰ä¿¡æ¯ï¼ŒæŠŠè¿™ä¸¤ç±»è§†è§‰ä¿¡æ¯å åŠ åœ¨ä¸€èµ·ï¼Œå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šåæ˜ ç”¨æˆ·åœ¨å½“å‰æœç´¢æ¡†æ¶ä¸‹ï¼Œæƒ³è¦å¾—åˆ°çš„æµé‡ä¾§å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæœ€åé€šè¿‡å¤šæ¨¡æ€è¡¨å¾åŒ¹é…æŠ€æœ¯ï¼Œæ•´ä¸ªç¦»çº¿å¬å›æ•ˆç‡ä¹Ÿæœ‰æå‡ã€‚

* ä½†è¿™ç§æ–¹å¼ä¹Ÿæ˜¯åŸºäºä¼ ç»Ÿçš„åˆ¤åˆ«å¼è¡¨å¾ï¼Œæ¯”å¦‚ç°åœ¨å¤§å®¶éƒ½åœ¨åšä¸ªæ€§åŒ–å‘é‡å¬å›ï¼Œç›¸å…³æ€§å’Œä¸ªæ€§åŒ–ä¹‹é—´æœ‰é€’è¿›å…³ç³»ï¼Œæœ€æµ…å±‚çš„éœ€è¦ä¿è¯ç›¸å…³æ€§ï¼Œç¬¬äºŒå±‚æ‰éœ€è¦åœ¨ç›¸å…³æ€§é‡ŒæŒ‘é€‰æ›´ä¸ªæ€§åŒ–ã€æ›´ç¬¦åˆç”¨æˆ·åå¥½çš„å€™é€‰é›†ï¼Œç»™åˆ°ä¸‹æ¸¸é“¾è·¯ã€‚
  * ä½†ä¼ ç»Ÿçš„åˆ¤åˆ«å¼æ–¹å¼ä¸€èˆ¬åœ¨ç‰¹å¾é˜¶æ®µå åŠ ä¸åŒç‰¹å¾ï¼Œé€šè¿‡å»ºæ¨¡ã€å¤šç›®æ ‡è½å®åå‘è¿ç§»æ–¹å¼ï¼Œä¸èƒ½å¾ˆå¥½çš„æ˜¾å¼å­¦ä¹ åˆ°ä¸åŒç›®æ ‡é—´çš„é€’è¿›å…³ç³»ï¼Œä½†SDç”Ÿæˆæ¨¡å‹æ¯”è¾ƒé€‚åˆè¿™ç§ç¨ å¯†å‘é‡ç”Ÿæˆï¼Œé€šè¿‡å¤šæ­¥è¿˜åŸè¿‡ç¨‹ï¼Œæœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªä¸æ–­æ¨ç†çš„ç”Ÿæˆå¼è¿‡ç¨‹ã€‚

* æˆ‘ä»¬å¸Œæœ›å‘é‡è¡¨å¾å…·å¤‡ä¸åŒä¿¡æ¯çš„æ¨ç†èƒ½åŠ›ï¼ŒSDçš„å¤šæ­¥åŠ å™ªå»å™ªè¿‡ç¨‹ç±»ä¼¼äºæ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥ç›¸ç»“åˆï¼Œåœ¨ä¸åŒæ­¥éª¤ä¸­å¼•å…¥ä¸åŒç»´åº¦çš„ä¿¡æ¯ï¼Œåšåˆ°å¤šç»´ä¿¡æ¯çš„æ˜¾å¼ç†è§£åŠèåˆã€‚
  * åœ¨æ­£å‘ç¼–ç è¿‡ç¨‹ä¸­ï¼Œå…ˆå°†itemé€šè¿‡ç¼–ç å™¨ç¼–ç æˆå‘é‡åï¼Œé€æ¸åŠ å™ªè¿˜åŸæˆç™½å™ªå£°ï¼Œåœ¨åå‘å»å™ªè¿˜åŸè¿‡ç¨‹ä¸­ï¼Œåœ¨å™ªå£°é‡Œåˆ†é˜¶æ®µæ·»åŠ ç”¨æˆ·Queryä»¥åŠside infoä¿¡æ¯ï¼Œé€šè¿‡å¤šæ­¥è¿˜åŸçš„æ–¹å¼ï¼Œè¿˜åŸå‡ºQueryæ‰€ä»£è¡¨çš„ä¿¡æ¯ã€‚å¹¶æœ‰ä¸¤ä¸ªå¯¹æ¯”çš„æ“ä½œï¼Œä¸€æ˜¯ä¼ ç»Ÿçš„æ ·æœ¬Paiwiseå­¦ä¹ ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ–¹å¼æ‹‰è¿‘Queryä¸ç›¸ä¼¼Itemçš„è¡¨å¾ï¼›äºŒæ˜¯æˆ‘ä»¬è®¤ä¸ºç›¸ä¼¼itemæœ‰ç±»ä¼¼çš„æ ‡å‡†è¿‡ç¨‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ‹‰è¿‘ç›¸ä¼¼itemä¹‹é—´åœ¨æ‰©æ•£ä¸­é—´è¿‡ç¨‹çš„è¡¨å¾ï¼Œè¿™æ˜¯æ•´ä¸ªå»ºæ¨¡è¿‡ç¨‹ã€‚
  * åœ¨è¿˜åŸé˜¶æ®µï¼Œæˆ‘ä»¬ä¼šæ˜¾å¼è¿˜åŸä¸­é—´æ­¥éª¤å åŠ ç›¸å…³æ€§ä¿¡æ¯ã€ä¸ªæ€§åŒ–ä¿¡æ¯ï¼Œé€šè¿‡å¯¹æ¯”æ–¹å¼è®©æ¨¡å‹åœ¨è¿˜åŸè¿‡ç¨‹ä¸­æ˜¾å¼ç›¸å…³æ€§å’Œä¸ªæ€§åŒ–ä¿¡æ¯ï¼Œæœ€ååœ¨æ¨¡å‹ç»“æœé‡Œèƒ½çœ‹åˆ°ï¼Œå¦‚ä¸‹å›¾å·¦è¾¹æ˜¯ä¼ ç»Ÿçš„åˆ¤åˆ«å¼æ¨¡å‹é‡Œæœ€å¥½çš„ä¸€ä¸ªBaselineï¼Œå®ƒèƒ½å¤Ÿè¾ƒå¥½åŒºåˆ†Queryå’Œæ­£æ ·æœ¬ä¿¡æ¯ï¼Œä½†å®ƒåœ¨ä¸ªæ€§åŒ–æ ·æœ¬å’Œç›¸å…³æ€§æ ·æœ¬é‡ŒåŸºæœ¬æ˜¯æ··åœ¨ä¸€èµ·çš„ï¼Œé€šè¿‡è¿™ç§æ‰©æ•£æ¨¡å‹æ–¹å¼ï¼Œç›¸å…³æ€§æ ·æœ¬å’Œä¸ªæ€§åŒ–æ ·æœ¬å°±æœ‰ä¸€å®šç¨‹åº¦åŒºåˆ†å¼€æ¥çš„èƒ½åŠ›ã€‚

![img](https://p0.meituan.net/travelcube/eb8c6c661c488af1801306944b08b8ff683001.png)



#### [äº¬ä¸œ] Towards Personalized and Semantic Retrieval : An End-to-End Solution for E-commerce Search via Embedding Learning

> https://zhuanlan.zhihu.com/p/465504164



#### [ç¬¬å››èŒƒå¼] å¦‚ä½•æ„å»ºä¸€ä¸ªå¥½çš„ç”µå•†æœç´¢å¼•æ“ï¼Ÿ

> https://www.infoq.cn/article/ixobeuyc5q0b1dmhrwh7

* å•†ä¸šé€»è¾‘ï¼š
  * æœç´¢ï¼Œæ˜¯ç”µå•† app éå¸¸é‡è¦çš„ä¸€ä¸ªæµé‡å…¥å£ï¼Œå¯èƒ½å¾ˆå¤šç”µå•† app æ¥è‡ªæœç´¢çš„æµé‡éƒ½ä¼šå è¿‡åŠä»¥ä¸Šã€‚
  * æœç´¢è¡Œä¸ºèƒŒåæ˜¯å·¨å¤§çš„UVä»·å€¼

![img](./MLSys/5cb85359f486ff64c45d24790572daef.png)

#### ä¸ªæ€§åŒ–

* DataSQRL + Flink https://www.datasqrl.com/blog/personalized-ai-search/
  * deduplicate the stream to get the most recent version for each product.



* * 

### Other MLSys

* [ä»‹ç» Facebook æ¨èç³»ç»Ÿçš„æ–‡ç« ](https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/)
  * å¤šç›®æ ‡ MMoEã€åˆ†æ•°èåˆ
  * ç›¸æ¯”æŠ–éŸ³ï¼Œç¤¾äº¤å±æ€§æ›´å¼º
    * unread bumping logic: æœªæ¥å¾—åŠå±•ç°çš„ä¿¡æ¯
    * action-bumping logic: äº¤äº’è¿‡çš„ä¿¡æ¯æœ‰å†æ¬¡äº¤äº’
  * serving æµç¨‹
    * integrity processes
    * pass 0: lightweight model é€‰å‡º 500 æ¡
    * pass 1: ç²¾æ’ 500 æ¡
      * ã€ŠObservational data for heterogeneous treatment effects with application to recommender systemsã€‹
      * People with higher correlation gain more value from that specific event, as long as we make this method incremental and control for potential confounding variables.
    * pass 2: æ··æ’ï¼Œcontextual features, such as content-type diversity rules

#### Facebook

##### Tech Stack

* ç½‘ç«™ï¼Œç§»åŠ¨ç«¯app -- Product
* Thrift Web API -- Product Infra
* æ•°æ®åº“ï¼Œæ¶ˆæ¯é˜Ÿåˆ—ï¼Œæµæ‰¹æ•°æ®ç³»ç»Ÿï¼Œæ–‡ä»¶ç³»ç»Ÿï¼ŒéŸ³è§†é¢‘è½¬ç å­˜å‚¨ -- Generic Infra
* Ads Rankingï¼Œæ¨èç³»ç»Ÿ
* æœºå™¨å­¦ä¹ å¹³å°ï¼ˆPyTorchï¼‰-- Generic Infra
* è™šæ‹Ÿç°å®ï¼ŒåŠ å¯†è´§å¸ï¼ˆOculusï¼ŒLibraï¼‰-- Cutting Edge, Future Product

#### å¿«æ‰‹

* sim åŸºäºembeddingèšç±»

* ppnetï¼šbaseç½‘ç»œä¸bpï¼Œgateç‹¬ç«‹ç½‘ç»œç‹¬ç«‹å­¦ï¼Œä¸å½±å“base embedding

* äº’åŠ¨ç‰¹å¾ç¨€ç–æ€ä¹ˆåŠï¼šmmoeç”±æ—¶é•¿ä¸»å¯¼ï¼Œæ”¹è¿›æ¯”è¾ƒä¼ ç»Ÿï¼Œä¸»è¦æ–¹å‘æ˜¯ç¨€ç–æ ·æœ¬åŠ æƒã€taskç½‘ç»œè®¾è®¡

* logæ—¶é—´æˆ³ ä¹Ÿæ˜¯ positon embedding ç¦»æ•£åŒ–

#### ç¾å›¢

* ç¾å›¢ä¼˜é€‰ å¼ äºšå³°ï¼šæ¨èç³»ç»Ÿç»“åˆå› æœæ¨æ–­

##### [TensorFlow åœ¨æ¨èç³»ç»Ÿä¸­çš„åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–å®è·µ](https://tech.meituan.com/2021/12/09/meituan-tensorflow-in-recommender-systems.html)

* tfåŸç”Ÿæ¶æ„çš„æŒ‘æˆ˜
  * æ‰€æœ‰å‚æ•°éƒ½æ˜¯ç”¨ Variable è¡¨è¾¾ï¼Œ å¯¹äºç™¾äº¿ä»¥ä¸Šçš„ç¨€ç–å‚æ•°å¼€è¾Ÿäº†å¤§é‡çš„å†…å­˜ï¼Œé€ æˆäº†èµ„æºçš„æµªè´¹ï¼›
  * åªæ”¯æŒç™¾çº§åˆ« Worker çš„åˆ†å¸ƒå¼æ‰©å±•ï¼Œå¯¹ä¸Šåƒ Worker çš„æ‰©å±•æ€§è¾ƒå·®ï¼›
  * ç”±äºä¸æ”¯æŒå¤§è§„æ¨¡ç¨€ç–å‚æ•°åŠ¨æ€æ·»åŠ ã€åˆ é™¤ï¼Œå¢é‡å¯¼å‡ºï¼Œå¯¼è‡´æ— æ³•æ”¯æŒ Online Learningï¼›
  * å¤§è§„æ¨¡é›†ç¾¤è¿è¡Œæ—¶ï¼Œä¼šé‡åˆ°æ…¢æœºå’Œå®•æœºï¼›ç”±äºæ¡†æ¶å±‚ä¸èƒ½å¤„ç†ï¼Œä¼šå¯¼è‡´ä»»åŠ¡è¿è¡Œå¼‚å¸¸
* æ ¸å¿ƒé—®é¢˜ï¼šæ— æ³•ä¸€ç›´æ¨ªå‘æ‰©PS
  * å¢åŠ æ‰‡å‡ºå¸¦æ¥çš„é“¾è·¯å»¶è¿ŸæŸå¤±è¶…è¿‡äº†åŠ PSç®—åŠ›å¹¶å‘çš„æ”¶ç›Š
  * ä¼˜åŒ–çš„æ ¸å¿ƒéš¾ç‚¹åœ¨äºï¼š**å¦‚ä½•åœ¨æœ‰é™çš„PSå®ä¾‹ä¸‹ï¼Œè¿›è¡Œåˆ†å¸ƒå¼è®¡ç®—çš„ä¼˜åŒ–**ã€‚
* è‡ªç ”HashTable
  * HashTableçš„å¤§å°å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¼¸ç¼©ï¼Œé¿å…äº†å¼€è¾Ÿå†—ä½™çš„å­˜å‚¨ç©ºé—´ï¼ŒåŒæ—¶ç”¨æˆ·æ— éœ€å…³æ³¨ç”³è¯·å¤§å°ï¼Œä»è€Œé™ä½äº†ä½¿ç”¨æˆæœ¬ã€‚
  * é’ˆå¯¹HashTableæ–¹æ¡ˆå®æ–½äº†ä¸€ç³»åˆ—å®šåˆ¶ä¼˜åŒ–ï¼Œè®­ç»ƒé€Ÿåº¦ç›¸æ¯”Variableæœ‰äº†å¾ˆå¤§çš„æé«˜ï¼Œå¯ä»¥è¿›è¡Œåƒäº¿è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒï¼Œæ‰©å±•æ€§è¾ƒå¥½ã€‚
  * å¾—ç›Šäºç¨€ç–å‚æ•°çš„åŠ¨æ€ä¼¸ç¼©ï¼Œæˆ‘ä»¬åœ¨æ­¤åŸºç¡€ä¸Šæ”¯æŒäº†Online Learningã€‚
  * APIè®¾è®¡ä¸Šä¿æŒä¸ç¤¾åŒºç‰ˆæœ¬å…¼å®¹ï¼Œåœ¨ä½¿ç”¨ä¸Šå‡ ä¹ä¸åŸç”ŸVariableä¸€è‡´ï¼Œå¯¹æ¥æˆæœ¬æä½ã€‚
  * ä¼˜åŒ–
    * ç¨€ç–åŸŸå‚æ•°èšåˆï¼šembã€momentumã€vã€cntå…±ç”¨ä¸€å¼ è¡¨
    * åœ¨åƒäº¿è§„æ¨¡ä¸‹TBBConcurrentHashTableæ¯”åŸç”ŸMutableDenseHashTableè®­ç»ƒé€Ÿåº¦ä¸Šå¿«äº†3å€
    * HashTable BucketPool
* åˆ†å¸ƒå¼è´Ÿè½½å‡è¡¡ä¼˜åŒ–
  * æŠŠæ‰€æœ‰ç¨€ç–å‚æ•°å’Œå¤§çš„ç¨ å¯†å‚æ•°è‡ªåŠ¨ã€å‡åŒ€çš„åˆ‡åˆ†åˆ°æ¯ä¸ªPSä¸Š
  * åŸç”ŸAdamä¼˜åŒ–å™¨ï¼Œå®ç°å¯¼è‡´PSè´Ÿè½½ä¸å‡è¡¡
* é€šä¿¡ä¼˜åŒ–ï¼ˆRDMAï¼‰
  * https://github.com/tensorflow/networking/pull/38/files
  * Memory Registrationä¼˜åŒ–
    * åœ¨RDMAä¼ è¾“æ•°æ®æ—¶ï¼Œéœ€è¦æå‰å¼€è¾Ÿå†…å­˜ç©ºé—´å¹¶å°†å…¶æ³¨å†Œåˆ°ç½‘å¡è®¾å¤‡ä¸Šï¼ˆMemory Registrationè¿‡ç¨‹ï¼Œä¸‹ç§°MRï¼‰ï¼Œä½¿å¾—è¿™ç‰‡ç©ºé—´å¯ä»¥è¢«ç½‘å¡ç›´æ¥æ“ä½œ
    * 10MB ~ 2ms
    * é—®é¢˜ï¼šç¤¾åŒºç‰ˆTensorflow RDMAå®ç°ï¼ŒTensoråˆ›å»ºä¾æ—§æ²¿ç”¨äº†ç»Ÿä¸€çš„BFC Allocatorï¼Œå¹¶å°†æ‰€æœ‰åˆ›å»ºçš„Tensoréƒ½æ³¨å†Œåˆ°MRä¸Š
    * ä¼˜åŒ–ï¼šä»…å¯¹è¿™äº›è·¨èŠ‚ç‚¹é€šä¿¡çš„Tensorè¿›è¡ŒMRæ³¨å†Œ
  * RDMAé™æ€åˆ†é…å™¨
    * reqé—´å¤ç”¨MR
    * shapeå’Œtensoræ‰“åŒ…åè®®ï¼Œé¿å…äº†åŸç”Ÿå®ç°ä¸­å› Tensorçš„Shapeå˜åŒ–è€Œäº§ç”Ÿçš„å¤šæ¬¡åå•†è¿‡ç¨‹
    * Allocation Analysisæ¨¡å—
      * åœ¨è®­ç»ƒå¼€å§‹çš„ä¸€æ®µæ—¶é—´ï¼Œæˆ‘ä»¬ä¼šå¯¹åˆ†é…çš„å†å²æ•°æ®è¿›è¡Œåˆ†æï¼Œä»¥å¾—åˆ°ä¸€ä¸ªå®é™…é¢„å¼€è¾ŸMRå¤§å°ä»¥åŠå„ä¸ªTensorçš„é¢„ç•™ç©ºé—´å¤§å°ã€‚ç„¶åæˆ‘ä»¬ä¼šæš‚åœè®­ç»ƒçš„è¿›ç¨‹ï¼Œå¯åŠ¨Allocatorçš„æ„é€ è¿‡ç¨‹ï¼ŒåŒ…æ‹¬MRçš„åˆ›å»ºä»¥åŠé€šä¿¡åŒç«¯çš„ä¿¡æ¯åŒæ­¥ã€‚åˆ©ç”¨ç›¸å…³ä¿¡æ¯æ„é€ MR Info Mapï¼Œè¿™ä¸ªMapçš„Keyæ˜¯ä¼ è¾“Tensorçš„å”¯ä¸€æ ‡è®°ï¼ˆParsedKeyï¼Œè®¡ç®—å›¾åˆ‡å›¾æ—¶ç¡®å®šï¼‰ï¼ŒInfoç»“æ„ä½“ä¸­åŒ…å«äº†æœ¬åœ°åœ°å€æŒ‡é’ˆã€offsetå¤§å°ã€ibv_send_wrç›¸å…³ä¿¡æ¯ç­‰ã€‚ç„¶åæ¢å¤è®­ç»ƒã€‚
  * Multi RequestBufferä¸CQè´Ÿè½½å‡è¡¡
  * Send-Driven & Rendezvous-Bypass

![å›¾10 MRé™æ€åˆ†é…å™¨](https://p1.meituan.net/travelcube/bc3415b2740a70d030c6464715676f4562230.png)

* Embedding Pipeling
  * è¿™ä¸ªè®¾è®¡æœ‰ç‚¹å‰å®³ã€‚ã€‚ã€‚å®Œå…¨éšè—embedding fetchçš„ç›¸å…³å»¶æ—¶
  * å‰ææ˜¯stalenessæŸå¤±å¯æ§

![å›¾16 Embeddingæµæ°´çº¿æ¶æ„æµç¨‹å›¾](https://p0.meituan.net/travelcube/e4b982ebcaa8b98f1bf370fb43af4cda237614.png)

* Unique&DynamicPartitionç®—å­èåˆ
  * uniqueç®—å­çš„ç¼ºç‚¹ï¼šå†…éƒ¨ä½¿ç”¨çš„å†…å­˜åˆ†é…ç­–ç•¥è¾ƒä¸ºä½æ•ˆã€‚ä½¿ç”¨äº†ä¸¤å€è¾“å…¥å‚æ•°ï¼ˆEmbedding IDï¼‰çš„å¤§å°è¿›è¡Œå†…å­˜åˆ†é…ï¼Œä½†ç”±äºè¾“å…¥å‚æ•°è¾ƒå¤§ï¼Œè€Œä¸”é‡å¤ç‡é«˜ï¼Œå¯¼è‡´HashTableåˆ›å»ºè¿‡å¤§ä¸”éå¸¸ç¨€ç–ã€‚å‡ ä¹æ¯æ¬¡æ’å…¥éƒ½ä¼šäº§ç”Ÿä¸€æ¬¡minor_page_faultï¼Œå¯¼è‡´HashTableæ€§èƒ½ä¸‹é™
  * Uniqueå’ŒDynamic Partitionç®—å­å­˜åœ¨å†—ä½™æ•°æ®éå†



##### [TensorFlowåœ¨ç¾å›¢å¤–å–æ¨èåœºæ™¯çš„GPUè®­ç»ƒä¼˜åŒ–å®è·µ](https://tech.meituan.com/2022/03/24/tensorflow-gpu-training-optimization-practice-in-meituan-waimai-recommendation-scenarios.html)

* **GPUæœåŠ¡å™¨ç‰¹ç‚¹**
  - **GPUå¡ç®—åŠ›å¾ˆå¼ºï¼Œä½†æ˜¾å­˜ä»æœ‰é™**ï¼šå¦‚æœè¦å……åˆ†å‘æŒ¥GPUç®—åŠ›ï¼Œéœ€è¦æŠŠGPUè®¡ç®—ç”¨åˆ°çš„å„ç§æ•°æ®æå‰æ”¾ç½®åˆ°æ˜¾å­˜ä¸­ã€‚è€Œä»2016å¹´~2020å¹´ï¼ŒNVIDIA Tesla GPUå¡[5]è®¡ç®—èƒ½åŠ›æå‡äº†10å€ä»¥ä¸Šï¼Œä½†æ˜¾å­˜å¤§å°åªæå‡äº†3å€å·¦å³ã€‚
  - **å…¶å®ƒç»´åº¦èµ„æºå¹¶ä¸æ˜¯å¾ˆå……è¶³**ï¼šç›¸æ¯”GPUç®—åŠ›çš„æå‡é€Ÿåº¦ï¼Œå•æœºçš„CPUã€ç½‘ç»œå¸¦å®½çš„å¢é•¿é€Ÿåº¦è¾ƒæ…¢ï¼Œå¦‚æœé‡åˆ°è¿™ä¸¤ç±»èµ„æºè´Ÿè·è¾ƒé‡çš„æ¨¡å‹ï¼Œå°†æ— æ³•å……åˆ†å‘æŒ¥GPUçš„èƒ½åŠ›ï¼ŒGPUæœåŠ¡å™¨ç›¸æ¯”CPUæœåŠ¡å™¨çš„æ€§ä»·æ¯”ä¸ä¼šå¤ªé«˜ã€‚

* æŒ‘æˆ˜
  * **æ•°æ®æµç³»ç»Ÿ**ï¼šå¦‚ä½•åˆ©ç”¨å¥½å¤šç½‘å¡ã€å¤šè·¯CPUï¼Œå®ç°é«˜æ€§èƒ½çš„æ•°æ®æµæ°´çº¿ï¼Œè®©æ•°æ®çš„ä¾›ç»™å¯ä»¥è·Ÿä¸ŠGPUçš„æ¶ˆè´¹é€Ÿåº¦ã€‚
  * **æ··åˆå‚æ•°è®¡ç®—**ï¼šå¯¹äºå¤§è§„æ¨¡ç¨€ç–å‚æ•°ï¼ŒGPUæ˜¾å­˜ç›´æ¥è£…ä¸ä¸‹çš„æƒ…å†µï¼Œå¦‚ä½•å……åˆ†åˆ©ç”¨GPUé«˜ç®—åŠ›ã€GPUå¡é—´çš„é«˜å¸¦å®½ï¼Œå®ç°ä¸€å¥—å¤§è§„æ¨¡ç¨€ç–å‚æ•°çš„è®¡ç®—ï¼ŒåŒæ—¶è¿˜éœ€è¦å…¼é¡¾ç¨ å¯†å‚æ•°çš„è®¡ç®—ã€‚
* ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°
  * settingï¼šå•æœºå¤šå¡ã€æ”¯æŒ100Gæ¨¡å‹
  * å‡å°‘å‚æ•°çš„æ€è·¯
    * **å»äº¤å‰ç‰¹å¾**ï¼šäº¤å‰ç‰¹å¾ç”±å•ç‰¹å¾é—´åšç¬›å¡å°”ç§¯äº§ç”Ÿï¼Œè¿™ä¼šç”Ÿæˆå·¨å¤§çš„ç‰¹å¾IDå–å€¼ç©ºé—´å’Œå¯¹åº”Embeddingå‚æ•°è¡¨ã€‚æ·±åº¦é¢„ä¼°æ¨¡å‹å‘å±•è‡³ä»Šï¼Œå·²ç»æœ‰å¤§é‡çš„æ–¹æ³•é€šè¿‡æ¨¡å‹ç»“æ„æ¥å»ºæ¨¡å•ç‰¹å¾é—´çš„äº¤äº’ï¼Œé¿å…äº†äº¤å‰ç‰¹å¾é€ æˆçš„Embeddingè§„æ¨¡è†¨èƒ€ï¼Œå¦‚FMç³»åˆ—[16]ã€AutoInt[17]ã€CAN[18]ç­‰ã€‚
    * **ç²¾ç®€ç‰¹å¾**ï¼šç‰¹åˆ«æ˜¯åŸºäºNASçš„æ€è·¯ï¼Œä»¥è¾ƒä½çš„è®­ç»ƒæˆæœ¬å®ç°æ·±åº¦ç¥ç»ç½‘ç»œè‡ªé€‚åº”ç‰¹å¾é€‰æ‹©ï¼Œå¦‚Dropout Rank[19]å’ŒFSCD[20]ç­‰å·¥ä½œã€‚
    * **å‹ç¼©Embeddingå‘é‡æ•°**ï¼šå¯¹ç‰¹å¾å–å€¼è¿›è¡Œå¤åˆIDç¼–ç å’ŒEmbeddingæ˜ å°„ï¼Œä»¥è¿œå°äºç‰¹å¾å–å€¼ç©ºé—´çš„Embeddingå‘é‡æ•°ï¼Œæ¥å®ç°ä¸°å¯Œçš„ç‰¹å¾Embeddingè¡¨è¾¾ï¼Œå¦‚Compositional Embedding[14]ã€Binary Code Hash Embedding[21]ç­‰å·¥ä½œã€‚
    * **å‹ç¼©Embeddingå‘é‡ç»´åº¦**ï¼šä¸€ä¸ªç‰¹å¾Embeddingå‘é‡çš„ç»´åº¦å†³å®šäº†å…¶è¡¨å¾ä¿¡æ¯çš„ä¸Šé™ï¼Œä½†æ˜¯å¹¶éæ‰€æœ‰çš„ç‰¹å¾å–å€¼éƒ½æœ‰é‚£ä¹ˆå¤§çš„ä¿¡æ¯é‡ï¼Œéœ€è¦Embeddingè¡¨è¾¾ã€‚å› æ­¤ï¼Œå¯ä»¥æ¯ä¸€ä¸ªç‰¹å¾å€¼è‡ªé€‚åº”çš„å­¦ä¹ ç²¾ç®€Embeddingç»´åº¦ï¼Œä»è€Œå‹ç¼©å‚æ•°æ€»é‡ï¼Œå¦‚AutoDim[22]å’ŒAMTL[23]ç­‰å·¥ä½œã€‚
    * **é‡åŒ–å‹ç¼©**ï¼šä½¿ç”¨åŠç²¾åº¦ç”šè‡³int8ç­‰æ›´æ¿€è¿›çš„æ–¹å¼ï¼Œå¯¹æ¨¡å‹å‚æ•°åšé‡åŒ–å‹ç¼©ï¼Œå¦‚DPQ[24]å’ŒMGQE[25]ã€‚

![å›¾1 ç³»ç»Ÿæ¶æ„](https://p0.meituan.net/travelcube/8efcb9ba1bb1a0f72f6b35366130192c907575.png)

![å›¾2 è¿›ç¨‹å†…éƒ¨æ‰§è¡Œé€»è¾‘](https://p1.meituan.net/travelcube/d5213c915f8d3e192e8e9987736c8bd2933367.png)

* ç³»ç»Ÿå®ç°
  * tf + horovodåŸç”Ÿ
  * æ•°æ®ã€è®¡ç®—ã€é€šä¿¡è§£è€¦
* embeddingå±‚ï¼š
  * å¤§çš„fcç”¨alltoallv
    * å‰å‘æ—¶ä¸¤æ¬¡å¡é—´alltoall
  * æ¢¯åº¦ï¼šå°çš„fc AllGatherï¼Œdense allreduce
    * å°=ç¨ å¯†="dense sparse"ï¼Œdense sparse emb table = tfåŸç”Ÿvariable
  * åœ¨cuCollectionsçš„GPU HashTableåŸºç¡€ä¸Šå®ç°äº†ç‰¹æ®Šæ¥å£ï¼ˆfind_or_insertï¼‰ï¼Œå¯¹å¤§è§„æ¨¡è¯»å†™æ€§èƒ½è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç„¶åå°è£…åˆ°äº†TensorFlowä¸­ï¼Œå¹¶åœ¨å…¶ä¸Šå®ç°äº†ä½é¢‘è¿‡æ»¤çš„åŠŸèƒ½ï¼Œèƒ½åŠ›ä¸Šå¯¹é½CPUç‰ˆæœ¬çš„ç¨€ç–å‚æ•°å­˜å‚¨æ¨¡å—
* æ•°æ®å±‚ä¼˜åŒ–
  * æ ·æœ¬æ‹‰å–ä¼˜åŒ–ï¼šper numaã€å¤šç½‘å¡ã€å¤šå¡ç‹¬ç«‹shared memory
  * ç‰¹å¾è§£æä¼˜åŒ–ï¼šSIMDä¼˜åŒ–protobuf::CodedInputStream::ReadVarint64Fallback
  * MemcpyH2Dæµæ°´çº¿ï¼š
    * PipelineDataset
    * CPUå†…å­˜éœ€è¦ä½¿ç”¨Pinned Memory
  * ç¡¬ä»¶è°ƒä¼˜
    * åœ¨ç½‘ç»œä¼ è¾“æ–¹é¢ï¼Œä¸ºäº†å‡å°‘ç½‘ç»œåè®®æ ˆå¤„ç†å¼€é”€ï¼Œæé«˜æ•°æ®æ‹·è´çš„æ•ˆç‡ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–ç½‘å¡é…ç½®ï¼Œå¼€å¯LROï¼ˆLarge-Receive-Offloadï¼‰ã€TC Flowerçš„ç¡¬ä»¶å¸è½½ã€Tx-Nocache-Copyç­‰ç‰¹æ€§ï¼Œæœ€ç»ˆç½‘ç»œå¸¦å®½æå‡äº†17%ã€‚
    * åœ¨CPUæ€§èƒ½ä¼˜åŒ–æ–¹é¢ï¼Œç»è¿‡æ€§èƒ½profilingåˆ†æï¼Œå‘ç°å†…å­˜å»¶è¿Ÿå’Œå¸¦å®½æ˜¯ç“¶é¢ˆã€‚äºæ˜¯æˆ‘ä»¬å°è¯•äº†3ç§NPSé…ç½®ï¼Œç»¼åˆä¸šåŠ¡åœºæ™¯å’ŒNUMAç‰¹æ€§ï¼Œé€‰æ‹©äº†NPS2ã€‚æ­¤å¤–ï¼Œç»“åˆå…¶ä»–BIOSé…ç½®ï¼ˆä¾‹å¦‚APBDISï¼ŒP-stateç­‰ï¼‰ï¼Œå¯ä»¥å°†å†…å­˜å»¶è¿Ÿé™ä½8%ï¼Œå†…å­˜å¸¦å®½æå‡6%ã€‚
* è®¡ç®—å±‚ä¼˜åŒ–
  * Embedding Pipeline
    * åœ¨GPUåœºæ™¯ä¸­ï¼ŒEGã€MGæ˜¯åœ¨åŒä¸€ä¸ªGPU Streamä¸Šæ‰§è¡ŒCUDA Kernelçš„ï¼Œæˆ‘ä»¬å°è¯•è¿‡EGã€MGåˆ†åˆ«åœ¨ç‹¬ç«‹çš„GPU Streamä¸Šæ‰§è¡Œï¼Œæ€§èƒ½ä¼šå˜å·®ï¼Œæ·±å±‚åŸå› ä¸CUDAåº•å±‚å®ç°æœ‰å…³ï¼Œè¿™ä¸ªé—®é¢˜æœ¬èº«è¿˜åœ¨ç­‰å¾…è§£å†³
  * ç®—å­ä¼˜åŒ–åŠXLA
    * ä»¥Uniqueç®—å­ä¸ºä¾‹ï¼ŒåŸç”ŸTensorFlowçš„Uniqueç®—å­è¦æ±‚è¾“å‡ºå…ƒç´ çš„é¡ºåºä¸è¾“å…¥å…ƒç´ çš„é¡ºåºä¸€è‡´ï¼Œè€Œåœ¨å®é™…åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬ä¿®æ”¹äº†Uniqueç®—å­çš„GPUå®ç°ï¼Œå‡å°‘äº†å› è¾“å‡ºæœ‰åºå¯¼è‡´çš„é¢å¤–æ‰§è¡Œçš„GPU Kernel
    * ç¼“è§£XLAå¯¹åŠ¨æ€shapeçš„æ”¯æŒé—®é¢˜
      * **å±€éƒ¨ä¼˜åŒ–**ï¼šå¯¹äºæˆ‘ä»¬æ‰‹åŠ¨å¼•å…¥çš„åŠ¨æ€shapeç®—å­ï¼ˆå¦‚Uniqueï¼‰ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å­å›¾æ ‡è®°ï¼Œä¸æ‰§è¡ŒXLAç¼–è¯‘ï¼ŒXLAåªä¼˜åŒ–å¯ä»¥ç¨³å®šåŠ é€Ÿçš„å­å›¾ã€‚
      * **OOMå…œåº•**ï¼šXLAä¼šæ ¹æ®ç®—å­çš„typeã€input typeã€shapeç­‰ä¿¡æ¯ï¼Œç¼“å­˜ç¼–è¯‘ä¸­é—´ç»“æœï¼Œé¿å…é‡å¤ç¼–è¯‘ã€‚ç„¶è€Œç”±äºç¨€ç–åœºæ™¯ä»¥åŠGPUæ¶æ„å®ç°çš„ç‰¹æ®Šæ€§ï¼Œå¤©ç„¶å­˜åœ¨Uniqueã€DynamicPartitionç­‰Output shapeæ˜¯åŠ¨æ€çš„ç®—å­ï¼Œè¿™å°±å¯¼è‡´è¿™äº›ç®—å­ä»¥åŠè¿æ¥åœ¨è¿™äº›ç®—å­ä¹‹åçš„ç®—å­ï¼Œåœ¨æ‰§è¡ŒXLAç¼–è¯‘æ—¶æ— æ³•å‘½ä¸­XLAç¼“å­˜è€Œé‡æ–°ç¼–è¯‘ï¼Œæ–°çš„ç¼“å­˜è¶Šæ¥è¶Šå¤šï¼Œè€Œæ—§çš„ç¼“å­˜ä¸ä¼šè¢«é‡Šæ”¾ï¼Œæœ€ç»ˆå¯¼è‡´CPUå†…å­˜OOMã€‚æˆ‘ä»¬åœ¨XLAå†…éƒ¨å®ç°äº†LRUCacheï¼Œä¸»åŠ¨æ·˜æ±°æ‰æ—§çš„XLAç¼“å­˜ï¼Œé¿å…OOMçš„é—®é¢˜ã€‚
      * **Const Memcpyæ¶ˆé™¤**ï¼šXLAåœ¨ä½¿ç”¨TF_HLOé‡å†™TensorFlowç®—å­æ—¶ï¼Œå¯¹ä¸€äº›ç¼–è¯‘æœŸå·²å›ºå®šçš„æ•°æ®ä¼šæ‰“ä¸ŠConstæ ‡è®°ï¼Œç„¶è€Œè¿™äº›Constç®—å­çš„Outputåªèƒ½å®šä¹‰åœ¨Hostç«¯ï¼Œä¸ºäº†å°†Hostç«¯çš„Outputé€ç»™Deviceç«¯éœ€è¦å†åŠ ä¸€æ¬¡MemcpyH2Dï¼Œè¿™å°±å ç”¨äº†TensorFlowåŸæœ‰çš„H2D Streamï¼Œå½±å“æ ·æœ¬æ•°æ®æå‰æ‹·è´åˆ°GPUç«¯ã€‚ç”±äºXLAçš„Const Outputåœ¨ç¼–è¯‘æœŸå·²ç»å›ºåŒ–ï¼Œå› æ­¤æ²¡æœ‰å¿…è¦æ¯ä¸€æ­¥éƒ½åšä¸€æ¬¡MemcpyH2Dï¼Œæˆ‘ä»¬å°†Deviceç«¯çš„Outputç¼“å­˜ä¸‹æ¥ï¼Œåç»­ä½¿ç”¨è¯¥Outputæ—¶ï¼Œç›´æ¥ä»ç¼“å­˜ä¸­è¯»å–ï¼Œé¿å…å¤šä½™çš„MemcpyH2Dã€‚
* é€šä¿¡å±‚ä¼˜åŒ–
  * å‘ç°å¡é—´é€šä¿¡ï¼ˆAllToAllã€AllReduceã€AllGatherç­‰ï¼‰åå•†çš„æ—¶é—´è¿œè¿œé«˜äºæ•°æ®ä¼ è¾“çš„æ—¶é—´
  * æ€€ç–‘ä¸åŒå¡ä¸Šç®—å­è°ƒåº¦çš„ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´äº†å„å¼ å¡å‘èµ·é€šä¿¡çš„æ—¶åˆ»ä¸åŒï¼Œå¹¶æœ€ç»ˆå¯¼è‡´äº†é€šä¿¡åå•†æ—¶é—´è¿‡é•¿
  * è§£å†³æ–¹æ¡ˆ
    * åˆå¹¶ç›¸åŒdim sizeçš„hashtableï¼Œå‡å°‘å¡é—´é€šä¿¡æ¬¡æ•°
    * Variable Fusion
      * éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒTensorFlowçš„Variableåˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯æ¯ä¸ªStepå…¨éƒ¨å‚æ•°å€¼éƒ½å‚ä¸è®­ç»ƒçš„Dense Variableï¼Œå¦‚MLPçš„Weightï¼›å¦ä¸€ç§æ˜¯ä¸“é—¨ç”¨äºembedding_lookupçš„Variableï¼Œæ¯ä¸ªStepåªæœ‰éƒ¨åˆ†å€¼å‚ä¸è®­ç»ƒï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºSparse Variableã€‚å¯¹äºå‰è€…ï¼ŒåšVariableåˆå¹¶ä¸ä¼šå½±å“åˆ°ç®—æ³•æ•ˆæœã€‚è€Œå¯¹äºåè€…ï¼Œå®ƒåå‘æ¢¯åº¦æ˜¯IndexedSliceså¯¹è±¡ï¼Œå¡é—´åŒæ­¥é»˜è®¤èµ°çš„æ˜¯AllGatheré€šä¿¡ï¼Œå¦‚æœä¸šåŠ¡æ¨¡å‹ä¸­å¯¹äºSparse Variablesçš„ä¼˜åŒ–é‡‡ç”¨çš„æ˜¯Lazyä¼˜åŒ–å™¨ï¼Œå³æ¯ä¸ªStepåªä¼˜åŒ–æ›´æ–°Variableä¸­çš„æŸäº›è¡Œï¼Œæ­¤æ—¶å¯¹Sparse Variablesåšåˆå¹¶ï¼Œä¼šå¯¼è‡´å…¶åå‘æ¢¯åº¦ä»IndexedSliceså¯¹è±¡è½¬ä¸ºTensorå¯¹è±¡ï¼Œå¡é—´åŒæ­¥å˜æˆAllReduceè¿‡ç¨‹ï¼Œå°±å¯èƒ½ä¼šå½±å“åˆ°ç®—æ³•æ•ˆæœã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€å…³ï¼Œç”±ä¸šåŠ¡å»æ§åˆ¶æ˜¯å¦åˆå¹¶Sparse Variablesã€‚ç»è¿‡æˆ‘ä»¬çš„å®æµ‹ï¼Œåœ¨æŸæ¨èæ¨¡å‹ä¸Šåˆå¹¶Sparse Variablesä¼šæé«˜5ï½10%çš„è®­ç»ƒæ€§èƒ½ï¼Œè€Œå¯¹å®é™…ä¸šåŠ¡æ•ˆæœçš„å½±å“åœ¨ä¸€ä¸ªåƒåˆ†ç‚¹ä»¥å†…ã€‚
* è®­ç»ƒæ•ˆæœ
  * å¤§Batchä¸‹è®­ç»ƒè¶…å‚è°ƒä¼˜çš„é—®é¢˜[26,27]ï¼šåœ¨ä¿è¯Epochä¸å˜çš„å‰æä¸‹ï¼Œæ‰©å¤§Batch Sizeä¼šå¯¼è‡´å‚æ•°æœ‰æ•ˆæ›´æ–°æ¬¡æ•°å‡å°‘ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹è®­ç»ƒçš„æ•ˆæœå˜å·®
  * Linear Scaling Rule[28]çš„åŸåˆ™æŒ‡å¯¼è°ƒæ•´å­¦ä¹ ç‡
  * ä½¿ç”¨å­¦ä¹ ç‡Warmupç­‰æ›´å¤æ‚çš„è®­ç»ƒç­–ç•¥[29]



#### DeepRec

[Github](https://github.com/alibaba/DeepRec)

[Docå…¥å£](https://deeprec.readthedocs.io/zh/latest/)

* ç¨€ç–åŠŸèƒ½
  * è‡ªé€‚åº”åŠ¨æ€å¼¹æ€§ç‰¹å¾
  * ç‰¹å¾æ·˜æ±°åŠå‡†å…¥
  * åŸºäºç‰¹å¾é¢‘ç‡çš„åŠ¨æ€å¼¹æ€§ç»´åº¦ï¼ˆFAEï¼‰

* å¼‚æ­¥è®­ç»ƒæ¡†æ¶ StarServer
  * é€šä¿¡åè®®ã€zerocopy
  * åŸºäºå›¾æ‹“æ‰‘åºçš„å›¾Fusion
  * Run To Completion and Lockless Graph Execution

* åŒæ­¥è®­ç»ƒæ¡†æ¶ hybridbackend
  * embeddingå±‚ï¼šå¤§çš„fcç”¨alltoallvï¼Œå°çš„fcç”¨allreduce
    * å°=ç¨ å¯†="dense sparse"ï¼Œdense emb table = tfåŸç”Ÿvariable
    * æ€è·¯å‚è€ƒmeituan https://discuss.tf.wiki/t/topic/2341

* prmalloc
  * æ± å­å…±äº«ï¼Œä¸å†ç”¨ TLS cacheï¼Œå› ä¸ºopå¯èƒ½æ˜¯ä¸åŒçº¿ç¨‹è¿è¡Œ

* ä¸šåŠ¡ç›¸å…³ä¼˜åŒ–
  * userç‰¹å¾ä¼˜åŒ–

* å›¾ä¼˜åŒ–-smartstage
* embedding store
  * embeddingå¤šçº§æ··åˆå­˜å‚¨ï¼šcpu cache dram pmem ssd
    * å¤šçº§æ··åˆå­˜å‚¨èƒ½æ”¯æŒå•æœºservingï¼Œä¸»è¦ä»ssdè¯»

#### Others

* [å½“æˆ‘ä»¬åœ¨è®¾è®¡æ¨èåœºæ™¯è®­ç»ƒç³»ç»Ÿ](https://zhuanlan.zhihu.com/p/376974245) è®¨è®ºäº†æ¨èç³»ç»Ÿæ ‡å‡†åŒ–çš„æ€è·¯
  * é…ç½®é©±åŠ¨ä¸ä»£ç é©±åŠ¨ï¼ŒæœŸæœ›å°½é‡ç»“åˆä¸Šè¿°ä¸¤ç§æ–¹æ¡ˆçš„ä¼˜ç‚¹ï¼Œå³ï¼š
    * å·¥ä½œæµæ•´ä½“è¿˜æ˜¯é€šè¿‡é…ç½®æ–‡ä»¶é©±åŠ¨ï¼Œç³»ç»Ÿä¼šå°†è§£æå¥½çš„é…ç½®æ–‡ä»¶ï¼Œæ ¹æ®ä¸åŒçš„å¤„ç†æ¨¡å—ï¼Œå°†å¯¹åº”éƒ¨åˆ†çš„é…ç½®ä¼ é€’ç»™è¿‡å»
    * å¯¹äºå¤§éƒ¨åˆ†çš„ä»»åŠ¡ï¼ŒåŸºæœ¬é€»è¾‘éƒ½æ˜¯é€šç”¨çš„ï¼Œå¦‚ä¸‹å›¾ä¸­çš„é»„è‰²æ¨¡å—ï¼Œå¯¹äºè¿™ä¸€éƒ¨åˆ†æä¾›é€šç”¨çš„å®ç°ã€‚å½“ç„¶ï¼Œç”±äºä»£ç æ˜¯å†…éƒ¨å¼€æºçš„ï¼Œå³ä¾¿æœ‰å°‘é‡ç‰¹æ®Šéœ€æ±‚ä¹Ÿå¯ä»¥è‡ªè¡Œå¼€å‘æäº¤
    * å¯¹äºè‡ªç”±åº¦è¾ƒé«˜çš„æ¨¡å—ï¼Œä¸»è¦æŒ‡çš„æ˜¯ä¸Šé¢è¯´çš„â€œæ¨¡å‹æ„å»ºâ€éƒ¨åˆ†ï¼Œåˆ™ç³»ç»Ÿæä¾›æŠ½è±¡çˆ¶ç±»ï¼ŒåŒ…å«åŸºç¡€çš„åŠŸèƒ½ã€‚è‡ªå®šä¹‰æ¨¡å‹æ—¶ï¼Œé€šè¿‡ç»§æ‰¿çˆ¶ç±»ï¼Œå¹¶é‡å†™â€œå‰å‘ä¼ æ’­â€ç­‰æ–¹æ³•å³å¯
  * è®­ç»ƒç¯å¢ƒdockeråŒ–

### æµ‹è¯•

- PyTorch å•æµ‹æ¡†æ¶

  - unittest + pytest
  - **Golden Test**

- TensorFlow Extended çš„æµ‹è¯•æ¨¡å¼

  - **Hermetic Testing**ï¼šä½¿ç”¨ Docker éš”ç¦»æµ‹è¯•ç¯å¢ƒ
  - **Artifact** **éªŒè¯é“¾**ï¼šå¯¹ checkpointã€saved_model ç­‰ä¸­é—´äº§ç‰©è¿›è¡Œç­¾åéªŒè¯

- HuggingFace çš„æµ‹è¯•å®è·µ

  - **Model Zoo æµ‹è¯•**ï¼šå¯¹æ¯ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå‰å‘ / åå‘ä¼ æ’­éªŒè¯

  - **ç²¾åº¦****å®¹å¿åº¦æ ‡è®°**ï¼š

    - ```Python
      @require_torch_gpu
      @slow
      @pytest.mark.tolerance(atol=1e-4)
      def test_large_model():
          ...
      ```

### MLSys Courses â€”â€” CSE 599W: Systems for ML

[cs294-2022](https://ucbrise.github.io/cs294-ai-sys-sp22/)

[cs294-2019](https://ucbrise.github.io/cs294-ai-sys-fa19/)

http://dlsys.cs.washington.edu/schedule

#### Lecture 1: Introduction to Deep Learning

* Ingredients in DL
  * æ¨¡å‹ã€ç›®æ ‡å‡½æ•°ã€æŠ€å·§ã€æ•°æ®
  * æŠ€å·§åŒ…æ‹¬Regularization, initialization (coupled with modeling)
    * Dropout: Overfitting prevention
    * Xavier Initialization
* æ¨¡å‹ç»“æ„
  * Fully Connected Layer
  * Convolution = Spatial Locality + Sharing
    * Convolution with Multiple Channels
  * Pooling Layer: Can be replaced by strided convolution
  * LeNet, AlexNet
  * Why ReLU?
    * Cheap to compute
    * It is roughly linear..
  * Dropout
    * Randomly zero out neurons with probability 0.5
    * During prediction, use expectation value (keep all neurons but scale output by 0.5)
  * GoogleNet: Multiple Pathways, Less Parameters
    * Inception Module
    * [1*1 å·ç§¯å‡å°channelç»´æ•°ï¼Œç”¨äºchannelèåˆï¼Œå‡å°‘è®¡ç®—é‡](https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network)
* Vanishing and Explosive Value Problem
  * In ConvNets, the weight are not tied, but their magnitude matters; Deep nets training was initialization sensitive
  * Batch Normalization: Stabilize the Magnitude
    * Subtract mean -> Divide by standard deviation -> Output is invariant to input scale! -> Scale input by a constant â€“> Output of BN remains the same
    * Impact: Easy to tune learning rate, less sensitive initialization
    * The Scale Normalization (Assumes zero mean)
  * ResNet: F(x) + x
* [lab1_mnist.ipynb](http://dlsys.cs.washington.edu/pdf/lab1_mnist.ipynb): MXNetå…¥é—¨ï¼ŒåŒ…æ‹¬Gluon APIã€å†™æ¨¡å‹ã€è®­ç»ƒæ¨ç†api

#### Lecture 3: Overview of Deep Learning System

![dlsys-stack](./MLSys/dlsys-stack.png)

* softmax in numpy
  * softmaxå†…è•´äº†logistic regression
  * æ‰‹ç®—æ¢¯åº¦ + SGD
    * æ¢¯åº¦æ¨å¯¼è§ã€Machine-Learningç¬”è®°ã€‘
* softmax in tinyflow
* The Declarative Language: Computation Graph
  * Nodes represents the computation (operation)
  * Edge represents the data dependency between operations

![computational-graph](./MLSys/computational-graph.png)

* Computation Graph Optimization
  * Deadcode elimination
  * Memory planning and optimization
  * Parallel Scheduling
    * Code need to run parallel on multiple devices and worker threads
    * Detect and schedule parallelizable patterns
  * GPU Acceleration
* Hardware backend
  * Each Hardware backend requires a software stack
  * New Trend: Compiler based Approach

#### Lecture 4: Backpropagation and Automatic Differentiation

* Symbolic Differentiation åŸºäºç¬¦å·å¾®åˆ†
  * åªèƒ½å¤„ç† closed-form expression
  * For complicated functions, the resultant expression can be exponentially large
  * Wasteful to keep around intermediate symbolic expressions if we only need a numeric value of the gradient in the end
  * Prone to error
  
* Numerical Differentiation åŸºäºæœ‰é™å·®åˆ†
  * Bad: rounding error, and slow to compute
  * A powerful tool to check the correctness of implementation, usually use h = 1e-6
* Backpropogation è‡ªåŠ¨å¾®åˆ†
  * æ€è·¯ï¼š
    * å°†åŸºç¡€æ“ä½œæ¢¯åº¦å…¬å¼hardcodeåœ¨ç³»ç»Ÿä¸­
    * æ¢¯åº¦ç´¯åŠ æ•°å€¼ï¼Œä»è€Œèƒ½å…¼å®¹æ¨¡å‹ç»“æ„ä¸­çš„é€»è¾‘åˆ¤æ–­

  * Easy to understand and implement
  * Bad for memory use and schedule optimization
    * You always need to keep intermediate data in the memory during the forward pass in case it will be used in the backpropagation.
    * Lack of flexibility, e.g., compute the gradient of gradient.

* Automatic Differentiation (autodiff)
  * Generate gradient computation to **entire** computation graphï¼Œè®¡ç®—è¿‡ç¨‹å…¨å›¾åŒ–
  * Better for system optimization
  * å…·ä½“ç®—æ³•è§ã€code-readingç¬”è®°ã€‘-- Tinyflow -- autodiff

![autodiff](./MLSys/autodiff.png)

##### Paper: ã€ŠAutomatic differentiation in PyTorchã€‹

* Features:

  * Dynamic, define-by-run execution; Immediate, eager execution
  * In-place operations; No tape; Core logic in C++
    * PyTorch (and Chainer) eschew this tape; instead, every intermediate result records only the subset of the computation graph that was relevant to their computation.

* ```python
  torch.autograd.grad(f(x, y, z), (x, y))
  
  from torch.autograd import Variable
  x, prev_h = Variable(torch.randn(1, 10)), Variable(torch.randn(1, 20))
  W_h, W_x = Variable(torch.randn(20, 20)), Variable(torch.randn(20, 10))
  i2h = torch.matmul(W_x, x.t())
  h2h = torch.matmul(W_h, prev_h.t())
  (i2h + h2h).tanh().sum().backward()
  ```

* API

  * â€œrequires gradâ€ and â€œvolatileâ€ flags
  * hooks:`x.register_hook(lambda grad: print(grad))`
  * Extensions

* Implementation

  * Variable

    * a wrapper around a Tensor
    * holds a reference to a graph of Function objects
    * mutated when an in-place operation occurs

  * Graph: immutable, purely functional representation of the derivative of computed function

  * Function: a closure that has all context necessary to compute vector-Jacobian products

  * å†…å­˜ç®¡ç†ï¼šPyTorchâ€™s Variable and Function must be designed to work well in a reference counted regime.

    * a Function records pointers to the Function which consumes its result

    * Another challenge is avoiding reference cycles. A naÄ±ve implementation of automatic differentiation can easily introduce such cycles (e.g. when a differentiable function would like to save a reference to its output). PyTorch breaks them by recording not a full-fledged variable, but instead a [â€œsaved variableâ€](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/saved_variable.cpp), which omits a pointer to the Function in such cases.

      * ```c++
        // We want grad_fn here to provide the most helpful debug message to the user
        // if versions don't match
        
        auto grad_fn = is_inplace_on_view_ ? weak_grad_fn_.lock()
          : !hooks_ ? saved_original_ ? data_.grad_fn() : nullptr
            : grad_fn_;
        ```

    * Supporting in-place operationsï¼šä¸å…¼å®¹Invalidationå’ŒAliasingè¿™ä¸¤ç§æƒ…å†µ

      * Invalidation: Every underlying storage of a variable is associated with a version counter, which tracks how many in-place operations have been applied to the storage.
      * Aliasing: the in-place addition to x also causes some elements of y to be updated; thus, yâ€™s computational history has changed as well.

##### Paper: ã€ŠAutomatic differentiation in ML: Where we are and where we should be goingã€‹

* Introduction
  * ç°çŠ¶ï¼šoperator overloading (OO) and source transformation (ST) used for AD
  * drawing insights from functional languages, graph-based IRs, and AD
* Forward modeå’ŒReverse mode
  * Forward mode has constant memory requirements and its runtime complexity scales with the number of inputs.
    * ç§‘å­¦è®¡ç®—åœºæ™¯ï¼Œè®¡ç®—é«˜é˜¶å¯¼æ•°

  * Reverse modeâ€™s runtime complexity scales with the number of outputs, and its memory complexity grows with the number of intermediate variables.
  * In principle, forward and reverse mode can be mixed, but finding the optimal way of doing so is NP-complete [27].
  * Since the number of inputs is significantly larger than the number of outputs, reverse mode AD is to be preferred

* Automatic differentiation: Two methods
  * Operator overloading (OO): record a tape
    * downside: Having an embedded interpreter inside of the host language can complicate debugging and performance analysis.
    * PyTorch, Autograd, and Chainer
  * source transformation (ST)
    * explicitly construct a program with a reversed control flow, which means that it needs transformation rules for function calls and control flow statements such as loops and conditionals åé™æ€
    * still ensure that intermediate variables from the forward pass are accessible by the adjoint
      * Tape-based
        * The tape used in ST stores only the intermediate variables, whereas the tape in OO is a program trace that stores the executed primitives as well.
      * Closure-based
        * no AD-specific compiler passes are needed: a functional language compiler will recognize the non-local use of the intermediate variables by the fact that they are free variables in the generated closure or continuation.
* Dataflow programming
  * Theano, TensorFlow, and MXNet
  * follow the dataflow program- ming paradigm [21] and use computation graphs as their **intermediate representation**
  * These graph representations do not have scoping or recursive function calls, which means that AD is much easier to implement with ST
  * è®¾è®¡å–èˆ
    * Function Calls: TensorFlow and Theano implement a type of subroutine through their Defun and OpFromGraph constructs, but these must be explicitly constructed by the user and donâ€™t support recursion.
    * Scoping: TensorFlow has a concept it refers to as â€˜scopingâ€™, but these scopes are not lexical and can be reentered at any time, so the lifetime of a value is not affected by its scope.
* Programming languages and compilers
  * The dataflow graph is an intermediate representation which is optimized using a series of compiler passes. The resulting program is compiled (e.g., XLA) and/or interpreted (e.g., the TensorFlow/Theano runtimes). Similarly, PyTorch has started optimizing its traced Python programs using just-in-time (JIT) compiler approaches.
  * Python because of its flexibility with the need for high performance and speed is an open question. ML frameworks have focused on metaprogramming and using C extensions, but other approaches are possible. For example, Cython [6] is a superset
  * performance and speed is an open question.
* Graph-based direct intermediate representation
  * graph based, purely functional, closure representation, strongly typed
  * IR specification
    * Concretely, our representation represents a function as a graph object with a list of parameter nodes and a single return node (multiple return values are supported through tuples). A node represents a function application and has an ordered list of incoming edges. The first incoming edge is a pointer to the function to apply, and the rest point to the arguments. Constants are represented as nodes with no incoming edges and a value field. Links between nodes are bidirectional, so that graphs can be traversed in either direction. Each non-constant node belongs to a single graph.
  * Source transformation
    * In order to ensure that our transformation can be applied again on the transformed program (so we can use reverse-over-reverse to compute second-order derivatives), it must be able to handle functions with free variables.
* Myia
  * Myia is a functioning proof of concept of a toolchain that uses the proposed graph representation
  * Python front end
    * Myia uses Pythonâ€™s inspect module to parse the function into an abstract syntax tree (AST), and converts that AST into the graph representation we previously described
  * Type inference
  * Optimization

> Lecture 5: GPU Programmingï¼Œå†…å®¹èå…¥ã€GPU.mdã€‘

#### Lecture 6: Optimize for Hardware Backends

* Where are we: gap between computation graph and hardware
  * Goal: High Level Program to Bare Metal Code
  * What are the tricks you can do to make your program run faster on CUDA/x86/any backend?
* Cache Line Aware Tiling
  * Output-tiled
  * cache line aware
  * æ”¶ç›Šæ¥æºäºmemory reuseï¼Œå‡å°‘load dram time cost

```c++
dram float A[n/b1][b1][n];
dram float B[n/b2][b2][n];
dram float C[n/b1][n/b2][b1][b2];
for (int i = 0; i < n/b1; ++i) {
  l1cache float a[b1][n] = A[i];
  for (int j = 0; j < n/b2; ++j) {
    l1cache b[b2][n] = B[j];
		for (int x = 0; x < b/v1; ++x) {
     for (int y = 0; x < b/v1; ++y) {
       register float c[v1][v2] = 0;
       for (int k = 0; k < n; ++k) {
         register float ar[v1] = a[x][k];
         register float br[v1] = b[y][k];
         C += dot(ar, br)
       }
 	    }
    }
  }
}
```

* operator fusion
* Optimizations = Too Many Variant of Operators
  * Different tiling patterns
  * Different fuse patterns
  * Different data layout
  * Different hardware backends

#### Lecture 7: Automatic Code Generation --- TVM Stack

https://tvm.apache.org/

https://github.com/apache/tvm

* Computational Graph as IR
  * Approach taken by: TensorFlow XLA, Intel NGraph, Nvidia TensorRT
  * XLA: Tensorflow Compiler
  * TensorRT: Rule based Fusion
    * relu+bias+conv --> CBR
    * Simple Graph-based Element-wise Kernel Generator: Fusion Pass + CodeGen Pass

![xla](./MLSys/xla.png)

* The Remaining Gap of "Computational Graph as IR"
  * need to build and optimize operators for each hardware, variant of layout, precision, threading pattern â€¦
  * hardware backendè¶Šå¤šï¼Œæ‰‹å·¥ä¼˜åŒ–opçš„æˆæœ¬è¶Šé«˜
* Emerging Tools Using Tensor Expression Language
  * Halide: Image processing language
  * Loopy: python based kernel generator
  * TACO: sparse tensor code generator
  * Tensor Comprehension

* TVM
  * Tensor Level Optimizations (Tensor Expression Language)
    * `C = t.compute((m, n), lambda i, j: t.sum(A[i, k] * B[j, k], axis=k))`
  * Tensor Index Expression

```python
# tvm
# Compute C = dot(A, B.T)
import tvm
m, n, h = tvm.var('m'), tvm.var('n'), tvm.var('h')
A = tvm.placeholder((m, h), name='A')
B = tvm.placeholder((n, h), name=â€˜B')

k = tvm.reduce_axis((0, h), name=â€˜k')
C = tvm.compute((m, n), lambda i, j: tvm.sum(A[i, k] * B[j, k], axis=k))
                    
# Convolution
out = tvm.compute((c, h, w), lambda i, x, y: tvm.sum(data[kc,x+kx,y+ky] * w[i,kx,ky], [kx,ky,kc]))
                    
# ReLU
out = tvm.compute(shape, lambda *i: tvm.max(0, out(*i))
```

* Schedule: Tensor Expression to Code
  * æ ¸å¿ƒæ€è·¯ï¼šSeparation of Compute and Schedule, introduced by Halide
* Key Challenge: Good Space of Schedule
  * Should contain any knobs that produces a logically equivalent program that runs well on backend models
  * Must contain the common manual optimization patterns
  * Need to actively evolve to incorporate new techniques

```python
# Example Schedule Transformation
C = tvm.compute((n,), lambda i: A[i] + B[i])
s = tvm.create_schedule(C.op)
xo, xi = s[C].split(s[C].axis[0], factor=32)  # 
s[C].recorder(xi, xo)
s[C].bind(xo, tvm.thread_axis(â€œblockIdx.xâ€)
s[C].bind(xi, tvm.thread_axis(â€œthreadIdx.xâ€)
```

* TVM Schedule Primitives
  * Loop Transformations, Thread Bindings, Cache Locality, Thread Cooperation, Tensorization, Latency Hiding
  * Schedule Space Exploration --> AutoTuner tuneå¤šä¸ªkernel

```python
# Extending Compute Primitives
# Symbolic Loop: Y = cumsum(X)
import tvm
m = tvm.var("m")
n = tvm.var("n")
X = tvm.placeholder((m, n), name="X")
s_state = tvm.placeholder((m, n))
s_init = tvm.compute((1, n), lambda _, i: X[0, i])
s_update = tvm.compute((m, n), lambda t, i: s_state[t-1, i] + X[t, i])
Y = tvm.scan(s_init, s_update, s_state, inputs=[X])
```

* Hardware designer: declare tensor instruction interface

```python
w, x = t.placeholder((8, 8)), t.placeholder((8, 8))
k = t.reduce_axis((0, 8))
y = t.compute((8, 8), lambda i, j: t.sum(w[i, k] * x[j, k], axis=k))
def gemm_intrin_lower(inputs, outputs):
 ww_ptr = inputs[0].access_ptr(â€œr")
 xx_ptr = inputs[1].access_ptr("r")
 zz_ptr = outputs[0].access_ptr("w")
 compute = t.hardware_intrin("gemm8x8", ww_ptr, xx_ptr, zz_ptr)
 reset = t.hardware_intrin("fill_zero", zz_ptr)
 update = t.hardware_intrin("fuse_gemm8x8_add", ww_ptr, xx_ptr, zz_ptr)
 return compute, reset, update

gemm8x8 = t.decl_tensor_intrin(y.op, gemm_intrin_lower)
```

* High Level Compilation Frontend

```python
import tvm
import nnvm.frontend
import nnvm.compiler
graph, params = nnvm.frontend.from_keras(keras_resnet50)
target = tvm.target.cuda()
graph, lib, params = nnvm.compiler.build(graph, target) 

module = runtime.create(graph, lib, tvm.gpu(0))
module.set_input(**params)
module.run(data=data_array)
output = tvm.nd.empty(out_shape, ctx=tvm.gpu(0))
module.get_output(0, output)
```

![tvm-remote](./MLSys/tvm-remote.png)

##### Paper: ã€ŠTVM: An Automated End-to-End Optimizing Compiler for Deep Learningã€‹

* Abstract/Conclusion
  * TVM solves optimization chal-lenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding.
* Introduction
  * TVM, a compiler that takes a high-level specification of a deep learning program from existing frameworks and generates low-level optimized code for a diverse set of hardware back-ends.
  * Leveraging Specific Hardware Features and Abstractions.
  * Large Search Space for Optimization
    * tensor expression language
    * automated program optimization framework
      * autotuneä¸æ‰‹å·¥ä¼˜åŒ–ï¼Œåè€…è§£å†³é€šç”¨é—®é¢˜ï¼Œå‰è€…è§£å†³é•¿å°¾é—®é¢˜å¹¶ä¼˜åŒ–åè€…å‚æ•°
    * graph rewriter

![tvm-1](./MLSys/tvm-1.png)

* Optimizing Computational Graphs
  * å’ŒIRçš„åŒºåˆ«ï¼šthe intermediate data items are large, multi-dimensional tensors.
  * Operator Fusionï¼šç»™opåˆ†äº†å››ç±»ï¼Œä¸åŒç±»å‹èƒ½ä»¥ä¸åŒå½¢å¼fusion
  * Data Layout Transformation.
  * constant-folding
  * static memory planning pass
* æ ¸å¿ƒæ€æƒ³ï¼šåˆ†ç¦»computeå’Œscheduleï¼Œé€šè¿‡å„ç§è°ƒåº¦å˜æ¢æœç´¢å‡ºæœ€é«˜æ•ˆçš„å®ç°
* Generating Tensor Operations
  * Tensor Expression and Schedule Space
    * Internally, TVM uses a data structure to keep track of the loop structure and other information as we apply schedule transformations. This information can then help generate low-level code for a given final schedule.
  * Nested Parallelism with Cooperation
    * in addition to being useful to GPUs, memory scopes let us tag special memory
      buffers and create special lowering rules when targeting specialized DL accelerators.
  * Tensorization
    * å¯¹æ™®é€šå¼ é‡ç¨‹åºï¼ˆä¸€èˆ¬å°±æ˜¯å¾ªç¯åµŒå¥—ç¨‹åºï¼‰ï¼Œè°ƒç”¨ç¡¬ä»¶åŠ é€Ÿå™¨æä¾›çš„ç‰¹å®šæŒ‡ä»¤å‡½æ•°ï¼ˆ intrinsicï¼‰è¿›è¡ŒåŠ é€Ÿã€‚æ¯”å¦‚ GPU é‡Œçš„ Tensor Core æä¾›çš„ä¸€äº› intrinsic å¯ä»¥ç›´æ¥å¤„ç†ç‰¹å®šå¤§å°çŸ©é˜µçš„è¿ç®—
    * We make tensorization extensible by separating the target hardware intrinsic from the schedule with a mechanism for tensor-intrinsic declaration.
  * Explicit Memory Latency Hiding

![tvm-primitives](./MLSys/tvm-primitives.png)

* Automating Optimization
  * Schedule Space Specification
  * ML-Based Cost Model
    * GDBT, ç‰¹å¾åŒ…æ‹¬ the memory access count and reuse ratio of each memory buffer at each loop level, as well as a one-hot encoding of loop annotations such as â€œvectorizeâ€, â€œun-rollâ€, and â€œparallel.â€
  * Schedule Exploration
    * a parallel simulated annealing algorithm
* Evaluation
* Related Work
  * é‡ç”³graph-based approachçš„ç¼ºç‚¹ï¼šservingå¤šç§hardware backendsè€—è´¹äººåŠ›
  * More importantly, we provide an end-to-end stack that can take descriptions directly from DL frameworks and jointly optimize together with the graph-level stack.
* æ›´å¤štvmåç»­è®ºæ–‡ï¼š
  * [Ansor : Generating High-Performance Tensor Programs for Deep Learning](https://arxiv.org/abs/2006.06762)
  * [NIMBLE: EFFICIENTLY COMPILING DYNAMIC NEURAL NETWORKS FOR MODEL INFERENCE](https://arxiv.org/pdf/2006.03031.pdf)

#### Lecture 8: Hardware Specialization in Deep Learning

* Hardware Specialization
  * â€¢ Idea: tailor your chip architecture to the characteristics of a **stable** workload

![evolution](./MLSys/evolution.png)

* Specialization Challenge
  * Tape-out costs for ASICs is exorbitant
    * 10x cost gap between 16nm and 65nm
    * 5nm ~ 800M$
  * Risky bet to design hardware accelerators for ever-changing applications
    * Flexibility vs. Efficiency Tradeoffs
    * Microprocessors(0.1) -> DSPs(1) -> Decicated HW(100)   (MOPS/mW)
* TPU: Googleâ€™s Entry in the Deep Learning Acceleration Race
  * Highlights (In-Datacenter Performance Analysis of a Tensor Processing Unit, ISCA 2017)
    * Custom ASIC deployed in datacenters since 2015
    * 65k 8-bit matrix multiply that offers peak throughput of 92 TOPS
    * Targets mainstream NN applications (MLPs, CNNs, and LSTMs)
    * Shows 30-80x improved TOPS/Watt over K80
  * Why Efficient
    * Integer inference (saves 6-30x energy over 16bit FP)
    * Large amount of MACs (25x over K80)
    * Large amount of on-chip memory (3.5x over K80)
  * TPU Roofline
    * 1350 Operations per byte of weight memory fetched
    * TPUçš„å†…å­˜å¸¦å®½å¤ªå°äº†ï¼Œ34GB/s

![tpu](./MLSys/tpu-block-diagram.png)

* HW/SW Co-Design - #1 Tensorization
* HW/SW Co-Design - #2 Memory Architecting

![memory-architecting](./MLSys/memory-architecting.png)

* HW/SW Co-Design - #3 Data Type

* VTA: Versatile Tensor Accelerator
  * a versatile and extendable deep learning accelerator for software codesign research and the development of next architectures
  * Features
    * Customizable tensor core, memory subsystem and data types based on bandwidth, storage and accuracy needs
    * Flexible CISC/RISC ISA for expressive and compact code
      * Goal: Provide the right tradeoff between expressiveness and code compactness 
      * Use CISC-ness to describe high-level operation (LD, ST, GEMM, ALU)
      * Use RISC-ness to describe low-level memory access patterns
      * Micro-op kernels are stored in a local micro op cache to implement different operators
    * Access-execute decoupling for memory latency hiding

* Latency Hiding: GEMM hide load latency
  * We want to enforce read-after-write (RAW) dependences
  * AND we want to enforce write-after-read (WAR) dependences
  * Takeaway: work partitioning and explicit dependence graph execution (EDGE) unlocks pipeline parallelism to hide the latency of memory accesses

![vta-design](./MLSys/vta-design.png)



* VTA Design	
  * Instruction fetch stage fetches high-level instructions from DRAM, decodes them, and pushes commands to the relevant queue (LD, EX, ST)
  * The load stage executes load commands to populate activation & kernel memories, the micro-op cache, and a load buffer for loading in values for the register file
  * Compute stage executes compute commands to perform vector ALU operations or GEMM operations to update the register file according to micro-coded kernels
  * Memory store stage executes store commands to store flushed register file values back to DRAM from the store buffer
  * Stages communicate via dependence token queues to indicate that they may proceed to execute the command theyâ€™re about to work on
  * Memories that connect pipeline stages follow a strict single producer, single consumer rule (fan-in=1, fan-out=1). This enables data flow execution, and makes this design modular
* TVM DSL allows for separation of schedule and algorithm

![vta-primitives](./MLSys/vta-primitives.png)

* Virtual Threading
  * How do we take advantage of pipeline parallelism with virtual threading?
  * Hardware-centric view: pipeline execution
  * Software-centric view: threaded execution
  * Final step: virtual thread lowering into a single instruction stream
    * Push and pop commands dictate how to interact with the hardware dependence queues

![virtual-threading](./MLSys/virtual-threading.png)

* Programming for VTA in TVM

  * How do we partition work and explicitly manage on-chip memories?

    * ```python
      // Tile
      yo, xo, yi, xi = s[OUT].tile(y, x, 4, 4)
      // Cache read
      INP_L = s.cache_read(INP, vta.act, [OUT])
      s[INP_L].compute_at(s[OUT], xo)
      ```

  * How do we take advantage of tensorization?

    * ```python
      // Tensorize
      s[OUT_L].tensorize(ni)
      ```

  * How do we take advantage of virtual threading?

    * ```python
      // Virtual Threading
      tx, co = s[OUT_L].split(co, factor=2)
      s[OUT_L].bind(tx, thread_axis(â€œcthreadâ€))
      ```

#### Lecture 9: Memory Optimization

* DL stack ä¸­çš„ Computational Graph Optimization and Execution ç¯èŠ‚
* Question for this lecture:
  * Why do we need automatic differentiation that extends the graph instead of backprop in graph?
* Executorçš„æ„å»ºï¼Œä¸­é—´èŠ‚ç‚¹åˆ†é…ä¸´æ—¶å†…å­˜ï¼ŒTemporary space linear to number of ops
  * Dynamic Memory Allocation
  * Static Memory Planning
    * Analog: register allocation algorithm in compiler
    * Inplace store the result in the input
      * We can only do inplace if result op is the only consumer of the current value
    * Normal Sharing reuse memory that are no longer needed
* Memory Allocation and Scheduling
  * Memory Planning Algorithm: ç»´æŠ¤å†…å­˜tagï¼Œä¸€ç§å®ç°è§ã€code-readingç¬”è®°ã€‘-- tvm -- å†…å­˜ç®¡ç†
  * Concurrency aware Heuristics:
    * Restrict memory reuse in the same colored path
    * coloré€šè¿‡ä¸æ–­åœ°æ‰¾æœ€é•¿è·¯å¾„ç”Ÿæˆï¼Œæ¯”å¦‚ç¬¬ä¸€æ¡æœ€é•¿è·¯å¾„ç”¨ä¸€ä¸ªé¢œè‰²
  * Introduces implicit control flow dependencies between ops
    * Solutions:
      * Explicitly add the control flow dependencies
        * Needed in TensorFlow
      * Enable mutation in the scheduler, no extra job needed
        * Both operation â€œmutateâ€ the same memory, supported in MXNet

![mlp-memory-opt](./MLSys/mlp-memory-opt.png)

* We are still Starved
  * For training, cost is still linear to the number of layers
  * Need to book-keep results for the gradient calculation
* Trade Computation with Memory
  * Only store a few of the intermediate result
  * Recompute the value needed during gradient calculation
  * tfä¸­éœ€è¦æ˜¾ç¤ºæ·»åŠ  control dependencyï¼ˆæŒ‡å‘å¸Œæœ›æ‰§è¡Œçš„opå‰çš„èŠ‚ç‚¹ï¼‰
  * **Sublinear Memory Complexity**
    * O(K) + O(N/K) ---> sqrt(N) memory cost plan

![memory-opt-recursion](./MLSys/memory-opt-recursion.png)

#### Lecture 10: Parallel Scheduling

* Questions to be answered
  * What are common patterns of parallelization
  * How can we easily achieve these patterns
  * What about dynamic style program 
* Model Parallel Training
  * Map parts of workload to different devicesï¼Œä¸»è¦æ˜¯è§£å†³GPUæ˜¾å­˜ä¸è¶³çš„é—®é¢˜
  * Require special dependency patterns (wave style)
    * e.g. LSTM

* Data Parallelism
  * Train replicated version of model in each machine
  * Synchronize the gradient
    * control dependency: weightæ›´æ–°æŒ‡å‘device forwardï¼ˆæå‰load dataï¼‰
* Goal of Scheduler Interface
  * ä¸²è¡Œçš„æ–¹å¼å†™ç¨‹åºï¼Œå†…éƒ¨å°½å¯èƒ½å¹¶è¡Œï¼Œæœ‰ç‚¹åƒ C++ ç¼–è¯‘å™¨ï¼Œåªè¦æ²¡æœ‰æ›´é«˜å±‚æ¬¡çš„å¹¶è¡Œå³å¯work
  * Schedule any resources
    * Data
    * Random number generator
    * Network communicator
* DAG Graph based scheduler
  * `engine.push(lambda op, deps=[])`
  * tfä½¿ç”¨ï¼ŒUseful when all results are immutable
* Discussion: How to schedule the following ops
  * Random number generator
  * Memory recyclingï¼Œç±»ä¼¼çš„è¿˜æœ‰ Write After Read Mutation
  * Cross device copy
  * Send data over network channel
* Mutation aware scheduler: solve these problems much easier than DAG based scheduler
  * Tag each Resource
  * Push Operation: è®°å½•read/mutateçš„èµ„æº
* Queue based Implementation of scheduler
  * Like scheduling problem in OS
  * Maintain a pending operation queueï¼Œå†ç»™æ¯ä¸ªèµ„æºç»´æŠ¤ä¸€ä¸ªqueue
  * Schedule new operations with event update

> Lecture 11  Distributed Training and Communication Protocols èå…¥ä¸Šé¢çš„ç¬”è®°

#### Lecture 12: Model Serving

* Model Compression
  * Tensor decomposition
    * Matrix decompostion
    * "Compression of deep convolutional neural networks for fast and low power mobile applications." ICLR (2016)
      * finetuneå‡å°æ•ˆæœæŸå¤±
  * Network pruning
    * "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding." ICLR (2016)
    * ![network-pruning](./MLSys/network-pruning.png)
    * ![network-pruning-2](./MLSys/network-pruning-2.png)
  * Quantization
    * pruning + quantization æ•ˆæœæœ€å¥½ï¼ˆç›¸æ¯”ä¸¤è€…çš„å•ç‹¬ä½¿ç”¨ä»¥åŠSVDï¼‰ï¼Œå¤§é“è‡³ç®€ï¼Ÿ
    * XNOR-Net: binary weights/binary input and weights
    * quantize during training
  * Smaller model
    * Knowledge distillation: "Fitnets: Hints for thin deep nets." ICLR (2015)
  * Others
    * Specialized hardware for sparse models
      * Song Han, et al. â€œEIE: Efficient Inference Engine on Compressed Deep Neural Network.â€ ISCA 2016
    * Accuracy and resource trade-off
      * Han, Seungyeop, et al. "MCDNN: An Approximation-Based Execution Framework for Deep Stream Processing Under Resource Constraints." MobiSys (2016).

* Serving system

  *  Goals:

    * High flexibility for writing applications
    * High efficiency on GPUs
    * Satisfy latency SLA

  * Challenges

    * Provide common abstraction for different frameworks
    * Achieve high efficiency
      * Sub-second latency SLA that limits the batch size
      * Model optimization and multi-tenancy causes long tail

  * ã€ŠNexus: efficient neural network serving systemã€‹

    * Frontend runtime library allows arbitrary app logic

    * Packing models to achieve higher utilization

    * A GPU scheduler allows new batching primitives

    * A batch-aware global scheduler allocates GPU cycles for each model

      * For high request rate, high latency SLA workload, saturate GPU efficiency by using large batch size

    * ```python
      class ModelHandler:
        # return output future
        def Execute(input)
      class AppBase:
        # return ModelHandlerï¼Œä¼ å…¥SLAåº”è¯¥æ˜¯ä¸ºäº†æ–¹ä¾¿è°ƒåº¦
        def GetModelHandler(framework, model, version, latency_sla)
        # Load models during setup time, implemented by developer
        def Setup()
        # Process requests, implemented by developer
        def Process(request)
      ```

  * æŒ‰modelç±»å‹åˆ†ç±»ï¼š

    * é«˜SLAé«˜QPSï¼šlarge batch size
    * ä½SLAé«˜QPSï¼šoptimized split batching
      * æ¯ä¸ªopå¯ä»¥æœ‰ä¸ä¸€æ ·çš„batch size...... ä¼˜åŒ–æœ€å¤§ååã€‚è¿™æ ·ä¸ä¼šå¢åŠ å†…å­˜å¸¦å®½æ¶ˆè€—ä¹ˆï¼Ÿ   GDRå¯ä»¥å‡å°‘åˆ°ä¸€æ¬¡
    * é«˜SLAä½QPSï¼šexecute multiple models on one GPU
      * Execute multiple models in round-robin fashionï¼Œå¯æœ€å°åŒ–ç­‰batchçš„latency
    * ä½SLAä½QPSï¼šSolution depends
      * If saturate GPU in temporal domain due to low latency: allocate dedicated GPU(s)
      * If not: can use multi-batching to share GPU cycles with other models

    * ![split-batching](./MLSys/split-batching.png)

    * Prefix batching for model specializationï¼šç±»ä¼¼äºsparse/dense opåˆ†ç¦»çš„æ€è·¯

  * Meet Latency SLA: Global scheduler

    * Best-fit decreasing algorithms



### MLSys in the Cloud

* MLflowå¼ºè°ƒæ˜“ç”¨æ€§å’Œæ”¯æŒå•æœºç¯å¢ƒï¼Œè€Œè°·æ­Œå¼ºè°ƒå¤§è§„æ¨¡å’Œå¹¶è¡Œ



https://outerbounds.com/blog/modern-data-stack-mlops/ MUSTDO

cs294çš„ä¸‰ç¯‡æ¨èè®ºæ–‡ï¼šMUSTDO

https://arxiv.org/abs/2205.07147

https://arxiv.org/abs/2006.07512

https://www.usenix.org/system/files/osdi21-qiao.pdf



https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.pdf





### è®ºæ–‡é˜…è¯»

#### MLSys: The New Frontier of Machine Learning Systems

#### Monolith: Real Time Recommendation System With Collisionless Embedding Table, RecSys 22

* Abstract & Conclusion
  * a collisionless embedding table with optimizations such as expirable embeddings and frequency filtering to reduce its memory footprint
  * we provide an production-ready online training architecture with high fault-tolerance
  * we proved that system reliability could be traded-off for real-time learning.
  * [BytePlus](https://www.byteplus.com/en/product/recommend)ï¼Œå‚è€ƒã€éæŠ€æœ¯çŸ¥è¯†ç¬”è®°ã€‘
* Intro
  * æ¨èåœºæ™¯ç‰¹ç‚¹
    * The features are mostly sparse, categorical and dynamically
      changing;
    * The underlying distribution of training data is non-stationary, a.k.a. Concept Drift [8].
  * Sparsity and Dynamism
    * embeddingå¤šä¸”æŒç»­å¢é•¿
    * Low-collision hashingä¸åˆé€‚ï¼ˆYoutube Recommendationsè®ºæ–‡ï¼‰
  * Non-stationary Distribution
    * è§£é‡Šå®æ—¶æ€§çš„æ”¶ç›Š
* Design
  * HashTable
    * tf.variableçš„å±€é™æ€§ï¼šä¸å¥½æ”¯æŒåŠ¨æ€add/delete variableï¼Œä¸æ”¯æŒæŒ‡å®šæ–°variableå¤ç”¨æ—§variable
    * cuckoo hashmap
    * ä¼˜åŒ–å†…å­˜çš„æ‰‹æ®µ
      * ouccurency/probabilistic filter
      * expire embeddings
  * Online Training
    * The online joiner concatenates features with labels from user actions and produces training examples, which are then written to a Kafka queue.
    * a unique key for each request so that user action and features could correctly pair up
      * è§£å†³æ ·æœ¬å›æµæ…¢çš„é—®é¢˜ï¼šå…ˆæŸ¥in-memory cacheå†æŸ¥kv
      * negative samplingï¼šsample bias (log odds correction [19] during serving)
  * Parameter Syncï¼šåˆ†é’Ÿçº§sync sparseã€å¤©çº§åˆ«dense
    * Sparse parameters are dominating the size of recommendation models;
    * Given a short range of time window, only a small subset of IDs gets trained and their embeddings updated;
    * Dense variables move much slower than sparse embeddings. This is because in momentum-based optimizers, the accumu- lation of momentum for dense variables is magnified by the gigantic size of recommendation training data, while only a few sparse embeddings receives updates in a single data batch.
      * å‚è€ƒã€Machine Learningç¬”è®°ã€‘-- AdaGrad çš„ naturally decaying learning rateæœ¬è´¨
  * Fault Toleranceï¼šå¤©çº§dump
* Evaluation
  * å†…éƒ¨æ¨èç³»ç»Ÿworkload
    * Each model has around 1000 embedding tables, and distribution of size of embedding tables are very uneven
    * a hashing trick by decomposing to curb the size of embedding table
      * conflictçš„æŸå¤±å¯ä»¥ç¼©å°ä¸ºâ€œå†·çš„embç”¨åˆ°äº†çƒ­embçš„ä¸€éƒ¨åˆ†â€œï¼Œè¿™ä¸ªå¯¹å†·çš„embå­¦ä¹ çš„å½±å“å¯èƒ½æ˜¯æœ‰é™çš„
  * å®éªŒç»“è®ºï¼š
    * collisonlessæå‡auc
    * online auc > batch auc ï¼ˆserving aucæå‡14%ï¼Ÿï¼‰ï¼Œæ­£å¸¸æƒ…å†µtraining aucå¤§æ¦‚æå‡åƒåˆ†ä½
      * åœ¨çº¿å­¦ä¹ çš„æ”¶ç›Šçœ‹åœºæ™¯ï¼šç‰¹å¾å®æ—¶æ€§çš„å½±å“ã€æ–°å‘æ–‡å æ¯”
    * å‡å°‘sync intervalæå‡auc
  * åˆ†å¸ƒå¼PSå®¹é”™è®¨è®ºï¼šä¸¢å¤±çš„sparseæœ‰é™ï¼Œæœºå™¨loss ï½ ç­‰æ¯”ä¾‹dauå—å½±å“
* Related workï¼š
  * è‡ªç ”PSï¼šYoutubeã€Instagramã€Grubhub
  * TF's PSï¼šXDLã€[ç¾å›¢](https://tech.meituan.com/2021/12/09/meituan-tensorflow-in-recommender-systems.html)ã€Krakenã€AIBox
  * online trainingï¼šXDLã€Persia

#### Persia: A Hybrid System Scaling Deep Learning Based Recommenders up to 100 Trillion Parameters

* å†™åœ¨å‰é¢ï¼šå…³äºåŒæ­¥è®­ç»ƒå’Œå¼‚æ­¥è®­ç»ƒ
  * åŒæ­¥è®­ç»ƒæ— gradient stalenessï¼Œå¯ä»¥ç”¨æ›´å¤§çš„batch size

* Intro & Conclusion
  * 100trillion ~ 100ä¸‡äº¿å‚æ•°ï¼Œfp16ä¸‹å°±æ˜¯200TB
  * èƒ½åœ¨google cloud platformè¿è¡Œï¼šhttps://github.com/PersiaML/tutorials/blob/main/src/kubernetes-integration/index.md

![persia-overall](./MLSys/persia-overall.png)

* Preliminaries
  * denseåŒæ­¥ sparseå¼‚æ­¥
  * It is worth mentioning that while the W_nn involved computation can be 1e7x more than the W_emb involved computation, the size of W_emb can be 1e7Ã— larger than that of W_nn, especially when W_emb contains many cross features
  * [Distributed Learning Systems with First-order Methods](https://arxiv.org/pdf/2104.05245.pdf)

* Hybrid Training Algorithm
  * æƒè¡¡sparse accessæ¨¡å¼ä¸‹çš„async updateçš„efficiencyå’Œstaleness
  * æ”¯æŒå¼‚æ„èµ„æº
  * ç®—æ³•ï¼š
    * W_embç›¸å…³çš„forwardå’Œbackwardä¸é˜»å¡
    * denseçš„forwardå’Œbackwardé˜»å¡
      * input: buffered embeddings from W_emb
      * output: activations' gradients

![persia-hybrid](./MLSys/persia-hybrid.png)

* System Design and Implementation
  * æ¶æ„
    * Embedding Worker: async, PS paradigm
    * NN Worker: AllReduce paradigm
  * Design Goals
    * Fill the Async/Sync Gap
      * NN worker buffer mechanismï¼šç¼“å­˜dense input + label
      * Embedding worker buffer mechanism: ç¼“å­˜æ ·æœ¬
    * Persia Memory Management: array-list based LRU cache
      * hashmapçš„valueå­˜array-listçš„index
        * Array-listçš„valueå­˜pre-index + post-index + entry
      * å¤šçº¿ç¨‹get/putï¼Œæœ‰é”
    * Communication Optimization
      * Optimized communication among NN workers: hiding communication overhead within the backward computation
        * BAGUA by å¿«æ‰‹ï¼šã€ŠBAGUA: Scaling up Distributed Learning with System Relaxationsã€‹
      * Optimized remote procedure callï¼šzero-copy serialization and deserialization mechanism targeting for tensors TODO
      * Workload balance of embedding PS: å¯¹æ¯ä¸ªfeature groupå†…çš„embeddingåšshuffleï¼Œç„¶åå¹³å‡åˆ†å¸ƒåœ¨PS shardsä¸Š
      * Communication compression
        * æ— æŸå‹ç¼©ï¼šunique fids + uint16 indexçš„è¡¨ç¤º
        * æœ‰æŸå‹ç¼©ï¼šfp32to16
          * Non-uniform mapping: å‹ç¼©å‰scale by $\frac{K}{\lVert v\rVert _{\infty}}$
    * Fault Tolerance
      * insightï¼špsèƒ½ä¸¢æ¢¯åº¦ï¼Œé‡è§†å®æ—¶å“åº”ï¼›denseä¸èƒ½ä¸¢æ¢¯åº¦
      * pså­˜shared-memoryæ–¹ä¾¿æ‹‰èµ·process

![image-20221121215329176](./MLSys/persia-system.png)

![image-20221121224430506](./MLSys/persia-PS.png)

* Theory Analysis
  * Assumptions
    * The assumptions of existence of global minimum, bounded variance,
      and bounded staleness are commonly used ones.
    * Bounded staleness: ç»éªŒå€¼ä¸º5
  * THEOREM 1: idç±»ç‰¹å¾çš„ä½é¢‘ç‰¹æ€§ --> æ”¶æ•›æ¥è¿‘åŒæ­¥è®­ç»ƒ

![image-20221121234602852](./MLSys/persia-theory.png)

* Evaluation
  * å†…éƒ¨cluster set-upï¼š100 * V100 + 100 * 50 CPU cores
  * GCP:
    * 8 a2-highgpu-8g instances (each with 8 Nvidia A100 GPUs) as NN workers;
    * 100 c2-standard-30 instances (each with 30vCPUs, 120GB RAM) as embedding workers;
    * 30 m2-ultramem-416 instances (each with 416vCPUs, 12TB RAM) as embedding PS
  * auc diff
    * sync/hybrid/async: base/-0.1%/-0.5%
  * Throughput diff
    * sync/hybrid/async: 1/2.6/3.1
* Related Work
  * xdlå°†æ„é€ tensorçš„ä»»åŠ¡äº¤ç»™cpu nodesï¼Œå¤§å‚ç»å…¸è®¾è®¡
  * ç™¾åº¦ç»™psåˆ†å±‚ï¼Œçƒ­çš„embeddingå­˜gpuä¸Šï¼Œå†·çš„embeddingå­˜åœ¨ssdé‡Œï¼ˆDeepRecæ˜¯ç±»ä¼¼çš„è®¾è®¡ï¼‰
    * ã€ŠDistributed hierarchical gpu parameter server for massive scale deep learning ads systemsã€‹
  * HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework.
    * GPU cache çƒ­çš„embedding
  * AWS SageMaker: model parallelism
    * https://arxiv.org/abs/2111.05972

#### Core Modeling at Instagram

https://instagram-engineering.com/core-modeling-at-instagram-a51e0158aa48

* Features
  * N-grams: We select the features by feature importance and smoothness of distribution, because rough distributions are harder to quantize
* Embeddings: è·å–embeddingsçš„æ–¹å¼
  * Word2Vec: ç¤¾äº¤äº’åŠ¨å…³ç³»å­¦ä¹ user embeddingï¼Œ(user1, [user2, user3])ï¼Œæœ¬è´¨ç±»ä¼¼GNN
  * DL
    * å‘é‡åŒ–å¬å›åº“ï¼šhttps://github.com/facebookresearch/faiss
* Pooling and Hashing
  * poolingçš„å«ä¹‰ï¼šå¯¹å†å²å…´è¶£poolingçš„ç»“æœæ˜¯å…´è¶£çš„centroidï¼Œæ¨é€çš„æ–°å…´è¶£è¶Šé è¿‘è¶Šå¥½
  * hashing: a better hashing strategy which took frequency into accountï¼Œé«˜çƒ­embeddingå‡æ‘Š
  * dimensionality: automatically perform dimensionality reductions on the learned embeddings , and alert if we are off in terms of dimensionality or hash size.
  * Pooling: bayesian optimizationä¼˜åŒ–poolingè¶…å‚ï¼ˆmax/avg/sum/attentionï¼‰
* Cold start and bootstrapping
  * At Instagram we monitor feature coverage fed into a model and if it is lower than a threshold we have fallback options that are less accurate but only use high fidelity features
  * userèšç±»embedding
  * Coming up we will be baking this into our training pipelines where each feature will have a â€œreliabilityâ€ score and we will automatically produce fallback models for every model trained.
* Offline vs Online vs Recurring
  * Recurring: æ¯å¤©é‡è®­ä¸€ä¸ªè®­ç»ƒå‰Nå¤©æ•°æ®çš„æ¨¡å‹
    * We usually evaluate against a fixed golden set, and a changing test set, as a good practice
  * model validationï¼šæ£€æµ‹ä¸åŒsnapshotä¹‹é—´çš„é¢„ä¼°å·®å¼‚
* Mixture of experts and sub-models
  * äººç¾¤å·®å¼‚é—®é¢˜ï¼š[Sparse MoE](https://arxiv.org/pdf/1701.06538.pdf)
* Offline analysis and backtesting
  * We have built a cohesive tool that replays past traffic using control and test treatments, and computes a panel of ecosystem and model metrics to help engineers with their project. This allows an engineer to quickly check that the expected results are moving in the intended fashion.
* Ranking-specific practices
  * Multi-stage Ranking
    * LambdaRank
  * Loss function and inverse propensity weighting
    * When the ranked list of items doesnâ€™t have a human-generatable ideal relevant ranking (unlike most information theory cases), most pipelines default to point-wise models instead of Learning-To-Rank framework.
    * For instance, one might rank the Instagram stories by computing P[tapping on the story] for each available medias and sorting by the probability. This works pretty well, albeit the loss function becomes an issue, because in most ranking use-cases the top items are much more impactful than the rest.
    * inverse propensity weighting: weight training examples by their positions
  * Position Bias: æœ€ç®€å•çš„æ–¹æ³•ï¼Œtrainingæ—¶åŠ position featureï¼Œservingç”¨é»˜è®¤å€¼å…¨é›¶ï¼Œä¼šæœ‰ç¦»åœ¨çº¿ä¸ä¸€è‡´

#### A Hitchhiker's Guide On Distributed Training Of Deep Neural Networks, JPDC 18

#### TFX: A TensorFlow-based production-scale machine learning platform

#### TensorFlow: A system for large-scale machine learning, OSDI 16

#### Clipper: A Low-Latency Online Prediction Serving System, NSDI 17

low latencies, high throughputs, and improved accuracy

prediction cache, batching queue

##### Model abstraction layer

ç”¨object storeå­˜æ¨¡å‹ï¼Œå‡å°‘åˆå§‹åŒ–å¼€é”€

prediction cacheï¼šæœ¬è´¨ä¸Šç±»ä¼¼SHAREDå±æ€§ï¼ˆåŒä¸€batchå†…çš„æŸä¸€ç‰¹å¾ç”¨ç›¸åŒçš„é¢„ä¼°ç»“æœï¼‰ã€‚ä¸¤è€…çš„åŒºåˆ«åœ¨äºï¼Œå‰è€…çš„è¾“å…¥æ›´ç®€å•ï¼Œä»¥æ¨¡å‹å’Œreq idä¸ºæ ‡è¯†ï¼Œæ˜“äºåšcacheæ“ä½œï¼›åè€…æ˜¯featureå±‚é¢ï¼Œæ›´ç²¾ç»†ã€‚æ¨èç³»ç»Ÿå…¥å›¾çš„ç‰¹å¾è¾“å…¥å¾ˆéš¾åšåˆ°å®Œå…¨ä¸€è‡´ï¼Œå› æ­¤åšprediction cacheæ“ä½œéš¾åº¦è¾ƒå¤§ã€‚

batchingï¼šåŠ¨æ€é€‰batch sizeçš„æ–¹å¼
* additive-increase-multiplicative-decrease (AIMD) scheme 
* quantile regression
* delayed batchingï¼šæŒ‰æ”’batchçš„timeoutæ¥delayï¼Œé€‚åˆå¹¶è¡Œä¼˜åŒ–æ˜æ˜¾çš„æ¨¡å‹

model container: æ— çŠ¶æ€æœåŠ¡
* Clipper performs adaptive batching independently for each replica

##### Model selection layer

åŠ¨æ€è°ƒæ•´é€‰ç”¨æ¨¡å‹çš„ç­–ç•¥ï¼Œæ¨èç³»ç»Ÿé‡‡ç”¨è¿™ç±»æŠ€æœ¯æ¯”CV/NLPéš¾åº¦æ›´å¤§

* Single Model Selection Policy
  * address the trade-off between exploring possible actions and exploiting the estimated best action. 
* Ensemble Model Selection Policies
  * Robust Prediction 
    * agreementè¡¡é‡prediction confidence 
    * æœ‰é’ˆå¯¹degradedæ¨¡å‹çš„é™çº§æœºåˆ¶
  * Straggler Mitigation
* Contextualization: instantiate a unique model selection state for each user, context, or session.



#### Hidden Technical Debt in Machine Learning Systems, NIPS 15

boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.

2. Complex Models Erode Boundaries
* Entanglement: å³ä½¿å¤šæ¨¡å‹/è¶…å‚çš„é…ç½®ç‹¬ç«‹ï¼Œæ•ˆæœä¹Ÿä¼šäº’ç›¸å½±å“
* Correction Cascade: æ¨¡å‹çº§è”æ˜¯ hidden debt
* Undeclared Consumers: éœ€è¦SLA (service-level agreement)

3. Data Dependencies Cost More than Code Dependencies
* Underutilized dependencies: legacy/bundled/epsilon/correlated, use exhaustive leave-one-feature-out evaluations to detect

4. Feedback Loops
* direct: related to bandit algorithms, costly
* hidden: two independent systems may interact

5. ML-System Anti-Patterns
* Glue Code: hard to achieve a domain-specific goal
* Pipeline Jungle: ç‰¹å¾å·¥ç¨‹çš„æ„ä¹‰æ‰€åœ¨ï¼Œthinking holistically about data collection and feature extraction
* Dead Experimental Codepaths
* Abstraction Debt
* Common Smells

6. Configuration Debts
* Feature A was incorrectly logged from 9/14 to 9/17
* Feature B is not available on data before 10/7
* The code used to compute feature C has to change for data before and after 11/1 because of changes to the logging format
* Feature D is not available in production, so a substitute features Dâ€² and Dâ€²â€² must be used when querying the model in a live setting
* If feature Z is used, then jobs for training must be given extra memory due to lookup tables or they will train inefficient
* Feature Q precludes the use of feature R because of latency constraints.

7. Dealing with Changes in the External World



#### Ad Click Prediction: a View from the Trenches, KDD 13

FTRLçš„æå‡ºç»å†äº†ä¸‹é¢å‡ ä¸ªå…³é”®çš„è¿‡ç¨‹ï¼š

1. **ä»æœ€è¿‘ç®€å•çš„SGDåˆ°OGD**ï¼ˆonline gradient descentï¼‰ï¼ŒOGDé€šè¿‡å¼•å…¥L1æ­£åˆ™åŒ–ç®€å•è§£å†³ç¨€ç–æ€§é—®é¢˜ï¼›
2. **ä»OGDåˆ°æˆªæ–­æ¢¯åº¦æ³•**ï¼Œé€šè¿‡æš´åŠ›æˆªæ–­å°æ•°å€¼æ¢¯åº¦çš„æ–¹æ³•ä¿è¯æ¨¡å‹çš„ç¨€ç–æ€§ï¼Œä½†æŸå¤±äº†æ¢¯åº¦ä¸‹é™çš„æ•ˆç‡å’Œç²¾åº¦ï¼›
3. **FOBOS**ï¼ˆForward-Backward Splittingï¼‰ï¼Œgoogleå’Œä¼¯å…‹åˆ©å¯¹OGDåšè¿›ä¸€æ­¥æ”¹è¿›ï¼Œ09å¹´æå‡ºäº†ä¿è¯ç²¾åº¦å¹¶å…¼é¡¾ç¨€ç–æ€§çš„FOBOSæ–¹æ³•ï¼›
4. **RDA**ï¼šå¾®è½¯æŠ›å¼ƒäº†æ¢¯åº¦ä¸‹é™è¿™æ¡è·¯ï¼Œç‹¬è¾Ÿè¹Šå¾„æå‡ºäº†æ­£åˆ™å¯¹å¶å¹³å‡æ¥è¿›è¡Œonline learningçš„æ–¹æ³•ï¼Œå…¶ç‰¹ç‚¹æ˜¯ç¨€ç–æ€§æä½³ï¼Œä½†æŸå¤±äº†éƒ¨åˆ†ç²¾åº¦ã€‚
5. Google**ç»¼åˆFOBOSåœ¨ç²¾åº¦ä¸Šçš„ä¼˜åŠ¿å’ŒRDAåœ¨ç¨€ç–æ€§ä¸Šçš„ä¼˜åŠ¿**ï¼Œå°†äºŒè€…çš„å½¢å¼è¿›è¡Œäº†è¿›ä¸€æ­¥ç»Ÿä¸€ï¼Œæå‡ºå¹¶åº”ç”¨FTRLï¼Œä½¿FOBOSå’ŒRDAå‡æˆä¸ºäº†FTRLåœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„ç‰¹æ®Šå½¢å¼ã€‚

ä½œè€…ï¼šç‹å–†
é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/61154299



2. Brief System Overviewï¼šGoogle åœºæ™¯æ˜¯æœç´¢å¹¿å‘Š

3. Online Learning and Sparsity

* FTRL-Proximal(Follow The Proximally Regularized Leader): get both the sparsity provided by RDA and the improved accuracy of OGD

* [åœ¨çº¿å­¦ä¹ ï¼ˆOnline Learningï¼‰å¯¼è¯» - å´æµ·æ³¢çš„æ–‡ç« ](https://zhuanlan.zhihu.com/p/36410780)
* FTRLçš„æ•°å­¦æœ¬è´¨ï¼šSGDï¼ˆæ¢¯åº¦ + L2ï¼‰+ç¨€ç–æ€§ï¼ˆL1ï¼‰

* æäº¦é”¬å¤§ä½¬çš„æœºå™¨å­¦ä¹ ç­”é¢˜é›†ï¼Œå¾ˆç²¾å½©ï¼Œå…¶ä¸­ä»‹ç»äº† FTRL çš„å®è·µæ„ä¹‰
  https://zhuanlan.zhihu.com/p/20693546

4. Saving Memory at Massive Scale

è¿›ä¸€æ­¥èŠ‚çœPSå†…å­˜çš„æ–¹å¼

* Probabilistic Feature Inclusion
  * å‡ºäºæ•ˆæœã€å›æº¯æ€§çš„è€ƒé‡ï¼Œåªè€ƒè™‘åœ¨ serving æ—¶çœå†…å­˜
  * Poisson Inclusion, Bloom Filter Inclusion
* Encoding Values with Fewer Bits
  * $\omega_{i,rounded}=2^{-13}\lfloor{2^{13}\omega_i+R}\rfloor$
* Training Many Similar Models
  * savings from not representing the key and the counts per model
* A Single Value Structure
  * åŠ¨æœºæ˜¯çœå†…å­˜ï¼Œæœ¬è´¨ä¸Šæœ‰ç‚¹åƒæ˜¯å¯¹æå…¶ç›¸ä¼¼çš„ models çš„å…¬å…±å‚æ•°åš cotrain
  * ç”¨äºç‰¹å¾æ·˜æ±°ã€ç‰¹å¾é€‰æ‹©ç­‰å®éªŒåœºæ™¯ (Fast prediction of new feature utility. ICML, 2012)
* Computing Learning Rates with Counts

* Subsampling Training Data: ç„¶åç»™è´Ÿæ ·æœ¬çš„ loss å¢åŠ æƒé‡ï¼Œä¿è¯â€œæœŸæœ›ä¸Šâ€ç›®æ ‡å‡½æ•°çš„ä¸€è‡´æ€§

5. Evaluating Model Performance

* Progressive Validation: online loss, relative changes

6. Confidence Estimates

* å®šä¹‰å¹¶ä¼°å‡ºäº†ä¸ç¡®å®šåº¦çš„ upper bound: å­¦ä¹ ç‡å‘é‡ç‚¹ä¹˜è¾“å…¥å‘é‡

7. Calibrating Predictions

* æœ‰ Poisson regressionã€isotonic regression ç­‰æ‰‹æ®µ
* ç³»ç»Ÿçš„ inherent feedback loop ä¸ä¿è¯ç†è®ºå‡†ç¡®æ€§

8. Automated Feature Management

* ç‰¹å¾å¹³å°åŒ–

9. Unsuccessful Experiments

* Feature Hashing, Dropout, Feature Bagging, Feature Vector Normalization



æœºå™¨å­¦ä¹ æ¡†æ¶æ˜“ç”¨æ€§

* a high-dimensional visualization tool was used to allow researchers to quickly see effects across many dimensions and slicings
* enables data sources and features to be annotated. Automated checks can then be run to ensure that all dependencies have the appropriate annotations, and dependency trees can be fully resolved.



#### XDL: An industrial deep learning framework for high-dimensional sparse data, KDD 19

MPI (All Reduce) å’Œ PSï¼Œä¸¤ç§åˆ†å¸ƒå¼è®¡ç®—çš„å‘å±•æ–¹å‘

Sparse + Dense

* SparseNet: Representation learning which captures information from high-dimensional sparse input and embeds them into a low-dimensional space

* DenseNet: Function fitting which models the relationship between dense embedding representation and supervised label

In order to facilitate deployment on various computing platforms, XDL can be scheduled by multiple resource management platform, like Yarn, and provides data I/O interfaces to various data storage systems, like HDFS and Kafka.

* I/O
  * Hierarchical sample compression: prefix tree
    * ç”¨æˆ·åœ¨åŒä¸€å±çš„å¤šæ¡æ›å…‰è®°å½•ï¼Œitemä¸åŒï¼Œä½†æ˜¯userç‰¹å¾æ˜¯ç›¸åŒçš„ã€‚åŸºäºè¿™ä¸ªç‰¹ç‚¹ï¼ŒXDLé‡‡ç”¨å¤šçº§å‰ç¼€æ ‘æ¥æ„é€ è®­ç»ƒæ ·æœ¬ï¼Œå‹ç¼©æ ·æœ¬ä¸­userç‰¹å¾ã€adç‰¹å¾çš„é‡ï¼ˆadç‰¹å¾çš„å‹ç¼©æ˜¯å¦æœ‰å¿…è¦ï¼Ÿï¼‰

![prefix-tree](./MLSys/prefix-tree.png)

* Workflow pipeline

  * I/O: read sample and group mini-batch -> prefetch (maybe cudaMemcpyAsync()) -> pull/forward/backward/push
  * SparseNet and DenseNet

* Optimization for Advanced Model Server

  * Network: [Seastar](https://github.com/scylladb/seastar) + zero-copy/CPU-binding

* Online Learning with XDL

  * Feature Entry Filter
  * Incremental Model Export
  * Feature Expire

#### Ethane: Taking control of the enterprise, SIGCOMM 2007

make networks more manageable and more secureï¼Œä¸€ç§æ€è·¯æ˜¯å…¨æ–¹ä½çš„å¢åŠ æ§åˆ¶ï¼Œç›¸å½“äºæ–°å¢ä¸€å±‚ï¼Œåªæ˜¯ hide äº†å¤æ‚åº¦ï¼›äºæ˜¯æå‡º ethane è§£å†³è¿™ä¸€é—®é¢˜

ethaneçš„æ€æƒ³ï¼š
* The network should be governed by policies declared over high-
level names
* Policy should determine the path that packets follow
* The network should enforce a strong binding between a packet
and its origin.

Ethaneçš„ä¼˜åŠ¿ï¼š
* Security follows management.

* Incremental deployability.

* Significant deployment experience.
  
#### Serving DNNs like Clockwork: Performance Predictability from the BottomUp, OSDI 2020

[presentation](https://www.usenix.org/conference/osdi20/presentation/gujarati) æŒºæœ‰æ„æ€

model serving: ML system's "narrow waist"

è¿™ç¯‡æ–‡ç« å°è¯•è§£å†³æœåŠ¡åŒ–è¯·æ±‚é•¿å°¾é—®é¢˜

é¦–å…ˆåˆ†æäº§ç”Ÿé•¿å°¾çš„åŸå› ï¼šout-of-order scheduling, interference from concurrency, power saving modes, and network queuing delays.
ç„¶ååŸºäºä»¥ä¸‹ä¸¤ä¸ªå‡è®¾ï¼š
1) â€œDNN inference is predictable.â€
2) èƒ½é™åˆ¶ç³»ç»Ÿåˆ°åº”ç”¨å±‚é¢çš„å†³ç­–èƒ½åŠ›ï¼ˆå‡å°‘workerå†…éƒ¨çš„å¹¶è¡Œï¼‰

æå‡ºè§£å†³æ–¹æ¡ˆï¼š
åˆ†å¸ƒå¼ç³»ç»Ÿå¸¸ç”¨çš„æ€è·¯ï¼Œrequestæ‰“åˆ°workerä¹‹å‰ï¼Œå…ˆè¿‡ä¸€ä¸ªä¸­å¿ƒcontrollerï¼Œä¸­å¿ƒcontrolleræŒæ¡å…¨å±€ä¿¡æ¯ï¼ˆæ¨¡å‹æ˜¯å¦loadã€workeræ˜¯å¦pendingç­‰ï¼‰ï¼Œé¢„æµ‹latencyæ˜¯å¦ä¼šè¶…è¿‡SLAï¼Œä»¥å†³å®šå°†è¯·æ±‚æ‰“åˆ°å“ªä¸ªworker

æ„Ÿè§‰è¿™ä¸€ç³»ç»Ÿéš¾ä»¥ç›´æ¥åº”ç”¨äºå¤§å…¬å¸çš„åœºæ™¯ï¼Œå› ä¸ºï¼š

1.éœ€è¦å’Œrpcæ¡†æ¶åšæ›´æ·±çš„ç»“åˆ

* é•¿å°¾é—®é¢˜æœ¬èº«æœ‰ä¸€éƒ¨åˆ†æ˜¯æ¥è‡ªäºæœåŠ¡åŒ–å¸¦æ¥çš„ç½‘ç»œä¼ è¾“å¼€é”€ï¼Œæ¯”å¦‚thrift workerè´Ÿæ‹…ï¼Œåªæœ‰rpcæ¡†æ¶èƒ½æŒæ¡æ›´å¤šä¿¡æ¯
* å¦‚æœè¦è½åœ°åˆ°ç”Ÿäº§åœºæ™¯ï¼Œè‡ªåˆ¶çš„ç®€é™‹ controller ä¸æ˜“æ¨å¹¿

2.è‡ªèº«çš„ä¼˜åŠ¿ä¸æ˜æ˜¾

* åˆ†ä¸šåŠ¡æœåŠ¡åŒ–éƒ¨ç½²ã€å¹¶ä¸”æ˜¯online learningçš„åœºæ™¯ï¼Œæ˜¾å­˜ä¸æ˜¯ç“¶é¢ˆï¼Œæ¨¡å‹æœ¬èº«å·²ç»æ˜¯preloadäº†
* scalableèƒ½åŠ›æœªç»è¿‡éªŒè¯ (6.6)ï¼Œcontrolleræˆä¸ºç“¶é¢ˆ

æœ‰å¯å‘çš„åœ°æ–¹
* æ¡†æ¶å†…çš„page cacheå¯ä»¥å€Ÿé‰´ä¸€ä¸‹ (https://gitlab.mpi-sws.org/cld/ml/clockwork/-/blob/master/src/clockwork/cache.h)

#### The Hardware Lottery, 2020

https://hardwarelottery.github.io/

* hardware, software and ML research communities evolve in isolation
  * Our own intelligence is both algorithm and machine.
  * Moore's Law ---> The predictable increases in compute and memory every two years meant hardware design became risk-averse.
  * machine learning researchers rationally began to treat hardware as a sunk cost to work around rather than something fluid that could be shaped
* The Hardware Lottery
  * "Happy families are all alike, every unhappy family is unhappy in itâ€™s own way." (Tolstoy & Bartlett, 2016)
  * e.g. Babbage çš„æ„æƒ³ç›´åˆ°äºŒæˆ˜ electronic vacuum tubes çš„ä½¿ç”¨æ‰æˆä¸ºç°å®ã€‚"being too early is the same as being wrong."
  * von Neumann Bottleneck â€” the available compute is restricted by â€œthe lone channel between the CPU and memory along which data has to travel sequentiallyâ€ (Time, 1985).
  * GPU å¹¶è¡Œèƒ½åŠ› ---> é«˜ FLOPS ---> èƒ½åšçŸ©é˜µä¹˜ ---> è®­å¾—åŠ¨æ·±åº¦ç¥ç»ç½‘ç»œ
* The Persistence of the Hardware Lottery
  * sparsity ~ Ampere Architecture
  * è¾ƒä¸ºå®‰å…¨çš„ç¡¬ä»¶ä¼˜åŒ–æ–¹å‘ï¼šmatrix multiplication, unstructured sparsity, weight specific quantization
  * the difficulty of trying to train a new type of image classification architecture called capsule networks on domain specialized hardware
* The Likelyhood of Future Hardware Lotteries
  * how much future algorithms will differ from models like deep neural networks?
    * è®¸å¤šå­é¢†åŸŸï¼Œå‚æ•°é‡å¯¹æ•ˆæœæå‡çš„è¾¹é™…æ•ˆåº”åœ¨ä¸‹é™ï¼ˆè¿‘ä¼¼å¯¹æ•°å…³ç³»ï¼‰
    * 100TB model (fp16) ~ 50T ~ 50ä¸‡äº¿å‚æ•°
    * Our own intelligence relies on decentralized local updates which surface a global signal in ways that are still not well understood
* The Way Forward
  * Producing a next generation chip typically costs $30-80 million dollars and takes 2-3 years to develop
  * A software evolution
    * one way is to focus on the development of domain-specific languages which cater to a narrow domain.
    * another way is to automatically auto-tune the algorithmic parameters of a program based upon the downstream choice of hardware.
* å¦ä¸€ç¯‡å¼ºè°ƒ General Method + ç®—åŠ› å¤§åŠ›å‡ºå¥‡è¿¹çš„ blog: http://www.incompleteideas.net/IncIdeas/BitterLesson.html

#### DCAF: A Dynamic Computation Allocation Framework for Online Serving System, DLP-KDD 2020

* åŠ å¼º å¬å›ã€ç²—æ’ã€ç²¾æ’ çš„è”åŠ¨ï¼Œå‘ç»Ÿä¸€åˆ†é…ç®—åŠ›çš„æ–¹å‘å‘å±•
* We formulate this resource allocation problem as a knapsack problem and propose a Dynamic Computation Allocation Framework (DCAF).

* åŸºäºèƒŒåŒ…é—®é¢˜çš„æœºåˆ¶ï¼Œæœ‰é™èµ„æºæœ€å¤§æ”¶ç›Š
  * ç†è®ºï¼šhttps://en.wikipedia.org/wiki/Duality_(optimization)ï¼Œå‡¸ä¼˜åŒ–ï¼Œè¯æ˜äº†åœ¨ç°å®ç®—åŠ›çº¦æŸçš„æ¡ä»¶ä¸‹ï¼ˆæœ‰ä¸¤ä¸ªç›´è§‰çš„å‰æï¼‰ï¼Œç”¨äºŒåˆ†æ¥æ‰¾ global optimal lambda å³å¯è·å–æœ€ä¼˜è§£
    * construct the Lagrangian

* ç³»ç»Ÿæœ‰ control èƒ½åŠ›ï¼Œèƒ½åŠ¨æ€å“åº”æµé‡æ³¢åŠ¨
  * ç†è®ºï¼šhttps://en.wikipedia.org/wiki/PID_controller

* Online Decision Maker
* Information Collection and Monitoring
* lambda ç¦»çº¿è®¡ç®—ï¼ŒQij åœ¨çº¿é¢„ä¼°
* Request Value Estimation.
* Policy Execution: assign j and PID controlï¼Œæˆ‘ç†è§£ PID controller æ˜¯ä¸ºäº†ç»™ lambda æ›´æ–°æ…¢çš„çš„æƒ…å†µæ¥å…œåº•
* Offline Estimator
* æ„Ÿè§‰æ˜¯ä¸ªç¦»çº¿ batch ä»»åŠ¡ï¼Œæ¨¡å‹é¢„ä¼°ä¸åŒç®—åŠ›ä¸‹çš„ctr

* Experimentsï¼šæ§ç²¾æ’æ¡æ•°ï¼Œå¢åŠ æ¡æ•°æœ‰æ˜æ˜¾çš„è¾¹é™…æ•ˆç›Š
* TODO: fairness é—®é¢˜ã€å…¨é“¾è·¯ç®—åŠ›åˆ†é…

* ä¸€äº›å¼•ç”¨çš„è®ºæ–‡
  * Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications
  * RobinHood: Tail latency aware cachingâ€“dynamic reallocation from cache-rich to cache-poor



#### A scalable pipeline for designing reconfigurable organisms, PNAS 2020

ML with bioengineering

å¦‚ä½•æ¢ç´¢æ›´é«˜æ•ˆçš„å™¨å®˜ç»„ç»‡

* æ¨¡æ‹Ÿ(silico)ï¼šperformant + conform to constraints
* æ¨¡æ‹Ÿ(silico) ->ç°å®(vivo)ï¼šnoise resistance + build filter
* ç›®æ ‡ï¼šè§ Object Manipulation å°èŠ‚
