# LLM MLSys

[toc]

## èµ„æº

* https://cs.stanford.edu/~chrismre/#papers

## Intro

* Intro
  * æœªæ¥ç¡¬ä»¶ï¼Œå†…å­˜äº’è¿å¾ˆå…³é”®
* æŠ€æœ¯å‘å±•
  * Memory Efficient Attention with Online Softmax (2021) -> FlashAttention in Megatron-LM (2022) 
  * Continuous Batching (2022), Paged Attention (2023) -> vLLM, TensorRT-LLM (2023) 
  * Speculative Sampling (2023) -> Everywhere in LLM Serving (2023)
  * Sequence Parallel (2023) ->  Megatron-LLM (2023) 
* ä¸šåŠ¡ç›®æ ‡ï¼šhttps://mp.weixin.qq.com/s/llalxX6miJRxy0-Vk8Ezpg
  * MFUï¼ˆModel FLOPs Utilizationï¼‰ï¼šè¯„ä¼°GPUç®—åŠ›çš„æœ‰æ•ˆåˆ©ç”¨ç‡ï¼Œå°±æ˜¯GPUçš„ç®—åŠ›åˆ°åº•æœ‰å¤šå°‘ç”¨æ¥å¹²æ´»çš„ã€‚å¦‚æœMFUä½äº50%ï¼Œåˆ™å±äºå¼‚å¸¸ï¼Œå¦‚æœèƒ½è¾¾åˆ°70-80%ï¼Œé‚£æ•ˆç‡å°±å¾ˆé«˜äº†ï¼›
  * æ•…éšœç‡ï¼šåœ¨å¤§è§„æ¨¡çš„é›†ç¾¤ä¸­ï¼Œæ¨ç†è¯·æ±‚çš„æ•…éšœç‡ï¼Œå› ä¸ºåœ¨ä¸€ä¸‡å¼ å¡çš„é›†ç¾¤ä¸­ï¼Œå¦‚æœæ¯å‡ åˆ†é’Ÿå°±æœ‰ä¸€å¼ å¡æŒ‚æ‰ï¼Œé‚£ä¹ˆè¿™ä¼šå½±å“æ•´ä½“æ•ˆç‡ï¼Œæˆ–è€…è¯´çœ‹æ•…éšœæ—¶é—´å åœ¨æ•´ä¸ªæœ‰æ•ˆè®­ç»ƒæ—¶é—´çš„å æ¯”ï¼Œå¦‚æœè¯´æ˜¯æ•…éšœçš„æ—¶é—´å è®­ç»ƒæ—¶é—´æ¯”ä¾‹è¶…è¿‡30%ï¼Œä¹Ÿéå¸¸å½±å“æ•ˆç‡ï¼›
  

## æˆæœ¬å’Œæ€§èƒ½è¯„ä¼°

* Intro
  * AIGCæ˜¯å¤§å›½çš„æ¸¸æˆ
    * æ¬§æ´²å—æ¬§ç›Ÿæ³•æ¡ˆå½±å“ï¼Œaiå‘å±•æ²¡è·Ÿä¸Š

  * AIç³»ç»Ÿï¼šè®°å½•æ•°æ®ã€ä¸äººäº¤äº’ã€æœºå™¨å­¦ä¹ åˆ†æã€é¢„æµ‹ã€å¹²é¢„äººçš„å†³ç­–

### FLOPS

* Am,k * Bk,n : `2*m*n*k` FLOPS
  * ä¹˜å’ŒåŠ å„ç®—ä¸€æ¬¡

### Token

```python
import tiktoken

def count_tokens(prompt):
    encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = len(encoding.encode(prompt))
    return num_tokens

prompt_text = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹prompt"
token_count = count_tokens(prompt_text)
print(f"Promptçš„tokenæ•°é‡ä¸º: {token_count}")
```

### æ€§èƒ½

* TTFTï¼štime to first tokenï¼Œå’Œinput tokené•¿åº¦ç›¸å…³
* TPOT


### è®­ç»ƒæˆæœ¬

* LLaMAï¼š2048 A100 21d
  * a100ä¸€ä¸ªæœˆå‡ ååˆ€ï¼Œè®­ä¸€ä¸ªå‡ åä¸‡
* äººåŠ›æˆæœ¬ï¼šè®­ç»ƒåŸºç¡€å¤§æ¨¡å‹ï¼Œå›¢é˜Ÿ20äºº
  * 6ä¸ªæœˆå‡†å¤‡ã€6ä¸ªæœˆè®­ç»ƒã€6ä¸ªæœˆå¾®è°ƒï¼Œ18ä¸ªæœˆè®­æ¨¡å‹
  * ä¸Šä¸‹æ–‡èƒ½åŠ›æå‡ä¹‹åï¼Œæ—¶æ•ˆæ€§ä¼šæ˜¾è‘—å¢å¼º

* Note
  * å’ŒèŠ¯ç‰‡çš„å¯¹æ¯”ï¼šThis â€œgrowthâ€ is strikingly similar to the one involved in chip evolution where as the number of transistors increases (higher density on a chip) the cost for plants manufacturing  those chips skyrocket.  In  the case of chip manufacturing  the economics remained viable because new plants did cost more but they also produced many more chips so that till the middle lf the last decade the cost per chip was actually  decreasing generation over generation (one effect captured in the Mooreâ€™s law).
  * As with chips one may  wonder if there is a limit to the economic affordability (there sure is, it is just difficult  to pinpoint!).
  * TODO: https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/

### GPU

* å­˜é‡å’Œå¢é‡

![image-20241019195324985](./LLM-MLSys/image-20241019195324985.png)

* åˆ†å¸ƒï¼š

![image-20241019195345714](./LLM-MLSys/image-20241019195345714.png)

### å”®ä»·

* https://tiktoken.aigc2d.com/
  * ç»Ÿè®¡tokenæ•°é‡
  * GPT-4o
    * outputï¼š15åˆ€/1M token
    * inputï¼š5åˆ€/1M token



## æ¨ç†ä¼˜åŒ–

### Intro

> https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/

* KV cache
  * LLMæ¨¡å‹é¢„æµ‹çš„æ—¶å€™ä½¿ç”¨çš„æ˜¯KV cacheçš„æŠ€æœ¯ï¼Œä¹Ÿå°±æ˜¯ç¼“å­˜å·²ç»æ¨ç†å‡ºçš„å‰t-1ä¸ªtokençš„KV matrixï¼Œé‚£ä¹ˆåœ¨ç¬¬tä¸ªtokenå¼€å§‹å°±æ— éœ€å†è®¡ç®—è¿™éƒ¨åˆ†KVï¼Œç›´æ¥è°ƒç”¨ç¼“å­˜çš„KVå°±å¯ä»¥ã€‚å…·ä½“è€Œè¨€ï¼Œæ•´ä¸ªMHAåœ¨casual maskä¸‹ï¼Œå¯ä»¥è¡¨ç¤ºä¸ºï¼š $$Logit_{t_h} = \sum_{i \leq t}softmax(\frac{Q_{t_h}K^T_{i_h}}{\sqrt d})V_{i_h}$$,å› æ­¤é¢„æµ‹ç¬¬tä¸ªtokençš„æ—¶å€™ï¼Œqueryçš„multi headï¼ˆhè¡¨ç¤ºï¼‰éœ€è¦é‡æ–°è®¡ç®—ï¼Œä»¥åŠç¬¬tä¸ªkeyå’Œqueryçš„multi headï¼ˆhè¡¨ç¤ºï¼‰è¡¨ç¤ºéœ€è¦é‡æ–°è®¡ç®—ï¼Œå…¶ä½™çš„å°±å¯ä»¥ç›´æ¥ç”¨é¢„æµ‹t-1ä¸ªtokenç¼“å­˜çš„KVè¿›è¡Œè®¡ç®—ã€‚æ•´ä½“ä¸Šä¼šå¤§å¤§èŠ‚çœé¢„æµ‹æ—¶é—´ã€‚é™„ï¼šä½†æ˜¯è¿™éƒ¨åˆ†çš„KVéœ€è¦å ç”¨GPUç¼“å­˜ï¼Œè€Œå¤§æ¨¡å‹ä¸­ç¼“å­˜å ç”¨è¿‡å¤šï¼Œä¼šå¯¼è‡´é¢„æµ‹çš„æ—¶å€™Batch sizeè¿‡å°ï¼Œé‚£ä¹ˆæ•´ä½“çš„é¢„æµ‹ååç‡ä¼šé™ä½ï¼Œæ‰€ä»¥åç»­å¾ˆå¤šå·¥ä½œéƒ½åœ¨å¯¹äºKV cacheåšä¼˜åŒ–ã€‚
* Prefix Cache
  * https://docs.vllm.ai/en/latest/automatic_prefix_caching/apc.html
  
* Mooncakeï¼šå°† P / D åˆ†ç¦»è¿›è¡Œåˆ°åº• https://zhuanlan.zhihu.com/p/1711346141

### Literature Review

* Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. ã€FlashAttentionã€‘
  * These methods range from sparse-approximation [51, 74] to low-rank approximation [12, 50, 84],
    and their combinations [3, 9, 92].
  * æ ¸å¿ƒé—®é¢˜ï¼šthey focus on FLOP reduction (which may not
    correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).

* Eï¬ƒcient ML Models with Structured Matrices. ã€FlashAttentionã€‘
  * Matrix multiply is the core computational bottleneck of most machine learning models. To reduce the computational complexity, there have been numerous approaches to learn over a more eï¬ƒcient set of matrices. These matrices are called structured matrices, which have subquadratic (ğ‘œ(ğ‘›2) for dimension ğ‘› Ã— ğ‘›) number of parameters and runtime. Most common examples of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several more general classes of structured matrices proposed in machine learning: Toeplitz-like [78], low-displacement rank [49], quasi-separable [25]). The butterï¬‚y pattern we use for our block-sparse attention is motivated
    by the fact that butterï¬‚y matrices [15, 64] and their products have been shown to be able to express any structured matrices with almost optimal runtime and number of parameters [16, 20]. However, even though structured matrices are eï¬ƒcient in theory, they have not seen wide adoption since it is hard to translate their eï¬ƒciency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation, a phenomenon known as the hardware lottery [41]. Extensions of butterï¬‚y matrices [17, 18] aimed to make butterï¬‚y matrices more hardware-friendly.
* Sparse Trainingã€FlashAttentionã€‘
  * Our block-sparse FlashAttention can be seen as a step towards making sparse model
    training more eï¬ƒcient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23, 38, 39, 55, 76]. For model training, the lottery tickets hypothesis [28, 29, 30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network.
* Eï¬ƒcient Transformer.ã€FlashAttentionã€‘
  * Transformer-based models have become the most widely-used architecture in
    natural language processing [22] and computer vision [24, 91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12, 54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52, 57, 79, 89]. One can also attend over the states from previous sequences
    to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details.
    There are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31, 36, 37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models. They combine the strengths of CNNs (eï¬ƒcient training), RNNs (eï¬ƒcient inference), and continuous models (robust to change in sampling rates). LambdaNetworks [2], AFT [93] and FLASH [42] are other attempts at replacing attention in the context of image classiï¬cation and language modeling.

### Best Practicesï¼šä½¿ç”¨ GemLiteã€TorchAO å’Œ SGLang åŠ é€Ÿ LLM æ¨ç†

> https://pytorch.org/blog/accelerating-llm-inference/
>
> é€‰å‹ï¼šint4 weight only quantization (both tinygemm and GemLite version), float8 dynamic quantization

* ç°æœ‰çš„ä½ç²¾åº¦æ¨ç†æ–¹æ¡ˆåœ¨å° batch size åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

  - å½“ batch size å¢å¤§æ—¶ï¼Œæ€§èƒ½ä¸‹é™

  - å¯¹é‡åŒ–ç±»å‹çš„é™åˆ¶ï¼Œä¾‹å¦‚ï¼Œä¸€äº›è®¡ç®—æ ¸ï¼ˆkernelsï¼‰ä»…æ”¯æŒå¯¹ç§°é‡åŒ–ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹åœ¨è¾ƒä½æ¯”ç‰¹ä¸‹çš„å‡†ç¡®æ€§

  - é‡åŒ–ã€åºåˆ—åŒ–å’Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰çš„ç›¸äº’å½±å“ï¼Œä½¿å¾—åŠ è½½é‡åŒ–æ¨¡å‹å˜å¾—å›°éš¾ï¼Œå¹¶ä¸”å¯èƒ½éœ€è¦å¯¹ç”¨æˆ·æ¨¡å‹è¿›è¡Œä¿®æ”¹

* é›†æˆï¼š

  * GemLite ï¼šä¸€ä¸ªåŸºäº Triton çš„è®¡ç®—æ ¸ï¼ˆkernelï¼‰åº“ï¼Œè§£å†³äº†å¤§ batch size åœºæ™¯ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æ”¯æŒæ›´çµæ´»çš„é‡åŒ–æ–¹å¼ã€‚
  * TorchAO ï¼šä¸€ä¸ªåŸç”Ÿ PyTorch åº“ï¼Œä¸ºé‡åŒ–ã€ç¨€ç–æ€§å’Œå¼ é‡å¹¶è¡Œï¼ˆä¸ DTensor ç»“åˆä½¿ç”¨ï¼‰æä¾›äº†ç®€åŒ–çš„ç”¨æˆ·ä½“éªŒã€‚
  * SGLang ï¼šä¸€ä¸ªå¿«é€Ÿã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„ LLM å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒå¹¿æ³›çš„æ¨¡å‹ç±»å‹ã€‚

* a summary of the results in **8xH100 machine on Llama 3.1-8B for decode**. 

  * **int4 Weight-Only Quantization**: This method significantly reduces memory footprint and **accelerates decode for memory-bound workloads**, with minimal impact on performance in compute-intensive scenarios like prefill or larger batch sizes. We present results for bf16, GemLite, and tinygemm kernels below, across various batch sizes and tensor parallel configurations
  * **float8 Dynamic Quantization**: While offering less memory savings, this method often provides higher accuracy and balanced speedups for both memory-bound and compute-bound tasks. With Hopper-grade hardware and native fp8 support, the efficient cutlass/cuBLAS kernels used by AO contribute to a significant speedup

![image-20250409022139221](./LLM-MLSys/image-20250409022139221.png)

> æ›´è¯¦ç»†çš„å®éªŒç»“è®ºï¼šhttps://developers.redhat.com/articles/2024/10/17/we-ran-over-half-million-evaluations-quantized-llms#real_world_benchmark_performance





### è®¿å­˜ä¼˜åŒ–

#### FlashAttention: Fast and Memory-Eï¬ƒcient Exact Attention
with IO-Awareness

> https://github.com/HazyResearch/flash-attention
>
> FlashAttn V1/V2/V3è®ºæ–‡ç²¾è¯» https://www.bilibili.com/video/BV1ExFreTEYa
>
> åŠ¨ç”»ï¼šhttps://www.bilibili.com/video/BV1HJWZeSEF4
>
> æ ¸å¿ƒæ´å¯Ÿï¼šattentionçŸ©é˜µN^2å¤ªå¤§äº†ï¼Œæ— æ³•åˆ©ç”¨192KBçš„SRAMç¼“å­˜

* Intro
  * uses tiling to reduce the number of memory reads/writes
    between GPU high bandwidth memory (HBM) and GPU on-chip SRAM
  * also extend FlashAttention to block-sparse attention
  * 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3Ã— speedup on
    GPT-2 (seq. length 1K), and 2.4Ã— speedup on long-range arena (seq. length 1K-4K).

![image-20250131230121452](./LLM-MLSys/image-20250131230121452.png)

![image-20250201014853281](./LLM-MLSys/image-20250201014853281.png)

* æ€è·¯

  * Our main goal is to avoid reading and writing the attention matrix to and from HBM.
    This requires
    *  (i) computing the softmax reduction without access to the whole input
    *  (ii) not storing the large intermediate attention matrix for the backward pass.

  * (i) We restructure the attention computation to split the input into blocks and make several
    passes over input blocks, thus incrementally performing the softmax reduction (also known as **tiling**).
  * (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.

* ç»“è®ºï¼š
  * in sub-quadratic HBM accesses
  * seq-len 512ï¼Œæ¯”ä»»ä½•ç®—æ³•å¿«
  * seq-len 1000ä»¥ä¸Šï¼Œapproximateç®—æ³•æ›´å¿«
* 3.1 An Eï¬ƒcient Attention Algorithm With Tiling and Recomputation
  * The main challenge in making attention memory-eï¬ƒcient is **the softmax that couples the columns of K (and columns of V).**
  * online softmax
    * Reformer: The eï¬ƒcient transformer
    * **Online normalizer calculation for softmax.**
    * Self-attention does not need ğ‘‚ (ğ‘›2) memory
  * recomputation
    * This can be seen as a form of **selective gradient checkpointing** [10, 34]. While gradient checkpointing has been suggested to reduce the maximum amount of memory required [66],

![image-20250201015720233](./LLM-MLSys/image-20250201015720233.png)



* **IOå¤æ‚åº¦å¯¹æ¯”ï¼š**
  * ![image-20250201022049588](./LLM-MLSys/image-20250201022049588.png)
  * For typical values of ğ‘‘ (64-128) and ğ‘€ (around 100KB), ğ‘‘2 is many times smaller than ğ‘€,
  * $$N^2d^2M^{-1}=(Nd)*N/(M/d)$$
* ![image-20250201024230130](./LLM-MLSys/image-20250201024230130.png)

* Evaluation
  * Path-X and Path-256: åƒç´ é¢„æµ‹ï¼Œä¸¤ä¸ªé»‘ç‚¹æ˜¯å¦ç›¸è¿
    * 256*256
    * Block-sparse FlashAttention: 64k seq len
  * æ€§èƒ½ç›¸æ¯”å…¶å®ƒtransformer![image-20250201025905877](./LLM-MLSys/image-20250201025905877.png)

##### From Online Softmax to FlashAttention

* (Safe) Softmax

  * é—®é¢˜ï¼šSRAMå­˜ä¸ä¸‹N^2çš„logitï¼Œå› æ­¤need to access Q and K three times

* Online Softmax

  * ç†è§£ï¼šdi'æ˜¯æ³¨æ„åŠ›æƒé‡çš„ç´¯ç§¯å’Œ
  * ![image-20250201033508793](./LLM-MLSys/image-20250201033508793.png)

  * ![image-20250201160812250](./LLM-MLSys/image-20250201160812250.png)

#### FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning

> æ ¸å¿ƒï¼šå…ˆéå†Qå†éå†KV
>
> - FAv1è¿˜æ˜¯æ²¡æœ‰åŠæ³•æ¥è¿‘GEMM,ä»…èƒ½è¾¾åˆ°ç¡¬ä»¶FLOPSçš„ç†è®ºä¸Šé™çš„30%,ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚
> - ä¸»è¦åŸå› æ˜¯FAåœ¨GPUä¸Šçº¿ç¨‹å—å’Œwarpä¹‹é—´çš„å·¥ä½œåˆ†é…ä¸ä½³,å¯¼è‡´ä¸èƒ½å‘æŒ¥ç¡¬ä»¶æ•ˆç‡ã€‚
> - é€šè¿‡ä¸‰ä¸ªåšæ³•æ¥è§£å†³ä¸Šè¿°çš„é—®é¢˜
>   - **æ”¹å†™ç®—æ³•ï¼Œå‡å°‘éçŸ©é˜µä¹˜æ³•çš„FLOPS**
>     - éçŸ©é˜µä¹˜åœ¨æ€»è®¡ç®—é‡ä¸­å æ¯”è¾ƒå°‘ï¼Œä½†è€—æ—¶å¾ˆé«˜ï¼ŒGPUå¯¹çŸ©é˜µä¹˜åšäº†å¾ˆå¤šä¼˜åŒ–é€šå¸¸èƒ½å¿«16Xï¼Œå› æ­¤å‡å°‘éçŸ©é˜µä¹˜çš„è®¡ç®—é‡éå¸¸é‡è¦ã€‚
>   - **æ›´å¥½çš„å¹¶è¡Œæ€§**ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå¤´ä¹Ÿå¯ä»¥åœ¨å¤šä¸ªçº¿ç¨‹å—ä¹‹é—´å¹¶è¡Œè®¡ç®—ã€‚
>     - é™¤äº†batchå’Œheadç»´åº¦ï¼Œåºåˆ—é•¿åº¦ä¹Ÿè¦æ”¯æŒå¹¶è¡ŒåŒ–ï¼Œè¿™æ ·èƒ½æé«˜GPUå ç”¨ç‡ã€‚
>   - çº¿ç¨‹å—å†…éƒ¨ï¼Œé€šè¿‡åˆç†çš„ç¼–æ’warpæ¥å‡å°‘å…±äº«å†…å­˜ä¸å¿…è¦çš„è®¿é—®ä»¥åŠé€šä¿¡ã€‚
> - ç»è¿‡ä¸Šé¢ä¸‰ä¸ªæ”¹é€ ç‚¹ï¼Œæ€§èƒ½ä¸Šv2æ¯”v1æå‡æ¥2å€ï¼Œæ•ˆç‡æ¥è¿‘GEMM,è¾¾åˆ°ç†è®ºFLOPSçš„70%

* è®¡ç®—é‡æ’åºï¼šå°†å¤–å¾ªç¯æ”¹ä¸ºéå†Qçš„å—å†…å¾ªç¯éå†K,Vçš„å—ï¼Œæé«˜äº†æ•°æ®å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§ã€‚
  * Qæ¯”KVåœ¨SRAMå¯ä»¥é©»ç•™æ›´é•¿çš„æ—¶é—´ï¼Œç¼“å­˜çš„å­˜æ´»æ—¶é—´æ›´é•¿ï¼Œæ›´èƒ½å‡å°‘HBMçš„è®¿é—®æ¬¡æ•°ã€‚
* å½’ä¸€åŒ–æ“ä½œï¼šæ”¾åœ¨å¾ªç¯å¤–ï¼Œ GPU çº¿ç¨‹ä¹‹é—´çš„å¹¶è¡Œæ€§å¾—åˆ°äº†æå¤§çš„æå‡ã€‚

* å¯¹äºå‰å‘è®¡ç®—:
  1. æ”¹æˆäº†å…ˆéå†Qåœ¨éå†kvï¼Œè¿™æ ·åšåˆ™æ˜¯åœ¨ä½¿å…¶åœ¨è¡Œå—å®ç°äº†å¹¶è¡Œã€‚ æ¯ä¸ªè¡Œå—åˆ†é…ç»™ä¸€ä¸ªGPUçš„thread blockï¼Œä»å…±äº«å†…å­˜çš„è§†è§’çœ‹Så¯ä»¥è¢«æ›´å¥½çš„å¤ç”¨ï¼Œä»å¹¶è¡Œåº¦ä¸Šå°†ï¼Œå¯ä»¥åˆ©ç”¨åºåˆ—ç»´åº¦çš„å¹¶è¡Œæ€§æ¥åŠ é€Ÿè®¡ç®—ã€‚
  2. å¯¹äºæ¯ä¸ªworkå†…éƒ¨çš„warpï¼Œå°½å¯èƒ½çš„å‡å°‘å…¶å¯¹å…±äº«å†…å­˜çš„è¯»å†™å¯ä»¥è·å¾—å¯è§‚çš„åŠ é€Ÿï¼Œfa1 æ˜¯å…±äº«kvï¼Œä½†æ˜¯qéœ€è¦è¢«æ‰€æœ‰warpè®¿é—®å’Œè¯»å†™ï¼Œä¸»è¦æ˜¯é¢‘ç¹çš„æ›´æ–°å…±äº«å†…å­˜ã€‚
  3. è€Œ fa2ï¼Œå…±äº«äº†qï¼Œæ¯ä¸ªwarpè¯»å–è‡ªå·±åˆ†å—å†…çš„kvï¼Œä¸éœ€è¦æ›´æ–°ä¸é€šä¿¡ï¼Œè·å¾—äº†åŠ é€Ÿã€‚
* å¯¹äºåå‘è®¡ç®—:
  1. è¿˜æ˜¯æŒ‰ç…§åˆ—å—è¿›è¡Œå¹¶è¡Œï¼Œè¿™æ ·å¹¶è¡Œçš„ç»„å—ä¹‹é—´æœ‰æœ€å°çš„é€šä¿¡è¡Œä¸ºï¼Œå¦åˆ™dkå’Œdvä¹Ÿè¦å…±äº«é€šä¿¡äº†ã€‚
  2. å°½ç®¡æŒ‰åˆ—åˆ†å—åï¼Œdqï¼Œdkï¼Œdvä¹‹é—´çš„ç›¸äº’ä¾èµ–å¾ˆå¤æ‚ï¼Œä½†é¿å…åˆ‡kï¼Œä¹Ÿä¾æ—§å¯ä»¥å‡å°‘warpå†…éƒ¨çš„å…±äº«å†…å­˜è¯»å†™çš„å¼€é”€ã€‚

#### FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision

**æ´å¯Ÿ**:  æ²¡æœ‰å……åˆ†åˆ©ç”¨æœ€æ–°ç¡¬ä»¶ä¸­çš„æ–°åŠŸèƒ½(tensor core ä¸ TMA)

1. å¼‚æ­¥åŒ–: åˆ©ç”¨ä¸“ç”¨warpé‡å è®¡ç®—ï¼Œmatmulå’Œsoftmaxã€‚
2. ä½ç²¾åº¦: åº”ç”¨FP8é‡åŒ–ï¼Œæ›´å¥½çš„åˆ©ç”¨tensor coreç‰¹æ€§ã€‚

æŒ‘æˆ˜: é‡å†™FA2æ¥é€‚é…å¼‚æ„ç¡¬ä»¶ï¼Œæœ€å°åŒ–FP8/4çš„é‡åŒ–è¯¯å·®ã€‚

**æ–¹æ¡ˆ**: 

1. ä¸“ç”¨warpå¼‚æ­¥åŒ–: é€šè¿‡æ‹†è§£ç”Ÿäº§è€…/æ¶ˆè´¹è€…warpæ¨¡å¼ï¼Œç§»åŠ¨æ•°æ®æ¥å®ç°æŒ‡ä»¤ä¸è®¿å­˜çš„é‡å 
2. éšè—softmaxè®¡ç®—:é€šè¿‡ä¼˜åŒ–ä¾èµ–å…³ç³»ï¼Œå°†éGEMMçš„è®¡ç®—éšè—åœ¨GEMMçš„å¯å¼‚æ­¥åŒ–é˜¶æ®µ
3. å—é‡åŒ–/éç›¸å¹²å¤„ç†:  è¡¥å¿FP8é‡åŒ–é€ æˆçš„ç²¾åº¦æŸå¤±ã€‚

* Tritonå®ç°ï¼šæ˜¾å­˜ä¸Šå®ç°ringbuffer

## SGLang

* Intro
  * known for its almost [zero-overhead batch scheduler](https://lmsys.org/blog/2024-12-04-sglang-v0-4/) and fast [constrained decoding](https://lmsys.org/blog/2024-02-05-compressed-fsm/)



## æ¨¡å‹è®­ç»ƒ

### æ˜¾å­˜ä¼˜åŒ–

*  Intro
  * 7Bæ¨¡å‹ï¼š
    * float32: 70*10^8 * 4B = 26.7GB
    * å¾®è°ƒï¼šè€ƒè™‘ä¸­é—´ç»“æœï¼Œ100GBä»¥ä¸Š
  * gpt-3ï¼š
    * 175B 700GB
      * Fp16 326GB
    * ç®—ä¸Šadamä¼˜åŒ–å™¨2100GB
    * æ··åˆç²¾åº¦è®­ç»ƒï¼š
      * fp16å‚æ•°ã€fp32å‚æ•°copyã€fp16æ¢¯åº¦ã€fp32æ¢¯åº¦ã€fp32å†å²æ¢¯åº¦æ»‘åŠ¨å¹³å‡ã€fp32å†å²æ¢¯åº¦å¹³æ–¹å’Œæ»‘åŠ¨å¹³å‡
      * `(1+2+1+2+2+2)*2*175=3,500 GB`

* Activations can take up a significant amount of memory [7] during training. As a concrete
  example, the 1.5B parameter GPT-2 model trained with sequence length of 1K and batch size of
  32 requires about 60 GB of memory.  ã€ZeRO paperã€‘
  * Activation checkpointing (or activation recomputation)
    is a common approach to reduce the activation memory by approximately the square root of
    the total activations at the expense of 33% re-computation overhead [7]. This would reduce
    the activation memory consumption of this model to about 8 GB.
  * The activation memory of a transformer-based model is proportional to the number of transformer layers Ã— hidden dimensions Ã— sequence length Ã— batch size. For a GPT-2 like architecture the total activations is about 12 Ã— hidden dim Ã— batch Ã— seq length Ã— transformer layers.

* ZeROï¼Œå‚è€ƒ ã€ŒMLSys.mdã€



### å¤§è§„æ¨¡é›†ç¾¤

* [ByteDance] MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs 
  * https://arxiv.org/pdf/2402.15627

### Ckpt

* å­—èŠ‚Ckpt https://mp.weixin.qq.com/s/4pIAZqH01Ib_OGGGD9OWQg
  * ByteCheckpoint ï¼Œä¸€ä¸ª PyTorch åŸç”Ÿï¼Œå…¼å®¹å¤šä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ Checkpoint çš„é«˜æ•ˆè¯»å†™å’Œè‡ªåŠ¨é‡æ–°åˆ‡åˆ†çš„å¤§æ¨¡å‹ Checkpointing ç³»ç»Ÿã€‚

## æ¨ç†æ¡†æ¶

* MLLMæ¨ç†
  * SGLang
  * LMDeploy
  * vLLM



## Vision Model æ¨ç†

* Swinï¼šmicrosoft/swinv2-large-patch4-window12-192-22k
  * pytorchåŸºç¡€ä½¿ç”¨
  * 1*V100
    * Batch_size=4, qps=32ï¼š æ˜¾å­˜ç“¶é¢ˆ 24G/32Gï¼Œ150W/300W
    * Batch_size=8, qps=20ï¼š æ˜¾å­˜ç“¶é¢ˆ 26G/32Gï¼Œ115W/300W
    * Batch_size=2, qps=27ï¼š æ˜¾å­˜ç“¶é¢ˆ 30G/32Gï¼ŒW/300W
  * æ³¨ï¼šqpså·²ç»è€ƒè™‘äº†batch_size

* Dinov2
  * Batch_size=4, qps=50: æ˜¾å­˜ 14Gï¼Œ120W

## åˆ†å¸ƒå¼ Agent

* [èš‚èš Ray](https://mp.weixin.qq.com/s/TFxzMJyQVoffV4SpiTh9AQ?open_in_browser=true)

  * è§£å†³çš„é—®é¢˜ï¼šagentçš„è´Ÿè½½ã€å½¢å¼å¤šæ ·ï¼Œä»POCåˆ°ä¸Šçº¿çš„gap
  * Ray-Agent ï¼ˆragentï¼‰
    * ä¸»è¦è€ƒè™‘ç‚¹å¦‚ä¸‹ï¼šâ‘ è¯¥æ¡†æ¶éœ€æä¾› Agent çš„ APIï¼›â‘¡åˆ©ç”¨ Ray å®ç°ä»æœ¬åœ°ä»£ç åˆ°æ”¯æŒå¼‚æ„èµ„æºçš„åˆ†å¸ƒå¼ä»£ç çš„æ‰©å±•ï¼›â‘¢åœ¨å¤š Agent åœºæ™¯ä¸­ï¼Œæ¯ä¸ª Agent éƒ½æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è¿›ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¡†æ¶æ¥åè°ƒè¿™äº›è¿›ç¨‹ï¼Œå³æ‰€è°“çš„ environmentï¼›â‘£è¦å…¼å®¹ä¸åŒçš„åº“ï¼Œå¦‚ MetaGPT å’Œ AutoGenï¼›â‘¤å¸Œæœ›åˆ©ç”¨ Ray çš„æ²™ç®±ï¼ˆsandboxï¼‰ã€æ‰¹å¤„ç†èƒ½åŠ›å’Œè·¨æºè°ƒåº¦åŠŸèƒ½ã€‚

  * ![image-20250228002001015](./LLM-MLSys/image-20250228002001015.png)

  * ![image-20250228002616025](./LLM-MLSys/image-20250228002616025.png)

  * ![image-20250228003128059](./LLM-MLSys/image-20250228003128059.png)
  * ![image-20250228003143293](./LLM-MLSys/image-20250228003143293.png)

  * ![image-20250228005839274](./LLM-MLSys/image-20250228005839274.png)

  * ![image-20250228010735262](./LLM-MLSys/image-20250228010735262.png)
  * æœªæ¥æœŸæœ›ï¼š
    * Agent Mesh/Agent Protocol
    * ç¦»åœ¨çº¿ä¸€ä½“æ¶æ„ï¼šå¯ä»¥ç”¨ Ray Data pipeline å®Œæˆç¦»çº¿å·¥ä½œ

## LLMOps

[Observability in LLMOps pipeline - Different Levels of Scale](https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline)