为什么AI搜索有前景，尤其在字节 —— 火山AI Force大会后杂谈

💡 技术趋势

1. Agent技术趋势
都说2025年是Agent的元年，本质在于近年来模型能力的发展，让Agent能力（任务流程中模型具备自主决策能力）到达了可用的门槛。但可用不等于好用，常有人提出超出模型能力边界或没用好模型的反例，AI搜索具备 1）解决发散问题而非封闭问题 2）侧重信息整合能力大于推理能力 这两大特点，使其产品容错率更高，易于在Agent技术浪潮中受益。

2. 突破模型能力的上限
业界存在共同认知，LLM Pretrain的发展已陷入瓶颈，原因是数据枯竭，Data Scaling跟不上参数Scaling的步伐。OpenAI发文称“如果要继续往前走，就需要算法创新，需要追求数据效率，用尽可能少的数量学习到尽可能多的智能模式。” 这诚然是一条路，但由于LLM训练范式相对成熟，模型结构的突破难度高（比如RWKV等新架构Scaling Up困难），因此另一条路是引入新的数据和任务形式。

从模型上限的角度思考，基于海量行为数据的搜推系统训练，和LLM训练互补：
弥补数据量少；
模型从“读取世界，学习预测客观、稠密知识”（人类高度提炼的文本内容）向“观察世界，学习预测主观、稀疏知识”演进（人类社会的幽微之处与情感倾向），对模型能力提出了更高的要求。
展望人工智能发展史，更高的任务要求+更多数据+更大参数量+更合理的模型+更高效的工程=更强的智能。

📈 商业趋势

1. toc和tob的产品融合趋势
如今toc和tob的产品形态正在趋同和融合，原因是伴随AI产品能力提升，客户陷入焦虑，不再仅需要中间链路上的一个组件（如数据库），而是急迫需要利用AI重塑自身系统，即需要端到端的最佳实践。以LLM API调用为例，其“端到端”的程度实质上是非常高的，能接受多种不同模态输入，并输出内容和格式繁多的结果，在这一基础上，客户对端到端的PaaS系统接受度增加、需求增加，对tob产品的能力要求将更多对标于toc产品，“所见即所得”。

2. 豆包塑造了用户心智
豆包探索了合理的产品形态、用户习惯了豆包，这会使toc产品在能力、交互方面的需求接近于豆包，更多toc需求也会带动tob需求。比如豆包浏览器具备视频理解能力，于是火山AI搜索结合视频理解能力打造了视频AI搜索产品，在某国企场景拿下大单。

3. 信息分发的价值
LLM大幅提升了人类世界信息生产和加工的效率，但在此之前已经创造了大量价值的领域是信息分发 —— 即搜广推，且可以预见的是这一价值将持续繁盛。因此在新的AI产品格局下，打磨信息分发能力、将信息分发与信息生产更充分结合，是有确定性收益的方向。

🚀 字节的行业优势

1. 火山引擎的占有率
火山引擎占中国公有云大模型服务调用量的**46.4%**，在tob市场，大模型和大模型应用是互相生长的：强能力的大模型催生新应用场景、清晰定义的大模型产品与数据促进增长大模型能力。因此豆包大模型乃至豆包App本体及其周边能力，是大模型应用的根基所在。

2. 搜索推荐ToB的优势
字节的推荐ToB基本是全球范围内最早、最强的，在国内市场效果第一，且ToB与ToC的信息流通也较好。因此做大模型与搜推在ToB的结合有较大的优势。

🌱 个人成长

9个月前我陆续参与了4-5个客户POC验证，这段时间是我毕业以来技术成长最快的日子，尤其是技术广度的成长。学习的领域包括LLM算法和应用算法、搜推Scaling Up的MLSys和算法、传统搜索推荐技术、大模型应用产品等，既学习开源技术，又结合内部闭源技术深入研究。
AI搜索对技术广度和深度的综合要求高，原因是为了拿到业务收益，需要服务于产品目标，解决端到端的技术问题，从产品目标的技术可行性验证（建立技术手段和产品目标的关联）到利用好已有技术（LLM应用算法，将各类模型和工具组织好），到针对领域场景应用差异化技术（业务策略与模型微调）再到定位业务的关键瓶颈问题，再到关键问题的建模、数据的组织和Scaling、模型的设计和参数Scaling、评测的建设和产品目标对齐等。综上，这是一个自顶而下的链路，且环节之间的反馈强烈，因此技术的广度和全局视野有利于避免单点优化相对于全局目标的偏差、每个节点的技术深度有利于做出正确的决策。

✍️ 写在最后

以上均是我假想中的投入字节火山AI搜索建设的决策因素，真实决策是因为服从组织和老板安排。

欢迎评论区交流！

#AI搜索 #Agent #字节跳动 #火山引擎 #AI技术 #大模型 #LLM #搜推广 #行业洞察 #科技评论