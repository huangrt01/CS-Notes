[toc]

# 深度学习推荐系统 —— 王喆

## Intro

* 推荐系统与做菜
  * 原材料如何，丰富不丰富，新鲜不新鲜
  * 厨艺好不好，有没有丰富的经验
  * 现场发挥，能不能物尽其用，充分展现厨艺
* 能力要求
  * 知识
  * 工具
  * 逻辑
  * 业务
  

## 算法心得

* 加特征：某些维度上高低估的情况，新增特征之前计算calibration
* 分析全链路，链路瓶颈在哪
  * 比如保送精排有收益说明召回粗排不准
* 是否有信息增益

### 采样

* 技巧：
  * 对靠前曝光位的负样本在采样的时候做降权（尤其baidu app这种，大部分用户意图不是feed流的场景）

### 搜广推的差异

> 王喆：排得更好VS估得更准VS搜的更全
>
> 「推荐、广告、搜索」算法间到底有什么区别？https://zhuanlan.zhihu.com/p/430431149

* 问题定义

  * 搜索：搜索的经典场景是用户主动输入查询query，较明确地表达需求，然后搜索引擎从数据库中检索得到topK最优结果，最后展现给用户，这是一个用户主动获取信息的过程。

  * 推荐：推荐一般不存在用户主动提供query，而是系统根据其用户画像（性别、职业...）以及历史行为（浏览、点击、收藏...）等，排出最可能使用户消费的内容展现出来，这是一个用户被动获取信息的过程。

  * 广告：一般广告的参与有三个角色：广告主、平台和用户。与搜推的为用户找信息的过程不同，广告则是为信息找人，目的也非常直接，纯粹就是为公司增加收入，可以说是离 最近的业务了。

* 业务目标

  - 搜索：相比与推荐和广告，搜索在某种意义上是存在『正确答案』的，即用户是带着明确目的来完成这次行为的。
    - 所以搜索第一目标就是相关性，围绕『搜索词』展开优化，将"正确答案"展现给用户；
    - [推荐算法](https://zhida.zhihu.com/search?content_id=183780223&content_type=Article&match_order=2&q=推荐算法&zhida_source=entity)强调的个性化永远只是搜索算法的补充。“围绕着搜索词的信息高效获取问题“才是搜索算法想解决的根本问题。
    - 其次目标才是CTR/CVR/GMV等。随着相关商品量和算法的发展，现在也越来越强调类似推荐的 **个性化/千X千面** 。

  - 推荐：推荐算法的预估目标就不尽相同，视频类更多倾向于预测观看时长，新闻类预测CTR，电商类预估客单价等等这些 **跟用户参与度最相关的业务指标** 。
    - 只有用户的参与度高了，才能让广告系统有更多的inventory，进而增加公司营收。

  - 广告：各大公司广告算法的预估目标非常统一，就是 **预估CTR和CVR**，增加收入

* 评价指标：

  * 搜索：**非常看重能否把这些正确答案给召回回来**，针对召回率、MAP、NDCG这些指标进行优化
  * 推荐：由于推荐系统的推荐场景是生成一个列表，所以更加关注item间的相对位置，因此评估阶段更倾向于用AUC，gAUC，MAP这些指标作为评价标准。
  * 广告：因为CPC和CPA计价仍是效果类广告系统的主流计价方式。所以只有预估出CTR和CVR，才能反向推导出流量的价值，并进一步给出合理的出价。所以针对这样的目标，广告算法非常看重把**预估偏差**当作首要的评价指标。

* 算法设计

  * 搜索：**强调搜索词的关键性，以及对搜索词的理解**
    * 搜索词与其他特征组成的交叉特征，组合特征，以及模型中的交叉部分是异常重要的
    * 一定程度上要抑制个性化的需求，更多把质量和权威性放在更重要的位置
  * 推荐：list level，page level
    * 对推荐内容的**多样性，新鲜度**有更高的要求，这就让探索与利用，强化学习等一些列方法在推荐场景下更受重视。
  * 广告：对calibration方法的严苛要求，就算模型训练的过程中存在偏差，比如使用了负采样、weighted sampling等方式改变了数据原始分布，也要根据正确的后验概率在各个维度上矫正模型输出。此外，因为广告是很少以列表的形式连续呈现的，要对每一条广告的CTR，CVR都估的准，**广告算法大都是point wise的训练方式**。

* 辅助策略和算法上的区别

  * 搜索：NLP技术，对大量内容进行预处理，embedding化
  * 推荐：照顾用户的长期兴趣，需要一些补充策略做出一些看似“非最优”的选择，**比如探索性的尝试一些长尾内容**
  * 广告：pacing，bidding，budget control，ads allocation

* 模型结构差异

  * 搜索：搜索词和item之间天然是一个双塔结构，因此在模型构建的时候各种交叉特征，模型中的各种交叉结构往往是搜索类模型的重点。
  * 推荐：如果不抓住用户兴趣的连续变化，是很难做好推荐模型的，因此利用sequential model来模拟用户兴趣变化往往是有收益的
  * 广告：兴趣是不那么连贯的，因此容易造成sequential model的失效，attention机制可能会更加重要一些
    * 解释了为什么DIN简单实用

* 系统痛点

  * 搜索：往往把重心放在**搜索词和item的内容理解上**，只要能做好这一点，模型结构本身反而不是改进的关键点了，但是在多模态的时代，图片、视频内容的理解往往是制约搜索效果的痛点。
  * 推荐：**问题往往卡在长期利益与短期利益的平衡上**，在模型结构红利消失殆尽的今天，如何破局是推荐算法工程师们做梦都在想的问题。
  * 广告：**各模块协同工作找到平台全局利润最大化方法的难度非常大**，系统异常复杂到难以掌控的地步

## chpt1 互联网的增长引擎——推荐系统

概念：

* UGC(User Generated Content)，典型如Youtube、Tiktok
* CVR (Conversion Rate)、观看时长(Youtube Recommendations)

推荐系统架构：数据与模型

* 物品信息、用户信息、场景信息
* 数据离线批处理、实时流处理

## chpt2 前深度学习时代——推荐系统的进化之路

演化关系图  p13

### 多种方法的对比

|                           | 信息             | 数学假设                                                     | 建模方式                                                     | 优点                                                         | 缺点                                                         |
| ------------------------- | ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| UserCF                    | $$E_C$$          | $$d=N, E_U={E_C}^T$$                                         | 用 User Embedding 检索相似 Users，获取 Users 的 Bhv Seq (Behaviour Sequence) |                                                              | 用户多，存不下矩阵不适用于正反馈获取困难、用户历史数据向量稀疏的场景 |
| ItemCF                    | $$E_C$$          | $$d=u, E_S=E_C$$                                             | 用 User Bhv Seq Embedding 检索相似 Item Embedding            |                                                              | 商品多，存不下矩阵头部效应较明显                             |
| MF (Matrix Factorization) | $$E_C$$          | $$E_C=E_S{E_U}^T$$                                           | 用 User Embedding 检索相似 Item Embedding                    | 泛化能力强空间复杂度低更好的扩展性和灵活性                   | 和UserCF/ItemCF一样，不方便加入context信息                   |
| STAR                      | $$E_C$$、$$E_S$$ | $${E_S^u=\frac{1}{n} (\bold{r\lambda^t})_u^T E_S}$$$${E_C^u=\frac{1}{n} (\bold{r\lambda^t})_u^T E_C}$$ | User Bhv Seq -> User Bhv Emb用 User Bhv Emb 做检索，两路 Merge第一路：用$${E_C}^u$$检索相似 Item第二路：用$${E_S}^u$$检索相似 Item注：STAR paper的实现，实际上不是两路做Merge，而是对全量商品做暴力计算STAR paper按User Bhv Seq顺序做了recency decay | 引入了预训练的Item Embedding，增加了语义信息显式引入协同过滤信息 | 计算复杂度较高，在线计算的挑战性大；仅用 User Bhv Seq 做检索，存在信息茧房 |

### 协同过滤(CF)

[一篇详细的介绍文章](https://zhuanlan.zhihu.com/p/80069337)

* UserCF (1994)

  * 用户相似度计算：余弦相似度/皮尔逊相关系数/引入物品平均分

  * 最终结果的排序：按相似度加权

  * 特点：社交特性更强

  * 缺点：
    * 用户太多，在线存储系统存不下矩阵。 
      * 不适用于正反馈获取困难的应用场景（酒店预定、大件商品购买），用户历史数据向量稀疏


* UserCF 和 ItemCF 的对比

  * UserCF 适合用在个性化需求不强，热点很明显的领域，比如新闻，电影推荐

  * ItemCF 适合用在个性化需求比较强，长尾比较长的领域，比如书，电商的推荐


* 协同过滤的缺点

  * 推荐系统的头部效应较明显，处理稀疏向量的能力弱 => MF用更稠密的隐向量表示用户和物品

  * 无法引入场景信息和更精细的用户/物品信息 => LR模型、机器学习模型




### 矩阵分解(MF)

* 用户和物品的隐向量通过分解协同过滤生成的共现矩阵得到
* 矩阵分解的方法：
  * 特征值分解（方阵，不适用）
  * SVD（计算量大、要求稠密）
  * 梯度下降法

* 矩阵分解可加入偏差向量
  * $$r_{ui}=\mu+b_i+b_u+q_i^Tp_u$$

* 优点：
  * 泛化能力强
  * 空间复杂度低
  * 更好的扩展性和灵活性
* 缺点
  * 同样不方便加入场景信息和更精细的用户/物品/context信息



* [Impl](https://medium.com/@eliasah/deep-dive-into-matrix-factorization-for-recommender-systems-from-basics-to-implementation-79e4f1ea1660)
  * **Alternating Least Squares (ALS)**
    * Fix the user matrix and optimize the item matrix
    * Fix the item matrix and optimize the user matrix
    * Repeat steps 1 and 2 until convergence





* paper
  * Online Learning for Matrix Factorization and Sparse Coding



### 逻辑回归 (LR)

将推荐问题转换为一个CTR预估问题

流行原因：1）数学含义上的支撑 2）可解释性强 3）工程化的需要



### 从FM到FFM——自动特征交叉的解决方案

* Poly2——特征交叉的开始
  * 两两特征交叉，缺点是交叉特征稀疏

* FM模型——隐向量特征交叉 (2012-2014)
  * 用两个特征隐向量的内积作为交叉特征的权重； 隐向量分解的思想扩展到所有特征
  * 参数减少到n*k，减小训练开销，增大收敛可能

* FFM模型——引入特征域的概念 (2015)
  * ![image-20241229013921950](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241229013921950.png)
  * 参数：nkf (f个特征域、hidden len k)
  * Features are empirically categorized into several groups
  * Within each group, features are further identified as user-side and ad-side features
  * For each group, user-side and ad-features are sum-pooled, respectively, followed by element-wise multiplication.
  * Limitations
    * Feature-level interactions are lost due to sum-pooling
    * Using element-wise multiplication of sum-pooled embeddings leads to high dimensional output.




### GBDT+LR——特征工程模型化的开端

* Intro

  * 组合模型，一定程度上解决特征交叉的问题
  * 树深度～特征交叉阶数

  * 优点：
    * e2e特征工程模型化

    * 能选一些能够显著区分正负样本的特征变化方式

  * 缺点：
    * 容易过拟合
    * 丢失大量特征的数值信息
    * 不容易上线，延时高

* 实现：

  * 所有叶子节点组成的向量，构成了LR模型输入 [0,1,0,0,1,0,1]



### LS-PLM —— alibaba曾经的主流推荐模型

* Large Scale Piece-wise Linear Model
  * 也称为MLR(Mix Logistic Regression)，在逻辑回归的基础上加入聚类的思想，其灵感来自对广告推荐领域样本特点的观察
  * $$f(x)=\sum_{i=1}^{m}\frac{e^{\mu_i\cdot x}}{\sum_{j=1}^me^{\mu_j\cdot x}}\cdot\frac{1}{1+e^{-w_i\cdot x}}$$
  * 先对样本分片，再分而治之
  * DL角度：注意力机制的三层模型

* 超参数：分片数m，阿里的经验值为12

* 优势：

  * 端到端的非线性学习能力

  * 模型稀疏性强


* 引申：
  * L1范数比L2范数更容易产生稀疏解，因为加入正则化项的损失函数最小值更容易在参数空间的顶点处取得，对应稀疏参数




## chpt3 浪潮之巅——深度学习在推荐系统中的应用

### Intro

演化关系图：p51

![image-20250102011634184](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250102011634184-5751797.png)

### [AutoRec](https://zhuanlan.zhihu.com/p/159087297) —— 单隐层神经网络推荐模型

* Intro
  * 结合了auto-encoder和协同过滤的思想，本质上是训练自编码器 auto-encoder保存推荐系统的泛化信息，输入一个“不完整”的、“真实”的评分条目列表

* I-AutoRec, U-AutoRec
  * U-AutoRec输入user评分，计算一次即可，但效果会受用户数据稀疏性影响


### Deep Crossing模型 —— 经典的深度学习架构

* Intro

  * 微软Bing搜索广告场景

* 特征：

  * query

    * query

  * ad

    * keyword
    * title

    * Landing page
    * match type
    * history ctr
    * Click prediction
    * 广告计划：预算、定向条件

* 模型：

  * Sparse&dense embedding连起来，过 多层残差网络

* 如何解决稀疏特征向量稠密化的问题？

  * 除数值类特征，进入Embedding层

* 如何解决特征自动交叉的问题？
  * 无人工特征交叉，由 **残差神经网络** 加强提取非线性特征和组合特征信息的能力
  * $$f(x) = Fc2(ReLU(Fc1(x)))+x$$

* 如何在输出层中达成问题设定的优化目标？
  * 最后一层逻辑回归

### NeuralCF模型——CF与深度学习的结合

* MF的缺点：输出层简单，容易欠拟合

* NeuralCF
  * 用神经网络替代CF最后一层打分的点积操作

* NeuralCF混合模型：
  * concat（原始NeuralCF模型的输出，以element-wise product为互操作的广义矩阵分解模型）

* 局限性和CF一致：没引入其他类型的特征、具体的互操作有待探索



### PNN模型——加强特征交叉能力

Product层：线性操作 + 乘积操作
* 乘积操作 = 内积(IPNN)/外积(OPNN)
* 叠加外积互操作矩阵：本质上是让所有embedding通过一个平均池化层后，再进行外积互操作
  * $$p=\sum_{i=1}^N\sum_{j=1}^Nf_if_j^T=(\sum_{i=1}^Nf_i)(\sum_{i=1}^Nf_i)^T$$
  * [embedding的意义](https://www.zhihu.com/question/374835153/answer/1042845667)
  * 简单的sumpooling会忽略一些有价值的信息，对不同类embedding的avg pooling需要谨慎
  



### Wide&Deep模型——记忆能力和泛化能力的综合

#### Wide & Deep learning for Recommender Systems, RecSys 17

* Introduction

  * Wide ~ Memorization: 模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力
    * LR能学到“强特征”
    * 输入特征：已安装应用list、target item


  * Deep ~ Generalization: 模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力
    * 输入特征：全部特征


  * Generalized linear models with nonlinear feature transformations

  * cross-product transformations: 特征工程的概念，交叉积变换，缺点是无法 generalize 没出现过的 query-item feature pairs


* 问题：输入是稀疏高秩矩阵，缺少 interactions，难以利用它学到合适的低维 embedding

WIDE & DEEP Learning

* The Wide Component
  * 利用 cross-product transformation 提供多个特征的非线性
    * $$\phi_k(\mathbf{x}) = \prod_{i = 1}^{d} x_i^{c_{ki}} \quad c_{ki} \in \{0, 1\}$$
    * [Explanation](https://datascience.stackexchange.com/questions/57435/how-is-the-cross-product-transformation-defined-for-binary-features)
  * 对比：[Deep Neural Networks for YouTube Recommendations ] 用平方 和 平方根项 提供单个特征的非线性

* Joint Training of Wide & Deep Model

  * 注意辨析 joint training 和 ensemble 的区别

    * 前者是共同训练，后者不是

    * 后者模型可以更大

    * 前者，Wide 只需要给 Deep 补少量 cross-product feature transformations


* System Implementation

  * Model Training

    * warm-starting system

    * dry run

    * sanity check


* Appendix

  * 概念：AUC：ROC曲线下方的面积，ROC横坐标FPR，纵坐标TPR

  * 资源：
    * 这个大佬的专栏很实用，讲解tensorflow和推荐系统，https://zhuanlan.zhihu.com/learningdeep


* 思考：可否联系到IRLS方法，最优化稀疏矩阵的秩，用一个类似的矩阵学习秩的表示

**改进：Deep&Cross模型 (DCN)**

* 多层交叉层: $x_{l+1}=x_0x_l^Tw_l+b_l+x_l$ 
  * 参数引入较为克制，增强模型的非线性学习能力
  * 解决了Wide&Deep模型人工组合特征的问题

#### [DLRM] Deep Learning Recommendation Model for Personalization and Recommendation Systems

* 模型：
  * dense特征相连之后过一个MLP也变成和Embedding维度一样的tensor A
  * 所有sparse + tensor A两两做内积，相连，再连上A，过MLP
* 训练：并行策略
  - DLRM 参数多，训练时间长，需高效并行。其嵌入部分参数多、内存和带宽需求大，不适用于数据并行；MLP 内存需求小但计算量大，适合数据并行。
  - 因此采用嵌入模型并行和 MLP 数据并行结合的方式。在实现中，需处理 top MLP 和交互算子的数据通信，通过自定义实现模型并行和蝴蝶洗牌算子实现All-to-all通信，并优化数据并行的参数更新。

* 机器：
  * Big Basin platform
  * Xeon 6138 CPU@2.00GHz + 8*Nvidia Tesla V100 16GB GPUs

* 结论：DLRM > DCN

### FM与深度学习模型的结合

* FNN——用FM的隐向量完成Embedding层初始化
  * 背景：Embedding层收敛速度慢 <= 参数多、输入向量稀疏

![image-20250103195716225](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103195716225.png)

* DeepFM——用FM代替Wide部分
  * embedding作为FM的隐向量
  * 对比Deep&Cross：用FM而不是Deep&Cross做特征交叉
* NFM——FM的神经网络化尝试
  * 在Embedding层和多层神经网络之间加入**特征交叉池化层**
    * 对embedding，两两
  * NFM和FM的关系：FM是一种浅层的线性模型, 可以看作是不带隐层的NFM
  * 相比Wide&Deep, Deep&Cross用concat的方法连接特征，NFM方法更侧重特征之间的深度交互



### 注意力机制在推荐模型中的应用

* AFM——引入注意力机制的FM
  * 在NFM的Pair-wise Interaction Lyaer和池化层之间加入注意力网络
  * 注意力网络 = 全连接层 + softmax
    * 全连接层网络输入是 element-wise 交叉的特征vector，输出是score
    * 所有score做softmax



#### (DIN) Deep Interest Network for Click-Through Rate Prediction ——引入注意力机制的深度学习网络

  * 核心动机是常见的 Embedding&MLP 模型，user 特征的表达能力受限于 fixed-length vector。user特征表达能力难以通过简单的翻倍方式来提升（将多个域的user特征连接起来），考虑到线上压力以及过拟合的风险，需要寻求算法上的突破 => weighted sum-pooling
  * Attention 技术：NLP、搜索领域，通常用来刻画context（比如用target词加权句子、用target广告加权最近的query）。本文创新性地用广告特征来加权user序列特征
  * 应用于淘宝的电商广告推荐场景，感觉在电商场景，推荐和广告天然结合地紧密（广告就是推荐的商品）；短视频场景，两者更割裂一些，并且用户兴趣更长期
      * 电商场景，搜索个性化的必要性也更强，和推荐区分不严格
    * 此外，搜索广告场景先通过相关性做召回，严格意义上进入推荐系统的候选数量少
  * 用户侧的embedding是对每次行为的embedding通过注意力加权得到，注意力权重受广告特征影响
    * 广告侧：论文中有 goods id, shop id, cate id；实践中可以pooling ad id, category id, position encoding (特征抽取时做分钟的sqrt，相比 log 对长期历史行为有区分度)
    * 商铺id只和用户历史行为中的商铺id序列发生作用，商品id也如此 <=> 注意力轻重更应该由同类信息相关性决定
  * $\textbf{V}_u=f(\textbf{V}_a)=\sum_{i=1}^N\omega_i·\textbf{V}_i=\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a)·\textbf{V}$ 
      * 注意力激活单元：
        * 元素减操作的embedding（原论文中是取外积向量）, concat两个原输入embedding
      * $\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a) \neq 1$ ，描述 an approximation of the intensity of activated user interests to some degree
  * Training Techniques
    * Mini-batch Aware Regularization：引入L2范数，通过“只对输入中有的sparse fc算”减少运算量
    * Data Adaptive Activation Function：PReLu -> Dice
  * Experiments
    * 新的AUC计算方式：GAUC (averaging AUC over users)
    * Regularization 实验：Mini-Batch Aware L2 > Occurrency Filter > DropOut > Regularization in DiFacto > Base

#### DIEN——序列模型与推荐系统的结合

> DIEN work的前提：1）场景存在兴趣进化；2）数据流完整保留兴趣的进化过程

* 尝试刻画 latent interest 而非用 behaviour 描述 interest
* 兴趣进化网络
  * 行为序列层：普通的Embedding层
  * 兴趣抽取层：GRU
    * auxiliary loss: extra supervision information, uses consecutive behavior to supervise the learning of hidden state at each step. which makes hidden state expressive enough to represent latent interest. 算法实现上，是让第i次的 hidden state 向量更接近第 i+1 次的正样本 item 向量
  * 兴趣进化层：(attentional update gate) AUGRU，引入注意力机制，与 DIN 相似，更有针对性地模拟与目标广告相关的兴趣进化路径
    * 文章中讨论了 attention 结合 GRU 的几种方法：AIGRU、AGRU、AUGRU

#### (DSIN) Deep Session Interest Network for Click-Through Rate Prediction

* multiple sessions：30 mins gap
* session内用户兴趣抽取：multi self-attention mechanism(AutoInt) with bias encoding
* session间兴趣进化：Bi-LSTM
* 聚合兴趣预估ctr：local activation unit

#### UIC+MIMN

[从阿里的User Interest Center看模型线上实时serving方法](https://zhuanlan.zhihu.com/p/111929212)

* system: UIC

* algorithm: MIMN = improved NTM with two designs of memory utilization regularization and memory induction unit

3.REALTIME CTR PREDICTION SYSTEM

- **电商领域，特征中 90% 的规模是 user behaviour features**
- long sequence 架构上的挑战主要在于 storage 和 latency

4.MULTI-CHANNEL USER INTEREST MEMORY NETWORK

- long sequence 算法上的挑战：
  - RNN 结构并不最适合刻画 long sequence:
    - hidden state 偏向于刻画 predicting target 而非 history；
    - 存储所有历史信息的方案较冗余


=> MIMN 

![image-20250111233228050](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111233228050.png)

- **NTM: Neural turing machines.**
- memory utilization regularization: to increase the expressive ability of memory tensor in UIC by increasing the utilization of memory
  - 解决 NTM 的 memory 被热点行为 dominate 的问题：1）LRU 将信息写进不同的 slot，有利有弊；2）Memory Utilization Regularization 
- memory induction unit: to help capture high-order information
  - 继续用GRU进化兴趣，MIMN的特点，兴趣进化是 multi-channel 的（channel间共享GRU的参数），**多通道分别生成记忆向量**
  - ![image-20250111233446127](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111233446127.png)



![image-20250111233647489](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111233647489.png)

#### [SIM] Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction

Search-based Interest Model (SIM) -> Sub user Behavior Sequence (SBS)

- General Search Unit (GSU)
  - soft-search and hard-search
- Exact Search Unit (ESU)
  - multi-attention; encode temporal distance information

training方案：

* ESU: multi-head attention

* GSU: soft-search 联合 ESU 学习，auxiliary CTR model 学习 behavior embedding

serving方案：

* hard-search: user behavior tree (UBT), uid->category id->behavior items

there indeed exists massive noise in original long-term behavior sequences which may undermine long-term user interest learning. 信息并非越多越好，本文也和 Youtube 论文[2] 一样，体现了过滤噪音信息的思想

#### MIND (Multi-Interest Network with Dynamic routing), CIKM 2019

* 与 match 特征的关系：本文是把 match 做到模型里，如果模型能学到 历史 item 到 interest 的转换关系，就相当于是有 match 能力了（match 的 key 我感觉就像是 interest）

* 本质上是捕捉用户兴趣做召回，利用胶囊网络去“压缩”序列特征成兴趣特征，multi-interest embedding 尝试克服 user embedding 的维数局限性
  * multi-interest extractor layer: soft-cluster user interests

* label-aware attention: to help learn a user representation with multiple vectors.

* Related Work
  * User Representation: sequence model; from word embedding
  * Capsule Network. The concept of "Capsule", a small group of neurons assembled to output a whole vector, is firstly proposed by Hinton [13] at 2011. 
    Instead of backpropagation, dynamic routing [21] is used to learn the weights on the connections between cap-sules, which is improved by utilizing Expectation-Maximization algorithm [14] to overcome several deficiencies and achieves better accuracy.

* Method
  * 3.3 Multi-Interest Extractor Layer
    * 由序列特征生成 user embedding，是基于胶囊网络的 user tower，生成 
      multiple representation vectors
  * Dynamic Routing
    * In a nutshell, capsule is a new kind of neuron represented by one vector instead of one scalar used in ordinary neural networks.
    * dynamic routing：胶囊网络中利用 bilinear 矩阵，学习low-level到high-level表征关系
    * 最后一层用 a non-linear "squash" function 处理
  * B2I dynamic routing
    * learn interest capsules from behavior capsules.
    * Shared bilinear mapping matrix：设计的一方面因素是因为 user behaviour 是变长的
    * Randomly initialized routing logits
    * Dynamic interest number: 根据历史序列长度动态调整兴趣数量

  * 3.4 Label-aware Attention Layer
    * 训练时用
    * Q: 这个 dynamic routing 怎么训练？

* Experiments
  * 实验指标 HitRate
  * hard attention scheme 效果最好

### 强化学习与推荐系统的结合

* 强化学习：“行动—反馈—状态更新”循环，收益点是在线学习
* DQN
  * 任何深度学习模型都可作为智能体的推荐模型
* DRN
  * 模型微更新：竞争梯度下降算法
    * dueling bandit gradient descent algorithm
  * 模型主更新：利用历史数据重 train
* dueling bandit gradient descent algorithm
  * 随机扰动、探索网络
  * interleaving然后推送结果
  * $$\Delta W=\alpha *rand(-1,1)\cdot W$$

* 意义：变静态为动态，增强模型学习的实时性
  * “重量”与“实时”的折中



## 新闻推荐

### Literature Review

* 数据增强
  * LLM生成category description https://arxiv.org/pdf/2405.13007
* 广告对推荐的影响
  * [2,10] [YAHOO]

### [YAHOO] Embedding-based News Recommendation for Millions of Users

* 技术发展
  * idrec和低秩分解，缺失语义信息；
  * word-based methods，涉及构造query、orthographical variants（拼写变体）、query生成

* An article is regarded as a collection of words included
  in its text. A user is regarded as a collection of words included
  in articles he/she has browsed. 

* 核心思路：
  * Start with distributed representations of articles based on
    a variant of the denoising autoencoder (which addresses
    the first issue in Section 3).
  * Generate user representations by using an RNN with brows-
    ing histories as input sequences (which addresses the sec-
    ond issue in Section 4).
  * Match and list articles for each user based on the inner
    product of article-user for relevance and article-article for
    de-duplication (outlined in Section 2).

* 业务流程：
  * Identify: Obtain user features calculated from user his-
    tory in advance.
  * Matching: Extract articles from all those available using
    user features.
  * Ranking: Rearrange list of articles on certain priorities.
  * De-duplication: Remove articles that contain the same
    information as others.
    * 贪心算法消重
  * Advertising: Insert ads if necessary

* 特征
  * the expected number of page views and freshness of each article, in addition
    to the relevance used for matching
* 模型
  * a **denoising autoencoder** [19] with weak supervision
    * ![image-20250103183530883](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103183530883.png)
    * 根据category信息，构建一个loss
    * ![image-20250103185036772](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103185036772.png)

* User行为建模

  * session和browse
  * 有可能一个browse却没有session（网页搜索进入）
  * ![image-20250103190827268](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103190827268.png)

  * 建模：考虑负例、position bias
    * ![image-20250103191736717](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103191736717.png)

  * word-based方法的局限性：

    * the sparseness of the representation
    * intensity

  * 优化：

    * ![image-20250103192355241](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250103192355241.png)

    * 缺点：指数遗忘

* 模型：

  * RNN
  * LSTM
  * GRU

* 评估：

  * 在线：user分类
    * Heavy: Users who have visited for more than five days
      during the previous week.
    * Medium: Users who have visited for between two and five
      days during the previous week.
    * Light: Users who have visited for less than two days dur-
      ing the previous week.

* 实时性：

  * article/user representation model，更新比较慢

* 结论：

  * distributed-representation比BoW强很多
  * 对browsing序列做简单的decay效果不好，GRU效果比较好
  * 在线指标：
    * a good recommendation model first increases clicks, and multiple click experiences
      encourage users to gradually use the service more frequently.
    * Light user提升大

### 微软新闻

* msnews.github.io
  * MIND acl2020

* bert noisytune 大模型finetune
* responsible的目标
  * [cprs: objective beyond click](https://www.ijcai.org/proceedings/2020/0418.pdf)
* fairness：fairrec 学习一个serving用的无偏网络，比如让模型估不出来性别

### Empowering News Recommendation with Pre-trained Language Models

* 模型

![image-20250102003946208](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250102003946208.png)

![image-20250102004310634](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250102004310634.png)

* 训练
  * used the titles of news
  * finetuned the last two Transformer layers
* 结论：
  * attention pooling > avg pooling > CLS token embarrassment
    * https://github.com/wuch15/PLM4NewsRec/blob/main/model_bert.py

## 视频推荐

### Intro

* 业务目标：
  * 时长 -> 用户满意度 -> 留存、DAU、ARPU
    * 反例：youtube推荐，无法提升留存和DAU，一直涨时长 -> 推的视频越来越长
    * -> 长视频的目标是 LT、LTV
* 业务特点（by 王喆）：首页播放率大概在1.5%左右，广告更低 0.5%以下

### Deep Neural Networks for Youtube Recommendations, RecSys 16

* 业务
  * 内容来自UGC

  * 头部效应不明显，商业模式不同于Netflix、爱奇艺
    * 时长更重要

* Intro: three major perspectives
  * scale
  * freshness: reponsive, exploitation
  * noise

  * DL在推荐系统中的应用：recommending news or music, citations, review ratings, collaborative filtering, autoencoders
* System Overview
  * candidate generation + ranking，一个two-staged的机制，本质上是对数据和用户行为细粒度做分层
  * 训练时利用offline信息，但测试是用A/B testing
* Candidate generation

![image-20250111225209942](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111225209942.png)

* Recommendation as Classification

  * item emb的学习
    * collaborative filtering + word2vec，类似airbnb的方法学习候选embedding向量
    * a non-linear generalization of factorization techniques，用分类的方法做推荐
      * 输出经过softmax处理，输入的是video向量和user向量的内积
  * user向量的学习：利用implicit feedback
  * 对类别数过多的处理
    * sample负样本再correct via importance weighting[10]，损失函数只涉及true label和sampled negative classes
    * hierachical softmax效果不好
    * serving阶段
      * approximate scoring scheme sublinear in the number of classes 
      * softmax对serving没用，转化为nearest neighbor search问题

* Model Architecture：Figure 3.

  * watch和search vector是对variable-length vectors求平均

  * Heterogeneous Signals
    * DL的优势是能方便地cat各种信息，性别、登入状态、年龄等作为[0,1]变量输入
    * 输入"Example Age" Feature，用来fresh信息，用户喜欢，可能有viral效应[11]
  * Label and Context Selection 
    * generate a fixed number of training examples per user，每个user的权重一致
    * training examples包括用户看的所有视频，而不仅仅是推荐给用户的视频
    * **withhold information from the classifier，问题的来源：推荐用户刚搜索的视频很蠢 =>** 
      **discarding sequence information and representing search queries with an unordered bag of tokens**
      * 感觉更多地适用于搜索+推荐综合场景（用户主动+被动接受信息）
    * asymmetric consumption patterns => Figure 5. predicting future watch
      * ![image-20250111230537204](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111230537204.png)
    * 总结：There is more art than science in selecting the surrogate problem for recommendations

  * Experiements with Features and Depth





![image-20250111225946384](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111225946384.png)

* 数据：

  * 对每个用户，提取等数量的训练样本，减少高度活跃用户对模型的影响

* Ranking

  * The primary role of ranking is to use impression data to specialize and calibrate candidate predictions for the particular user interface.

* 特征 Feature Representation

  * 引入univalent和multivalent的概念对变量分类
    * whether they describe properties of the item (“impression”) or properties of the user/context (“query”), Query features are computed once per request while impression features are computed for each item scored.

  * 主要的挑战是1）时序动作 2）与impression相关的时序动作如何处理
  * 业务特征：
    * 该用户自上次观看**同频道视频**的时间（time since last watch）
    * 该视频已曝光给用户的次数（previous impression）

  * Embedding Categorical Features
    * 维度压缩，对数关系
    * categorical features in the same ID space also share underlying emeddings
    * 占据了模型大部分参数
  * Normalizing Continuous Features
    * 线性插值估算CDF来归一化
    * **输入x的次方和开方，获取非线性特性**

* Modeling Expected Watch Time

  * weighted logistic regression
  * 数学含义：预测Odds
    * $$p=\frac{1}{1+e^{-(Wx+b)}}$$
    * $$Odds=p/(1-p)=e^{Wx+b}$$  ，**机会比**
    * logit函数：$$logit(p)=ln(\frac{p}{1-p})$$
    * Weighted LR后，正样本i的Odds：$$Odds(i)=\frac{T_ip}{1-T_ip}\approx T_ip=E(T_i)=期望观看时长$$
  * 细节分析：正样本加权 <-> Odds变成w倍
    * https://bourneli.github.io/recommendation/2020/07/19/notes-of-youtube-recommendation-2016.html
  * ![image-20250116220810022](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250116220810022.png)
    * 《Simple and scalable response prediction for display advertising》

* 处理用户对新视频的偏好：Example Age

  * training：训练样本产生的时刻距离当前时刻的时间
  * serving：0或者很小的负值
  * 思考：为什么不用视频的days since upload，原因可能是会导致时间分布过于分散，无法集中描述近期的变化趋势

* Note

  * 长尾视频embedding用全零向量替代
  * 特征长度的可变性：在输入特征之前再加可变长的神经网络？

* 资料

  * [Youtube推荐系统的变迁](http://www.datagrand.com/blog/youtube.html)

#### 和 Word2Vec 的联系

* 这里的最后一个 hidden 层和 softmax 层组成的结构与 word2vec 一模一样
  * 唯一的区别是 word2vec 用输入层与 hidden 层之间的权重作为 word embedding，没用 hidden 层与输出层的权重；
  * 这里用了 hidden 层与输出层的权重作为 video embedding，用 hidden 层作为 user embedding

* 为什么input和output不在一个空间，又通过点积预测
  * 《word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method》
  * One motivation for making this assumption is the following: consider the case where both the word dog and the context dog share the same vector v. Words hardly appear in the contexts of themselves, and so the **model should assign a low probability to p(dog|dog)**, which entails assigning a low value to v·v which is impossible.

## 电商推荐

* Title: value aware recommendation baased on reinforcement profit maximization

## 传统搜索

* 历史：https://www.vantagediscovery.com/post/ecommerce-search-transcended-for-the-ai-age
  * PageRank
  * As search engines become more personalized and user-focused, traditional ecommerce SEO tactics based on keyword optimization and backlinking are giving way to more sophisticated strategies that prioritize user intent and experience. 
* 传统的Indexing技术
  * [Suffix tree](https://en.wikipedia.org/wiki/Suffix_tree) 
    * Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [trie](https://en.wikipedia.org/wiki/Trie). Tries support [extendible hashing](https://en.wikipedia.org/wiki/Extendible_hashing), which is important for search engine indexing.[[8\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-8) Used for searching for patterns in [DNA](https://en.wikipedia.org/wiki/DNA) sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.[[9\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-Gus97-9) An alternate representation is a [suffix array](https://en.wikipedia.org/wiki/Suffix_array), which is considered to require less virtual memory and supports data compression such as the [BWT](https://en.wikipedia.org/wiki/Burrows–Wheeler_transform) algorithm.
  * [Inverted index](https://en.wikipedia.org/wiki/Inverted_index)
    * Stores a list of occurrences of each atomic search criterion,[[10\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-10) typically in the form of a [hash table](https://en.wikipedia.org/wiki/Hash_table) or [binary tree](https://en.wikipedia.org/wiki/Binary_tree).[[11\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-11)[[12\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-12)
  * [Citation index](https://en.wikipedia.org/wiki/Citation_index)
    * Stores citations or hyperlinks between documents to support citation analysis, a subject of [bibliometrics](https://en.wikipedia.org/wiki/Bibliometrics).
  * [*n*-gram index](https://en.wikipedia.org/wiki/N-gram)
    * Stores sequences of length of data to support other types of retrieval or [text mining](https://en.wikipedia.org/wiki/Text_mining).[[13\]](https://en.wikipedia.org/wiki/Search_engine_indexing#cite_note-13)
  * [Document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix)
    * Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix).
* faceted search
  * https://www.sparq.ai/blogs/ecommerce-faceted-search
* TF-IDF
  * Term Frequency–Inverse Document Frequency
  * 指标定义：TF-IDF = TF * IDF。其中，TF（Term Frequency）指的是词频，即某个词在文本中出现的次数。IDF（Inverse Document Frequency）指的是逆文档频率，即该词在整个语料库中出现的文档数的倒数。
  * 应用场景：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
* beam search 
  * https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24
  * Beam search is an algorithm used in many NLP and speech recognition models as a final decision making layer to choose the best output given target variables like maximum probability or next output character.   
  * 核心在于LM generate output的地方
  * Greedy Search: A Naïve Approach
  * Beam Search: **Using Conditional Probability** ,多次计算Decoder
    * The algorithm can take any number of N best alternatives through a hyperparameter know as Beam width


![image-20241005231947598](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/translation.png)

### ElasticSearch

* Intro

  * https://www.elastic.co/guide/en/elasticsearch/reference/8.15/release-highlights.html

  * ES基于Lucene引擎，其核心是基于关键词的倒排索引，关键组件包括分词、倒排索引、相关性计算等。其中相关性计算通常采用TF-IDF和BM25等基于词频的算法

  * denormalization
    * inverted index
      * An index is a collection of documents.
      * index: <word -> [documents]>
      * document: <field -> [values]>
      * 用stopword处理


  * document oriented tool

* Usage

  * term关键字，不能用于text，要用于keywords类型

    * https://stackoverflow.com/questions/21933787/elasticsearch-not-returning-results-for-terms-query-against-string-property

  * phrase匹配

    * ```
      "query": {
          "multi_match": {
                  "query": "严有兵",
                  "fields": [
                      ...
                  ],
                  "operator": "OR",
                  "type": "phrase"
              }
        }
      ```

  * 前缀匹配 query_string

  * 分类汇总

    * HyperLogLog:  cardinality关键字

    * ```
      "aggs": {
          "count_unlisted_companies": {
            "cardinality": {
              "field": "company_id" // 假设这里有一个代表公司唯一标识的company_id字段
            }
          }
        }
      ```

    * ```
      "aggs": {
          "count_active_companies": {
            "value_count": {
              "field": "_id"
            }
          }
        }
      ```

### 排序

#### 干预排序

```
"relevance_score + exp((-timestamp_diff(last_update_timestamp)/86400)) + exp(-(1/(click_cnt+1)))"
```



### Evaluation

* Recall & NDCG
  * ndcg影响用户价值
  * recall影响商家价值

* F-score: 精确率和召回率的调和平均数
* GSB：Good-Same-Bad

### Case Analysis

* 点击和相关性逆序比较大的case

### Case Examples

- “母亲节给妈妈买什么”
- “一个有趣的夜晚外出的衬衫”
- running shoes
- casual red Nike sneakers for summer
  - sparse: 'red', 'Nike', and 'sneakers'
  - dense: 'casual' and 'summer'
- recipes for a 6-year-old's birthday party
- recipes for a family dinner
- graduation garden party
  - floral sundress
  - wide-brim sunhat
- skin protection for outdoor concert
  - sunshine savior
- I need a stylish, comfortable dress for a summer wedding
- gear for a weekend camping trip
- eco-friendly sneakers
- what to wear to a beach party
- long floral dress with short sleeves.
- 菠萝
  - -X-> 马可菠萝火腿肠
- 生姜
  - -X-> 生姜洗发水
- 蒙牛牛奶
- 口条=猪舌
- 角瓜=茭瓜=西葫芦
- Redmi
- 烧烤（泛意图）
  - -> 烤羊肉串、烤羊腿
- 康师傅方便面
  - 品牌：康师傅
  - 商品：【袋装面、桶装面、泡面】
  - 类目：粮油调味
- 品牌搜索
  - 安慕希
- 基础单词
  - 苹果
  - 鱼
  - 水
    - -X-> 柔肤水
    - -X-> 丰水梨
- 同义词搜索
  - 圣女果
- 纠错
  - 平果、pingguo 和 pinguo

![9236e387-c3c1-4c80-b3a5-699c71e9299e](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/9236e387-c3c1-4c80-b3a5-699c71e9299e-3938325-3938327.png)

##### 对话式

- a camping backpack for an upcoming trip
  - the material that the backpack is made of, its volume, its straps, and its storage compartments.
- 更贵的xxx
  - 需要retrieve商品信息

### Ranking relevance in yahoo search

* Intro

  * ranking functions + semantic matching features + query rewriting
  * 挑战：1）query和item的空间不一致 (how much / price)；2）行为稀疏性；3）语义query
    * Developing semantic matching features including click simi-
      larity, deep semantic matching, and translated text matching.
  * 其它因素：时效性、地理位置
    * recency sensitive relevance and location sensitive relevance

* overview

  * 分词，求term检索的交集，uniquing（such as a limit on the num-
    ber of documents from a single host or domain），Core Ranking Function

* 特征

  * WebGraph（PageRank）
  * Document statistics
  * Document classiﬁer
  * Query Features：number of terms, frequency of the query and of its terms,
    click-through rate of the query
  * Text Match：computed from different sections of the document (ti-
    tle, body, abstract, keywords) as well as from the **anchor text（锚文本）** and
    the URL.
    * There are also proximity features which try to quantify how far in the document are the query
      terms (the closer the better) [23].
  * Topical matching:
  * Click
  * Time

* 算法

  * a uniﬁed method for web search which is **based on logistic loss and incorporates the Perfect, Excellent and Good information into the model though scaling the gradient for**
    **GBDT**
    * 加权pseudo-response
  * Contextual Reranking
    * 动机：所有候选的平均相似度
    * (1) Rank: sorting URLs by the feature value in ascending order
      to get the ranks of speciﬁc URLs. (2) Mean: calculating the mean
      of the feature values of top 30 URLs. (3) Variance: calculating
      the variance of the feature values of top 30 URLs. (4) Normalized
      feature: normalizing the feature by using mean and standard devia-
      tion. (5) Topic model feature: aggregating the topical distributions
      of 30 URLs to create a query topic model vector, and calculating
      similarity with each individual result.
    * In practice, we found that it is **more**
      **robust to use the ranks of the results** as a feature instead of directly
      using the core-ranking function’s scores

* semantic matching feature

  * 动机：The documents relevant to tail queries are often lacking anchor text
  * three novel features: **click similarity, translated text matching, deep semantic matching,**
  * **click similarity：** **vector propagation algorithm**
    * bipartite click graph构建
    * 根据coclick关系，不断迭代获取 QV^n, DV^n
    * trim top-K可以加速收敛
    * ![image-20250120030945033](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250120030945033.png)
    * ![image-20250120032806492](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250120032806492.png)

  * “Translated Text Matching” (TTM).
    * ![image-20250120033451861](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250120033451861.png)

* 工程
  * 根据url md5分shard server
  * export features from the index serving nodes，给重排使用
    * 30-50个primitive feature
  * coclick feature：一个月重建一次

* 评估
  * human labeling (e.g., professional editor’s judgment) and user be-
    havioral metrics (e.g., click-through rate, query reformulation rate,
    dwell time).
  * DCG
    * employ the Wilcoxon T-test to report p-value
  * 按query频次分类：top, torso and tail.



## 电商搜索

### 业务理解

* 商品搜索中的用户强意图场景，召回率/MAP/NDCG指标要求高，因为不能让一些商品永远没有曝光的机会 —— 第四范式
* 商品搜索对个性化的要求高于网页/视频/文字搜索，比如搜索的时候，不同的人消费能力的不同，那么排序时，需要考虑用户的消费能力，返回合适价格的商品
* 电商场景出于商家生态考虑，商品覆盖率要求高

### 实体识别

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/ab517b8391561f900d538776c1bc0381.png)

* 领域知识积累
  * e.g.
    * 口条=猪舌
    * 角瓜=茭瓜=西葫芦
    * Redmi
  * 词库挖掘
    * 同义词挖掘
      * 基于word2vec共现关系（噪声大）
      * 百科爬取
      * 运营提供
      * 现有词库
    * 上位词挖掘
      * 类目作为上位词
      * 爬取类目体系
  * 商品知识图谱构建
    * 知识图谱其实是做了一个非个性化全局的知识构建，通过商品库去分析静态概率，最后根据用户点击行为会做一些动态调整，调整完的知识图谱再用在后面的排序上。
    * ![image-20241011154227917](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241011154227917.png)
  * LLM都能搞定

### 意图识别

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/a0dd83557a74b8d07f3bed5e4a6fd0ef.png)

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/43f0a0f7c0b801a7be62446738bf1b6a.png)

* FastText分类器 https://fasttext.cc/

### 召回

* ES

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/d350bbd78199bc8b214e81bc6a387820.png)

### 粗排

* 当召回数量较大时，粗排截断
* 相关性、时间、热度、销量、好评数和收藏数等等特征，训练出简单的模型，做一些粗排的排序，进行截断
  * 找出核心的特征，做加权平均也可以。

### Rerank

> [搜推广生死判官：重排技术发展](https://mp.weixin.qq.com/s/oSxtpVuoTFQGsVxbZelGQg)

* LR、GBDT

![img](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/690ce3a3bbc3f407bc453ebc35ceb18b.png)

### 模型微调

[“阿里灵杰”问天引擎电商搜索算法赛--季军经验分享](https://mp.weixin.qq.com/s/sfFZpttGqf_f4sBFpQIiyg)

* ![image-20241010195711640](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010195711640.png)

* ![image-20241010195813065](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010195813065.png)

* 领域数据后训练
  * 动机：大量corpus未曝光，未经过训练
  * 预训练模型：Chinese-roberta-wwm-ext, Whole World MLM
* 召回任务微调
  * 数据构造：
    * 正样本：标注document
    * 负样本：
      * in-batch-negative: batch内其它query的标注document
      * random-negative：随机在corpus采样非标注document
        * 动机：如果仅使用in-batch-negative，会影响非标注document的学习过程

![image-20241010202306817](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202306817.png)

* 精排任务微调
  * 均值变换：相似程度较高的召回，区分度增强，增加梯度

![image-20241010202519864](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202519864.png)

![image-20241010202709678](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202709678.png)

![image-20241010202740700](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241010202740700.png)

#### 各种Tricks

* **数据增强**

  - **训练bart生成模型，使用document生成query构造伪标签**（还可以生成时随机替换一些阿拉伯数字、拼音），同时使用一定规则修正生成的伪标签query

  - 反向翻译（不确定是否有效）

  - **把query中长度小于4的拿出来，加入词表构建新词**

  - 把document分词后随机抽几个拼接作为新doc（不确定是否有效）

* **召回模型：**

  - 降维时可以使用无监督PCA作为降维矩阵初始化

  - 训练交互式精排模型去蒸馏非交互式召回模型

  - 难负例构造：使用训练好的召回模型+faiss召回，取相似度中等水平（30左右）的document作为难负例

  - infoNCE loss的温度：可以首先设定一个比较小的，然后慢慢变大，但不能太大

  - 集成使用提交的线上得分作为系数加权求和embedding（不确定是否有效）

* **精排模型：**
  - **可以使用ES引擎先召回query对应top-k doc，然后经过规则过滤（字面相似度高，向量余弦相似度不低）出过滤doc，然后把query和这些doc的向量表示按照一定规则相加，作为新query向量表示**

* **召回模型融合：**
  - **不能直接取均值，而是使用stacking的方法，把多个模型的输出向量使用一个训练后的FFN网络融合。**

## Two-sided marketplace 搜推

### Real-time Personalization using Embeddings for Search Ranking at Airbnb

> 解决转化稀疏的问题：user type & listing type

* 业务场景
  * Airbnb, a short-term rental marketplace, search and recom-
    mendation problems are quite unique, being a **two-sided market-**
    **place** in which one needs to optimize for host and guest preferences,
    in a world where a user rarely consumes the same item twice and
    one listing can accept only one guest for a certain set of dates
    * Example industries include accommodation (Airbnb), ride sharing (Uber, Lyft), online
      shops (Etsy),
  * 短租平台
  * 搜索个性化、相似推荐
    * query： location、dates、guests、map、filters
* 业务特征
  * given an **input query** with **location and trip dates** we need to rank high listings whose
    **location, price, style, reviews**, etc. are appealing to the guest and,
    at the same time, are a good match in terms of host preferences for
    **trip duration and lead days.**
  * **bad reviews, pets, length of stay, group size**
  * session特征：
    * **clicks, host contacts**
    * **skips of high ranked listings**
* 核心思路：
  * 训练泛化user type embedding，冷启动（由于旅客的行为过于稀疏，一年几次）
    * 在同一个空间训练 listing type embeddings
  * **Adapting Training for Congregated Search** - Unlike in
    Web search, the search on travel platforms is often congre-
    gated, where users frequently search only within a certain
    market, e.g. Paris., and rarely across different markets. We
    adapted the embedding training algorithm to take this into
    account when doing negative sampling, which lead to cap-
    turing better within-market listings similarities.

* 参数/数据
  * 800 million search clicks sessions -> listing emb
  * 50 million users -> user/listing type emb

* listing embeddings的学习

  * session定义：
    * A new session is started whenever there is a time
      gap of more than 30 minutes between two consecutive user clicks
    * 详情页停留30秒才算数据点
    * 至少2个有效clicks
  * ![image-20250111131009803](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111131009803.png)
  * 思路：
    * 基于context window，对比学习，类似CBOW的思想
      * 共现的标准：滑动窗口内共现
    * Mn项：强调类内的负例，同一market下的随机负例

  * 冷启动：地理位置相似 + 类型一致 + 价格bucket一致，3个取平均

* User-type & Listing-type Embeddings

  * 动机：book（预订）行为相比点击行为非常稀疏，session长度仅有1，学好某个item embedding至少需要5-10条样本
  * 两个目标交替训练，从而user-type和listing-type在一个空间
  * ![image-20250111132736294](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111132736294-6573257.png)

  * ![image-20250111132909510](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111132909510.png)

  * 双边市场：考虑Host rejections（listing type对user type的rejection）
    * 作为explicit negatives

* 模型，在基线gbdt&lr model的基础上，加入上面的embeddings

  * ![image-20250111135137098](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111135137098.png)
  * ![image-20250111135539502](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111135539502.png)
    * 加特征：规则+score

* 训练

  * 数据
    * session划分
    * we removed accidental and short clicks, i.e. clicks for which user stayed on the
      listing page for less than 30 seconds, and kept only sessions con-
      sisting of 2 or more clicks
    * oversampled booked sessions by 5x in the training data
  * Daily Training.
  * listing types
    * trained embeddings for 500K user
      types and 500K listing types using **50 million user booking sessions.**
      Embeddings were d = 32 dimensional and were trained using a
      **sliding window of m = 5** over booking sessions

* Evaluation

  * ![image-20250111134240296](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111134240296.png)

  * ![image-20250111140037550](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250111140037550.png)
  * 离线 NDCU (Normalized Discounted Cumulative Utility)
    * rejections -0.4
    * Clicks 0.01
    * contacts 0.25
    * bookings 1

## 广告

### Intro

* 广告的预估CTR具有物理意义
  * calibration

### Practical Lessons from Predicting Clicks on Ads at Facebook, KDD 2014

> GBDT+LR、在线学习、Online Joiner、负样本降采样

**2.指标**

* Normalized Entropy: the average log loss per impression divided by what the average log loss per impression would be if a model predicted the background click through rate (CTR) for every impression. 
  * 用 background CTR 给 loss 做 normalize
* RIG (Relative Information Gain) = 1 - NE
* Calibration: the ratio of the average estimated CTR and empirical CTR
* AUC(Area-Under-ROC): 衡量排序， 忽略低估与高估

**超参**

* GBDT
  * 子树600；几天更新一次

**3.Prediction Model Structure**

* BOPR (Bayesian online learning scheme for probit regression): 假定高斯分布，在线学习分布的参数
  * Both SGD-based LR and BOPR described above are stream learners as they adapt to training data one by one.
  * BOPR 相比 SGD-based LR 的区别在于，梯度下降的 step-size 由 belief uncertainty $\sigma$ 控制，也是在线更新的参数3
* 3.1 Decision tree feature transforms
  * bin the feature
  * build tuple input features
    *  joint binning, using for example a k-d tree
    *  boosted decision trees
  * follow the Gradient Boosting Machine (GBM) [5], where the classic L2-TreeBoost algorithm is used
  * We can understand boosted decision tree based transformation as a supervised feature encoding that converts a real-valued vector into a compact binary-valued vector.

* 3.2 Data freshness
  * The boosted decision trees can be trained daily or every couple of days, but the linear classifier can be trained in near real-time by using some flavor of online learning.

* Experiment result for different learning rate schmeas for LR with SGD
  * NE: per weight > global > constant > per weight sqrt > per coordinate

* BOPR 与 LR 对比
  * LR's model size is half
  * BOPR provides a full predictive distribution over the probability of click. This can be used to compute percentiles of the predictive distribution, which can be used for explore/exploit learning schemes

**4.Online Data Joiner**

* length of waiting time window: 定义"no click"，需要 tune
  * 定义：曝光发生后，等待多久判定是否产生了点击
  * 过长会增加buffer、影响"recency"实时性
  * 过短会影响"click coverage" => the empirical CTR that is somewhat lower than the ground truth

* 数据结构：HashQueue 缓存曝光记录
  * consisting of a First-In-First-Out queue as a buffer window and a hash map for fast random access to label impressions.
  * operations: enqueue, dequeue, lookup

* Anomaly detection mechanisms
  * 监测到数据剧烈变化，断流训练器
  * e.g. 点击数据流由于action id的bug无法与曝光数据流正确join

**5.Containing Memory and Latency**

* number of boosting trees: 500个比较折中
* boosting feature importance
  * the cumulative loss reduction attributable to a feature
  * 对多个 trees 的 reduction 相加
* features
  * contextual features: local time of day, day of week, device, current page
  * historical features: ctr of the ad in lask week, avg ctr of the user
  * **historical features 明显比 contextual features 重要**
  * contextual features 更需要 data freshness

**6.Coping with Massive Training Data**

* Uniform subsampling: sample rate 10% 比较合适

* Negative down sampling: sample rate 2.5% 效果反而更好

* Model Re-Calibration: $q=\frac{p}{p+\frac{1-p}{w}}$
  * 负采样带来ctr飘移



## 简历 Job Market 匹配

### Personalized Job Recommendation System at LinkedIn





## chpt4 Embedding 技术在推荐系统中的应用

### Intro

* 定义：用低维稠密向量“表示”一个对象 
  *	向量“表示”对象特征
  *	向量距离“表示”相似性
  *	具有本体论哲学层面上的意义
* 意义
  * 高维稀疏特征向量 转 低维稠密向量
  * 本身表达能力强，可引入任何信息进行编码(Graph Embedding)
  * 对物品、用户相似度的计算是常用的推荐系统召回层技术

* embedding的学习
  * 预训练
  * e2e
* embedding的获取方式：
  * word2vec
  * 在线学习
  * gbdt也可以视作embedding



### Word2vec——经典的Embedding方法

> 参考 Machine-Learning.md

### Item2Vec——Word2vec在推荐系统领域的推广

* 和Word2Vec的区别：
  * 不限制上下文长度
* 广义的Item2Vec：粗排双塔模型

### Graph Embedding——引入更多结构信息的图嵌入技术

#### DeepWalk

* 生成物品图：根据短时间内的物品序列
* 随机游走生成多个序列
  * 唯一的参数：v_i到v_j的跳转概率
* 用序列训embedding

#### Node2Vec

* Intro
  * 同质性和结构性的权衡
  * 同质性 -> DFS
    * In-out parameter越小，随机游走到远方节点的概率越大，越突出同质性
  * 结构性 -> BFS
    * return parameter越小，游回节点的概率越大，越突出结构性
* 关于Node2vec算法中Graph Embedding同质性和结构性的进一步探讨 - 王喆的文章 - 知乎
  https://zhuanlan.zhihu.com/p/64756917

#### EGES: Enhanced Graph Embedding With Side Infomation

* 核心思想：
  * 知识图谱学习类目向量，提升商品冷启动
    * 根据行为序列构成有向图
  * 学习 $$e^{\alpha_j}$$作为系数，weighted avg pooling的每类embedding的权重

#### Graph Service

* 基于Euler改造
  * 支持全图节点遍历，支持按时间戳采样

* 高性能采样
  * 节点生成前缀和，二分查找随机数
  * 全局点采样：shard有权（shard上所有节点的权重），shard内再次按权
* Graph Embedding on GPU: sample和worker分离
* GE应用
  * 预训练：利用uid与author间的finish关系，构造双向异构图。边权重用finish视频数量，点权重用finish视频总数。
    * 不用gid而用author是为了减小图结构随时间的变化
    * 正负例构造：正例是user及其finish的节点，负例随机采样
  * end2end training
    * 利用uid和cid之间的click关系，边权重是click次数，点权重是click总数
    * 采样：有放回按权采样
    * 优点：训练目标和LTR任务一致
    * 缺点：1.全局图的拟合能力不如业务图 2.end2end结构更自由，可以用node2vec等
  * Finetune BERT/ResNet
    * cid和cid的co-click，边权重用交并比，点权重用click总数，点特征用广告id类泛化特征

## chpt5 多角度审视推荐系统

* 特征工程
* 召回算法

### 实时性

* Literature Review

  * Facebook用gdbt+LR做实验，更新间隔越长，loss越大

* Online Learning
  * BOPR
  
* 特征实时性

  * 端上：缓存session内部行为，一个session内（比如3分钟一个session）点击的几篇文章
  * 流处理：简单的统计特征
    * 某个item在时间窗口内的曝光次数
    * 某个user在时间窗口内的点击话题分布
  * 批处理：
    * 数据join
    * 延迟信号的合并
  * ![image-20250105011214868](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250105011214868.png)

* Embedding实时性

* 模型实时性

  * 全量更新
  * 增量更新
    * 可以和全量更新相结合，过一段时间全量更新一次
  * online learning
    * 核心问题是兼顾模型效果和稀疏性
    * FOBOS和FTRL
  * 策略：局部更新
    * GBDT天级更新、LR实时更新
    * Embedding预训练、Dense实时更新

  * 端上的模型实时性
    * 讨论了端上User Embedding的实时更新，我觉得和端上实时特征的区别不大

### 推荐系统的优化目标

* youtube视频推荐系统的目标
  * 业务场景：视频播放前和中间的广告
  * 优化时长而不是ctr
  * 视频长短和质量都重要
  * 把播放时长转化为正样本权重，输出用加权逻辑回归，预测用$$e^{Wx+b}$$预测时长
    * 【上次观看时间】也是重要特征

* 电商推荐：预估cvr
  * 解决【曝光点击转化】的问题，阿里esmm

* 协商好优化目标——技术、产品、运营、内容编辑团队，解耦工作，最大化效率

### 优化不限手段

* [Netflix artwork personalization](https://netflixtechblog.com/artwork-personalization-c589f074ad76)
  * https://www.youtube.com/watch?v=UjQMEjkrUGo

* roku：右滑某一类目tab，作为特征



## 特征工程

* 原则：

  * 尽可能保留推荐环境及用户行为过程中的所有有用信息，尽量摒弃冗余信息

* 原始特征分类：

  * 用户行为数据
  * 用户关系数据
  * 属性、标签类数据
    * 电商类目打标
    * 豆瓣“社交化”让用户添加收藏，用户打标
  * 内容类数据
  * context特征
    * 时间、GPS地点、季节、月份、节假日、天气、社会大事件
  * 统计类特征
    * 历史ctr、历史cvr、物品热门程度、物品流行程度
  * match特征

* 特征举例：

  * ![image-20250105002547819](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250105002547819-6007955.png)

  * ![image-20250105002610024](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250105002610024.png)

* 特征处理：

  * dense特征：
    * 归一化、离散分桶
    * 非线性变换，一起加进来：$$x^a, log_a(x), log_a(\frac{x}{1-x})$$

  * categorical特征：
    * one-hot/multi-hot编码
    * embedding化

## 召回算法

### 多路召回

* 新闻场景的N路：
  * 热门新闻
  * 兴趣标签
  * 协同过滤
  * 最近流行
  * 朋友喜欢
* Embedding召回
  * EGES一定程度上能融合多路
  * 评分具有连续性

### Evaluation

* 后验概率：
  * 发生点击行为后，用户对item感兴趣的概率

## 长序列建模

### Literature Review

* 一些算法 [star]
  * KNN is a user-based collaborative
    filtering method that finds the top 10 most similar users to a given
    user and averages their ratings to score a specific item
  * Caser uses convolution neural networks to model user interests [54]; 
  * HGN uses hierarchical gating networks to capture both long and
    short-term user behaviors [55];
  * GRU4Rec employs GRU to model user action sequences [56];
  * FDSA uses a self-attentive model to learn feature transition patterns [57];
  * SASRec uses a self-attention mechanism to capture item correlations within a user’s action sequence [58];
  * BERT4Rec applies a masked language modeling (MLM) objective for bi-directional sequential recommendation [9];
  * S3-Rec extends beyond the MLM objective by pre-training with four self-supervised objectives to learn better item representations [59].
  * P5 fine-tunes a pre-trained LM for use in multi-task recommendation systems by generating tokens based on randomly assigned item IDs [40];
  * TIGER also fine-tunes LMs to predict item IDs directly, but these IDs are semantic, meaning they are learned based on the content of the items [21];
  * IDGenRec goes further by extending semantic IDs to textual IDs,
    enriching the IDs with more detailed information [44].

### [美团] 用transformer做序列特征交叉

https://tech.meituan.com/2020/04/16/transformer-in-meituan.html

* 用transformer做序列特征交叉
  * 将transformer的输出结果和target item做din

### SASREC

https://arxiv.org/abs/1808.09781

## 模型结构创新

### [DHE] Learning to Embed Categorical Features without Embedding Tables for Recommendation

* Intro
  * 挑战1: Highly-skewed data distribution: The categorical features
    in recommendation data usually follow highly skewed power-
    law distributions. The small number of training examples on
    infrequent feature values hurts the embedding quality for the
    tail items significantly.

![image-20241226232352140](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20241226232352140.png)

* 思路：
  * 解决one-hot hashing的collision问题，兼顾压缩+唯一性
  * 一种思路是concat多个hash embedding
* 编码设计原则
  - **唯一性**：确保每个特征值的编码唯一，避免冲突影响模型性能。
  - **等相似性**：避免引入错误的归纳偏差，使任意两个编码在相似性上无差异。
  - **高维度**：便于后续解码函数区分不同特征值，高维空间可分性更强。
  - **高香农熵**：从信息论角度防止冗余维度，最大化每个维度的熵。

- **密集哈希编码** Dense Hash Encoding：使用多个通用哈希函数将特征值映射为维密集实值编码，再通过适当变换近似均匀分布或高斯分布。此过程无需存储，计算时间复杂度为，可并行计算。
- Deep Embedding Network
  - 发现 Mish 激活函数和批量归一化（BN）能提高性能。网络结构上，约五层隐藏层的效果较好，且简单等宽 MLP 架构表现最佳
  -  side feature enhanced encodings for DHE
    - directly concatenating the generalizable features and the hash encodings.
    - the hash encoding provides a unique identifier for memorization while the other features enable the generalization ability.

* 结论：
  * 1024个hash function效果最好
  * RQ4: 批量归一化（BN）能显著稳定和加速训练并提高性能，在激活函数方面，Mish 激活函数优于 ReLU
  * 4.8 简单等宽 MLP 表现最佳，添加残差连接反而略降性能

## 冷启动 Cold-start 问题

* 问题定义：
  * 用户冷启动
  * 物品冷启动
    * pretrained embedding
    * 根据“价格、房屋属性、距离近”进行聚类，用相似物品的embedding
  * 系统冷启动

### 基于规则

* 用户冷启动
  * 热门、最流行、最高评分
  * 根据年龄、性别、地址，构建榜单
* 物品冷启动
  * pretrained embedding
  * 根据“价格、房屋属性、距离近”进行聚类，用相似物品的embedding

### 丰富特征

* 注册信息
  * 根据IP、GPS推断地址信息
* 第三方DMP（Data Management Platform）提供的信息
  * 低阶和高阶特征
* 物品内容特征：
  * 演员、年代、风格
* 引导用户输入：
  * 音乐风格
  * 喜欢的电影

### 主动学习、迁移学习、Exploit-Exploration

* 主动学习
  * 实际效果：给用户推荐最大聚类的物品

```
for j = 1,2,..., totalIterations do
	foreach q_j in potentialQueries do
		Evaluate Loss(q_j)
	end foreach
	Ask query q_j for which Loss(q_j) is the lowest
	Update model M with query q_j and response (q_j, y_j)
end for
return model M
```

#### bandit算法 —— 非个性化的探索利用

* 问题定义：
  * Multi-Armed Bandit Problem
  * 假设了每个Item对不同用户的效益一视同仁
* $$\epsilon-Greedy$$算法
  * 每次$$\epsilon$$概率随机选择，$$1-\epsilon$$概率选择平均收益最大的
  * 缺点：需要启发式，动态调节探索和利用的比例

> [专治选择困难症——bandit算法](https://zhuanlan.zhihu.com/p/21388070)

* Thompson sampling算法
  * 假设赢钱的概率p服从beta分布
    * beta分布是伯努利分布的共轭先验分布
  * `choice = numpy.argmax(pymc.rbeta(1 + self.wins, 1 + self.trials - self.wins))`
* UCB算法：Upper Confidence Bound
  * 利于工程实现
  * $$UCB(j) = \bar{x}_j + \sqrt{\frac{2 \ln n}{n_j}}$$
* Epsilon-Greedy算法：有点类似模拟退火
  * 一定的概率做纯随机决策
* 总结：
  * UCB算法和Thompson采样算法显著优秀一些，两者都是给explore bonus，给出explore bonus的方式不同：
    * UCB是频率学派，假定了大数定律收益收敛，显式的通过试验次数给出的置信度上界
    * Thompson是靠分布刻画不确定性
  * 至于你实际上要选哪一种bandit算法，你可以选一种bandit算法来选bandit算法。。。

#### 个性化探索利用

> Contextual-Bandit Algorithm

* LinUCB： 针对线性模型，找到探索的公式
  * $$E[\textbf{r}_{t,a}|\textbf{x}_{t,a}]=\textbf{x}_{t,a}\theta_a^*$$
  * Ridge Regression岭回归
  * ...

#### 基于模型探索利用

* 强化学习DRN，扰动模型参数，起到类似的作用
* 多样性探索：消重、打散、部分流量未知兴趣



## Evaluation (chpt 7)

- 资料
  - https://recbole.io/evaluation.html

### 离线评估

**实验setting**

* holdout实验
  * 7:3
* cross-validation 交叉验证
  * k-fold
  * leave-one-out validation
    * K-fold k=1
* 自助法 bootstrap
  * N次有放回，没抽到的作为验证集

**指标**

* 准确率 Accuracy：
  * 缺点是只把99%负样本分类对，准确率也很高
* Precision + Recall
  * Precision@N 认为前N个为模型预测的正例
  * F1-Score 调和平均
* RMSE
  * 解决离群点影响大的问题：$$MAPE=\sum_{i=1}^N|\frac{y_i-\hat{y_i}}{y_i}|\times\frac{100}{n}$$ 
* LogLoss
  * 二分类，评估模型收敛情况

**直接评估推荐序列的离线指标**

* P-R 曲线
  * 横轴Recall，纵轴Precision
  * AUC：Area Under Curve
* ROC 曲线
  * the Receiver Operating Characteristic
  * 横轴 FPR = FP/N，纵轴 TPR = TP/P
  * 计算方法：可以对样本排序、遍历，遇到正样本，纵轴绘制 1/P，遇到负样本，横轴绘制 1/N
  * 经过 (0,0) 和 (1,1)
* mAP
  * mean average precision
  * 对每个用户的样本排序，计算AP
  * 按全部用户平均，计算mAP

**其它指标**

* NDCG
* coverage
* diversity

#### 关于AUC

评价指标：[AUC和GAUC](https://www.jianshu.com/p/03a11a083a6d)、[AUC不等于线上效果可能的原因](https://zhuanlan.zhihu.com/p/58152702)

### 更接近线上环境的离线评估方法 —— Replay

* 动态离线评估
  * 训练集 - 测试集1 - 测试集2 - 测试集3
* Netflix Time Machine
  * 避免“数据穿越” （样本里包含未来信息）

### A/B Testing

* Intro：为什么A/B测试无法被替代
  * 离线评估无法消除 data bias
  * 离线评估无法完全还原线上工程环境
    * 延迟、数据丢失、标签数据缺失
  * 线上系统某些商业指标，在离线评估中无法计算
    * CTR、ST、PV

* 分桶原则
  * 分桶的随机性
  * Overlapping Experiment Infrastructure: More, Better, Faster Experimentation
    * 层与层之间的流量正交
      * 同层之间的流量互斥

* 真实指标：

![image-20250107001513104](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20250107001513104.png)

### 快速线上评估方法 —— Interleaving

* Intro
  * A/B资源有限 -> 快速筛选出“优胜”算法
  * A/B测试的统计学问题 -> 不区分A/B组，把不同的被测对象同时提供给受试者，根据受试者喜好得出评估结果
* 实现：
  * 整合算法A和算法B
* 结论：
  * p-value 5%: interleaving需要10^3样本，A/B testing需要10^5样本

* 局限性：
  * 工程实现复杂（需要大量辅助性的数据标识添加到数据流）
  * 只能得到偏好，无法得到真实业务指标提升的数据

## Potpourri

* [Inside TikTok's Algorithm: A WSJ Video Investigation](https://www.wsj.com/articles/tiktok-algorithm-video-investigation-11626877477)
  * https://www.youtube.com/watch?v=nfczi2cI6Cs

