# LLM MLSys

[toc]

> https://cs.stanford.edu/~chrismre/#papers

## è®­æ¨ç³»ç»Ÿ

### Intro

#### æ¨ç†ç³»ç»Ÿ Overview

![image-20251004020634191](./LLM-MLSys/image-20251004020634191.png)

![image-20251005030023014](./LLM-MLSys/image-20251005030023014.png)

#### ç³»ç»Ÿã€ç®—æ³•ã€æ•°æ®çš„å…±åŒæ¼”è¿›

* **ç ”ç©¶å³å·¥ç¨‹**ï¼šGemini 3 è´Ÿè´£äººæŒ‡å‡ºï¼Œå¤§æ¨¡å‹ç ”å‘å·²ä¸å†æ˜¯å•çº¯è®­ç»ƒä¸€ä¸ªç½‘ç»œï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªå›´ç»•ç¥ç»ç½‘ç»œçš„**å¤æ‚ç³»ç»Ÿ**ï¼ˆSystemï¼‰ã€‚
* ç®—æ³•ä¸ç³»ç»Ÿçš„è¾¹ç•Œæ—¥ç›Šæ¨¡ç³Šï¼Œç¡¬ä»¶æ¶æ„ï¼ˆå¦‚ TPU Podsï¼‰ä¸æ¨¡å‹æ¶æ„ï¼ˆå¦‚ MoEï¼‰éœ€ååŒè®¾è®¡ã€‚

![image-20251005213702242](./LLM-MLSys/image-20251005213702242.png)

#### ç¡¬ä»¶ï¼Œå†…å­˜äº’è¿ã€å¼‚æ„ååŒå¾ˆå…³é”®

* LLMæ¨ç†åˆ°åº•éœ€è¦ä»€ä¹ˆæ ·çš„èŠ¯ç‰‡ï¼Ÿ https://wallstreetcn.com/articles/3709523

![image-20251005214302319](./LLM-MLSys/image-20251005214302319.png)

### æŠ€æœ¯å‘å±•

* Memory Efficient Attention with Online Softmax (2021) -> FlashAttention in Megatron-LM (2022) 
* Continuous Batching (2022), Paged Attention (2023) -> vLLM, TensorRT-LLM (2023) 
* Speculative Sampling (2023) -> Everywhere in LLM Serving (2023)
* Sequence Parallel (2023) ->  Megatron-LLM (2023) 

### ä¸šåŠ¡ç›®æ ‡ï¼šMFUã€æ•…éšœç‡ç­‰

> https://mp.weixin.qq.com/s/llalxX6miJRxy0-Vk8Ezpg

* MFUï¼ˆModel FLOPs Utilizationï¼‰
* æ•…éšœç‡ï¼šåœ¨å¤§è§„æ¨¡çš„é›†ç¾¤ä¸­ï¼Œæ¨ç†è¯·æ±‚çš„æ•…éšœç‡ï¼Œå› ä¸ºåœ¨ä¸€ä¸‡å¼ å¡çš„é›†ç¾¤ä¸­ï¼Œå¦‚æœæ¯å‡ åˆ†é’Ÿå°±æœ‰ä¸€å¼ å¡æŒ‚æ‰ï¼Œé‚£ä¹ˆè¿™ä¼šå½±å“æ•´ä½“æ•ˆç‡ï¼Œæˆ–è€…è¯´çœ‹æ•…éšœæ—¶é—´å åœ¨æ•´ä¸ªæœ‰æ•ˆè®­ç»ƒæ—¶é—´çš„å æ¯”ï¼Œå¦‚æœè¯´æ˜¯æ•…éšœçš„æ—¶é—´å è®­ç»ƒæ—¶é—´æ¯”ä¾‹è¶…è¿‡30%ï¼Œä¹Ÿéå¸¸å½±å“æ•ˆç‡ï¼›

### LLMæ¨¡å‹&èµ„æºå†³ç­–

> * å¾®è°ƒçš„æ˜¾å­˜æ¶ˆè€—å°
> * å¯¹äºè®¸å¤šä¸éœ€è¦ H ç³»åˆ—æ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼ˆå¦‚æœ€é«˜å¸¦å®½çš„ NVLinkã€å…¨é¢çš„ ECC å†…å­˜ã€ç‰¹å®šçš„è™šæ‹ŸåŒ–æ”¯æŒæˆ–å•å¡æœ€å¤§æ˜¾å­˜ï¼‰çš„åœºæ™¯ï¼Œ4090 æ˜¯ä¸€ä¸ªæ›´ç»æµçš„é€‰æ‹©
>   * æ³¨æ„4090åŠŸè€—&æ•£çƒ­åƒäºï¼Œ32B+ æ¨¡å‹éœ€é«˜åŠŸç‡ç”µæºï¼ˆ1000W+ï¼‰å’Œæ•£çƒ­ç³»ç»Ÿ

![image-20250507030154296](./LLM-MLSys/image-20250507030154296.png)

* **ä½é…ä½¿ç”¨ï¼ˆè®¡ç®—èµ„æºæœ‰é™ï¼‰**
  * Int4é‡åŒ–ï¼Œçº¦2Kä¸Šä¸‹æ–‡

<table align="left">
<thead>
<tr>
<th style="text-align:center">æ¨¡å‹ï¼ˆint4ï¼‰</th>
<th style="text-align:center">æ‰€éœ€æ˜¾å­˜GB</th>
<th>æ¨èGPU</th>
<th>å‚è€ƒæ¨¡å‹</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.5B</td>
<td style="text-align:center">&lt;5G</td>
<td></td>
<td>Qwen2-0.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">1.5B</td>
<td style="text-align:center">&lt;3G</td>
<td></td>
<td>Qwen-1_8B-Chat, Qwen2-1.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">6B</td>
<td style="text-align:center">4G</td>
<td></td>
<td>Yi-6B-Chat-4bits</td>
</tr>
<tr>
<td style="text-align:center">7B</td>
<td style="text-align:center">&lt;11G</td>
<td></td>
<td>Qwen2-7B-Instructï¼ŒQwen-7B-Chat-Int4</td>
</tr>
<tr>
<td style="text-align:center">14B</td>
<td style="text-align:center">13G</td>
<td></td>
<td>Qwen-14B-Chat-Int4</td>
</tr>
<tr>
<td style="text-align:center">34B</td>
<td style="text-align:center">20G</td>
<td></td>
<td>Yi-34B-Chat-4bits</td>
</tr>
<tr>
<td style="text-align:center">57B</td>
<td style="text-align:center">&lt;35G</td>
<td></td>
<td>Qwen2-57B-A14B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">72B</td>
<td style="text-align:center">&lt;47G</td>
<td></td>
<td>Qwen2-72B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">130B</td>
<td style="text-align:center">-</td>
<td>8 * RTX 2080 Ti(11G) <br> 4 * RTX 3090(24G)</td>
<td>GLM-130B</td>
</tr>
<tr>
<td style="text-align:center">236B</td>
<td style="text-align:center">130G</td>
<td>8xA100(80G)</td>
<td>DeepSeek-V2-Chat</td>
</tr>
</tbody>
</table>
























* ä¸­é…
  * int8ã€4k/6kä¸Šä¸‹æ–‡

<table align="left">
<thead>
<tr>
<th style="text-align:center">æ¨¡å‹ï¼ˆint8ï¼‰</th>
<th style="text-align:center">æ‰€éœ€æ˜¾å­˜GB</th>
<th>æ¨èGPU</th>
<th>å‚è€ƒæ¨¡å‹</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.5B</td>
<td style="text-align:center">6G</td>
<td></td>
<td>Qwen2-0.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">1.5B</td>
<td style="text-align:center">8G</td>
<td></td>
<td>Qwen2-1.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">6B</td>
<td style="text-align:center">8G</td>
<td></td>
<td>Yi-6B-Chat-8bits</td>
</tr>
<tr>
<td style="text-align:center">7B</td>
<td style="text-align:center">14G</td>
<td></td>
<td>Qwen2-7B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">14B</td>
<td style="text-align:center">27G</td>
<td></td>
<td>Qwen-14B-Chat-Int8</td>
</tr>
<tr>
<td style="text-align:center">34B</td>
<td style="text-align:center">38G</td>
<td></td>
<td>Yi-34B-Chat-8bits</td>
</tr>
<tr>
<td style="text-align:center">57B</td>
<td style="text-align:center">117G (bf16)</td>
<td></td>
<td>Qwen2-57B-A14B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">72B</td>
<td style="text-align:center">80G</td>
<td></td>
<td>Qwen2-72B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">130B</td>
<td style="text-align:center">-</td>
<td>8xRTX3090 (24G)</td>
<td>GLM-130B</td>
</tr>
<tr>
<td style="text-align:center">236B</td>
<td style="text-align:center">490G(bf16)</td>
<td>8xA100 (80G)</td>
<td>DeepSeek-V2-Chat</td>
</tr>
<tr>
<td style="text-align:center">340B</td>
<td style="text-align:center">-</td>
<td>16xA100(80G) <br>  16xH100(80G) <br>  8xH200</td>
<td>Nemotron-4-340B-Instruct</td>
</tr>
</tbody>
</table>




























* é«˜é…
  * Bf16ï¼Œ32Kä¸Šä¸‹æ–‡

<table align="left">
<thead>
<tr>
<th style="text-align:center">æ¨¡å‹ï¼ˆfb16ï¼‰</th>
<th style="text-align:center">æ‰€éœ€æ˜¾å­˜GB</th>
<th>æ¨èGPU</th>
<th>å‚è€ƒæ¨¡å‹</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.5B</td>
<td style="text-align:center">27G</td>
<td></td>
<td>Qwen2-0.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">1.5B</td>
<td style="text-align:center">30G</td>
<td></td>
<td>Qwen2-1.5B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">6B</td>
<td style="text-align:center">20G</td>
<td></td>
<td>Yi-6B-200K</td>
</tr>
<tr>
<td style="text-align:center">7B</td>
<td style="text-align:center">43G</td>
<td></td>
<td>Qwen2-7B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">14B</td>
<td style="text-align:center">39G(8k)</td>
<td></td>
<td>Qwen-14B-Chat</td>
</tr>
<tr>
<td style="text-align:center">34B</td>
<td style="text-align:center">200G(200k)</td>
<td>4 x A800 (80 GB)</td>
<td>Yi-34B-200K</td>
</tr>
<tr>
<td style="text-align:center">57B</td>
<td style="text-align:center">117G</td>
<td></td>
<td>Qwen2-57B-A14B-Instruct</td>
</tr>
<tr>
<td style="text-align:center">72B</td>
<td style="text-align:center">209G</td>
<td></td>
<td>Qwen2-72B-Instruct</td>
</tr>
</tbody>
</table>























### DeepSeek-V3 (MoE)

* prefill
  * The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The
    attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), com-
    bined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP
    communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that
    each expert processes a sufficiently large batch size, thereby enhancing computational efficiency.
    For the MoE all-to-all communication, we use the same method as in training: first transferring
    tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In
    particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP
    communication.
    * **redundant experts**ï¼šFor each GPU, besides the original 8 experts it
      hosts, it will also host one additional redundant expert
  * simultaneously process two micro-batches with similar computational workloads, **overlapping the attention and MoE of one micro-batch with the dispatch and combine of another.**
    * exploring a dynamic redundancy strategy for experts, where each GPU hosts
      more experts (e.g., 16 experts), but only 9 will be activated during each inference step

* decoding
  * The minimum deployment unit of the decoding stage consists of 40 nodes with 320 GPUs.
  * The attention part employs TP4 with SP, combined with DP80,
  * the MoE part uses EP320.
    * each GPU hosts only one expert, and 64 GPUs
      are responsible for hosting redundant experts and shared experts
    * the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation
      * **allocate only a small portion of SMs to dispatch+MoE+combine.**
  * é€šä¿¡ä¼˜åŒ–
    * DeepEpï¼šleverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency.
    * **overlap the attention of one micro-batch with the dispatch+MoE+combine of another.**

## æˆæœ¬å’Œæ€§èƒ½è¯„ä¼°

* Intro
  * AIGCæ˜¯å¤§å›½çš„æ¸¸æˆ
    * æ¬§æ´²å—æ¬§ç›Ÿæ³•æ¡ˆå½±å“ï¼Œaiå‘å±•æ²¡è·Ÿä¸Š

  * AIç³»ç»Ÿï¼šè®°å½•æ•°æ®ã€ä¸äººäº¤äº’ã€æœºå™¨å­¦ä¹ åˆ†æã€é¢„æµ‹ã€å¹²é¢„äººçš„å†³ç­–

### MFUã€HFU

* Hardware FLOPS Utilization

  * è€ƒè™‘äº†è®¡ç®—æ¢ç©ºé—´

* MFUï¼ˆModel FLOPs Utilizationï¼‰ï¼š

  * è¯„ä¼°GPUç®—åŠ›çš„æœ‰æ•ˆåˆ©ç”¨ç‡

* | æ¨¡å‹          | å‚æ•°è§„æ¨¡ | MFU    | ç¡¬ä»¶é…ç½®   |
  | ------------- | -------- | ------ | ---------- |
  | PaLM          | 540B     | 46.2%  | 6144 TPUv4 |
  | Megatron-LM   | 530B     | 56.0ï¼… | 3072 A100  |
  | Mosaic ML     | 70B      | 43.36% | 128 H100   |
  | å­—èŠ‚MegaScale | 175B     | 55.2%  | 12,288 GPU |

### FLOPS

* Am,k * Bk,n : `2*m*n*k` FLOPS
  * ä¹˜å’ŒåŠ å„ç®—ä¸€æ¬¡
* transformer
  * è®¾Cä¸ºemb sizeã€Tä¸ºseq len
  * ä¸€å±‚Transformer
    * FLOPSï¼š `24BTC^2 + 4BCT^2` 
    * Paramsï¼š`12C^2+13C`
  
  * attnçš„è®¡ç®—å æ¯”æ˜¯$$\frac{4BCT^2}{24BTC^2+4BCT^2} = \frac{T}{6C+T}$$
    * åœ¨$$T < 6 \times C$$æ—¶ï¼Œæ•´ä½“è®¡ç®—å‹åŠ›åœ¨ FFN+QKVO Proj éƒ¨åˆ†ï¼›åœ¨ $$T > 6 \times C$$æ—¶ï¼Œæ•´ä½“è®¡ç®—å‹åŠ›åœ¨Attention éƒ¨åˆ†ã€‚
    * GPT3-175B C = 12288, T = 8192
  

```Python
# x : [B, T, C]
# B : batch_size
# T : seq_len
# C : dimension

x = layernorm(x)
q, k, v = qkv_proj(x).split()
# [B, T, C] x [C, 3C] -> [B, T, 3C]: 6BTC^2 FLOPS
attn = q @ k.T
# [B, T, C] x [B, C, T] = [B, T, T] : 2BT^2C FLOPS
attn = softmax(attn)
# 3BT^2*n_h, softmaxè®¡ç®—é‡è¢«å¿½ç•¥
y = attn @ v
# [B, T, T] x [B, T, C] -> [B,T, C] : 2BT^2C FLOPS
y = proj(y)
# [B, T, C] x [C, C] -> [B, T, C] : 2BTC^2
y = layernorm(y)
y = fc1(y)
# [B, T, C] x [C, 4C] -> [B, T, 4C] : 8BTC^2
y = gelu(y)
y = fc2(y)
# [B, T, 4C] x [4C, C] -> [B, T, C] : 8BTC^2
```

* GPT decoderæ¨ç†
  * ç»“åˆGPUçš„FLOPSå’ŒDRAMå†…å­˜å¸¦å®½ï¼Œå®¹æ˜“è®¡ç®—å¾—åˆ°GPTçš„è®­ç»ƒæ˜¯compute boundï¼Œæ¨ç†æ˜¯MBW bound

```Python
# qkv_cache : [B, T-1, 3C]
# x : [B, 1, C]
# B : batch_size
# T : seq_len
# C : dimension

x = layernorm(x)
qkv = qkv_proj(x)
# [B, 1, C] x [C, 3C] -> [B, 1, 3C]: 6BC^2 FLOPS
qkv = concat(qkv, qkv_cache)
# [B, 1, 3C], [B, T-1, 3C] -> [B, T, 3C]
q, k, v = qkv.split()
attn = q[:, -1, :] @ k.T
# [B, 1, C] x [B, C, T] = [B, 1, T] : 2BTC FLOPS
attn = softmax(attn)
y = attn @ v
# [B, 1, T] x [B, T, C] -> [B,1, C] : 2BTC FLOPS
y = proj(y)
# [B, 1, C] x [C, C] -> [B, 1, C] : 2BC^2
y = layernorm(y)
y = fc1(y)
# [B, 1, C] x [C, 4C] -> [B, 1, 4C] : 8BC^2
y = gelu(y)
y = fc2(y)
# [B, 1, 4C] x [4C, C] -> [B, 1, C] : 8BC^2
```



### æ˜¾å­˜

#### è®­ç»ƒæ˜¾å­˜

![image-20250416153914190](./LLM-MLSys/image-20250416153914190.png)

* 7Bæ¨¡å‹ï¼š
  
  * float32: 70*10^8 * 4B = 26.7GB
  * å¾®è°ƒï¼šè€ƒè™‘ä¸­é—´ç»“æœï¼Œ100GBä»¥ä¸Š
* gpt-3ï¼š
  * Fp32å‚æ•°ï¼š175B * 4 = 700GB
    * Fp16å‚æ•°ï¼š326GB
  * ç®—ä¸Šadamä¼˜åŒ–å™¨2100GB
  * æ··åˆç²¾åº¦è®­ç»ƒï¼š
    * fp16å‚æ•°ã€fp32å‚æ•°copyã€fp16æ¢¯åº¦ã€fp32æ¢¯åº¦ã€fp32å†å²æ¢¯åº¦æ»‘åŠ¨å¹³å‡ã€fp32å†å²æ¢¯åº¦å¹³æ–¹å’Œæ»‘åŠ¨å¹³å‡
      * fp16æ¢¯åº¦ åœ¨è½¬æ¢ä¸º fp32æ¢¯åº¦ åå¯ä»¥è¢«é‡Šæ”¾
      * fp16å‚æ•° åœ¨fwdä¹‹åå¯ä»¥é‡Šæ”¾ï¼ˆäº‹å®ä¸ŠPyTorchçš„ampå®ç°å¹¶ä¸ä¼šè¿™æ ·ï¼‰
    * ä¿å®ˆä¼°è®¡ï¼š`(1+2+1+2+2+2)*2*175=20*175=3500 GB`
    * æ¿€è¿›ä¼°è®¡ï¼š`(2+2+2+2)*2*175=16*175GB`
  
* the 1.5B parameter GPT-2 model trained with sequence length of 1K and batch size of 32 requires about 60 GB of memory. 
  
  * Activation checkpointing reduce the activation memory by approximately the square root of the total activations. -> 8GB
  
  * For a GPT-2 like architecture the total activations is about 12 Ã— hidden dim Ã— batch Ã— seq length Ã— transformer layers.

#### æ¨ç†æ˜¾å­˜

![image-20251005140306148](./LLM-MLSys/image-20251005140306148.png)

* 8bité‡åŒ–æ¨¡å‹ï¼š å‚æ•°é‡1B å ç”¨ 1G æ˜¾å­˜ä»¥ä¸Š

### Token

```python
import tiktoken

def count_tokens(prompt):
    encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = len(encoding.encode(prompt))
    return num_tokens

prompt_text = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹prompt"
token_count = count_tokens(prompt_text)
print(f"Promptçš„tokenæ•°é‡ä¸º: {token_count}")
```

### æ€§èƒ½ã€å»¶æ—¶

* TTFTï¼štime to first tokenï¼Œå’Œinput tokené•¿åº¦ç›¸å…³
* TPOT / ITL
* TBT: time between tokens


### è®­ç»ƒæˆæœ¬

* O(10k) è§„æ¨¡çš„ GPU / TPU é›†ç¾¤
* LLaMAï¼š2048 A100 21d
  * a100ä¸€ä¸ªæœˆå‡ ååˆ€ï¼Œè®­ä¸€ä¸ªå‡ åä¸‡
* äººåŠ›æˆæœ¬ï¼šè®­ç»ƒåŸºç¡€å¤§æ¨¡å‹ï¼Œå›¢é˜Ÿ20äºº
  * 6ä¸ªæœˆå‡†å¤‡ã€6ä¸ªæœˆè®­ç»ƒã€6ä¸ªæœˆå¾®è°ƒï¼Œ18ä¸ªæœˆè®­æ¨¡å‹
  * ä¸Šä¸‹æ–‡èƒ½åŠ›æå‡ä¹‹åï¼Œæ—¶æ•ˆæ€§ä¼šæ˜¾è‘—å¢å¼º

* Note
  * å’ŒèŠ¯ç‰‡çš„å¯¹æ¯”ï¼šThis â€œgrowthâ€ is strikingly similar to the one involved in chip evolution where as the number of transistors increases (higher density on a chip) the cost for plants manufacturing  those chips skyrocket.  In  the case of chip manufacturing  the economics remained viable because new plants did cost more but they also produced many more chips so that till the middle lf the last decade the cost per chip was actually  decreasing generation over generation (one effect captured in the Mooreâ€™s law).
  * As with chips one may  wonder if there is a limit to the economic affordability (there sure is, it is just difficult  to pinpoint!).
  * TODO: https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/

### å…¨çƒGPUä¾›ç»™

* å­˜é‡å’Œå¢é‡

![image-20241019195324985](./LLM-MLSys/image-20241019195324985.png)

* åˆ†å¸ƒï¼š

![image-20241019195345714](./LLM-MLSys/image-20241019195345714.png)

### èƒ½æº

* ä¸€å¼ H100 = 700W * 61% å¹´åˆ©ç”¨ç‡ = 2.51ä¸ªäººçš„ç¾å›½å®¶åº­

![image-20251007005621688](./LLM-MLSys/image-20251007005621688.png)

### å”®ä»·

* https://tiktoken.aigc2d.com/
  * ç»Ÿè®¡tokenæ•°é‡
  * GPT-4o
    * outputï¼š15åˆ€/1M token
    * inputï¼š5åˆ€/1M token

## AIå¯¹è¯ç³»ç»Ÿ

### Intro

![image-20251005013903889](./LLM-MLSys/image-20251005013903889.png)

### ä»æ•…äº‹ç»­å†™åˆ°AIå¯¹è¯

#### Chat Template

![image-20251005012753518](./LLM-MLSys/image-20251005012753518.png)



#### ä¼šè¯è®°å¿† = kv cache

* ![image-20251005013326741](./LLM-MLSys/image-20251005013326741.png)

#### ã€Œè°ƒåº¦ä¼˜åŒ–ã€

## Long-Contextä¼˜åŒ–

### Intro

![image-20251005174802221](./LLM-MLSys/image-20251005174802221.png)

### Linear/Sparse Attention å·¥ç¨‹

#### Intro

> [ä¸ºä»€ä¹ˆM2æ˜¯FullAttention](https://www.xiaohongshu.com/explore/69018843000000000703bb85?app_platform=ios&app_version=8.86&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBeHBH61bxaVIZ4pyF_-YomNi6kafK79CLuiuoWHO9E9w=&author_share=1&xhsshare=CopyLink&shareRedId=N0lEN0Y6Rk82NzUyOTgwNjc5OTg2NUpP&apptime=1761736838&share_id=9f706ecf4b8f49f48240b83852de05d3)





### åˆ†å¸ƒå¼å¹¶è¡Œæ³¨æ„åŠ›

#### Ring Attention â€”â€” Sequence Parallel Attention Across Devices

> GPU Mode Lecture 13 https://www.youtube.com/watch?v=ws7angQYIxI

* æ˜¾å­˜ï¼šflash-attnçš„æ˜¾å­˜éšseq-lençº¿æ€§å¢é•¿
  * flash-attnå°†æ˜¾å­˜ä»O(s^2)é™åˆ°äº†O(s)
  * ![image-20250512023151453](./LLM-MLSys/image-20250512023151453.png)

* é•¿ä¸Šä¸‹æ–‡FLOPS
  * ![image-20250512024143990](./LLM-MLSys/image-20250512024143990.png)

* blockwise attn
  * åŠ¨ç”»ï¼šhttps://www.youtube.com/watch?v=JhR_xo9S0_E
  * ![image-20250513215122201](./LLM-MLSys/image-20250513215122201.png)

* SP
  * å‚è€ƒã€ŒMLSys.md â€”â€”å¹¶è¡Œè®­ç»ƒ â€”â€” SPã€

* ring-attn

  * ![image-20250513225747367](./LLM-MLSys/image-20250513225747367.png)
  * ![image-20250514001654067](./LLM-MLSys/image-20250514001654067.png)

  * ring attentionçš„é—®é¢˜ï¼šidle worker
    * ![image-20250514002313692](./LLM-MLSys/image-20250514002313692.png)
    * ![image-20250514002501925](./LLM-MLSys/image-20250514002501925.png)

#### Striped Attention (Reorder QKV)

![image-20250514003125607](./LLM-MLSys/image-20250514003125607.png)

![image-20250514003221884](./LLM-MLSys/image-20250514003221884.png)

### ç®—å­SMåˆ©ç”¨ç‡ä¼˜åŒ–

#### Flash-Decoding (For Long-Context)

> æ€è·¯ï¼šparallelize KVè®¡ç®—ï¼Œç”¨æ»¡GPU

* è§£å†³çš„é—®é¢˜ï¼š
  * **FlashAttention is Sub-optimal for Long-Context Inference**
    * parallelizes across blocks of queries and batch size only, and does not manage to occupy the entire GPU during token-by-token decoding.

* https://crfm.stanford.edu/2023/10/12/flashdecoding.html æœ‰åŠ¨ç”»

#### POD-Attention: Unlocking Full Prefill-Decode Overlap

* Intro
  * åŠ¨æœºï¼šhybrid batchingæ—¶ï¼Œæ”¾åœ¨ä¸€èµ·çš„prefill attnå’Œdecode attnæ²¡æœ‰ä»»ä½•é‡ç”¨ï¼Œæ˜¯è·¨è¯·æ±‚å„è‡ªç‹¬ç«‹çš„ï¼Œèƒ½å¦æœ‰ä¼˜åŒ–ç©ºé—´ï¼Ÿ
  * Goal: Overlap compute-heavy prefill with memory-banedwidth-heavy
    decode to fully utilize GPU resources.
    * è®©prefill kernelå’Œdecode kernelå…±äº«åŒä¸€ä¸ªSMçš„èµ„æº

![image-20251005174305512](./LLM-MLSys/image-20251005174305512.png)

##### ç°æœ‰kernel fusionæŠ€æœ¯çš„å±€é™æ€§

![image-20251005175614866](./LLM-MLSys/image-20251005175614866.png)

* CTA-parallelå’Œkernel-parallelï¼šæ— æ³•ä¿è¯åŒä¸€SMæ‰§è¡Œ
  * ![image-20251005175900584](./LLM-MLSys/image-20251005175900584.png)
* warp-parallelï¼šè´Ÿè½½ä¸å‡è¡¡æ¯”è¾ƒä¸¥é‡
* intra-threadï¼šåŒæ­¥å¼€é”€å¤§

##### POD-Attention

* POD-Attention: Combines prefills and decodes into a single kernel with guaranteed SM co-location.
* Key idea: SM-aware CTA scheduling
  * Guarantees each SM runs prefill and decode CTAs in parallel
  * Enables the CTA scheduler to overlap the two operations
  * Utilizes compute and memory bandwidth simultaneously.

![image-20251005180053939](./LLM-MLSys/image-20251005180053939.png)

![image-20251005181013683](./LLM-MLSys/image-20251005181013683.png)

* ç»“è®ºï¼š
  * ![image-20251005181218437](./LLM-MLSys/image-20251005181218437.png)
  * ![image-20251005181252622](./LLM-MLSys/image-20251005181252622.png)





## è®­ç»ƒè°ƒåº¦

### å¼‚æ„GPUé›†ç¾¤è°ƒåº¦å™¨

#### Metis: Heterogeneous GPUs + DP + TP + PP

> InfiniTensor Paperè®²è§£ï¼šhttps://www.bilibili.com/video/BV1oEZ1Y6EBv

* Today's Practice: Auto-parallelier to find optimal parallelissm plans on homogeneous GPUs (e.g., **Alpa**)
* ![image-20251005212433711](./LLM-MLSys/image-20251005212433711.png)

* ![image-20251005212728814](./LLM-MLSys/image-20251005212728814.png)

* å¼‚æ„ï¼ˆA100/V100ï¼‰éœ€è¦è€ƒè™‘çš„äº‹æƒ…ï¼š
  * load balancingï¼Œæ¯”å¦‚PPæ›´å¤šlayeræ”¾åˆ°A100ä¸Š
  * break 2d-abstractionï¼Œæ¯”å¦‚4 V100 = 2 A100
* è§£æ³•ï¼šplannerè§„åˆ’å™¨
  * ![image-20251005213015879](./LLM-MLSys/image-20251005213015879.png)
  * ![image-20251005213121093](./LLM-MLSys/image-20251005213121093.png)



## æ¨ç†è°ƒåº¦

### Continuous Batching: Orca

> Orca: A distributed serving system for transformer-based generative model
>
> Continuous Batchingè§£å†³çš„æ˜¯ã€Œè¯·æ±‚è°ƒåº¦é—®é¢˜ã€ï¼Œå¯ä»¥å’Œvarlen flash attnç›¸ç»“åˆ

* èƒŒæ™¯ï¼šAI Chatbotä¸­çš„batching
  * è®¡ç®—æµªè´¹ã€å»¶è¿Ÿã€ä¸­æ–­
  * ![image-20251005014058271](./LLM-MLSys/image-20251005014058271.png)

* Orca
  * ![image-20251005014456699](./LLM-MLSys/image-20251005014456699.png)
  * æ ¸å¿ƒæ€è·¯
    * å¯ä»¥è¿›è¡Œbatchingçš„è®¡ç®—åŒæ—¶è¿›è¡Œ
      * qkv linear
      * out linear
    * æ— æ³•batchingçš„è®¡ç®—åˆ†è¯·æ±‚è¿›è¡Œ
      * attn

### LLM Hybrid Batching

> ç”¨TTFTæ¢TPOT
>
> å’Œchunked prefillå½¢æˆé…åˆ

![image-20251005173748976](./LLM-MLSys/image-20251005173748976.png)

- Prefill and decode inputs of multiple requests are batched as a single input
- Improves throughput by reducing scheduling latencies.





### Prefill-Decode Disaggregating (PDåˆ†ç¦»)

> DistServeã€Splitwiseã€TetriInfer
>
> TODO [zartbot: å†æ¥è°ˆè°ˆå¤§æ¨¡å‹çš„åˆ†ç¦»å¼æ¨ç†æ¶æ„](https://mp.weixin.qq.com/s/oRQMEsAj3LoD8UbVtST3Lw)

#### Intro

* ![image-20250912201454071](./LLM-MLSys/image-20250912201454071.png)(semi-PD)
  * å°è®¡ç®—é‡çš„decodeï¼Œå æ»¡GPUèµ„æºï¼Œå¯¼è‡´å¤§è®¡ç®—é‡çš„prefillè¿›è¡Œwait

####  Mooncake: ä»¥KV Cacheä¸ºä¸­å¿ƒï¼ŒPDåˆ†ç¦»æ¨ç†æ¶æ„

> TODO Mooncakeï¼šå°† P / D åˆ†ç¦»è¿›è¡Œåˆ°åº• https://zhuanlan.zhihu.com/p/1711346141
>
> TODO https://www.zhihu.com/question/649192998/answer/3546745976

##### Intro: PDåˆ†ç¦» + KV Cache Pool

![image-20251005220419286](./LLM-MLSys/image-20251005220419286.png)

![image-20251005215005826](./LLM-MLSys/image-20251005215005826.png)

* æ ¸å¿ƒæ€è·¯ï¼š
  * prefillå’Œdecodeç”¨å¼‚æ„é›†ç¾¤
  * KVCache Poolï¼Œé›†ç¾¤é—´æ„æˆä¸€ä¸ªå¤§çš„KV Cache Pool
* æŒ‘æˆ˜ï¼šKV Cacheå å†…å­˜å¤§ï¼Œä¸”éœ€è¦å°½å¿«ä¼ è¾“
  * äºæ˜¯è€ƒè™‘ç”¨å»‰ä»·CPU DRAMå­˜å‚¨

##### KV Cacheå¤šçº§ç¼“å­˜æ± ã€Transfer Engine

![image-20251005215358591](./LLM-MLSys/image-20251005215358591.png)

* transfer engine
  * æ ¸å¿ƒæ˜¯RDMA zero copy

![image-20251005220132617](./LLM-MLSys/image-20251005220132617.png)

##### å¼€æºæ¡†æ¶èåˆï¼švLLM(LMCache)/SGLang(deepseek-v3ã€NVL72è¶…èŠ‚ç‚¹)/Dynamo

> vllm PR #12957

![image-20251005221150234](./LLM-MLSys/image-20251005221150234.png)

* SGLang + Mooncacke: deepseek-v3/r1åå5å€æå‡ã€è¶…èŠ‚ç‚¹NVL72æ”¯æŒ

![image-20251005221459614](./LLM-MLSys/image-20251005221459614.png)

![image-20251005221553375](./LLM-MLSys/image-20251005221553375.png)

* Dynamo + Mooncake

![image-20251005221701671](./LLM-MLSys/image-20251005221701671.png)

##### å’Œå¼ºåŒ–å­¦ä¹ ç»“åˆ

* RL Infraçš„æŒ‘æˆ˜ï¼š
  * ckptï¼Œå¿«é€Ÿä»è®­ç»ƒèŠ‚ç‚¹updateåˆ°æ¨ç†èŠ‚ç‚¹
  * long cotçš„RLï¼Œprompté•¿åº¦ä¸å‡è¡¡ï¼Œé•¿å°¾è¯·æ±‚
    * è§£å†³æ–¹æ¡ˆï¼špartial rolloutï¼Œæˆªæ–­ç‰¹åˆ«é•¿çš„é•¿å°¾ï¼Œæ”¾è¿›ä¸‹ä¸€è½®
    * æ ¸å¿ƒè¦ç‚¹ï¼šæš‚å­˜kv cacheï¼Œæ”¾åˆ°ä¸‹ä¸€è½®

![image-20251005221956055](./LLM-MLSys/image-20251005221956055.png)

#### semi-PD: åˆ†é˜¶æ®µè§£è€¦è®¡ç®— + ç»Ÿä¸€å­˜å‚¨

> https://github.com/infinigence/Semi-PD

* Intro
  * ç°æœ‰ LLM æœåŠ¡ç³»ç»Ÿåˆ†ä¸º**ç»Ÿä¸€ç³»ç»Ÿ**ï¼ˆprefill ä¸ decode é˜¶æ®µåŒ GPUï¼Œå­˜åœ¨å»¶è¿Ÿå¹²æ‰°ï¼‰å’Œ**è§£è€¦ç³»ç»Ÿ**ï¼ˆä¸¤é˜¶æ®µåˆ†å±ä¸åŒ GPUï¼Œå­˜åœ¨å­˜å‚¨å¤±è¡¡ã€KV ç¼“å­˜ä¼ è¾“å¼€é”€ã€èµ„æºè°ƒæ•´æˆæœ¬é«˜ã€æƒé‡å†—ä½™å››å¤§é—®é¢˜ï¼‰
  * ä¸ºæ­¤æå‡º**semi-PD**ç³»ç»Ÿï¼Œé€šè¿‡**åˆ†é˜¶æ®µè§£è€¦è®¡ç®—**ï¼ˆåŸºäº MPS å®ç° SM çº§èµ„æºåˆ†é…ï¼Œæ¶ˆé™¤ä¸¤é˜¶æ®µå»¶è¿Ÿå¹²æ‰°ï¼‰ä¸**ç»Ÿä¸€å­˜å‚¨**ï¼ˆç”¨ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨åè°ƒæƒé‡ä¸ KV ç¼“å­˜è®¿é—®ï¼Œè§£å†³å­˜å‚¨ç—›ç‚¹ï¼‰ï¼Œæ­é…**ä½å¼€é”€èµ„æºåˆ‡æ¢æœºåˆ¶**å’Œ**SLO-aware åŠ¨æ€åˆ†åŒºç®—æ³•**ï¼Œæœ€ç»ˆåœ¨ DeepSeek ç³»åˆ—æ¨¡å‹ä¸Šé™ä½å•è¯·æ±‚å¹³å‡ç«¯åˆ°ç«¯å»¶è¿Ÿ**1.27-2.58Ã—**ï¼Œåœ¨ Llama ç³»åˆ—æ¨¡å‹ä¸Šæ»¡è¶³ SLO çº¦æŸçš„è¯·æ±‚é‡æå‡**1.55-1.72Ã—**ã€‚
  * <img src="./LLM-MLSys/image-20250912202038298.png" alt="image-20250912202038298" style="zoom:50%;" />

![image-20250912201840304](./LLM-MLSys/image-20250912201840304.png)

* | ç³»ç»Ÿç±»å‹ | ä»£è¡¨æ–¹æ¡ˆ                         | æ ¸å¿ƒç‰¹ç‚¹                           | å…³é”®é—®é¢˜                                                     |
  | -------- | -------------------------------- | ---------------------------------- | ------------------------------------------------------------ |
  | ç»Ÿä¸€ç³»ç»Ÿ | vLLMã€SGLangã€FasterTransformer  | prefill ä¸ decode åŒ GPUï¼Œå…±äº«èµ„æº | 1. **å»¶è¿Ÿå¹²æ‰°**ï¼šä¼˜å…ˆ prefill ä¼šæ¶åŒ– TPOTï¼Œä¼˜å…ˆ decode ä¼šæ¶åŒ– TTFTï¼› 2. æ— æ³•åŒæ—¶æ»¡è¶³ TTFT ä¸ TPOT çš„ SLO |
  | è§£è€¦ç³»ç»Ÿ | DistServeã€Splitwiseã€TetriInfer | prefill ä¸ decode åˆ†å±ä¸åŒ GPU     | 1. **å­˜å‚¨å¤±è¡¡**ï¼šdecode éœ€å­˜å®Œæ•´ KV ç¼“å­˜ï¼Œprefill ä»…å­˜éƒ¨åˆ†ï¼Œæœ€é«˜æµªè´¹ 89.33% GPU å†…å­˜ï¼› 2. **KV ç¼“å­˜ä¼ è¾“å¼€é”€**ï¼šè·¨ GPU ä¼ è¾“è€—æ—¶ï¼Œä½ç«¯ GPU æ—  NVLink æ—¶å¼€é”€æ˜¾è‘—ï¼› 3. **èµ„æºè°ƒæ•´æˆæœ¬é«˜**ï¼šGPU çº§ç²—ç²’åº¦è°ƒæ•´ï¼ŒDistServe é‡è½½æƒé‡éœ€åˆ†é’Ÿçº§ï¼› 4. **æƒé‡å†—ä½™**ï¼šä¸¤é˜¶æ®µå„å­˜å®Œæ•´æƒé‡ï¼ŒLlama3.1-405B éœ€é¢å¤–ç¿»å€ GPU |

* **è®¡ç®—èµ„æºæ§åˆ¶å™¨**ï¼š

  - è§£è€¦è®¡ç®—å®ç°ï¼šåŸºäº**NVIDIA MPS**ï¼ˆå¤šè¿›ç¨‹æœåŠ¡ï¼‰ï¼Œæ”¯æŒ SM çº§èµ„æºåˆ†é…ï¼Œé€šè¿‡ (x,y) é…ç½® prefill/decode çš„ SM å æ¯”ï¼ˆå¦‚ x=60ã€y=40 è¡¨ç¤º prefill ç”¨ 60% SMï¼‰ï¼›
  - ä½å¼€é”€èµ„æºåˆ‡æ¢ï¼šä¿è¯å½“é…æ¯”å˜åŒ–æ—¶ï¼ŒæœåŠ¡ä¸æŠ–åŠ¨
    1. **å¸¸é©»è¿›ç¨‹**ï¼šæŒæœ‰å…³é”®weightä¸ KV ç¼“å­˜ï¼Œé€šè¿‡ IPC å…±äº«å†…å­˜æŒ‡é’ˆï¼Œé¿å…è¿›ç¨‹é‡å¯æ—¶çš„æƒé‡é‡è½½ä¸ KV å¤åˆ¶ï¼›
    2. **å»¶è¿Ÿåˆ‡æ¢**ï¼šæ–° (x,y) é…ç½®å‡†å¤‡å®Œæˆåå†ç”Ÿæ•ˆï¼Œéšè— IPC ä¸åˆå§‹åŒ–å»¶è¿Ÿï¼›
    3. **å¼‚æ­¥åˆ‡æ¢**ï¼šä»…ç»ˆæ­¢å®Œæˆå½“å‰è¿­ä»£çš„ workerï¼Œç¡®ä¿ç³»ç»Ÿå§‹ç»ˆæœ‰ worker è¿è¡Œï¼Œé¿å…ç©ºé—²ã€‚
  - ![image-20250912202246690](./LLM-MLSys/image-20250912202246690.png)

* **ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨**ï¼š

  - æƒé‡ç®¡ç†ï¼šåˆ©ç”¨æƒé‡ â€œåªè¯»â€ ç‰¹æ€§ï¼Œæ”¯æŒ prefill/decode worker å…±äº«è®¿é—®ï¼Œæ¶ˆé™¤æƒé‡å†—ä½™ï¼›
  - KV ç¼“å­˜ç®¡ç†ï¼š
    1. åŸºäº vLLM çš„**åˆ†é¡µå­˜å‚¨**ï¼Œé€šè¿‡å—è¡¨ç´¢å¼•è®¿é—® KV ç¼“å­˜ï¼›
    2. ç”¨**åŸå­æ“ä½œ**ï¼ˆåŒ…è£¹ query-get-update ä¸‰æ­¥ï¼‰è§£å†³ prefill/decode å¼‚æ­¥åˆ†é…å¯¼è‡´çš„ WARï¼ˆå†™åè¯»ï¼‰å†²çªï¼Œç¡®ä¿å†…å­˜åˆ©ç”¨ç‡å‡†ç¡®ã€‚

* SLO-aware åŠ¨æ€è°ƒæ•´æ–¹æ³•

  * TTFTï¼šç»“åˆ M/M/1 æ’é˜Ÿæ¨¡å‹ï¼Œè€ƒè™‘ç­‰å¾…å»¶è¿Ÿ + å¤„ç†å»¶è¿Ÿ



### Attention-FFN Disaggregation (AFD)

> kimiã€ç«å±±MegaScale-Inferã€é˜¶è·ƒï¼Œéƒ½æ˜¯ç±»ä¼¼æ€è·¯

#### Intro

![image-20251005222618516](./LLM-MLSys/image-20251005222618516.png)

#### è¦ç‚¹æ˜¯é€šä¿¡ä¼˜åŒ–

![image-20251005230600558](./LLM-MLSys/image-20251005230600558.png)

#### Mooncake

* å’Œç«å±±å¼•æ“æœ‰åˆä½œ

![image-20251005222604681](./LLM-MLSys/image-20251005222604681.png)

![image-20251005230631404](./LLM-MLSys/image-20251005230631404.png)





## æ¨ç†ä¼˜åŒ–&ç®—å­ä¼˜åŒ–

### Intro

> https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/
>
> InfiniTensor å…¥é—¨ææ–™ https://www.bilibili.com/video/BV1zifEYMELb
>
> [InfiniTensor ç« æ˜æ˜Ÿ - ä»åŒæ„èµ°å‘åˆ†ç¦»çš„å¤§æ¨¡å‹æ¨ç†ç³»ç»Ÿ](https://www.bilibili.com/video/BV11aYfz3EPC)

### Literature Review

* Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. ã€FlashAttentionã€‘
  * These methods range from sparse-approximation [51, 74] to low-rank approximation [12, 50, 84],
    and their combinations [3, 9, 92].
  * æ ¸å¿ƒé—®é¢˜ï¼šthey focus on FLOP reduction (which may not
    correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).

* Eï¬ƒcient ML Models with Structured Matrices. ã€FlashAttentionã€‘
  * Matrix multiply is the core computational bottleneck of most machine learning models. To reduce the computational complexity, there have been numerous approaches to learn over a more eï¬ƒcient set of matrices. These matrices are called structured matrices, which have subquadratic (ğ‘œ(ğ‘›2) for dimension ğ‘› Ã— ğ‘›) number of parameters and runtime. Most common examples of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several more general classes of structured matrices proposed in machine learning: Toeplitz-like [78], low-displacement rank [49], quasi-separable [25]). The butterï¬‚y pattern we use for our block-sparse attention is motivated
    by the fact that butterï¬‚y matrices [15, 64] and their products have been shown to be able to express any structured matrices with almost optimal runtime and number of parameters [16, 20]. However, even though structured matrices are eï¬ƒcient in theory, they have not seen wide adoption since it is hard to translate their eï¬ƒciency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation, a phenomenon known as the hardware lottery [41]. Extensions of butterï¬‚y matrices [17, 18] aimed to make butterï¬‚y matrices more hardware-friendly.
* Sparse Trainingã€FlashAttentionã€‘
  * Our block-sparse FlashAttention can be seen as a step towards making sparse model
    training more eï¬ƒcient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23, 38, 39, 55, 76]. For model training, the lottery tickets hypothesis [28, 29, 30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network.
* Eï¬ƒcient Transformer.ã€FlashAttentionã€‘
  * Transformer-based models have become the most widely-used architecture in
    natural language processing [22] and computer vision [24, 91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12, 54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52, 57, 79, 89]. One can also attend over the states from previous sequences
    to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details.
    There are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31, 36, 37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models. They combine the strengths of CNNs (eï¬ƒcient training), RNNs (eï¬ƒcient inference), and continuous models (robust to change in sampling rates). LambdaNetworks [2], AFT [93] and FLASH [42] are other attempts at replacing attention in the context of image classiï¬cation and language modeling.

### ç®—æ³•å·¥ç¨‹co-design

* å‚è€ƒã€ŒAI-Algorithmã€ï¼šã€ŒKVå‹ç¼©ã€ã€ŒQå‹ç¼©ã€ç­‰ã€ã€ŒMoEã€

### Best Practices

#### ä½¿ç”¨ GemLiteã€TorchAO å’Œ SGLang åŠ é€Ÿ LLM æ¨ç†

> https://pytorch.org/blog/accelerating-llm-inference/
>
> é€‰å‹ï¼šint4 weight only quantization (both tinygemm and GemLite version), float8 dynamic quantization

* ç°æœ‰çš„ä½ç²¾åº¦æ¨ç†æ–¹æ¡ˆåœ¨å° batch size åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

  - å½“ batch size å¢å¤§æ—¶ï¼Œæ€§èƒ½ä¸‹é™

  - å¯¹é‡åŒ–ç±»å‹çš„é™åˆ¶ï¼Œä¾‹å¦‚ï¼Œä¸€äº›è®¡ç®—æ ¸ï¼ˆkernelsï¼‰ä»…æ”¯æŒå¯¹ç§°é‡åŒ–ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹åœ¨è¾ƒä½æ¯”ç‰¹ä¸‹çš„å‡†ç¡®æ€§

  - é‡åŒ–ã€åºåˆ—åŒ–å’Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰çš„ç›¸äº’å½±å“ï¼Œä½¿å¾—åŠ è½½é‡åŒ–æ¨¡å‹å˜å¾—å›°éš¾ï¼Œå¹¶ä¸”å¯èƒ½éœ€è¦å¯¹ç”¨æˆ·æ¨¡å‹è¿›è¡Œä¿®æ”¹

* é›†æˆï¼š

  * GemLite ï¼šä¸€ä¸ªåŸºäº Triton çš„è®¡ç®—æ ¸ï¼ˆkernelï¼‰åº“ï¼Œè§£å†³äº†å¤§ batch size åœºæ™¯ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æ”¯æŒæ›´çµæ´»çš„é‡åŒ–æ–¹å¼ã€‚
  * TorchAO ï¼šä¸€ä¸ªåŸç”Ÿ PyTorch åº“ï¼Œä¸ºé‡åŒ–ã€ç¨€ç–æ€§å’Œå¼ é‡å¹¶è¡Œï¼ˆä¸ DTensor ç»“åˆä½¿ç”¨ï¼‰æä¾›äº†ç®€åŒ–çš„ç”¨æˆ·ä½“éªŒã€‚
  * SGLang ï¼šä¸€ä¸ªå¿«é€Ÿã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„ LLM å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒå¹¿æ³›çš„æ¨¡å‹ç±»å‹ã€‚

* a summary of the results in **8xH100 machine on Llama 3.1-8B for decode**. 

  * **int4 Weight-Only Quantization**: This method significantly reduces memory footprint and **accelerates decode for memory-bound workloads**, with minimal impact on performance in compute-intensive scenarios like prefill or larger batch sizes. We present results for bf16, GemLite, and tinygemm kernels below, across various batch sizes and tensor parallel configurations
  * **float8 Dynamic Quantization**: While offering less memory savings, this method often provides higher accuracy and balanced speedups for both memory-bound and compute-bound tasks. With Hopper-grade hardware and native fp8 support, the efficient cutlass/cuBLAS kernels used by AO contribute to a significant speedup

![image-20250409022139221](./LLM-MLSys/image-20250409022139221.png)

> æ›´è¯¦ç»†çš„å®éªŒç»“è®ºï¼šhttps://developers.redhat.com/articles/2024/10/17/we-ran-over-half-million-evaluations-quantized-llms#real_world_benchmark_performance

### KV Cache

> æœ¬è´¨å’Œ casual mask æœ‰å¯†åˆ‡å…³ç³»ï¼Œfull maskä¸‹æ— æ³•ä½¿ç”¨ KV cache
>
> å¤§æ¨¡å‹æ¨ç†æ€§èƒ½ä¼˜åŒ–ä¹‹KV Cacheè§£è¯» https://zhuanlan.zhihu.com/p/630832593
>
> llama3æºç 

#### Intro

* Intro
  * ç¼“å­˜å½“å‰è½®å¯é‡å¤åˆ©ç”¨çš„è®¡ç®—ç»“æœï¼Œä¸‹ä¸€è½®è®¡ç®—æ—¶ç›´æ¥è¯»å–ç¼“å­˜ç»“æœ
  * æ¯è½®æ¨ç†å¯¹åº”çš„ cache æ•°æ®é‡ä¸º 2âˆ—bâˆ—sâˆ—hâˆ—n_layers ï¼Œè¿™é‡Œ s å€¼ç­‰äºå½“å‰è½®æ¬¡å€¼ã€‚ä»¥GPT3-175Bä¸ºä¾‹ï¼Œå‡è®¾ä»¥ float16 æ¥ä¿å­˜ KV cacheï¼Œsenquenceé•¿åº¦ä¸º100ï¼Œbatchsize=1ï¼Œåˆ™ KV cacheå ç”¨æ˜¾å­˜ä¸º 2Ã—100Ã—12288Ã—96Ã—2 Byte= 472MBã€‚
  * LLMæ¨¡å‹é¢„æµ‹çš„æ—¶å€™ä½¿ç”¨çš„æ˜¯KV cacheçš„æŠ€æœ¯ï¼Œä¹Ÿå°±æ˜¯ç¼“å­˜å·²ç»æ¨ç†å‡ºçš„å‰t-1ä¸ªtokençš„KV matrixï¼Œé‚£ä¹ˆåœ¨ç¬¬tä¸ªtokenå¼€å§‹å°±æ— éœ€å†è®¡ç®—è¿™éƒ¨åˆ†KVï¼Œç›´æ¥è°ƒç”¨ç¼“å­˜çš„KVå°±å¯ä»¥ã€‚å…·ä½“è€Œè¨€ï¼Œæ•´ä¸ªMHAåœ¨casual maskä¸‹ï¼Œå¯ä»¥è¡¨ç¤ºä¸ºï¼š $$Logit_{t_h} = \sum_{i \leq t}softmax(\frac{Q_{t_h}K^T_{i_h}}{\sqrt d})V_{i_h}$$,å› æ­¤é¢„æµ‹ç¬¬tä¸ªtokençš„æ—¶å€™ï¼Œqueryçš„multi headï¼ˆhè¡¨ç¤ºï¼‰éœ€è¦é‡æ–°è®¡ç®—ï¼Œä»¥åŠç¬¬tä¸ªkeyå’Œqueryçš„multi headï¼ˆhè¡¨ç¤ºï¼‰è¡¨ç¤ºéœ€è¦é‡æ–°è®¡ç®—ï¼Œå…¶ä½™çš„å°±å¯ä»¥ç›´æ¥ç”¨é¢„æµ‹t-1ä¸ªtokenç¼“å­˜çš„KVè¿›è¡Œè®¡ç®—ã€‚æ•´ä½“ä¸Šä¼šå¤§å¤§èŠ‚çœé¢„æµ‹æ—¶é—´ã€‚é™„ï¼šä½†æ˜¯è¿™éƒ¨åˆ†çš„KVéœ€è¦å ç”¨GPUç¼“å­˜ï¼Œè€Œå¤§æ¨¡å‹ä¸­ç¼“å­˜å ç”¨è¿‡å¤šï¼Œä¼šå¯¼è‡´é¢„æµ‹çš„æ—¶å€™Batch sizeè¿‡å°ï¼Œé‚£ä¹ˆæ•´ä½“çš„é¢„æµ‹ååç‡ä¼šé™ä½ï¼Œæ‰€ä»¥åç»­å¾ˆå¤šå·¥ä½œéƒ½åœ¨å¯¹äºKV cacheåšä¼˜åŒ–ã€‚
  * ![image-20250630201017484](./LLM-MLSys/image-20250630201017484.png)
  

#### prefillå’Œdecode

![image-20251005175111505](./LLM-MLSys/image-20251005175111505.png)

* prefillï¼šå‘ç”Ÿåœ¨è®¡ç®—ç¬¬ä¸€ä¸ªè¾“å‡ºtokenè¿‡ç¨‹ä¸­ï¼Œè¿™æ—¶Cacheæ˜¯ç©ºçš„ï¼ŒFLOPsåŒKV Cacheå…³é—­ä¸€è‡´ï¼Œå­˜åœ¨å¤§é‡gemmæ“ä½œï¼Œæ¨ç†é€Ÿåº¦æ…¢ã€‚
* Decodeï¼š
  * å‘ç”Ÿåœ¨è®¡ç®—ç¬¬äºŒä¸ªè¾“å‡ºtokenè‡³æœ€åä¸€ä¸ªtokenè¿‡ç¨‹ä¸­ï¼Œè¿™æ—¶Cacheæ˜¯æœ‰å€¼çš„ï¼Œæ¯è½®æ¨ç†åªéœ€è¯»å–Cacheï¼ŒåŒæ—¶å°†å½“å‰è½®è®¡ç®—å‡ºçš„æ–°çš„Keyã€Valueè¿½åŠ å†™å…¥è‡³Cacheï¼›
  * FLOPsé™ä½ï¼Œgemmå˜ä¸ºgemvæ“ä½œï¼Œæ¨ç†é€Ÿåº¦ç›¸å¯¹ç¬¬ä¸€é˜¶æ®µå˜å¿«ï¼Œè¿™æ—¶å±äºMemory-boundç±»å‹è®¡ç®—ã€‚

![image-20251005214442152](./LLM-MLSys/image-20251005214442152.png)

![image-20251005012559887](./LLM-MLSys/image-20251005012559887.png)

#### Prefix Caching

> https://docs.vllm.ai/en/latest/automatic_prefix_caching/apc.html

##### Intro

![image-20251005214742619](./LLM-MLSys/image-20251005214742619.png)

![image-20251005214806732](./LLM-MLSys/image-20251005214806732.png)



### è®¿å­˜ä¼˜åŒ–

#### FlashAttention: IO-Awareness
> https://github.com/HazyResearch/flash-attention
>
> **flashattn + flash-decoding https://zhuanlan.zhihu.com/p/685020608**
>
> FlashAttn V1/V2/V3è®ºæ–‡ç²¾è¯» https://www.bilibili.com/video/BV1ExFreTEYa
>
> åŠ¨ç”»ï¼šhttps://www.bilibili.com/video/BV1HJWZeSEF4
>
> æ ¸å¿ƒæ´å¯Ÿï¼šattentionçŸ©é˜µN^2å¤ªå¤§äº†ï¼Œæ— æ³•åˆ©ç”¨192KBçš„SRAMç¼“å­˜
>
> ç›´è§‚ç†è§£ï¼šåˆ†å—è®¡ç®—æ³¨æ„åŠ›ï¼Œå‰é¢å—çš„æ³¨æ„åŠ›æ˜¯ä¸€ä¸ªå±€éƒ¨æ³¨æ„åŠ›ï¼Œå½“è¿›ä¸€æ­¥è®¡ç®—åé¢æ³¨æ„åŠ›æ—¶ï¼Œéœ€è¦å¯¹å‰é¢çš„å±€éƒ¨æ³¨æ„åŠ›åŠ æƒï¼Œå’Œåé¢çš„æ³¨æ„åŠ›æƒé‡ç›¸åŠ 

##### Attnè®¡ç®—

* 1 SM: â€œ1 head + no batch dimension"
  * å› æ­¤attnçš„head dimè¾ƒå°ï¼Œå¦åˆ™æ— æ³•mapåˆ°ä¸€ä¸ªSMå®Œæˆ
* tilingä¼˜åŒ–æ€è·¯
  * å¯¹contraction axisåštiling

```
for t_tile:
    load(Q[t_tile]) to shared, init O[t, d] = o
    for s_tile:
        load(K[s_tile], V[stile]) to shared;
        compute I[t, s] = Q[t_tile] @ Káµ€[s_tile] (compute p[t, s])
        O[t, d] += p[t_tile, s_tile] @ V[s_tile]
    write O[t, d] 
```

##### FlashAttn

* Intro
  * uses tiling to reduce the number of memory reads/writes
    between GPU high bandwidth memory (HBM) and GPU on-chip SRAM
  * also extend FlashAttention to block-sparse attention
  * 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3Ã— speedup on
    GPT-2 (seq. length 1K), and 2.4Ã— speedup on long-range arena (seq. length 1K-4K).

![image-20250131230121452](./LLM-MLSys/image-20250131230121452.png)

![image-20250201014853281](./LLM-MLSys/image-20250201014853281.png)

* æ€è·¯

  * Our main goal is to avoid reading and writing the attention matrix to and from HBM.
    This requires
    *  (i) computing the softmax reduction without access to the whole input
    *  (ii) not storing the large intermediate attention matrix for the backward pass.

  * (i) We restructure the attention computation to split the input into blocks and make several
    passes over input blocks, thus incrementally performing the softmax reduction (also known as **tiling**).
  * (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.

* ç»“è®ºï¼š
  * in sub-quadratic HBM accesses
  * seq-len 512ï¼Œæ¯”ä»»ä½•ç®—æ³•å¿«
  * seq-len 1000ä»¥ä¸Šï¼Œapproximateç®—æ³•æ›´å¿«
* 3.1 An Eï¬ƒcient Attention Algorithm With Tiling and Recomputation
  * The main challenge in making attention memory-eï¬ƒcient is **the softmax that couples the columns of K (and columns of V).**
  * online softmax
    * Reformer: The eï¬ƒcient transformer
    * **Online normalizer calculation for softmax.**
    * Self-attention does not need ğ‘‚ (ğ‘›2) memory
  * recomputation
    * This can be seen as a form of **selective gradient checkpointing** [10, 34]. While gradient checkpointing has been suggested to reduce the maximum amount of memory required [66],

![image-20250201015720233](./LLM-MLSys/image-20250201015720233.png)

![image-20250503014225408](./LLM-MLSys/image-20250503014225408.png)

* **IOå¤æ‚åº¦å¯¹æ¯”ï¼š**
  * ![image-20250201022049588](./LLM-MLSys/image-20250201022049588.png)
  * For typical values of ğ‘‘ (64-128) and ğ‘€ (around 100KB), ğ‘‘2 is many times smaller than ğ‘€,
  * $$N^2d^2M^{-1}=(Nd)*N/(M/d)$$
* ![image-20250201024230130](./LLM-MLSys/image-20250201024230130.png)

* Evaluation
  * Path-X and Path-256: åƒç´ é¢„æµ‹ï¼Œä¸¤ä¸ªé»‘ç‚¹æ˜¯å¦ç›¸è¿
    * 256*256
    * Block-sparse FlashAttention: 64k seq len
  * æ€§èƒ½ç›¸æ¯”å…¶å®ƒtransformer![image-20250201025905877](./LLM-MLSys/image-20250201025905877.png)

##### Online Softmax

> ã€ŠFrom Online Softmax to FlashAttentionã€‹ã€ã€ŠOnline normalizer calculation for softmaxã€‹

* (Safe) Softmax

  * é—®é¢˜ï¼šSRAMå­˜ä¸ä¸‹N^2çš„logitï¼Œå› æ­¤**need to access Q and K three times**
  * **3 read + 1 store per element**
  * ![image-20250503020010235](./LLM-MLSys/image-20250503020010235.png)

* Online Softmax
  * $$ \sum_{j} \left( \exp(l_j - m_{\text{new}}) \right) = \exp(m - m_{\text{new}}) \sum_{j} \left( \exp(l_j - m) \right) $$ 
    * can also do this for partial sum $\to$ do summing and max in one go 
  
  * **2 read + 1 store per element**
  * ç†è§£ï¼šdi'æ˜¯æ³¨æ„åŠ›æƒé‡çš„ç´¯ç§¯å’Œ
  * ![image-20250201033508793](./LLM-MLSys/image-20250201033508793.png)
  
  * ![image-20250201160812250](./LLM-MLSys/image-20250201160812250.png)
  

#### FlashAttention-2: Better Parallelism and Work Partitioning

> æ ¸å¿ƒï¼šå…ˆéå†Qå†éå†KV
>
> - FAv1è¿˜æ˜¯æ²¡æœ‰åŠæ³•æ¥è¿‘GEMM,ä»…èƒ½è¾¾åˆ°ç¡¬ä»¶FLOPSçš„ç†è®ºä¸Šé™çš„30%,ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚
> - ä¸»è¦åŸå› æ˜¯FAåœ¨GPUä¸Šçº¿ç¨‹å—å’Œwarpä¹‹é—´çš„å·¥ä½œåˆ†é…ä¸ä½³,å¯¼è‡´ä¸èƒ½å‘æŒ¥ç¡¬ä»¶æ•ˆç‡ã€‚
> - é€šè¿‡ä¸‰ä¸ªåšæ³•æ¥è§£å†³ä¸Šè¿°çš„é—®é¢˜
>   - **æ”¹å†™ç®—æ³•ï¼Œå‡å°‘éçŸ©é˜µä¹˜æ³•çš„FLOPS**
>     - éçŸ©é˜µä¹˜åœ¨æ€»è®¡ç®—é‡ä¸­å æ¯”è¾ƒå°‘ï¼Œä½†è€—æ—¶å¾ˆé«˜ï¼ŒGPUå¯¹çŸ©é˜µä¹˜åšäº†å¾ˆå¤šä¼˜åŒ–é€šå¸¸èƒ½å¿«16Xï¼Œå› æ­¤å‡å°‘éçŸ©é˜µä¹˜çš„è®¡ç®—é‡éå¸¸é‡è¦ã€‚
>   - **æ›´å¥½çš„å¹¶è¡Œæ€§**ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå¤´ä¹Ÿå¯ä»¥åœ¨å¤šä¸ªçº¿ç¨‹å—ä¹‹é—´å¹¶è¡Œè®¡ç®—ã€‚
>     - é™¤äº†batchå’Œheadç»´åº¦ï¼Œåºåˆ—é•¿åº¦ä¹Ÿè¦æ”¯æŒå¹¶è¡ŒåŒ–ï¼Œè¿™æ ·èƒ½æé«˜GPUå ç”¨ç‡ã€‚
>   - çº¿ç¨‹å—å†…éƒ¨ï¼Œé€šè¿‡åˆç†çš„ç¼–æ’warpæ¥å‡å°‘å…±äº«å†…å­˜ä¸å¿…è¦çš„è®¿é—®ä»¥åŠé€šä¿¡ã€‚
> - ç»è¿‡ä¸Šé¢ä¸‰ä¸ªæ”¹é€ ç‚¹ï¼Œæ€§èƒ½ä¸Šv2æ¯”v1æå‡æ¥2å€ï¼Œæ•ˆç‡æ¥è¿‘GEMM,è¾¾åˆ°ç†è®ºFLOPSçš„70%
> - ä½¿ç”¨äº†cutlass



![image-20250511165841859](./LLM-MLSys/image-20250511165841859.png)

* è®¡ç®—é‡æ’åºï¼šå°†å¤–å¾ªç¯æ”¹ä¸ºéå†Qçš„å—å†…å¾ªç¯éå†K,Vçš„å—ï¼Œæé«˜äº†æ•°æ®å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§ã€‚
  * Qæ¯”KVåœ¨SRAMå¯ä»¥é©»ç•™æ›´é•¿çš„æ—¶é—´ï¼Œç¼“å­˜çš„å­˜æ´»æ—¶é—´æ›´é•¿ï¼Œæ›´èƒ½å‡å°‘HBMçš„è®¿é—®æ¬¡æ•°ã€‚
* å½’ä¸€åŒ–æ“ä½œï¼šæ”¾åœ¨å¾ªç¯å¤–ï¼Œ GPU çº¿ç¨‹ä¹‹é—´çš„å¹¶è¡Œæ€§å¾—åˆ°äº†æå¤§çš„æå‡ã€‚

* å¯¹äºå‰å‘è®¡ç®—:
  1. æ”¹æˆäº†å…ˆéå†Qåœ¨éå†kvï¼Œè¿™æ ·åšåˆ™æ˜¯åœ¨ä½¿å…¶åœ¨è¡Œå—å®ç°äº†å¹¶è¡Œã€‚ æ¯ä¸ªè¡Œå—åˆ†é…ç»™ä¸€ä¸ªGPUçš„thread blockï¼Œä»å…±äº«å†…å­˜çš„è§†è§’çœ‹Så¯ä»¥è¢«æ›´å¥½çš„å¤ç”¨ï¼Œä»å¹¶è¡Œåº¦ä¸Šå°†ï¼Œå¯ä»¥åˆ©ç”¨åºåˆ—ç»´åº¦çš„å¹¶è¡Œæ€§æ¥åŠ é€Ÿè®¡ç®—ã€‚
  2. å¯¹äºæ¯ä¸ªworkå†…éƒ¨çš„warpï¼Œå°½å¯èƒ½çš„å‡å°‘å…¶å¯¹å…±äº«å†…å­˜çš„è¯»å†™å¯ä»¥è·å¾—å¯è§‚çš„åŠ é€Ÿï¼Œfa1 æ˜¯å…±äº«kvï¼Œä½†æ˜¯qéœ€è¦è¢«æ‰€æœ‰warpè®¿é—®å’Œè¯»å†™ï¼Œä¸»è¦æ˜¯é¢‘ç¹çš„æ›´æ–°å…±äº«å†…å­˜ã€‚
  3. è€Œ fa2ï¼Œå…±äº«äº†qï¼Œæ¯ä¸ªwarpè¯»å–è‡ªå·±åˆ†å—å†…çš„kvï¼Œä¸éœ€è¦æ›´æ–°ä¸é€šä¿¡ï¼Œè·å¾—äº†åŠ é€Ÿã€‚
* å¯¹äºåå‘è®¡ç®—:
  1. è¿˜æ˜¯æŒ‰ç…§åˆ—å—è¿›è¡Œå¹¶è¡Œï¼Œè¿™æ ·å¹¶è¡Œçš„ç»„å—ä¹‹é—´æœ‰æœ€å°çš„é€šä¿¡è¡Œä¸ºï¼Œå¦åˆ™dkå’Œdvä¹Ÿè¦å…±äº«é€šä¿¡äº†ã€‚
  2. å°½ç®¡æŒ‰åˆ—åˆ†å—åï¼Œdqï¼Œdkï¼Œdvä¹‹é—´çš„ç›¸äº’ä¾èµ–å¾ˆå¤æ‚ï¼Œä½†é¿å…åˆ‡kï¼Œä¹Ÿä¾æ—§å¯ä»¥å‡å°‘warpå†…éƒ¨çš„å…±äº«å†…å­˜è¯»å†™çš„å¼€é”€ã€‚

#### FlashAttention-3: Asynchrony and Low-precision

**æ´å¯Ÿ**:  æ²¡æœ‰å……åˆ†åˆ©ç”¨æœ€æ–°ç¡¬ä»¶ä¸­çš„æ–°åŠŸèƒ½(tensor core ä¸ TMA)

1. å¼‚æ­¥åŒ–: åˆ©ç”¨ä¸“ç”¨warpé‡å è®¡ç®—ï¼Œmatmulå’Œsoftmaxã€‚
2. ä½ç²¾åº¦: åº”ç”¨FP8é‡åŒ–ï¼Œæ›´å¥½çš„åˆ©ç”¨tensor coreç‰¹æ€§ã€‚

æŒ‘æˆ˜: é‡å†™FA2æ¥é€‚é…å¼‚æ„ç¡¬ä»¶ï¼Œæœ€å°åŒ–FP8/4çš„é‡åŒ–è¯¯å·®ã€‚

**æ–¹æ¡ˆ**: 

1. ä¸“ç”¨warpå¼‚æ­¥åŒ–: é€šè¿‡æ‹†è§£ç”Ÿäº§è€…/æ¶ˆè´¹è€…warpæ¨¡å¼ï¼Œç§»åŠ¨æ•°æ®æ¥å®ç°æŒ‡ä»¤ä¸è®¿å­˜çš„é‡å 
2. éšè—softmaxè®¡ç®—:é€šè¿‡ä¼˜åŒ–ä¾èµ–å…³ç³»ï¼Œå°†éGEMMçš„è®¡ç®—éšè—åœ¨GEMMçš„å¯å¼‚æ­¥åŒ–é˜¶æ®µ
3. å—é‡åŒ–/éç›¸å¹²å¤„ç†:  è¡¥å¿FP8é‡åŒ–é€ æˆçš„ç²¾åº¦æŸå¤±ã€‚

* Tritonå®ç°ï¼šæ˜¾å­˜ä¸Šå®ç°ringbuffer

#### FlashMask: Rich Mask Extension

https://arxiv.org/pdf/2410.01359



### Decodingä¼˜åŒ–

* Speculative Decoding, Lookahead Decoding, Flash-Decoding, Flash-decoding++, Deja Vu, Atom, Continunous Batchingï¼ŒPrefill-Decode Disaggregating

#### Speculative Decoding

> * [GPU Mode Lecture 22: Hacker's Guide to Speculative Decoding in VLLM](https://www.youtube.com/watch?v=9wNAgpX6z_4)ã€1ã€‘
>   * Cade Daniel
>   * Working on LLM inference in vLLM
>   * Software Engineer at [Anyscale](https://www.anyscale.com/)
>   * Previously, model parallelism systems at AWS 
>     - https://arxiv.org/abs/2111.05972 
>   * https://x.com/cdnamz 
> * Recommended reading: Andrej Karpathyâ€™s tweet on speculative decoding ã€2ã€‘
>   - https://x.com/karpathy/status/1697318534555336961 
> * ã€ŠAccelerating LLM Inference with Staged Speculative Decodingã€‹ã€3ã€‘
> * ã€ŠSpeculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generationã€‹ã€4ã€‘
>
> * ã€ŠFast Inference from Transformers via Speculative Decodingã€‹ã€5ã€‘

##### Intro

![image-20251007142108701](./LLM-MLSys/image-20251007142108701.png)

- Memory-boundedness
  - In memory-bound LLM inference, the full GPU compute capacity is underutilized
  - The unused compute can be used, if we can find a way to use it
- Not all parameters required for every token
  - Do we really need 70B parameters to answer â€œWhat is the capital of Californiaâ€? Probably notâ€¦
- Idea:
  - Try to predict what large model will say
  - Get probabilities of predictions
  - Use heuristic to accept or rejection the predictions based on probabilities

* Draft model
  * use a small and cheap draft model to first **generate a candidate sequence of K tokens - a "draft"**. 
* large model
  * Then we feed all of these together through the big model in a batch. 
  * This is almost as fast as feeding in just one token, per the above. Then we go from left to right over the logits predicted by the model and sample tokens. Any sample that agrees with the draft allows us to immediately skip forward to the next token. 
  * If there is a disagreement then we throw the draft away and eat the cost of doing some throwaway work (sampling the draft and the forward passing for all the later tokens).

![image-20250531020213917](./LLM-MLSys/image-20250531020213917.png)ã€3ã€‘

![image-20250531015417368](./LLM-MLSys/image-20250531015417368.png)ã€3ã€‘

##### ç»“è®º

![image-20250531021603085](./LLM-MLSys/image-20250531021603085.png)

![image-20250531021629279](./LLM-MLSys/image-20250531021629279.png)

- Î³ (gamma) : æ¨æµ‹æ­¥æ•°æˆ–å€™é€‰Tokenæ•°é‡ ã€‚å®ƒå¾ˆå¯èƒ½è¡¨ç¤ºå°å‹çš„â€œè‰ç¨¿æ¨¡å‹â€ï¼ˆdraft modelï¼‰ä¸€æ¬¡æè®®çš„å€™é€‰Tokençš„æ•°é‡ã€‚
  - æ›´å¤§çš„ Î³ æ„å‘³ç€è‰ç¨¿æ¨¡å‹ä¸€æ¬¡ä¼šç”Ÿæˆæ›´å¤šçš„å€™é€‰Tokenï¼Œä¾›å¤§å‹çš„â€œç›®æ ‡æ¨¡å‹â€ï¼ˆtarget modelï¼‰å¹¶è¡ŒéªŒè¯ã€‚
- Î± (alpha) : æ¨æµ‹è§£ç çš„æ•ˆç‡æˆ–æ¥å—ç‡ç›¸å…³çš„æŒ‡æ ‡ ã€‚å®ƒå¯èƒ½ä¸è‰ç¨¿æ¨¡å‹æè®®çš„Tokenè¢«ç›®æ ‡æ¨¡å‹æ¥å—çš„å¹³å‡æ¯”ä¾‹æˆ–è´¨é‡æœ‰å…³ã€‚
  - æ›´é«˜çš„ Î± é€šå¸¸æ„å‘³ç€è‰ç¨¿æ¨¡å‹çš„æè®®æ›´å‡†ç¡®ï¼Œæˆ–è€…æ¨æµ‹è¿‡ç¨‹æ›´æœ‰æ•ˆï¼Œå¯¼è‡´æ›´å¤šçš„å€™é€‰Tokenè¢«æ¥å—ã€‚
- æ¨æµ‹è§£ç çš„æ½œåŠ› : å½“ Î± è¾ƒé«˜æ—¶ï¼ˆä¾‹å¦‚ Î± > 0.8ï¼‰ï¼Œå³ä½¿ Î³ è¾ƒå¤§ï¼ˆå¦‚ Î³=10ï¼‰ï¼Œè¿ç®—é‡çš„å¢åŠ ä¹Ÿç›¸å¯¹å¯æ§ï¼ˆä¾‹å¦‚å°äº2å€ï¼‰ï¼ŒåŒæ—¶èƒ½è·å¾—æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœï¼ˆä¾‹å¦‚5-7å€ï¼‰ã€‚

![image-20250531033629475](./LLM-MLSys/image-20250531033629475.png)



##### Losslessness --> rejection sampling

> (proof of losslessness): [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/pdf/2302.01318) 

- Is the output of speculative decoding different than the target model?

  - TL;DR No if using rejection sampling, subject to hardware numerics
  - Diagram https://github.com/vllm-project/vllm/pull/2336 
  - Yes if using lossly sampling technique, e.g. Medusaâ€™s typical acceptance (but higher acceptance rate!)

- æ‹’ç»é‡‡æ ·çš„æ ‡å‡†æ–¹æ³•è§„å®šï¼š

  - å¯¹äºè‰ç¨¿æ¨¡å‹æå‡ºçš„è¯å…ƒ d ï¼Œæˆ‘ä»¬ä»¥æ¦‚ç‡ min(1, P_target(d) / P_draft(d)) æ¥å—å®ƒã€‚å…¶ä¸­ P_target(d) æ˜¯ç›®æ ‡æ¨¡å‹è®¤ä¸ºè¯å…ƒ d çš„æ¦‚ç‡ï¼Œ P_draft(d) æ˜¯è‰ç¨¿æ¨¡å‹è®¤ä¸ºè¯å…ƒ d çš„æ¦‚ç‡ã€‚

    - æƒ…å†µä¸€ï¼š P_target(d) <= P_draft(d)

      - æ¥å—æ¦‚ç‡ alpha = min(1, P_target(d) / P_draft(d)) = P_target(d) / P_draft(d) ã€‚
        - è¿™æ„å‘³ç€è‰ç¨¿æ¨¡å‹å¯¹äºè¯å…ƒ d çš„é¢„æµ‹è¦ä¹ˆæ˜¯â€œè¿‡äºè‡ªä¿¡â€ï¼ˆå³ P_draft(d) è¿œå¤§äº P_target(d) ï¼‰ï¼Œè¦ä¹ˆæ˜¯æ°å¥½ç¬¦åˆæˆ–ç•¥å¾®é«˜ä¼°äº†ç›®æ ‡æ¨¡å‹çš„æ¦‚ç‡ã€‚
      - å½“è‰ç¨¿æ¨¡å‹æå‡ºè¯å…ƒ d æ—¶ï¼ˆä»¥æ¦‚ç‡ P_draft(d) å‘ç”Ÿï¼‰ï¼Œæˆ‘ä»¬ä»¥ alpha çš„æ¦‚ç‡æ¥å—å®ƒã€‚
      - é€šè¿‡è¿™æ¡è·¯å¾„ï¼ˆè‰ç¨¿æè®® d å¹¶è¢«æ¥å—ï¼‰,ç¡®ä¿äº†è¯å…ƒ d çš„è¾“å‡ºæ¦‚ç‡æ°å¥½ç­‰äºç›®æ ‡æ¨¡å‹å¸Œæœ›çš„æ¦‚ç‡æ˜¯ P_draft(d) * alpha = P_draft(d) * (P_target(d) / P_draft(d)) = P_target(d) ã€‚

  - å¦‚æœè¯å…ƒ d è¢«æ‹’ç»ï¼ˆå³ä¸Šè¿°æ¥å—æ¡ä»¶æœªæ»¡è¶³ï¼‰ï¼Œä¸ºäº†ä»ç„¶ä»ç›®æ ‡åˆ†å¸ƒ P_target ä¸­é‡‡æ ·ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸€ä¸ªè°ƒæ•´åçš„â€œå‰©ä½™â€æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ã€‚è¿™ä¸ªåˆ†å¸ƒæ­£æ¯”äº max(0, P_target(x) - P_draft(x)) ï¼Œå…¶ä¸­ x æ˜¯è¯æ±‡è¡¨ä¸­çš„ä»»æ„è¯å…ƒã€‚

  - **Recovered token:** If all tokens are rejected, we can use math trick to sample a correct token from the target model distribution

    - â†’ We always get >=1 token

    - **æ¢å¤è¯å…ƒ**å°±æ˜¯ä»è¿™ä¸ªè°ƒæ•´åçš„â€œå‰©ä½™â€æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°çš„è¯å…ƒ ã€‚

  - é€šè¿‡è¿™ç§æ–¹å¼ï¼ˆæ¥å—è‰ç¨¿è¯å…ƒï¼Œæˆ–åœ¨æ‹’ç»æ—¶ä½¿ç”¨æ¢å¤è¯å…ƒï¼‰ï¼Œç®—æ³•ç¡®ä¿äº†åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­é€‰å‡ºçš„è¯å…ƒéƒ½ä¸¥æ ¼éµå¾ªç›®æ ‡æ¨¡å‹ P_target çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™æ˜¯æ¢å¤è¯å…ƒæœ€æ ¹æœ¬çš„æ„ä¹‰ã€‚

- **Bonus token:** All speculative tokens may be accepted. We can sample from target model distribution normally in this case

  - â†’ we get an additional token in the happy-path!
  - ![image-20250601011458695](./LLM-MLSys/image-20250601011458695.png)

##### top1 vs top-k â€œtree attentionâ€

> - https://sites.google.com/view/medusa-llm 
> - https://arxiv.org/pdf/2305.09781 
> - https://www.together.ai/blog/sequoia 

- Top-1: proposal method suggests 1 token per sequence per slot
- Top-k: proposal method suggests k tokens per sequence per slot
- Currently only top-1 proposal and scoring is supported
  - Top-k is a future work
  - Most aggressive speedups require top-k attention masking
  - FlashInfer going to support masking
  - https://github.com/vllm-project/vllm/issues/3960 

##### å·¥ç¨‹å®ç°

![image-20250531034753144](./LLM-MLSys/image-20250531034753144.png)

* How to evaluate speedup?

  - Simplified version:

    - Inter-token latency = step time / number of tokens per step in expectation
    - Example without speculative decoding: 30ms / 1 â†’ 1 token per 30ms
    - Example with speculative decoding: 40ms / 2.5 â†’ 1 token per 16ms

    - Key factors
      - How long does it take to propose?
      - How accurate are the proposals?
      - How long does it take to verify / other spec framework overheads?

  - In practice:
    - https://github.com/vllm-project/vllm/blob/main/vllm/v1/spec_decode/metrics.py
      - Acceptance rate â€“ â€œHow aligned is the proposal method with the target model?â€
      - System efficiency â€“ â€œHow efficient is the deployment compared to 100% acceptance rate?â€

##### Lookahead scheduling

- Problem: Scoring speculative tokens generates KV. How can we save accepted KV to skip regeneration and reduce FLOPs requirements?
- Recommended reading: [What is lookahead scheduling in vLLM?](https://docs.google.com/document/d/1Z9TvqzzBPnh5WHcRwjvK2UEeFeq5zMZb5mFE8jR0HCs/edit#heading=h.1fjfb0donq5a)
- TL;DR:
  - vLLMâ€™s scheduler allocates additional space for KV
  - The SpecDecodeWorker uses the space to store KV of speculative tokens
  - Accepted token KV is stored correctly

##### Dynamic speculative decoding

- Problem: As batch size increases, spare FLOPs is reduced. How can we ensure spec decode performs no worse than no spec decode?
- Recommended reading: https://github.com/vllm-project/vllm/issues/4565 
  - Work by Lily Liu and Cody Yu
- TL;DR
  - Based on the batch size, adjust which sequences have speculations (or disable spec dec altogether)
  - Future work: per-sequence speculation length

![image-20250601012653322](./LLM-MLSys/image-20250601012653322.png)

##### Batch expansion

- Problem: How to support scoring when **PagedAttention only supports 1 query token per sequence**?
- Recommended reading: [Optimizing attention for spec decode can reduce latency / increase throughput](https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit#heading=h.kk7dq05lc6q8)
- TL;DR
  - We create â€œvirtual sequencesâ€ in SpecDecodeWorker each with 1 query token
  - This expands the batch (and duplicates KV loads in the attention layers)
  - We can remove this with an attention kernel which supports PagedAttention + multiple query tokens per sequence
  - Contact https://github.com/LiuXiaoxuanPKU for more information

##### Future Contribution Ideas

- More engineering
  - Retrieval-acceleration https://arxiv.org/html/2401.14021v1 
  - Chunked prefill + spec decode
  - Prefix caching + spec decode
  - Guided decoding + spec decode
  - Inferentia / TPU / CPU support
- More modeling
  - Meta-model for speculation length
  - Meta-model for speculation type
- Large / mixed engineering+modeling
  - Multi-LoRA draft model (specialize to domains)
  - Online learning draft model https://arxiv.org/abs/2310.07177 
  - Batched parallel decoding https://github.com/vllm-project/vllm/issues/4303 

#### æ ‘çŠ¶æŠ•æœº

##### Sequoia: åŸºç¡€æ ‘çŠ¶

![image-20251007142328516](./LLM-MLSys/image-20251007142328516.png)

##### EAGLE: é«˜é€Ÿæ ‘çŠ¶æŠ•æœºï¼Œç”¨äºDeepSeek MTP

![image-20251007142456171](./LLM-MLSys/image-20251007142456171.png)

##### FR-Spec: ä¼˜åŒ–EAGLEçš„èµ·è‰æ¨¡å‹çš„è¯è¡¨æ•ˆç‡

![image-20251007142712622](./LLM-MLSys/image-20251007142712622.png)

![image-20251007142812542](./LLM-MLSys/image-20251007142812542.png)

![image-20251007142837046](./LLM-MLSys/image-20251007142837046.png)

### MoE æ¨ç† â€”â€” Expert Parallelism

#### Intro

![image-20251003005914473](./LLM-MLSys/image-20251003005914473.png)

![image-20251003010149803](./LLM-MLSys/image-20251003010149803.png)

* Arcticã€DeepSeek-V3

![image-20251003010751098](./LLM-MLSys/image-20251003010751098.png)



#### Seed Paper

https://arxiv.org/abs/2504.02263

#### DeepSeekè§£æ³•



## æ¨ç†æ¡†æ¶

> èŒƒå¼ï¼šé¢„è®­ç»ƒEmbedding+è½»é‡åŒ–çº¿ä¸Šæ¨¡å‹

### Intro

* MLLMæ¨ç†
  * SGLang
  * LMDeploy
  * vLLM
* ![utilize-gpu](./LLM-MLSys/utilize-gpu.png)
* æ¨¡å‹åŠ é€Ÿï¼šTensorRTã€DL complier
  * Layer & Tensor Fusion: æ¨ªå‘/çºµå‘çš„èåˆï¼Œå‡å°‘copyæ˜¾å­˜; layer merge
  * Weights & Activation Precision Calibration
    * Symmetric quantization: è¶…å‚thresholdï¼Œè¶…è¿‡ä¼šæˆªæ–­ï¼Œæé«˜è½¬æ¢ç²¾åº¦
    * ç”¨KL-divergenceæ¥è¡¡é‡threshold

  * Kernel Auto-Tuning: æ‰¾å½“å‰ç¡¬ä»¶ä¸‹æœ€ä¼˜çš„å·ç§¯ç®—æ³•ã€kernelsã€tensor layouts
  * Dynamic Tensor Memory: ç»™å±‚åŠ å¼•ç”¨è®¡æ•° 

#### vLLM v.s. Sarathi v.s. Sarathi + POD

![image-20251005182019260](./LLM-MLSys/image-20251005182019260.png)

#### Triton v.s. In-house inference server

![image-20251021140125431](./LLM-MLSys/image-20251021140125431.png)



### Triton Inference Server

> https://developer.nvidia.com/blog/power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features/#multi-gpu_multi-node_inference
>
> bç«™è´¨é‡æ¯”è¾ƒé«˜çš„å…¥é—¨æ•™å­¦è§†é¢‘ https://www.bilibili.com/video/BV1KS4y1v7zd
>
> https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/overview.html#overview

#### Intro

* æœ¬å›¾ï¼šæ¨ç†æœåŠ¡çš„è¦ç´ é½å…¨

![image-20251021011746074](./LLM-MLSys/image-20251021011746074.png)



![image-20251021013003706](./LLM-MLSys/image-20251021013003706.png)



##### æ¨ç†æ¡†æ¶çš„è®¾è®¡

![image-20251021013611335](./LLM-MLSys/image-20251021013611335.png)

* æ¨¡å‹åˆ†ä¸ºä¸‰ç±»
  * ç‹¬ç«‹çš„
  * ensembled with a model pipeline
  * stateful modelï¼Œæ¯”å¦‚LLM
    * oldestæ˜¯ä¸€ç§å…è®¸æ‰“sequence batchçš„ç­–ç•¥ï¼Œåªè¦ä¿è¯é¡ºåºå³å¯
  * ![image-20251021014149599](./LLM-MLSys/image-20251021014149599.png)

#### åº”ç”¨

![image-20251021140306069](./LLM-MLSys/image-20251021140306069.png)

#### schedulingã€batchingã€streaming

* é»˜è®¤æ˜¯default scheduler

* dynamic_batching


```
dynamic_batching {
	preferred_batch_size:[4,8],
	max_queue_delay_microseconds: 100,
}
```

* streaming

![image-20251021014859343](./LLM-MLSys/image-20251021014859343.png)

##### ä¼˜åŒ–ç»†èŠ‚

* Client/serveråœ¨æœ¬åœ°ï¼š
  * Reduces HTTP/gRPC overhead: Inputs/outputs needed to be passed to/from Triton are stored in system/CUDA shared memory. 

#### multi model servingã€instance-group

* [instance-group](https://docs.nvidia.com/deeplearning/triton-inference-server/archives/triton_inference_server_1140/user-guide/docs/model_configuration.html#section-instance-groups)
  * ![image-20250917200712691](./LLM-MLSys/image-20250917200712691.png)
  * ![image-20251024004214705](./LLM-MLSys/image-20251024004214705.png)
  
* ä½¿ç”¨ multi streams å®ç°
  * ![image-20250917201955610](./LLM-MLSys/image-20250917201955610.png)

#### Stateful model

![image-20251022004941200](./LLM-MLSys/image-20251022004941200.png)

#### Model Pipelines

##### Ensemble model

https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/ensemble_models.html

![image-20251021014957347](./LLM-MLSys/image-20251021014957347.png)

##### BLS(Business Logic Scripting)

https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/bls.html



#### Step-by-step

##### Prepare the Model Repository

![image-20251021151841663](./LLM-MLSys/image-20251021151841663.png)

![image-20251021163316269](./LLM-MLSys/image-20251021163316269.png)

##### Configure the Served Model

![image-20251021180832920](./LLM-MLSys/image-20251021180832920.png)

![image-20251021181028150](./LLM-MLSys/image-20251021181028150.png)

* version policy

![image-20251021181939060](./LLM-MLSys/image-20251021181939060.png)

* instance groups

![image-20251021182528915](./LLM-MLSys/image-20251021182528915.png)

* Scheduling

###### warmup

![image-20251022005418293](./LLM-MLSys/image-20251022005418293.png)

###### TensorRTä¼˜åŒ–

![image-20251022005113058](./LLM-MLSys/image-20251022005113058.png)

##### 

##### Launch Triton Server

![image-20251022015043843](./LLM-MLSys/image-20251022015043843.png)

![image-20251024004412153](./LLM-MLSys/image-20251024004412153.png)





##### Configure an Ensemble Model

* [AI æ¨ç†å…¥é—¨å¿…çœ‹ | Triton Inference Server ç¼–ç¨‹å®æˆ˜å…¥é—¨æ•™ç¨‹å››](https://www.bilibili.com/video/BV1tt4y1h75i)

##### Send Requests to Triton Server

* ä¸‰ç§ï¼šhttpã€grpcã€capi





#### æ›´å¤šèƒ½åŠ›

##### Metrics

![image-20251021140234653](./LLM-MLSys/image-20251021140234653.png)

##### Model Analyzer

> https://www.bilibili.com/video/BV1R3411g7VR?spm_id_from=333.788.videopod.sections&vd_source=65f5ae8ea74e17ab3f49a362930881e1

* Performance&memory analysis 

##### PyTriton

https://github.com/triton-inference-server/pytriton

##### Triton Management Service

NVIDIA Triton Management Service provides model orchestration functionality for efficient multimodel inference. This functionality, which runs as a production service, loads models on demand and unloads models when not in use. 

It efficiently allocates GPU resources by placing as many models as possible on a single GPU server and helps to optimally group models from different frameworks for efficient memory use. It now supports autoscaling of NVIDIA Triton instances based on high utilization from inference and encrypted (AES-256) communication with applications. [Apply for early access to NVIDIA Triton Management Service.](https://developer.nvidia.com/tms-early-access)

### TensorRT

> https://developer.nvidia.com/blog/power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features

#### Intro

* Model Parser è§£æTensorFlow/Caffeæ¨¡å‹
  * [ONNX Parser](https://github.com/onnx)
* TensorRT Network Definition API
  * è‡ªå®šä¹‰ç®—å­éœ€è¦è‡ªå·±å†™
* TF-TRT (TensorFlow integration with TensorRT) parses the frozen TF graph or saved model, and **converts each supported subgraph to a TRT optimized node** (TRTEngineOp), allowing TF to execute the remaining graph.

#### ä¼˜åŒ–åŸç†

> https://zhuanlan.zhihu.com/p/667727749

* Hardware Aware Optimazation
  * ä¸åŒç¡¬ä»¶ï¼Œæ¨¡æ‹Ÿkernelæœ€ä¼˜è§£`trtexec --timingCacheFile=`
  * Type of hardwareï¼ˆDLA/Hardware capability...ï¼‰
  * Memory footprintï¼ˆShare, Cache, Global...ï¼‰
  * Input and output shape
  * Weight shapes
  * Weight sparsity
  * Level of quantization ï¼ˆso, reconsider memory)
* å¼ºåˆ¶é€‰æ‹©kernelï¼š`AlgorithmSelector`

#### Torch-TensorRT

https://github.com/pytorch/TensorRT/releases/tag/v2.8.0

> https://docs.pytorch.org/TensorRT/
>
> https://docs.pytorch.org/TensorRT/dynamo/dynamo_export.html
>
> https://docs.pytorch.org/TensorRT/fx/getting_started_with_fx_path.html

#### æ›´å¤šèƒ½åŠ›

* Multi-GPU multi-node inference



### Nvidia Dynamo

> https://github.com/ai-dynamo

* Core Framework
* **[LLM Optimized Components](https://github.com/ai-dynamo/dynamo/tree/main/lib/llm)**
  - Disaggregated Serving Engine: Decoupling of prefill and decode to optimize for throughput at latency SLOs
  - Intelligent Routing System: Prefix-based and load-aware request distribution
  - KV Cache Management: Distributed KV Cache management
* NIXL
  * *NVIDIA Inference Xfer Library* (*NIXL*) is targeted for accelerating point to point communications in AI inference frameworks such as NVIDIA Dynamo.



### Faster Transformer

![faster transformer](./LLM-MLSys/faster-transformer.png)

* è®¾è®¡æ€è·¯

  * decoderå’Œdecodingä¸¤å±‚æŠ½è±¡ï¼Œé€‚ç”¨äºä¸åŒçµæ´»æ€§çš„åœºæ™¯

* æ¨¡å‹ç»“æ„

  * GPT-2 model
    * Only one attention block
    * No beam search
    * Support sequence length <= 4096
  * Faster Transformerçš„å®ç°ï¼š
    * encoderå‚è€ƒBERT
    * decoderå’Œdecodingå‚è€ƒOpenNMT-tf (Attention is all you need)ã€GPT-2

* encoderå’Œdecoderçš„è®¨è®º

  * encoderä¸€æ¬¡è¾“å…¥çš„è¯å¤šã€è¿è¡Œæ¬¡æ•°å°‘ã€å¯¹GPUæ›´å‹å¥½
  * decoderå’Œä¸Šè¿°ç›¸åï¼Œä½†ä¾æ®Amdahl's Lawï¼Œåœ¨encoderå’Œdecoderå…±ç”¨çš„åœºæ™¯ï¼Œdecoderæ˜¯ç“¶é¢ˆ

* ä¼˜åŒ–çš„è®¨è®º

  * encoderï¼šç“¶é¢ˆæ˜¯kernel launch boundï¼Œkernels are too small
    * Fused Encoder: Fuse the kernels except GEMMs (General Matrix Multiplication)  as much as possibleï¼ŒGEMMç”¨tensorcoreä¼˜åŒ–ã€‚æ›´è¿›ä¸€æ­¥å¯ä»¥åˆ©ç”¨cutlasså·¥å…·fuse multi-head attention
  * decoderï¼šæ›´å¤šsmall kernels
    * **Fuse multi-head attentionï¼šåŸå› æ˜¯decoderçš„batch sizeæ˜¯1ï¼Œä¸å¿…è¦å¯¹GEMMä¼˜åŒ–**
  * decoding : 
    * fuse the softmax and top k operations by [online-softmax](https://github.com/NVIDIA/online-softmax)
    * use [CUB](https://nvidia.github.io/cccl/cub/) to accelerate the reduce operations
    * [beam search](https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f) ä¹‹å‰è¦ FP16 è½¬ FP32
      * Beam width
    * [effective_transformer by ByteDance](https://github.com/bytedance/effective_transformer): è®°å½•æ¯ä¸ªsentenceçš„paddingå‰ç¼€å’Œï¼ŒçŸ©é˜µè®¡ç®—å‰ç§»é™¤æ— ç”¨çš„paddingï¼Œåšattentionæ—¶å†æ˜ å°„å›æ¥ï¼Œæœ¬è´¨ä¸Šæ˜¯è¿½æ±‚tensorçš„ç´§è‡´ç»„ç»‡ã€‚
  * INT8 optimizationï¼šQAT + **without quantizing residuals** => ç²¾åº¦æŸå¤±å°‘

  ![INT8](./LLM-MLSys/INT8-optimization.png)



### SGLang

* Intro
  * known for its almost [zero-overhead batch scheduler](https://lmsys.org/blog/2024-12-04-sglang-v0-4/) and fast [constrained decoding](https://lmsys.org/blog/2024-02-05-compressed-fsm/)

#### æ€§èƒ½ä¼˜åŒ–

* [SGLang æ€§èƒ½ä¼˜åŒ–çŸ¥è¯†ç‚¹2025-9æœˆæœˆæŠ¥](https://mp.weixin.qq.com/s/6AVsx9FavxCjVmnmzVyoLw)
  * è®¸å¤šç»†èŠ‚æ€§èƒ½ä¼˜åŒ–ï¼Œé€‚åˆèµ°è¯»æºç 



### vLLM

#### Intro

* core principles
  * Ease-of-use
  * Great performance
    * æ¼”è¿›è·¯çº¿æ˜¯ä¼˜å…ˆä¼˜åŒ–throughputï¼Œåä¼˜åŒ–latency
  * Hardware agnosticity

* ä¼˜åŒ–ç‰¹æ€§
  * PagedAttention/tensor parallelism
  * Optimized multi-LoRA
  * Chunked prefill
  * Automatic prefix caching
    * block levelå®ç°
  * Guided decoding
    * é™åˆ¶tokenç±»å‹ï¼Œæ¯”å¦‚jsonè¯­æ³•
  * Quantization (fp8 WIP, and others)
  * Pipeline-parallelism (WIP)
  * Prefill disaggregation (WIP)

* Hardware agnosticity
  * NVIDIA, AMD, Inferentia, TPU (WIP), CPU 



### ollama

* æ›´é€‚åˆæœ¬åœ°å®éªŒ
* [ollama deepseek-r1](https://ollama.com/library/deepseek-r1:8b)
* open-webui ç‰ˆæœ¬ï¼šdyrnq/open-webui:latest

### NVIDIA ASR & TTS SOLUTIONS

#### ASR WFST decoding

* ASR Pipeline

  * å¤šçº§çš„è½¬æ¢ï¼šspeech -> phoneme -> character -> word -> sentence
    * å³ä½¿æ˜¯æ·±åº¦å­¦ä¹ å…´èµ·ï¼Œå·¥ä¸šç•Œå°‘æœ‰ç”¨e2e
    * å¤šçº§å¸¦æ¥æµ·é‡choicesï¼Œéœ€è¦æ„å»ºä¸€ä¸ªdecoderè§£å†³è¯†åˆ«ä»»åŠ¡(a search problem)

  * ASR system overview

![ASR-system](./LLM-MLSys/ASR-system.png)

> *Q: How do we combine HMM, Lexicon & LM together?*
>
> *A: WFST (Weighted Finite State Transducer)*

* WFSTæ˜¯ä¸€ç§å›¾çš„è¡¨ç¤ºæ–¹å¼ï¼Œèƒ½é€šç”¨åœ°è¡¨ç¤ºä¸Šè¿°ä¸‰ç§æ¨¡å‹ï¼Œç„¶åè¿™ä¸‰å¼ å›¾å¯ä»¥åˆå¹¶ã€‚

  * æ¨¡å‹çº§è” :

    - HMM (å£°å­¦æ¨¡å‹) çš„è¾“å‡ºæ˜¯éŸ³ç´ phonemeã€‚
    - è¯å…¸å°†éŸ³ç´ åºåˆ—æ˜ å°„åˆ°è¯è¯­ã€‚
    - è¯­è¨€æ¨¡å‹è¯„ä¼°è¯è¯­åºåˆ—çš„åˆç†æ€§ã€‚åœ¨ WFST æ¡†æ¶ä¸‹ï¼Œè¿™äº›æ¨¡å‹çš„è¾“å‡ºå’Œè¾“å…¥å¯ä»¥è‡ªç„¶åœ°è¿æ¥èµ·æ¥ã€‚

  * WFST Decoding: 
    * å›¾çš„æœ€çŸ­è·¯å¾„é—®é¢˜ : åœ¨ç»„åˆåçš„ HCLG å›¾ä¸­ï¼Œä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„è·¯å¾„ä»£è¡¨äº†ä¸€ä¸ªå¯èƒ½çš„è¯†åˆ«ç»“æœï¼Œè·¯å¾„ä¸Šçš„æƒé‡ç´¯ç§¯ä»£è¡¨äº†è¿™ä¸ªç»“æœçš„å¯èƒ½æ€§ã€‚è§£ç çš„ç›®æ ‡å°±æ˜¯åœ¨è¿™ä¸ªå›¾ä¸­æ‰¾åˆ°æƒé‡æœ€å°ï¼ˆæˆ–æ¦‚ç‡æœ€å¤§ï¼‰çš„è·¯å¾„ã€‚
    * ä»¤ç‰Œä¼ é€’ (Token Passing) : è¿™æ˜¯ä¸€ç§åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œç”¨äºåœ¨ WFST å›¾ä¸­è¿›è¡Œæœç´¢ã€‚è§£ç å™¨é€å¸§å¤„ç†éŸ³é¢‘ï¼Œå°†â€œä»¤ç‰Œ (token)â€ åœ¨å›¾ä¸­çš„çŠ¶æ€é—´ä¼ é€’å’Œæ‰©å±•ï¼Œæ¯ä¸ªä»¤ç‰Œè®°å½•äº†åˆ°è¾¾å½“å‰çŠ¶æ€çš„è·¯å¾„å’Œç´¯ç§¯å¾—åˆ†ã€‚

#### Kaldi CUDA decoding pipeline

> - Blogs: https://developer.nvidia.com/blog/gpu-accelerated-speech-to-text-with-kaldi-a-tutorial-on-getting-started/
> - Kaldi integration with Triton: https://github.com/NVIDIA/DeepLearningExamples/tree/master/Kaldi/SpeechRecognition
> - Kaldi GPU decoder
>   - NGC: nvcr.io/nvidia/kaldi:20.08-py3
>   - Kaldi github: github.com/kaldi-asr/src/cudadecoder

* WFST Decodingé€»è¾‘åˆ¤æ–­å’Œå¯¹è±¡copyè¾ƒå¤šï¼Œä¹‹å‰å¾ˆé•¿æ—¶é—´CPUå®ç°
* GPU DECODE CHALLENGES
  * Dynamic workload
    * Amount of parallelism varies greatly throughout decode process
    * Can have few or many candidates moving from frame to frame
  * Limited parallelism
    * Even with many candidates, the amount of parallelism is still far smaller to saturate a GPU
  * Complex data structure
    * Need a GPU-friendly data layout to obtain high performance on GPU
* CUDA DECODER
  * Operate FST on GPU
    * CudaFst takes ~1/3 of its original size
  * Accelerate decoding by parallelization
    * Batch processing: batchä¸åŒè¯­å¥çš„chunksï¼Œæ”¯æŒcontext switch
    * Token Passing in parallel
  * Process in streaming manner
* ASR GPU PIPELINE: e2e acceleration, feature extraction + Acoustic Model + Language Model
  * ç»“åˆTriton Inference Server


![asr-pipeline](./LLM-MLSys/asr-pipeline.png)

#### Text To Speech(TTS) Synthesis

* Modern TTS Solution

  * Synthesizer: TACOTRON 2æ¨¡å‹ï¼Œåˆæˆå‘éŸ³ç‰¹å¾

  * Vocoderï¼šå£°ç å™¨ WAVENETã€WAVEGLOW
    * æ€è·¯ï¼šåˆ©ç”¨å¯é€†ç½‘ç»œç”Ÿæˆå£°éŸ³, affine coupling layerå¾ˆå…³é”®

![waveglow](./LLM-MLSys/waveglow.png)



* BERT
  * æŒ‘æˆ˜ (åœ¨ TTS æˆ–ç›¸å…³å£°å­¦/éŸµå¾‹å»ºæ¨¡ä¸­) :
    - å¤šéŸ³å­—æ¶ˆæ­§ (Polyphone disambiguation) : ç¡®å®šå¤šéŸ³å­—åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­çš„æ­£ç¡®å‘éŸ³ã€‚
    - éŸµå¾‹ç»“æ„é¢„æµ‹ (Prosodic structure prediction) : é¢„æµ‹è¯­éŸ³çš„åœé¡¿ã€é‡éŸ³ã€è¯­è°ƒç­‰éŸµå¾‹ç‰¹å¾ï¼Œä½¿åˆæˆè¯­éŸ³æ›´è‡ªç„¶ã€‚

* BERT Optimization: 
  * å¯¹self-attention layeråškernel fusion
  * Amp

## æ¨¡å‹è®­ç»ƒ

> å¹¶è¡Œè®­ç»ƒå‚è€ƒ MLSys.md

### å¤§è§„æ¨¡é›†ç¾¤

- [Meta LIama 3](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)ï¼Œ16k GPU å¹¶è¡Œè®­ç»ƒï¼ŒèƒŒé ï¼ˆç‹¬ç«‹çš„ï¼‰ä¸¤ä¸ªè§„æ¨¡è¾¾24K çš„ H100 é›†ç¾¤ï¼Œåˆ†åˆ«åŸºäº RoCE å’Œ IB æ„å»ºå•é“¾è·¯å¸¦å®½400Gbpsçš„èŠ‚ç‚¹äº’è”ã€‚
- [Google Gemini 1.5](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)ï¼ŒåŸºäºæ•°åä¸ª 4k TPUv4 Pod å¹¶è¡Œè®­ç»ƒï¼ŒPod å†…éƒ¨ 3D-Torus ICI äº’è”ï¼Œå•é“¾è·¯å¸¦å®½ 800Gbpsã€‚
  - TPUæœ‰SuperPodå¤§è§„æ¨¡ICIçš„ä¼˜åŠ¿
- [å­—èŠ‚ MegaScale](https://arxiv.org/abs/2402.15627)ï¼Œ12k GPU å¹¶è¡Œè®­ç»ƒã€‚

### å¤šæ¨¡æ€è®­ç»ƒ

#### VeOmni

> https://mp.weixin.qq.com/s/A1CdiEiSaGrh_aH_ggBINg
>
> **arXivï¼š**https://arxiv.org/pdf/2508.02317
>
> **GitHubï¼š**https://github.com/ByteDance-Seed/VeOmni

* ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„åˆ†å¸ƒå¼è®­ç»ƒ
  * VeOmni å°†æ¨¡å‹å®šä¹‰ä¸åº•å±‚åˆ†å¸ƒå¼è®­ç»ƒä»£ç è§£è€¦ï¼Œä½¿ FSDPã€SPã€EP ç­‰åˆ†å¸ƒå¼ç­–ç•¥ï¼Œå¯çµæ´»ç»„åˆåº”ç”¨äºä¸åŒçš„æ¨¡å‹ç»„ä»¶ï¼ˆå¦‚ç¼–ç å™¨ã€MoE å±‚ï¼‰ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç ã€‚
  * åŒæ—¶ï¼ŒVeOmni æä¾›è½»é‡æ¥å£ï¼Œæ”¯æŒæ–°æ¨¡æ€æ— ç¼é›†æˆï¼Œè§£å†³äº†ç°æœ‰æ¡†æ¶å› æ¨¡å‹ä¸å¹¶è¡Œé€»è¾‘è€¦åˆè€Œå¯¼è‡´çš„æ‰©å±•æ€§å·®ã€å·¥ç¨‹æˆæœ¬é«˜ç­‰é—®é¢˜ã€‚
  * ![image-20250820184311361](./LLM-MLSys/image-20250820184311361.png)



### Ckpt

#### Intro

* safetensors
  * config.json
  * model.safetensors
  * tokenizer.model

![image-20251004232301701](./LLM-MLSys/image-20251004232301701.png)

![image-20251004232511080](./LLM-MLSys/image-20251004232511080.png)

* ç»†èŠ‚ï¼š
  * å­˜å‚¨æ—¶æ˜¯Wï¼Œè®¡ç®—æ—¶W^T

#### ByteCheckpoint

* å­—èŠ‚Ckpt https://mp.weixin.qq.com/s/4pIAZqH01Ib_OGGGD9OWQg
  * ByteCheckpoint ï¼Œä¸€ä¸ª PyTorch åŸç”Ÿï¼Œå…¼å®¹å¤šä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ Checkpoint çš„é«˜æ•ˆè¯»å†™å’Œè‡ªåŠ¨é‡æ–°åˆ‡åˆ†çš„å¤§æ¨¡å‹ Checkpointing ç³»ç»Ÿã€‚

#### [é˜¿é‡Œå¤§æ¨¡å‹åˆ›ä½œå¹³å° MuseAI æé€Ÿæ¨¡å‹åˆ‡æ¢](https://mp.weixin.qq.com/s?__biz=Mzg4NTczNzg2OA==&mid=2247507136&idx=1&sn=4a3f589481aa8b9808e4e37cd13684d9&scene=21&poc_token=HHa65GijwfEt24fRL4yDooJWlGzE7F3NfBC3qFKb)

* æœ¬æ–‡ä¸»è¦åˆ†æäº†å¹³å°ç”±äºé¢‘ç¹åˆ‡æ¢ Diffusion Pipeline å¼•èµ·çš„ç”¨æˆ·ä½“éªŒä¸èµ„æºæµªè´¹é—®é¢˜ï¼Œå¹¶ä»ç½‘ç»œä¼ è¾“ã€å†…å­˜ç®¡ç†ã€Host-to-Deviceã€æ¨¡å‹é‡åŒ–ç­‰æ–¹é¢ç€æ‰‹ä¼˜åŒ–ã€‚





## é€šä¿¡ä¼˜åŒ– -> MLSys+RecSys.md

## è½¯ç¡¬ååŒ

### [Trends in Deep Learning Hardware: Bill Dally (NVIDIA)](https://www.youtube.com/watch?v=kLiwvnr4L80)

### DeepSeek-V3 çš„ç¡¬ä»¶ç•…æƒ³

* the **SMs** primarily perform the following tasks for **all-to-all communication:** ï¼ˆ 20/132 SMs for H800ï¼‰
  â€¢ Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB
  traffic destined for multiple GPUs within the same node from a single GPU.
  â€¢ Transporting data between RDMA buffers (registered GPU memory regions) and in-
  put/output buffers.
  â€¢ Executing reduce operations for all-to-all combine.
  â€¢ Managing fine-grained memory layout during chunked data transferring to multiple
  experts across the IB and NVLink domain.
  * æœŸæœ›ç”¨ç±»ä¼¼ NVIDIA SHARP Graham et al. (2016). æ¥åš
  * aim for this hardware to unify the IB (scale-out) and NVLink
    (scale-up) networks from the perspective of the computation units
* ScaleUPå’ŒScaleOutè¯­ä¹‰çš„èåˆæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„å·¥ä½œ, å‡†ç¡®çš„æ¥è¯´åœ¨ScaleOutä½¿ç”¨RDMAå°±æ˜¯ä¸€ä¸ªé”™è¯¯, å¹¶ä¸”æƒ³ç®€å•çš„åœ¨ScaleUPä½¿ç”¨RDMAä¹Ÿæ˜¯ä¸€ä¸ªé”™è¯¯.
  * [ã€ŠHotChip2024åè®°: è°ˆè°ˆåŠ é€Ÿå™¨äº’è”åŠScaleUPä¸ºä»€ä¹ˆä¸èƒ½ç”¨RDMAã€‹](https://mp.weixin.qq.com/s?__biz=MzUxNzQ5MTExNw==&mid=2247492300&idx=1&sn=8a239883c831233e7e06659ec3425ea2&scene=21#wechat_redirect)

### Fire-Flyer AI-HPC: **A Cost-Effective** Software-Hardware Co-Design for Deep Learning

> https://blog.csdn.net/m0_59163425/article/details/143349082

* ä½¿ç”¨äº†Pcleæ¥å£çš„A100èŠ¯ç‰‡ï¼ˆä¾¿å®œç‰ˆæœ¬ï¼Œè€Œéæ›´æ˜‚è´µçš„NVIDIA DGXï¼‰ï¼Œæ¯”åŸæ¥AIè®­ç»ƒçš„ä¸“ç”¨èŠ¯ç‰‡ç›´æ¥å°‘äº†ä¸€åŠçš„æˆæœ¬ã€‚åœ¨10,000 GPUé›†ç¾¤ä¸Šï¼Œå®ç°äº†DGX-A100 80%çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½50%æˆæœ¬å’Œ40%èƒ½è€—ï¼Œè¯æ˜äº†è¯¥è®¾è®¡çš„æˆæœ¬æ•ˆç›Šã€‚
* æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬ï¼š
  * è‡ªç ”**HFReduce é€šä¿¡åº“**æå‡ AllReduce æ•ˆç‡ï¼Œé€šè¿‡ CPU å¼‚æ­¥å¤„ç†å‡å°‘ PCIe å¸¦å®½å ç”¨ï¼›
  * ä¼˜åŒ–**HaiScale æ¡†æ¶**æ”¯æŒæ•°æ®ã€æµæ°´çº¿ã€å¼ é‡å¹¶è¡Œç­‰å¤šç§å¹¶è¡Œç­–ç•¥ï¼›
  * è®¾è®¡**ä¸¤å±‚ Fat-Tree ç½‘ç»œ**æ•´åˆè®¡ç®—ä¸å­˜å‚¨æµé‡ï¼Œé€šè¿‡ 3FS åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå®ç° 8TB/s è¯»å–ååé‡ï¼›HAI å¹³å°æä¾›ä»»åŠ¡è°ƒåº¦ä¸æ•…éšœæ¢å¤ï¼Œä¿éšœå¤§è§„æ¨¡é›†ç¾¤ç¨³å®šæ€§ã€‚
* HF Reduce
  * **å¼‚æ­¥æ¢¯åº¦èšåˆ**ï¼šé€šè¿‡ CPU é¢„å¤„ç†æ¢¯åº¦ï¼ˆD2H ä¼ è¾“ + èŠ‚ç‚¹å†… Reduceï¼‰ï¼Œå†ç» IB ç½‘ç»œè·¨èŠ‚ç‚¹ AllReduceï¼Œè¾ƒ NCCL æå‡ 2-3 å€å¸¦å®½åˆ©ç”¨ç‡ï¼ˆå›¾ 7aï¼‰ã€‚
  * **NVLink å¢å¼º**ï¼šé›†æˆ NVLink æ¡¥æ¥åï¼Œè·¨åŒºé€šä¿¡å¸¦å®½çªç ´ 10GB/sï¼ˆå›¾ 7bï¼‰ï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œé«˜æ•ˆè®¡ç®—ã€‚
  * ![image-20250502121007854](./LLM-MLSys/image-20250502121007854.png)
* **HaiScale è®­ç»ƒæ¡†æ¶**ï¼š
  - å¤šå¹¶è¡Œç­–ç•¥
    - æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ï¼šå¼‚æ­¥ AllReduce é‡å è®¡ç®—é€šä¿¡ï¼ŒVGG16 è®­ç»ƒæ—¶é—´è¾ƒ PyTorch DDP å‡åŠï¼ˆå›¾ 8aï¼‰ã€‚
    - æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ï¼šé€šè¿‡èŠ‚ç‚¹å†… GPU åˆ†å±ä¸åŒ DP ç»„ï¼Œå‡å°‘ç½‘ç»œæ‹¥å¡ï¼ŒLLaMA-13B è®­ç»ƒå¹¶è¡Œæ•ˆç‡è¾¾ 91%ï¼ˆå›¾ 9aï¼‰ã€‚
  - **FSDP ä¼˜åŒ–**ï¼šå†…å­˜ç®¡ç†æ›´é«˜æ•ˆï¼ŒGPT2-Medium è®­ç»ƒå¹¶è¡Œ scalability è¾¾ 95%ï¼ˆå›¾ 8bï¼‰ã€‚
* **3FS åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ**ï¼š
  - **ç¡¬ä»¶é…ç½®**ï¼š180 èŠ‚ç‚¹ Ã—16 NVMe SSDï¼Œæä¾›**8TB/s è¯»å–ååé‡**ä¸ 20PiB å­˜å‚¨å®¹é‡ã€‚
  - æŠ€æœ¯äº®ç‚¹
    - é“¾å¼å¤åˆ¶ï¼ˆCRAQï¼‰ä¿è¯æ•°æ®ä¸€è‡´æ€§ï¼Œè¯·æ±‚ - å‘é€æ§åˆ¶æœºåˆ¶é¿å…ç½‘ç»œæ‹¥å¡ã€‚
    - é›†æˆ 3FS-KV æ”¯æŒé”®å€¼å­˜å‚¨ï¼Œé™ä½ LLM æœåŠ¡æˆæœ¬ä¸€ä¸ªæ•°é‡çº§ã€‚
* **HAI å¹³å°**ï¼š
  - **æ—¶é—´å…±äº«è°ƒåº¦**ï¼šæŒ‰èŠ‚ç‚¹ç²’åº¦åˆ†é…èµ„æºï¼Œåˆ©ç”¨ç‡è¾¾ 99%ï¼Œæ”¯æŒä»»åŠ¡æ–­ç‚¹ç»­ä¼ ã€‚
  - **æ•…éšœæ¢å¤**ï¼šCheckpoint Manager æ¯ 5 åˆ†é’Ÿå¼‚æ­¥ä¿å­˜ï¼Œä»…ä¸¢å¤±æœ€æ–° 5 åˆ†é’Ÿæ•°æ®ï¼›Validator å·¥å…·å‘¨æ£€ç¡¬ä»¶çŠ¶æ€ï¼Œæå‰è¯†åˆ« GPU Xid é”™è¯¯ï¼ˆè¡¨ VIï¼‰ã€‚

### å…¶å®ƒ

MTP ~ [**Zen5çš„2-Ahead Branch Predictor**](https://chipsandcheese.com/p/zen-5s-2-ahead-branch-predictor-unit-how-30-year-old-idea-allows-for-new-tricks)

## Agent ç³»ç»Ÿä¼˜åŒ–

> https://zhuanlan.zhihu.com/p/1931375587781501201

- **ç®—æ³•å±‚é¢ä¼˜åŒ–æ–¹æ³•ï¼š**
  - æ–‡ç« æå‡º Local Attention æ–¹æ³•ï¼Œå³é€šè¿‡ LightTransferã€LoLCATs ç­‰æ–¹æ³•ï¼Œå°† Transformer çš„å…¨å±€æ³¨æ„åŠ›æ›¿æ¢ä¸ºå±€éƒ¨æˆ–ä½ç§©æœºåˆ¶ï¼Œå¤§å¹…é™ä½å¤æ‚åº¦è‡³è¿‘çº¿æ€§ï¼Œä¸”æ€§èƒ½æŸå¤±æå°ï¼›
  - æ–‡ç« åˆ©ç”¨ Layer Collapseã€SlimGPT ç­‰ç»“æ„åŒ–å‰ªææŠ€æœ¯ï¼Œåˆ å‡å†—ä½™å±‚ã€æ³¨æ„åŠ›å¤´æˆ–é€šé“ï¼Œåœ¨æ— éœ€å¤§è§„æ¨¡é‡è®­ç»ƒçš„æƒ…å†µä¸‹å‹ç¼©æ¨¡å‹å‚æ•°ï¼Œå¹¶ä¿æŒå‡ ä¹ç›¸åŒçš„æ•ˆæœã€‚
- **æ¶æ„å±‚é¢ä¼˜åŒ–æ–¹æ³•ï¼š**
  - ç¼©çŸ­è¾“å‡ºé•¿åº¦ï¼ˆOutput Length Reductionï¼‰
  - è¯­ä¹‰ç¼“å­˜ï¼ˆSemantic Cachingï¼‰
  - é‡åŒ–ï¼ˆQuantizationï¼‰
  - é¢„å¡«å……ä¸è§£ç åˆ†ç¦»ï¼ˆPrefill-Decode Separationï¼‰
  - æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰

![image-20251105175253591](./LLM-MLSys/image-20251105175253591.png)



## Vision Model æ¨ç†

* Swinï¼šmicrosoft/swinv2-large-patch4-window12-192-22k
  * pytorchåŸºç¡€ä½¿ç”¨
  * 1*V100
    * Batch_size=4, qps=32ï¼š æ˜¾å­˜ç“¶é¢ˆ 24G/32Gï¼Œ150W/300W
    * Batch_size=8, qps=20ï¼š æ˜¾å­˜ç“¶é¢ˆ 26G/32Gï¼Œ115W/300W
    * Batch_size=2, qps=27ï¼š æ˜¾å­˜ç“¶é¢ˆ 30G/32Gï¼ŒW/300W
  * æ³¨ï¼šqpså·²ç»è€ƒè™‘äº†batch_size

* Dinov2
  * Batch_size=4, qps=50: æ˜¾å­˜ 14Gï¼Œ120W



## åˆ†å¸ƒå¼ Agent

* [èš‚èš Ray](https://mp.weixin.qq.com/s/TFxzMJyQVoffV4SpiTh9AQ?open_in_browser=true)

  * è§£å†³çš„é—®é¢˜ï¼šagentçš„è´Ÿè½½ã€å½¢å¼å¤šæ ·ï¼Œä»POCåˆ°ä¸Šçº¿çš„gap
  * Ray-Agent ï¼ˆragentï¼‰
    * ä¸»è¦è€ƒè™‘ç‚¹å¦‚ä¸‹ï¼šâ‘ è¯¥æ¡†æ¶éœ€æä¾› Agent çš„ APIï¼›â‘¡åˆ©ç”¨ Ray å®ç°ä»æœ¬åœ°ä»£ç åˆ°æ”¯æŒå¼‚æ„èµ„æºçš„åˆ†å¸ƒå¼ä»£ç çš„æ‰©å±•ï¼›â‘¢åœ¨å¤š Agent åœºæ™¯ä¸­ï¼Œæ¯ä¸ª Agent éƒ½æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è¿›ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¡†æ¶æ¥åè°ƒè¿™äº›è¿›ç¨‹ï¼Œå³æ‰€è°“çš„ environmentï¼›â‘£è¦å…¼å®¹ä¸åŒçš„åº“ï¼Œå¦‚ MetaGPT å’Œ AutoGenï¼›â‘¤å¸Œæœ›åˆ©ç”¨ Ray çš„æ²™ç®±ï¼ˆsandboxï¼‰ã€æ‰¹å¤„ç†èƒ½åŠ›å’Œè·¨æºè°ƒåº¦åŠŸèƒ½ã€‚

  * ![image-20250228002001015](./LLM-MLSys/image-20250228002001015.png)

  * ![image-20250228002616025](./LLM-MLSys/image-20250228002616025.png)

  * ![image-20250228003128059](./LLM-MLSys/image-20250228003128059.png)
  * ![image-20250228003143293](./LLM-MLSys/image-20250228003143293.png)

  * ![image-20250228005839274](./LLM-MLSys/image-20250228005839274.png)

  * ![image-20250228010735262](./LLM-MLSys/image-20250228010735262.png)
  * æœªæ¥æœŸæœ›ï¼š
    * Agent Mesh/Agent Protocol
    * ç¦»åœ¨çº¿ä¸€ä½“æ¶æ„ï¼šå¯ä»¥ç”¨ Ray Data pipeline å®Œæˆç¦»çº¿å·¥ä½œ

## LLMOps

[Observability in LLMOps pipeline - Different Levels of Scale](https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline)