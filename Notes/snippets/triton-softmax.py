import triton
import triton.language as tl

@triton.jit
def softmax(Y, stride_ym, stride_yn, X, stride_xm, stride_xn, M, N, BLOCK_SIZE: tl.constexpr):
  # row index
  m = tl.program_id(0)
  # col indices
  # this specific kernel only works for matrices that
  # have less than BLOCK_SIZE columns
  n = tl.arange(0, BLOCK_SIZE)
  # the memory address of all the elements
  # that we want to load can be computed as follows
  X = X + m * stride_xm + n * stride_xn
  # load input data; pad out-of-bounds elements with 0
  x = tl.load(X, mask=n < N, other=-float('inf'))
  # compute numerically-stable softmax
  z = x - tl.max(x, axis=0)
  num = tl.exp(z)
  denom = tl.sum(num, axis=0)
  y = num / denom
  # write back to Y
  Y = Y + m * stride_ym + n * stride_yn
  tl.store(Y, y, mask=n < N)

import torch
# Allocate input/output tensors
X = torch.normal(0, 1, size=(583, 931), device='cuda')
Y = torch.empty_like(X)
# SPMD launch grid
grid = (X.shape[0], )
BLOCK_SIZE = 1024
# enqueue GPU kernel
softmax[grid](Y, Y.stride(0), Y.stride(1),
              X, X.stride(0), X.stride(1),
              X.shape[0]    , X.shape[1], BLOCK_SIZE=BLOCK_SIZE)
print(Y)


@torch.jit.script
def softmax(x):
    x_max = x.max(dim=1)[0]
    z = x - x_max[:, None]
    numerator = torch.exp(x)
    denominator = numerator.sum(dim=1)
    return numerator / denominator[:, None]